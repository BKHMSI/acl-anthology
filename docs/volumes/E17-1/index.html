<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/E17-1.pdf>Proceedings of the 15th Conference of the <span class=acl-fixed-case>E</span>uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></h2><p class=lead><a href=/people/m/mirella-lapata/>Mirella Lapata</a>,
<a href=/people/p/phil-blunsom/>Phil Blunsom</a>,
<a href=/people/a/alexander-koller/>Alexander Koller</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>E17-1</dd><dt>Month:</dt><dd>April</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Valencia, Spain</dd><dt>Venue:</dt><dd><a href=/venues/eacl/>EACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/E17-1>https://aclanthology.org/E17-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/E17-1.pdf>https://aclanthology.org/E17-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/E17-1.pdf title="Open PDF of 'Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+15th+Conference+of+the+European+Chapter+of+the+Association+for+Computational+Linguistics%3A+Volume+1%2C+Long+Papers" title="Search for 'Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1000/>Proceedings of the 15th Conference of the <span class=acl-fixed-case>E</span>uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></strong><br><a href=/people/m/mirella-lapata/>Mirella Lapata</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1001/>Gated End-to-End Memory Networks</a></strong><br><a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/j/julien-perez/>Julien Perez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1001><div class="card-body p-3 small">Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks (MemN2N) have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>. Concretely, we develop a Gated End-to-End trainable Memory Network architecture (GMemN2N). From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>. Then, we show improvements on the Dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> sets the new state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1002/>Neural Tree Indexers for Text Understanding</a></strong><br><a href=/people/t/tsendsuren-munkhdalai/>Tsendsuren Munkhdalai</a>
|
<a href=/people/h/hong-yu/>Hong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1002><div class="card-body p-3 small">Recurrent neural networks (RNNs) process input text sequentially and model the conditional transition between word tokens. In contrast, the advantages of recursive networks include that they explicitly model the <a href=https://en.wikipedia.org/wiki/Composition_(language)>compositionality</a> and the recursive structure of natural language. However, the current <a href=https://en.wikipedia.org/wiki/Recursion_(computer_science)>recursive architecture</a> is limited by its dependence on <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>syntactic tree</a>. In this paper, we introduce a robust syntactic parsing-independent tree structured model, Neural Tree Indexers (NTI) that provides a middle ground between the sequential RNNs and the syntactic treebased recursive models. NTI constructs a full n-ary tree by processing the input text with its node function in a bottom-up fashion. Attention mechanism can then be applied to both <a href=https://en.wikipedia.org/wiki/Mathematical_structure>structure</a> and node function. We implemented and evaluated a binary tree model of NTI, showing the model achieved the state-of-the-art performance on three different NLP tasks : natural language inference, answer sentence selection, and sentence classification, outperforming state-of-the-art recurrent and recursive neural networks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1003/>Exploring Different Dimensions of Attention for Uncertainty Detection</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1003><div class="card-body p-3 small">Neural networks with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> have proven effective for many natural language processing tasks. In this paper, we develop <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> for uncertainty detection. In particular, we generalize standardly used <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> by introducing external attention and sequence-preserving attention. These novel architectures differ from standard approaches in that they use external resources to compute attention weights and preserve <a href=https://en.wikipedia.org/wiki/Sequence>sequence information</a>. We compare them to other <a href=https://en.wikipedia.org/wiki/Configuration_(geometry)>configurations</a> along different dimensions of attention. Our novel architectures set the new state of the art on a Wikipedia benchmark dataset and perform similar to the state-of-the-art model on a biomedical benchmark which uses a large set of linguistic features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1004/>Classifying Illegal Activities on <a href=https://en.wikipedia.org/wiki/Tor_(anonymity_network)>Tor Network</a> Based on Web Textual Contents</a></strong><br><a href=/people/m/mhd-wesam-al-nabki/>Mhd Wesam Al Nabki</a>
|
<a href=/people/e/eduardo-fidalgo/>Eduardo Fidalgo</a>
|
<a href=/people/e/enrique-alegre/>Enrique Alegre</a>
|
<a href=/people/i/ivan-de-paz/>Ivan de Paz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1004><div class="card-body p-3 small">The freedom of the Deep Web offers a safe place where people can express themselves anonymously but they also can conduct <a href=https://en.wikipedia.org/wiki/Crime>illegal activities</a>. In this paper, we present and make publicly available a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for Darknet active domains, which we call Darknet Usage Text Addresses (DUTA). We built DUTA by sampling the <a href=https://en.wikipedia.org/wiki/Tor_(anonymity_network)>Tor network</a> during two months and manually labeled each address into 26 classes. Using DUTA, we conducted a comparison between two well-known text representation techniques crossed by three different <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifiers</a> to categorize the <a href=https://en.wikipedia.org/wiki/Tor_(anonymity_network)>Tor hidden services</a>. We also fixed the pipeline elements and identified the aspects that have a critical influence on the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results. We found that the combination of TFIDF words representation with <a href=https://en.wikipedia.org/wiki/Logistic_regression>Logistic Regression classifier</a> achieves 96.6 % of 10 folds cross-validation accuracy and a macro F1 score of 93.7 % when classifying a subset of illegal activities from DUTA. The good performance of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> might support potential tools to help the authorities in the detection of these activities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1005/>When is <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> effective? Semantic sequence prediction under varying data conditions</a></strong><br><a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1005><div class="card-body p-3 small">Multitask learning has been applied successfully to a range of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, mostly <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphosyntactic</a>. However, little is known on when <a href=https://en.wikipedia.org/wiki/Machine_to_machine>MTL</a> works and whether there are data characteristics that help to determine the success of <a href=https://en.wikipedia.org/wiki/Machine_to_machine>MTL</a>. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary task configurations, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. When successful, <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>auxiliary tasks</a> with compact and more uniform label distributions are preferable.<i>when</i> MTL works and whether there are data characteristics that help to determine the success of MTL. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary task configurations, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 tasks. When successful, auxiliary tasks with compact and more uniform label distributions are preferable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1006/>Learning Compositionality Functions on Word Embeddings for Modelling Attribute Meaning in Adjective-Noun Phrases</a></strong><br><a href=/people/m/matthias-hartung/>Matthias Hartung</a>
|
<a href=/people/f/fabian-kaupmann/>Fabian Kaupmann</a>
|
<a href=/people/s/soufian-jebbara/>Soufian Jebbara</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1006><div class="card-body p-3 small">Word embeddings have been shown to be highly effective in a variety of lexical semantic tasks. They tend to capture meaningful relational similarities between individual words, at the expense of lacking the capabilty of making the underlying <a href=https://en.wikipedia.org/wiki/Semantic_relation>semantic relation</a> explicit. In this paper, we investigate the <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>attribute relation</a> that often holds between the constituents of <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>adjective-noun phrases</a>. We use CBOW word embeddings to represent word meaning and learn a compositionality function that combines the individual constituents into a phrase representation, thus capturing the compositional attribute meaning. The resulting embedding model, while being fully interpretable, outperforms count-based distributional vector space models that are tailored to attribute meaning in the two tasks of attribute selection and phrase similarity prediction. Moreover, as the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> captures a generalized layer of attribute meaning, it bears the potential to be used for predictions over various attribute inventories without <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>re-training</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1007/>Hypernyms under Siege : Linguistically-motivated Artillery for Hypernymy Detection</a></strong><br><a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1007><div class="card-body p-3 small">The fundamental role of <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has motivated the development of many methods for the automatic identification of this relation, most of which rely on word distribution. We investigate an extensive number of such <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised measures</a>, using several distributional semantic models that differ by <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context type</a> and feature weighting. We analyze the performance of the different <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> based on their linguistic motivation. Comparison to the state-of-the-art supervised methods shows that while supervised methods generally outperform the unsupervised ones, the former are sensitive to the distribution of training instances, hurting their reliability. Being based on general linguistic hypotheses and independent from training data, unsupervised measures are more robust, and therefore are still useful artillery for hypernymy detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1008/>Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network</a></strong><br><a href=/people/k/kim-anh-nguyen/>Kim Anh Nguyen</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1008><div class="card-body p-3 small">Distinguishing between <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> and <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In addition to the lexical and syntactic information, we successfully integrate the distance between the related words along the syntactic path as a new pattern feature. The results from <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> experiments show that AntSynNET improves the performance over prior <a href=https://en.wikipedia.org/wiki/Pattern_recognition>pattern-based methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1009/>Unsupervised Does Not Mean Uninterpretable : The Case for Word Sense Induction and Disambiguation</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/e/eugen-ruppert/>Eugen Ruppert</a>
|
<a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1009><div class="card-body p-3 small">The current trend in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> is the use of highly opaque models, e.g. neural networks and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. While these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> yield state-of-the-art results on a range of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, their drawback is poor interpretability. On the example of word sense induction and disambiguation (WSID), we show that it is possible to develop an interpretable <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that matches the state-of-the-art models in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Namely, we present an unsupervised, knowledge-free WSID approach, which is interpretable at three levels : word sense inventory, sense feature representations, and disambiguation procedure. Experiments show that our model performs on par with state-of-the-art word sense embeddings and other <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised systems</a> while offering the possibility to justify its decisions in <a href=https://en.wikipedia.org/wiki/Human-readable_medium>human-readable form</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1010/>Word Sense Disambiguation : A Unified Evaluation Framework and Empirical Comparison</a></strong><br><a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1010><div class="card-body p-3 small">Word Sense Disambiguation is a long-standing task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>, lying at the core of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>human language understanding</a>. However, the evaluation of <a href=https://en.wikipedia.org/wiki/Automation>automatic systems</a> has been problematic, mainly due to the lack of a reliable evaluation framework. In this paper we develop a unified evaluation framework and analyze the performance of various Word Sense Disambiguation systems in a fair setup. The results show that <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a> clearly outperform knowledge-based models. Among the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a>, a <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a> trained on conventional local features still proves to be a hard baseline to beat. Nonetheless, recent approaches exploiting <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> on unlabeled corpora achieve promising results, surpassing this hard baseline in most test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1011/>Which is the Effective Way for Gaokao : <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a> or <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a>?<span class=acl-fixed-case>G</span>aokao: Information Retrieval or Neural Networks?</a></strong><br><a href=/people/s/shangmin-guo/>Shangmin Guo</a>
|
<a href=/people/x/xiangrong-zeng/>Xiangrong Zeng</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1011><div class="card-body p-3 small">As one of the most important test of China, <a href=https://en.wikipedia.org/wiki/Gaokao>Gaokao</a> is designed to be difficult enough to distinguish the excellent high school students. In this work, we detailed the Gaokao History Multiple Choice Questions(GKHMC) and proposed two different approaches to address them using various resources. One approach is based on entity search technique (IR approach), the other is based on text entailment approach where we specifically employ deep neural networks(NN approach). The result of experiment on our collected real Gaokao questions showed that they are good at different categories of questions, that is IR approach performs much better at entity questions(EQs) while NN approach shows its advantage on sentence questions(SQs). We achieve state-of-the-art performance and show that it&#8217;s indispensable to apply hybrid method when participating in the real-world tests.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1012/>If You Ca n’t Beat Them Join Them : Handcrafted Features Complement Neural Nets for Non-Factoid Answer Reranking</a></strong><br><a href=/people/d/dasha-bogdanova/>Dasha Bogdanova</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a>
|
<a href=/people/d/daria-dzendzik/>Daria Dzendzik</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1012><div class="card-body p-3 small">We show that a neural approach to the task of non-factoid answer reranking can benefit from the inclusion of tried-and-tested handcrafted features. We present a neural network architecture based on a combination of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> that are used to encode questions and answers, and a <a href=https://en.wikipedia.org/wiki/Multilayer_perceptron>multilayer perceptron</a>. We show how this approach can be combined with additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, in particular, the discourse features used by previous research. Our neural approach achieves state-of-the-art performance on a public dataset from Yahoo ! Answers and its performance is further improved by incorporating the discourse features. Additionally, we present a new dataset of Ask Ubuntu questions where the hybrid approach also achieves good results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1013/>Chains of Reasoning over Entities, Relations, and Text using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/arvind-neelakantan/>Arvind Neelakantan</a>
|
<a href=/people/d/david-belanger/>David Belanger</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1013><div class="card-body p-3 small">Our goal is to combine the rich multi-step inference of symbolic logical reasoning with the generalization capabilities of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We are particularly interested in complex reasoning about entities and relations in <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs to compose the distributed semantics of multi-hop paths in KBs ; however for multiple reasons, the approach lacks accuracy and practicality. This paper proposes three significant modeling advances : (1) we learn to jointly reason about <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>, entities, and entity-types ; (2) we use neural attention modeling to incorporate multiple paths ; (3) we learn to share strength in a single RNN that represents logical composition across all relations. On a large-scale Freebase+ClueWeb prediction task, we achieve 25 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a>, and a 53 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> on sparse relations due to shared strength. On chains of reasoning in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> we reduce error in mean quantile by 84 % versus previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.<i>entities, and entity-types</i>; (2) we use neural attention modeling to incorporate <i>multiple paths</i>; (3) we learn to <i>share strength in a single RNN</i> that represents logical composition across all relations. On a large-scale Freebase+ClueWeb prediction task, we achieve 25% error reduction, and a 53% error reduction on sparse relations due to shared strength. On chains of reasoning in WordNet we reduce error in mean quantile by 84% versus previous state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1015/>Multitask Learning for Mental Health Conditions with Limited Social Media Data</a></strong><br><a href=/people/a/adrian-benton/>Adrian Benton</a>
|
<a href=/people/m/margaret-mitchell/>Margaret Mitchell</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1015><div class="card-body p-3 small">Language contains information about the author&#8217;s demographic attributes as well as their <a href=https://en.wikipedia.org/wiki/Mental_state>mental state</a>, and has been successfully leveraged in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> to predict either one alone. However, demographic attributes and mental states also interact with each other, and we are the first to demonstrate how to use them jointly to improve the prediction of mental health conditions across the board. We model the different conditions as tasks in a multitask learning (MTL) framework, and establish for the first time the potential of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> in the prediction of mental health from online user-generated text. The framework we propose significantly improves over all baselines and single-task models for predicting mental health conditions, with particularly significant gains for conditions with limited data. In addition, our best MTL model can predict the presence of conditions (neuroatypicality) more generally, further reducing the error of the strong feed-forward baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1017/>Computational Argumentation Quality Assessment in <a href=https://en.wikipedia.org/wiki/Natural_language>Natural Language</a></a></strong><br><a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/n/nona-naderi/>Nona Naderi</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/y/yonatan-bilu/>Yonatan Bilu</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/t/tim-alberdingk-thijm/>Tim Alberdingk Thijm</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1017><div class="card-body p-3 small">Research on computational argumentation faces the problem of how to automatically assess the quality of an argument or argumentation. While different quality dimensions have been approached in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, a common understanding of argumentation quality is still missing. This paper presents the first holistic work on computational argumentation quality in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. We comprehensively survey the diverse existing theories and approaches to assess logical, rhetorical, and dialectical quality dimensions, and we derive a systematic taxonomy from these. In addition, we provide a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with 320 arguments, annotated for all 15 dimensions in the <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomy</a>. Our results establish a common ground for research on computational argumentation quality assessment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1018/>A method for in-depth comparative evaluation : How (dis)similar are outputs of pos taggers, dependency parsers and coreference resolvers really?</a></strong><br><a href=/people/d/don-tuggener/>Don Tuggener</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1018><div class="card-body p-3 small">This paper proposes a generic <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for the comparative evaluation of system outputs. The approach is able to quantify the pairwise differences between two outputs and to unravel in detail what the differences consist of. We apply our approach to three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>Computational Linguistics</a>, i.e. POS tagging, <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parsing</a>, and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. We find that system outputs are more distinct than the (often) small differences in evaluation scores seem to suggest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1020/>Integrating <a href=https://en.wikipedia.org/wiki/Meaning_(philosophy_of_language)>Meaning</a> into Quality Evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/o/osman-baskaya/>Osman Başkaya</a>
|
<a href=/people/e/eray-yildiz/>Eray Yildiz</a>
|
<a href=/people/d/doruk-tunaoglu/>Doruk Tunaoğlu</a>
|
<a href=/people/m/mustafa-tolga-eren/>Mustafa Tolga Eren</a>
|
<a href=/people/a/a-seza-dogruoz/>A. Seza Doğruöz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1020><div class="card-body p-3 small">Machine translation (MT) quality is evaluated through comparisons between MT outputs and the human translations (HT). Traditionally, this <a href=https://en.wikipedia.org/wiki/Validity_(logic)>evaluation</a> relies on form related features (e.g. lexicon and syntax) and ignores the transfer of meaning reflected in HT outputs. Instead, we evaluate the <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> of MT outputs through meaning related features (e.g. polarity, subjectivity) with two experiments. In the first experiment, the meaning related features are compared to human rankings individually. In the second experiment, combinations of meaning related features and other quality metrics are utilized to predict the same human rankings. The results of our experiments confirm the benefit of these features in predicting human evaluation of translation quality in addition to traditional <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> which focus mainly on form.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1021/>Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages</a></strong><br><a href=/people/m/michael-schlichtkrull/>Michael Schlichtkrull</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1021><div class="card-body p-3 small">In cross-lingual dependency annotation projection, information is often lost during <a href=https://en.wikipedia.org/wiki/Language_transfer>transfer</a> because of early decoding. We present an end-to-end graph-based neural network dependency parser that can be trained to reproduce matrices of edge scores, which can be directly projected across word alignments. We show that our approach to cross-lingual dependency parsing is not only simpler, but also achieves an absolute improvement of 2.25 % averaged across 10 languages compared to the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1022/>Parsing Universal Dependencies without training<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies without training</a></strong><br><a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/z/zeljko-agic/>Željko Agić</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1022><div class="card-body p-3 small">We present UDP, the first training-free parser for Universal Dependencies (UD). Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is based on <a href=https://en.wikipedia.org/wiki/PageRank>PageRank</a> and a small set of specific dependency head rules. UDP features two-step decoding to guarantee that <a href=https://en.wikipedia.org/wiki/Function_word>function words</a> are attached as <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>leaf nodes</a>. The <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> requires no training, and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD. The <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> has very few parameters and distinctly robust to domain change across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1024/>Stance Classification of Context-Dependent Claims</a></strong><br><a href=/people/r/roy-bar-haim/>Roy Bar-Haim</a>
|
<a href=/people/i/indrajit-bhattacharya/>Indrajit Bhattacharya</a>
|
<a href=/people/f/francesco-dinuzzo/>Francesco Dinuzzo</a>
|
<a href=/people/a/amrita-saha/>Amrita Saha</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1024><div class="card-body p-3 small">Recent work has addressed the problem of detecting relevant claims for a given controversial topic. We introduce the complementary task of Claim Stance Classification, along with the first <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark dataset</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We decompose this problem into : (a) open-domain target identification for topic and claim (b) sentiment classification for each target, and (c) open-domain contrast detection between the topic and the claim targets. Manual annotation of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> confirms the applicability and validity of our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. We describe an implementation of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, focusing on a novel <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for contrast detection. Our approach achieves promising results, and is shown to outperform several baselines, which represent the common practice of applying a single, monolithic classifier for stance classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1025/>Exploring the Impact of Pragmatic Phenomena on Irony Detection in Tweets : A Multilingual Corpus Study</a></strong><br><a href=/people/j/jihen-karoui/>Jihen Karoui</a>
|
<a href=/people/f/farah-benamara/>Farah Benamara</a>
|
<a href=/people/v/veronique-moriceau/>Véronique Moriceau</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a>
|
<a href=/people/c/cristina-bosco/>Cristina Bosco</a>
|
<a href=/people/n/nathalie-aussenac-gilles/>Nathalie Aussenac-Gilles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1025><div class="card-body p-3 small">This paper provides a linguistic and pragmatic analysis of the phenomenon of <a href=https://en.wikipedia.org/wiki/Irony>irony</a> in order to represent how Twitter&#8217;s users exploit <a href=https://en.wikipedia.org/wiki/Irony>irony devices</a> within their communication strategies for generating textual contents. We aim to measure the impact of a wide-range of pragmatic phenomena in the interpretation of irony, and to investigate how these <a href=https://en.wikipedia.org/wiki/Phenomenon>phenomena</a> interact with contexts local to the tweet. Informed by linguistic theories, we propose for the first time a multi-layered annotation schema for <a href=https://en.wikipedia.org/wiki/Irony>irony</a> and its application to a corpus of French, English and Italian tweets. We detail each layer, explore their interactions, and discuss our results according to a qualitative and quantitative perspective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1026/>A Multi-View Sentiment Corpus</a></strong><br><a href=/people/d/debora-nozza/>Debora Nozza</a>
|
<a href=/people/e/elisabetta-fersini/>Elisabetta Fersini</a>
|
<a href=/people/e/enza-messina/>Enza Messina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1026><div class="card-body p-3 small">Sentiment Analysis is a broad task that involves the analysis of various aspect of the <a href=https://en.wikipedia.org/wiki/Natural_language>natural language text</a>. However, most of the <a href=https://en.wikipedia.org/wiki/List_of_art_media>approaches</a> in the state of the art usually investigate independently each aspect, i.e. Subjectivity Classification, Sentiment Polarity Classification, <a href=https://en.wikipedia.org/wiki/Emotion_recognition>Emotion Recognition</a>, Irony Detection. In this paper we present a Multi-View Sentiment Corpus (MVSC), which comprises 3000 English microblog posts related the movie domain. Three independent annotators manually labelled MVSC, following a broad annotation schema about different aspects that can be grasped from natural language text coming from <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. The contribution is therefore a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> that comprises five different views for each message, i.e. subjective / objective, sentiment polarity, implicit / explicit, <a href=https://en.wikipedia.org/wiki/Irony>irony</a>, <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>. In order to allow a more detailed investigation on the human labelling behaviour, we provide the annotations of each human annotator involved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1027/>A Systematic Study of Neural Discourse Models for Implicit Discourse Relation</a></strong><br><a href=/people/a/attapol-rutherford/>Attapol Rutherford</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1027><div class="card-body p-3 small">Inferring implicit discourse relations in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language text</a> is the most difficult subtask in discourse parsing. Many neural network models have been proposed to tackle this problem. However, the comparison for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is not unified, so we could hardly draw clear conclusions about the effectiveness of various <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>. Here, we propose neural network models that are based on feedforward and long-short term memory architecture and systematically study the effects of varying structures. To our surprise, the best-configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Further, we compare our best feedforward system with competitive convolutional and recurrent networks and find that feedforward can actually be more effective. For the first time for this task, we compile and publish outputs from previous neural and non-neural systems to establish the standard for further comparison.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1029/>Dialog state tracking, a machine reading approach using Memory Network</a></strong><br><a href=/people/j/julien-perez/>Julien Perez</a>
|
<a href=/people/f/fei-liu-unimelb/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1029><div class="card-body p-3 small">In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> and the natural language understanding modules. This paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an End-to-End Memory Network, MemN2N, a memory-enhanced neural network architecture. We evaluate the proposed approach on the second Dialog State Tracking Challenge (DSTC-2) dataset. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> has been converted for the occasion in order to frame the hidden state variable inference as a question-answering task based on a sequence of utterances extracted from a dialog. We show that the proposed <a href=https://en.wikipedia.org/wiki/Music_tracker>tracker</a> gives encouraging results. Then, we propose to extend the DSTC-2 dataset with specific reasoning capabilities requirement like <a href=https://en.wikipedia.org/wiki/Counting>counting</a>, list maintenance, yes-no question answering and indefinite knowledge management. Finally, we present encouraging results using our proposed MemN2N based tracking model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1030/>Sentence Segmentation in Narrative Transcripts from Neuropsychological Tests using Recurrent Convolutional Neural Networks</a></strong><br><a href=/people/m/marcos-treviso/>Marcos Treviso</a>
|
<a href=/people/c/christopher-shulby/>Christopher Shulby</a>
|
<a href=/people/s/sandra-aluisio/>Sandra Aluísio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1030><div class="card-body p-3 small">Automated discourse analysis tools based on Natural Language Processing (NLP) aiming at the diagnosis of language-impairing dementias generally extract several textual metrics of narrative transcripts. However, the absence of sentence boundary segmentation in the transcripts prevents the direct application of NLP methods which rely on these marks in order to function properly, such as taggers and <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. We present the first steps taken towards automatic neuropsychological evaluation based on narrative discourse analysis, presenting a new automatic sentence segmentation method for impaired speech. Our model uses recurrent convolutional neural networks with prosodic, Part of Speech (PoS) features, and word embeddings. It was evaluated intrinsically on impaired, spontaneous speech as well as normal, prepared speech and presents better results for healthy elderly (CTL) (F1 = 0.74) and Mild Cognitive Impairment (MCI) patients (F1 = 0.70) than the Conditional Random Fields method (F1 = 0.55 and 0.53, respectively) used in the same context of our study. The results suggest that our model is robust for impaired speech and can be used in automated discourse analysis tools to differentiate narratives produced by MCI and CTL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1031/>Joint, Incremental Disfluency Detection and Utterance Segmentation from <a href=https://en.wikipedia.org/wiki/Speech>Speech</a></a></strong><br><a href=/people/j/julian-hough/>Julian Hough</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1031><div class="card-body p-3 small">We present the joint task of incremental disfluency detection and utterance segmentation and a simple deep learning system which performs it on transcripts and ASR results. We show how the constraints of the two <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> interact. Our joint-task system outperforms the equivalent individual task systems, provides competitive results and is suitable for future use in conversation agents in the psychiatric domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1032/>From Segmentation to Analyses : a Probabilistic Model for Unsupervised Morphology Induction</a></strong><br><a href=/people/t/toms-bergmanis/>Toms Bergmanis</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1032><div class="card-body p-3 small">A major motivation for unsupervised morphological analysis is to reduce the sparse data problem in under-resourced languages. Most previous work focus on segmenting surface forms into their constituent morphs (taking : tak + ing), but surface form segmentation does not solve the sparse data problem as the analyses of take and taking are not connected to each other. We present a system that adapts the MorphoChains system (Narasimhan et al., 2015) to provide morphological analyses that aim to abstract over spelling differences in functionally similar morphs. This results in analyses that are not compelled to use all the orthographic material of a word (stopping : stop + ing) or limited to only that material (acidified : acid + ify + ed). On average across six typologically varied languages our <a href=https://en.wikipedia.org/wiki/System>system</a> has a similar or better F-score on EMMA (a measure of underlying morpheme accuracy) than three strong baselines ; moreover, the total number of distinct morphemes identified by our <a href=https://en.wikipedia.org/wiki/System>system</a> is on average 12.8 % lower than for Morfessor (Virpioja et al., 2013), a state-of-the-art surface segmentation system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1034/>Universal Dependencies and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a> for Hungarian-and on the Price of Universality<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies and Morphology for <span class=acl-fixed-case>H</span>ungarian - and on the Price of Universality</a></strong><br><a href=/people/v/veronika-vincze/>Veronika Vincze</a>
|
<a href=/people/k/katalin-ilona-simko/>Katalin Simkó</a>
|
<a href=/people/z/zsolt-szanto/>Zsolt Szántó</a>
|
<a href=/people/r/richard-farkas/>Richárd Farkas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1034><div class="card-body p-3 small">In this paper, we present how the principles of universal dependencies and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> have been adapted to <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>. We report the most challenging <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical phenomena</a> and our solutions to those. On the basis of the adapted guidelines, we have converted and manually corrected 1,800 sentences from the Szeged Treebank to universal dependency format. We also introduce experiments on this manually annotated corpus for evaluating automatic conversion and the added value of language-specific, i.e. non-universal, annotations. Our results reveal that converting to universal dependencies is not necessarily trivial, moreover, using language-specific morphological features may have an impact on overall performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1035/>Addressing the Data Sparsity Issue in Neural AMR Parsing<span class=acl-fixed-case>AMR</span> Parsing</a></strong><br><a href=/people/x/xiaochang-peng/>Xiaochang Peng</a>
|
<a href=/people/c/chuan-wang/>Chuan Wang</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1035><div class="card-body p-3 small">Neural attention models have achieved great success in different <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>. However, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1037/>Enumeration of Extractive Oracle Summaries</a></strong><br><a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nishino/>Masaaki Nishino</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1037><div class="card-body p-3 small">To analyze the limitations and the future directions of the extractive summarization paradigm, this paper proposes an Integer Linear Programming (ILP) formulation to obtain extractive oracle summaries in terms of ROUGE-N. We also propose an algorithm that enumerates all of the <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle summaries</a> for a set of reference summaries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following : (1) room still exists to improve the performance of extractive summarization ; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1038/>Neural Semantic Encoders</a></strong><br><a href=/people/t/tsendsuren-munkhdalai/>Tsendsuren Munkhdalai</a>
|
<a href=/people/h/hong-yu/>Hong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1038><div class="card-body p-3 small">We present a memory augmented neural network for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> : Neural Semantic Encoders. NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves over time and maintains the understanding of input sequences through read, compose and write operations. NSE can also access 1 multiple and shared memories. In this paper, we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, sentence classification, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>document sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> where NSE achieved state-of-the-art performance when evaluated on publically available benchmarks. For example, our shared-memory model showed an encouraging result on <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, improving an attention-based baseline by approximately 1.0 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1039/>Efficient Benchmarking of NLP APIs using Multi-armed Bandits<span class=acl-fixed-case>NLP</span> <span class=acl-fixed-case>API</span>s using Multi-armed Bandits</a></strong><br><a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/t/tuan-tran/>Tuan Dung Tran</a>
|
<a href=/people/m/mark-carman/>Mark Carman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1039><div class="card-body p-3 small">Comparing NLP systems to select the best one for a task of interest, such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, is critical for practitioners and researchers. A rigorous approach involves setting up a hypothesis testing scenario using the performance of the <a href=https://en.wikipedia.org/wiki/System>systems</a> on query documents. However, often the hypothesis testing approach needs to send a lot of document queries to the systems, which can be problematic. In this paper, we present an effective alternative based on the multi-armed bandit (MAB). We propose a hierarchical generative model to represent the uncertainty in the performance measures of the competing systems, to be used by <a href=https://en.wikipedia.org/wiki/Thompson_sampling>Thompson Sampling</a> to solve the resulting MAB. Experimental results on both synthetic and real data show that our approach requires significantly fewer queries compared to the standard benchmarking technique to identify the best <a href=https://en.wikipedia.org/wiki/System>system</a> according to <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1040/>Character-Word LSTM Language Models<span class=acl-fixed-case>LSTM</span> Language Models</a></strong><br><a href=/people/l/lyan-verwimp/>Lyan Verwimp</a>
|
<a href=/people/j/joris-pelemans/>Joris Pelemans</a>
|
<a href=/people/h/hugo-van-hamme/>Hugo Van hamme</a>
|
<a href=/people/p/patrick-wambacq/>Patrick Wambacq</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1040><div class="card-body p-3 small">We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating <a href=https://en.wikipedia.org/wiki/Word_embedding>word and character embeddings</a>, we achieve up to 2.77 % relative improvement on <a href=https://en.wikipedia.org/wiki/English_language>English</a> compared to a baseline model with a similar amount of parameters and 4.57 % on <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>. Moreover, we also outperform baseline word-level models with a larger number of parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1041/>A Hierarchical Neural Model for Learning Sequences of Dialogue Acts</a></strong><br><a href=/people/q/quan-hung-tran/>Quan Hung Tran</a>
|
<a href=/people/i/ingrid-zukerman/>Ingrid Zukerman</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1041><div class="card-body p-3 small">We propose a novel hierarchical Recurrent Neural Network (RNN) for learning sequences of Dialogue Acts (DAs). The input in this task is a sequence of utterances (i.e., conversational contributions) comprising a sequence of tokens, and the output is a sequence of DA labels (one label per utterance). Our model leverages the hierarchical nature of dialogue data by using two nested RNNs that capture long-range dependencies at the dialogue level and the utterance level. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is combined with an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> that focuses on salient tokens in utterances. Our experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms strong baselines on two popular datasets, Switchboard and MapTask ; and our detailed empirical analysis highlights the impact of each aspect of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1042/>A Network-based End-to-End Trainable Task-oriented Dialogue System</a></strong><br><a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/d/david-vandyke/>David Vandyke</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/s/steve-young/>Steve Young</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1042><div class="card-body p-3 small">Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of <a href=https://en.wikipedia.org/wiki/Handicraft>handcrafting</a>, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> easily and without making too many assumptions about the task at hand. The results show that the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1043 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1043/>May I take your order? A Neural Model for Extracting Structured Information from Conversations<span class=acl-fixed-case>I</span> take your order? A Neural Model for Extracting Structured Information from Conversations</a></strong><br><a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/m/michael-seltzer/>Michael Seltzer</a>
|
<a href=/people/y/y-c-ju/>Y.C. Ju</a>
|
<a href=/people/g/geoffrey-zweig/>Geoffrey Zweig</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1043><div class="card-body p-3 small">In this paper we tackle a unique and important problem of extracting a structured order from the conversation a customer has with an order taker at a restaurant. This is motivated by an actual <a href=https://en.wikipedia.org/wiki/System>system</a> under development to assist in the order taking process. We develop a sequence-to-sequence model that is able to map from unstructured conversational input to the structured form that is conveyed to the kitchen and appears on the customer receipt. This problem is critically different from other tasks like <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> where sequence-to-sequence models have been used : the input includes two sides of a conversation ; the output is highly structured ; and logical manipulations must be performed, for example when the customer changes his mind while ordering. We present a novel sequence-to-sequence model that incorporates a special attention-memory gating mechanism and conversational role markers. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves performance over both a phrase-based machine translation approach and a standard sequence-to-sequence model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1044/>A Two-stage Sieve Approach for Quote Attribution</a></strong><br><a href=/people/g/grace-muzny/>Grace Muzny</a>
|
<a href=/people/m/michael-fang/>Michael Fang</a>
|
<a href=/people/a/angel-chang/>Angel Chang</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1044><div class="card-body p-3 small">We present a deterministic sieve-based system for attributing quotations in literary text and a new dataset : QuoteLi3. Quote attribution, determining who said what in a given text, is important for tasks like creating dialogue systems, and in newer areas like computational literary studies, where it creates opportunities to analyze novels at scale rather than only a few at a time. We release QuoteLi3, which contains more than 6,000 annotations linking quotes to speaker mentions and quotes to speaker entities, and introduce a new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for quote attribution. Our two-stage algorithm first links quotes to mentions, then mentions to entities. Using two stages encapsulates difficult sub-problems and improves system performance. The modular design allows us to tune for overall performance or higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, which is useful for many real-world use cases. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves an average <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 87.5 across three novels, outperforming previous systems, and can be tuned for <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> of 90.4 at a <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>recall</a> of 65.1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1045/>Out-of-domain FrameNet Semantic Role Labeling<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et Semantic Role Labeling</a></strong><br><a href=/people/s/silvana-hartmann/>Silvana Hartmann</a>
|
<a href=/people/i/ilia-kuznetsov/>Ilia Kuznetsov</a>
|
<a href=/people/m/m-teresa-martin-valdivia/>Teresa Martin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1045><div class="card-body p-3 small">Domain dependence of NLP systems is one of the major obstacles to their application in large-scale text analysis, also restricting the applicability of FrameNet semantic role labeling (SRL) systems. Yet, current FrameNet SRL systems are still only evaluated on a single in-domain test set. For the first time, we study the domain dependence of FrameNet SRL on a wide range of <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark sets</a>. We create a novel test set for FrameNet SRL based on user-generated web text and find that the major bottleneck for out-of-domain FrameNet SRL is the frame identification step. To address this problem, we develop a simple, yet efficient <a href=https://en.wikipedia.org/wiki/System>system</a> based on distributed word representations. Our <a href=https://en.wikipedia.org/wiki/System>system</a> closely approaches the state-of-the-art in-domain while outperforming the best available frame identification system out-of-domain. We publish our <a href=https://en.wikipedia.org/wiki/System>system</a> and test data for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1046/>TDParse : Multi-target-specific sentiment recognition on Twitter<span class=acl-fixed-case>TDP</span>arse: Multi-target-specific sentiment recognition on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/b/bo-wang/>Bo Wang</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a>
|
<a href=/people/a/arkaitz-zubiaga/>Arkaitz Zubiaga</a>
|
<a href=/people/r/rob-procter/>Rob Procter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1046><div class="card-body p-3 small">Existing target-specific sentiment recognition methods consider only a single target per tweet, and have been shown to miss nearly half of the actual targets mentioned. We present a <a href=https://en.wikipedia.org/wiki/Twitter>corpus of UK election tweets</a>, with an average of 3.09 entities per tweet and more than one type of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> in half of the tweets. This requires a method for multi-target specific sentiment recognition, which we develop by using the context around a target as well as syntactic dependencies involving the target. We present results of our method on both a benchmark corpus of single targets and the multi-target election corpus, showing state-of-the art performance in both corpora and outperforming previous approaches to multi-target sentiment task as well as deep learning models for single-target sentiment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1048/>An Extensive Empirical Evaluation of Character-Based Morphological Tagging for 14 Languages</a></strong><br><a href=/people/g/georg-heigold/>Georg Heigold</a>
|
<a href=/people/g/gunter-neumann/>Guenter Neumann</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1048><div class="card-body p-3 small">This paper investigates neural character-based morphological tagging for languages with complex morphology and large tag sets. Character-based approaches are attractive as they can handle rarely- and unseen words gracefully. We evaluate on 14 languages and observe consistent gains over a state-of-the-art morphological tagger across all languages except for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>, where we match the state-of-the-art. We compare two architectures for computing character-based word vectors using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent (RNN)</a> and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional (CNN) nets</a>. We show that the CNN based approach performs slightly worse and less consistently than the RNN based approach. Small but systematic gains are observed when combining the two <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> by <a href=https://en.wikipedia.org/wiki/Assembly_language>ensembling</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1049/>Neural Multi-Source Morphological Reinflection</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1049><div class="card-body p-3 small">We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one <a href=https://en.wikipedia.org/wiki/Form_(document)>source form</a> since different <a href=https://en.wikipedia.org/wiki/Form_(document)>source forms</a> can provide complementary information, e.g., different <a href=https://en.wikipedia.org/wiki/Word_stem>stems</a>. We further present a novel extension to the encoder-decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1050/>Online Automatic Post-editing for MT in a Multi-Domain Translation Environment<span class=acl-fixed-case>MT</span> in a Multi-Domain Translation Environment</a></strong><br><a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/g/gebremedhen-gebremelak/>Gebremedhen Gebremelak</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1050><div class="card-body p-3 small">Automatic post-editing (APE) for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> aims to fix recurrent errors made by the <a href=https://en.wikipedia.org/wiki/Machine_translation>MT decoder</a> by learning from correction examples. In controlled evaluation scenarios, the representativeness of the training set with respect to the test data is a key factor to achieve good performance. Real-life scenarios, however, do not guarantee such favorable learning conditions. Ideally, to be integrated in a real professional translation workflow (e.g. to play a role in computer-assisted translation framework), APE tools should be flexible enough to cope with continuous streams of diverse data coming from different domains / genres. To cope with this problem, we propose an online APE framework that is : i) robust to data diversity (i.e. capable to learn and apply correction rules in the right contexts) and ii) able to evolve over time (by continuously extending and refining its knowledge). In a comparative evaluation, with English-German test data coming in random order from two different domains, we show the effectiveness of our approach, which outperforms a strong <a href=https://en.wikipedia.org/wiki/Batch_processing>batch system</a> and the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> in online APE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1051" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1051/>An Incremental Parser for Abstract Meaning Representation<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation</a></strong><br><a href=/people/m/marco-damonte/>Marco Damonte</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/g/giorgio-satta/>Giorgio Satta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1051><div class="card-body p-3 small">Abstract Meaning Representation (AMR) is a semantic representation for natural language that embeds annotations related to traditional tasks such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>, <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> and co-reference resolution. We describe a transition-based parser for AMR that parses sentences left-to-right, in <a href=https://en.wikipedia.org/wiki/Time_complexity>linear time</a>. We further propose a test-suite that assesses specific subtasks that are helpful in comparing AMR parsers, and show that our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is competitive with the state of the art on the LDC2015E86 dataset and that it outperforms state-of-the-art parsers for recovering named entities and handling polarity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1052/>Integrated Learning of Dialog Strategies and Semantic Parsing</a></strong><br><a href=/people/a/aishwarya-padmakumar/>Aishwarya Padmakumar</a>
|
<a href=/people/j/jesse-thomason/>Jesse Thomason</a>
|
<a href=/people/r/raymond-mooney/>Raymond J. Mooney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1052><div class="card-body p-3 small">Natural language understanding and <a href=https://en.wikipedia.org/wiki/Dialog_management>dialog management</a> are two integral components of interactive dialog systems. Previous research has used machine learning techniques to individually optimize these <a href=https://en.wikipedia.org/wiki/Component_(graph_theory)>components</a>, with different forms of direct and indirect supervision. We present an approach to integrate the learning of both a dialog strategy using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, and a semantic parser for robust <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, using only natural dialog interaction for supervision. Experimental results on a simulated task of robot instruction demonstrate that joint learning of both <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> improves dialog performance over learning either of these <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1053/>Unsupervised AMR-Dependency Parse Alignment<span class=acl-fixed-case>AMR</span>-Dependency Parse Alignment</a></strong><br><a href=/people/w/wei-te-chen/>Wei-Te Chen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1053><div class="card-body p-3 small">In this paper, we introduce an Abstract Meaning Representation (AMR) to Dependency Parse aligner. Alignment is a preliminary step for AMR parsing, and our aligner improves current AMR parser performance. Our aligner involves several different features, including named entity tags and semantic role labels, and uses Expectation-Maximization training. Results show that our aligner reaches an 87.1 % F-Score score with the experimental data, and enhances AMR parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1054/>Improving Chinese Semantic Role Labeling using High-quality Surface and Deep Case Frames<span class=acl-fixed-case>C</span>hinese Semantic Role Labeling using High-quality Surface and Deep Case Frames</a></strong><br><a href=/people/g/gongye-jin/>Gongye Jin</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1054><div class="card-body p-3 small">This paper presents a method for applying automatically acquired knowledge to semantic role labeling (SRL). We use a large amount of automatically extracted knowledge to improve the performance of SRL. We present two varieties of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a>, which we call surface case frames and deep case frames. Although the surface case frames are compiled from syntactic parses and can be used as rich syntactic knowledge, they have limited capability for resolving semantic ambiguity. To compensate the deficiency of the <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>surface case frames</a>, we compile <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>deep case frames</a> from automatic semantic roles. We also consider <a href=https://en.wikipedia.org/wiki/Quality_management>quality management</a> for both types of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> in order to get rid of the noise brought from the automatic analyses. The experimental results show that Chinese SRL can be improved using automatically acquired knowledge and the <a href=https://en.wikipedia.org/wiki/Quality_management>quality management</a> shows a positive effect on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1055/>Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1055><div class="card-body p-3 small">Entities are essential elements of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. In this paper, we present <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for learning multi-level representations of entities on three complementary levels : character (character patterns in entity names extracted, e.g., by neural networks), word (embeddings of words in entity names) and entity (entity embeddings). We investigate state-of-the-art learning methods on each level and find large differences, e.g., for <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, traditional ngram features and the subword model of fasttext (Bojanowski et al., 2016) on the character level ; for <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> (Mikolov et al., 2013) on the word level ; and for the order-aware model wang2vec (Ling et al., 2015a) on the entity level. We confirm experimentally that each level of <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> contributes complementary information and a joint representation of all three levels improves the existing embedding based baseline for fine-grained entity typing by a large margin. Additionally, we show that adding information from entity descriptions further improves multi-level representations of entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1056/>The ContrastMedium Algorithm : Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links<span class=acl-fixed-case>C</span>ontrast<span class=acl-fixed-case>M</span>edium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links</a></strong><br><a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1056><div class="card-body p-3 small">In this paper, we present ContrastMedium, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manually selected input root and leaf concepts. This is achieved by leveraging structural information from a companion reference taxonomy, to which the input <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> is linked (either automatically or manually). When used in conjunction with methods for hypernym acquisition and knowledge base linking, our methodology provides a complete solution for end-to-end taxonomy induction. We conduct experiments using automatically acquired knowledge graphs, as well as a SemEval benchmark, and show that our method is able to achieve high performance on the task of taxonomy induction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1057/>Probabilistic Inference for Cold Start Knowledge Base Population with Prior World Knowledge</a></strong><br><a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/m/marjorie-freedman/>Marjorie Freedman</a>
|
<a href=/people/t/talya-meltzer/>Talya Meltzer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1057><div class="card-body p-3 small">Building knowledge bases (KB) automatically from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> is crucial for many applications such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search</a>. The problem is very challenging and has been divided into sub-problems such as mention and named entity recognition, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> and <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. However, combining these components has shown to be under-constrained and often produces KBs with supersize entities and common-sense errors in relations (a person has multiple birthdates). The errors are difficult to resolve solely with IE tools but become obvious with <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> at the corpus level. By analyzing <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> and a large text collection, we found that per-relation cardinality and the popularity of entities follow the <a href=https://en.wikipedia.org/wiki/Power_law>power-law distribution</a> favoring flat long tails with low-frequency instances. We present a probabilistic joint inference algorithm to incorporate this <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> during KB construction. Our approach yields state-of-the-art performance on the TAC Cold Start task, and 42 % and 19.4 % relative improvements in F1 over our baseline on Cold Start hop-1 and all-hop queries respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1058" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1058/>Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema</a></strong><br><a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/a/arvind-neelakantan/>Arvind Neelakantan</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1058><div class="card-body p-3 small">Universal schema predicts the types of entities and relations in a knowledge base (KB) by jointly embedding the union of all available schema typesnot only types from multiple structured databases (such as <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> or Wikipedia infoboxes), but also types expressed as textual patterns from raw text. This prediction is typically modeled as a matrix completion problem, with one type per column, and either one or two entities per row (in the case of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity types</a> or <a href=https://en.wikipedia.org/wiki/Binary_relation>binary relation types</a>, respectively). Factorizing this sparsely observed matrix yields a learned vector embedding for each row and each column. In this paper we explore the problem of making predictions for entities or entity-pairs unseen at training time (and hence without a pre-learned row embedding). We propose an approach having no per-row parameters at all ; rather we produce a row vector on the fly using a learned aggregation function of the vectors of the observed columns for that row. We experiment with various aggregation functions, including neural network attention models. Our approach can be understood as a natural language database, in that questions about KB entities are answered by attending to textual or database evidence. In experiments predicting both relations and entity types, we demonstrate that despite having an order of magnitude fewer parameters than traditional universal schema, we can match the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the traditional model, and more importantly, we can now make predictions about unseen rows with nearly the same <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> as rows available at training time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1059/>Learning to Generate Product Reviews from Attributes</a></strong><br><a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1059><div class="card-body p-3 small">Automatically generating product reviews is a meaningful, yet not well-studied task in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Traditional natural language generation methods rely extensively on hand-crafted rules and predefined templates. This paper presents an attention-enhanced attribute-to-sequence model to generate product reviews for given attribute information, such as <a href=https://en.wikipedia.org/wiki/User_(computing)>user</a>, product, and rating. The attribute encoder learns to represent input attributes as vectors. Then, the sequence decoder generates reviews by conditioning its output on these vectors. We also introduce an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to jointly generate <a href=https://en.wikipedia.org/wiki/Review_article>reviews</a> and align words with input attributes. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained end-to-end to maximize the likelihood of target product reviews given the <a href=https://en.wikipedia.org/wiki/Variable_and_attribute_(research)>attributes</a>. We build a publicly available dataset for the review generation task by leveraging the Amazon book reviews and their metadata. Experiments on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> show that our approach outperforms baseline methods and the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> significantly improves the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1060" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1060/>Learning to generate one-sentence biographies from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a><span class=acl-fixed-case>W</span>ikidata</a></strong><br><a href=/people/a/andrew-chisholm/>Andrew Chisholm</a>
|
<a href=/people/w/will-radford/>Will Radford</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1060><div class="card-body p-3 small">We investigate the generation of one-sentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> incorporates a novel secondary objective that helps ensure <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> generates sentences that contain the input facts. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a BLEU score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1061" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1061/>Transition-Based Deep Input Linearization</a></strong><br><a href=/people/r/ratish-puduppully/>Ratish Puduppully</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1061><div class="card-body p-3 small">Traditional methods for deep NLG adopt pipeline approaches comprising stages such as constructing syntactic input, predicting function words, linearizing the syntactic input and generating the surface forms. Though easier to visualize, pipeline approaches suffer from error propagation. In addition, information available across modules can not be leveraged by all modules. We construct a transition-based model to jointly perform <a href=https://en.wikipedia.org/wiki/Linearization>linearization</a>, function word prediction and morphological generation, which considerably improves upon the accuracy compared to a pipelined baseline system. On a standard deep input linearization shared task, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves the best results reported so far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1064" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1064/>Tackling Error Propagation through <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> : A Case of Greedy Dependency Parsing</a></strong><br><a href=/people/m/minh-le/>Minh Lê</a>
|
<a href=/people/a/antske-fokkens/>Antske Fokkens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1064><div class="card-body p-3 small">Error propagation is a common problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes are made early in a process. In this paper, we apply <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to greedy dependency parsing which is known to suffer from <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a> and confirm that <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> reduces the occurrence of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1065 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1065/>Noisy-context surprisal as a human sentence processing cost model</a></strong><br><a href=/people/r/richard-futrell/>Richard Futrell</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1065><div class="card-body p-3 small">We use the noisy-channel theory of human sentence comprehension to develop an incremental processing cost model that unifies and extends key features of expectation-based and memory-based models. In this model, which we call noisy-context surprisal, the processing cost of a word is the surprisal of the word given a noisy representation of the preceding context. We show that this model accounts for an outstanding puzzle in sentence comprehension, language-dependent structural forgetting effects (Gibson and Thomas, 1999 ; Vasishth et al., 2010 ; Frank et al., 2016), which are previously not well modeled by either expectation-based or memory-based approaches. Additionally, we show that this model derives and generalizes locality effects (Gibson, 1998 ; Demberg and Keller, 2008), a signature prediction of memory-based models. We give <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus-based evidence</a> for a key assumption in this derivation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1066 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1066/>Task-Specific Attentive Pooling of Phrase Alignments Contributes to Sentence Matching</a></strong><br><a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1066><div class="card-body p-3 small">This work studies comparatively two typical sentence matching tasks : textual entailment (TE) and answer selection (AS), observing that weaker phrase alignments are more critical in TE, while stronger phrase alignments deserve more attention in AS. The key to reach this observation lies in phrase detection, phrase representation, phrase alignment, and more importantly how to connect those aligned phrases of different matching degrees with the final <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a>. Prior work (i) has limitations in phrase generation and representation, or (ii) conducts <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> at word and phrase levels by handcrafted features or (iii) utilizes a single framework of <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> without considering the characteristics of specific tasks, which limits the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>&#8217;s effectiveness across tasks. We propose an architecture based on Gated Recurrent Unit that supports (i) representation learning of phrases of arbitrary granularity and (ii) task-specific attentive pooling of phrase alignments between two sentences. Experimental results on TE and AS match our observation and show the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1067" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1067/>On-demand Injection of Lexical Knowledge for Recognising Textual Entailment</a></strong><br><a href=/people/p/pascual-martinez-gomez/>Pascual Martínez-Gómez</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a>
|
<a href=/people/d/daisuke-bekki/>Daisuke Bekki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1067><div class="card-body p-3 small">We approach the recognition of textual entailment using logical semantic representations and a <a href=https://en.wikipedia.org/wiki/Automated_theorem_proving>theorem prover</a>. In this setup, lexical divergences that preserve semantic entailment between the source and target texts need to be explicitly stated. However, recognising subsentential semantic relations is not trivial. We address this problem by monitoring the proof of the theorem and detecting unprovable sub-goals that share <a href=https://en.wikipedia.org/wiki/Predicate_(mathematical_logic)>predicate arguments</a> with <a href=https://en.wikipedia.org/wiki/Premise>logical premises</a>. If a <a href=https://en.wikipedia.org/wiki/Binary_relation>linguistic relation</a> exists, then an appropriate <a href=https://en.wikipedia.org/wiki/Axiom>axiom</a> is constructed on-demand and the theorem proving continues. Experiments show that this approach is effective and precise, producing a <a href=https://en.wikipedia.org/wiki/System>system</a> that outperforms other <a href=https://en.wikipedia.org/wiki/Logic_programming>logic-based systems</a> and is competitive with state-of-the-art <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1068/>Learning to Predict Denotational Probabilities For Modeling Entailment</a></strong><br><a href=/people/a/alice-lai/>Alice Lai</a>
|
<a href=/people/j/julia-hockenmaier/>Julia Hockenmaier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1068><div class="card-body p-3 small">We propose a framework that captures the denotational probabilities of words and phrases by embedding them in a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, and present a method to induce such an <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> from a dataset of denotational probabilities. We show that our model successfully predicts denotational probabilities for unseen phrases, and that its predictions are useful for textual entailment datasets such as SICK and SNLI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1070 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1070/>Argument Strength is in the Eye of the Beholder : Audience Effects in <a href=https://en.wikipedia.org/wiki/Persuasion>Persuasion</a></a></strong><br><a href=/people/s/stephanie-lukin/>Stephanie Lukin</a>
|
<a href=/people/p/pranav-anand/>Pranav Anand</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a>
|
<a href=/people/s/steve-whittaker/>Steve Whittaker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1070><div class="card-body p-3 small">Americans spend about a third of their time online, with many participating in online conversations on social and political issues. We hypothesize that social media arguments on such issues may be more engaging and persuasive than traditional media summaries, and that particular types of people may be more or less convinced by particular styles of argument, e.g. emotional arguments may resonate with some personalities while factual arguments resonate with others. We report a set of experiments testing at large scale how audience variables interact with argument style to affect the persuasiveness of an argument, an under-researched topic within <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. We show that belief change is affected by <a href=https://en.wikipedia.org/wiki/Personality_psychology>personality factors</a>, with conscientious, open and agreeable people being more convinced by emotional arguments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1071/>A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts</a></strong><br><a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/j/julien-perez/>Julien Perez</a>
|
<a href=/people/s/scott-nowson/>Scott Nowson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1071><div class="card-body p-3 small">There have been many attempts at automatically recognising author personality traits from text, typically incorporating <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> with conventional <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>, e.g. linear regression or <a href=https://en.wikipedia.org/wiki/Support_vector_machine>Support Vector Machines</a>. In this work, we propose to use deep-learning-based models with atomic features of text the characters to build hierarchical, vectorial word and sentence representations for the task of trait inference. On a corpus of tweets, this method shows state-of-the-art performance across five traits and three languages (English, Spanish and Italian) compared with prior work in <a href=https://en.wikipedia.org/wiki/Author_profiling>author profiling</a>. The results, supported by preliminary visualisation work, are encouraging for the ability to detect <a href=https://en.wikipedia.org/wiki/Complex_traits>complex human traits</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1072/>A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments</a></strong><br><a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1072><div class="card-body p-3 small">While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> remain vague. We observe that whether or not an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> uses a particular feature set (sentence IDs) accounts for a significant performance gap among these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1073/>Online Learning of Task-specific Word Representations with a Joint Biconvex Passive-Aggressive Algorithm</a></strong><br><a href=/people/p/pascal-denis/>Pascal Denis</a>
|
<a href=/people/l/liva-ralaivola/>Liva Ralaivola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1073><div class="card-body p-3 small">This paper presents a new, efficient method for learning task-specific word vectors using a variant of the Passive-Aggressive algorithm. Specifically, this <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> learns a word embedding matrix in tandem with the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier parameters</a> in an online fashion, solving a <a href=https://en.wikipedia.org/wiki/Convex_optimization>bi-convex constrained optimization</a> at each iteration. We provide a theoretical analysis of this new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> in terms of regret bounds, and evaluate it on both synthetic data and NLP classification problems, including text classification and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. In the latter case, we compare various pre-trained word vectors to initialize our word embedding matrix, and show that the <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a> learned by our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> vastly outperforms the initial <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a>, with performance results comparable or above the state-of-the-art on these tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1074/>Nonsymbolic Text Representation</a></strong><br><a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1074><div class="card-body p-3 small">We introduce the first generic text representation model that is completely nonsymbolic, i.e., it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text. This applies to training the parameters of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on a training corpus as well as to applying it when computing the representation of a new text. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better than prior work on an <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and a text denoising task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1076/>Event extraction from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> using Non-Parametric Bayesian Mixture Model with Word Embeddings<span class=acl-fixed-case>T</span>witter using Non-Parametric <span class=acl-fixed-case>B</span>ayesian Mixture Model with Word Embeddings</a></strong><br><a href=/people/d/deyu-zhou/>Deyu Zhou</a>
|
<a href=/people/x/xuan-zhang/>Xuan Zhang</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1076><div class="card-body p-3 small">To extract structured representations of newsworthy events from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised models</a> typically assume that tweets involving the same named entities and expressed using similar words are likely to belong to the same event. Hence, they group tweets into clusters based on the <a href=https://en.wikipedia.org/wiki/Co-occurrence>co-occurrence patterns</a> of <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> and <a href=https://en.wikipedia.org/wiki/Index_term>topical keywords</a>. However, there are two main limitations. First, they require the number of events to be known beforehand, which is not realistic in practical applications. Second, they do n&#8217;t recognise that the same named entity might be referred to by multiple mentions and tweets using different mentions would be wrongly assigned to different events. To overcome these limitations, we propose a non-parametric Bayesian mixture model with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event extraction</a>, in which the number of events can be inferred automatically and the issue of lexical variations for the same named entity can be dealt with properly. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has been evaluated on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with sizes ranging between 2,499 and over 60 million tweets. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the baseline approach on all <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> by 5-8 % in <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1077/>End-to-end Relation Extraction using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> and <a href=https://en.wikipedia.org/wiki/Markov_logic_network>Markov Logic Networks</a><span class=acl-fixed-case>M</span>arkov <span class=acl-fixed-case>L</span>ogic <span class=acl-fixed-case>N</span>etworks</a></strong><br><a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/g/girish-palshikar/>Girish Palshikar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1077><div class="card-body p-3 small">End-to-end relation extraction refers to identifying boundaries of entity mentions, entity types of these mentions and appropriate <a href=https://en.wikipedia.org/wiki/Semantic_relation>semantic relation</a> for each pair of mentions. Traditionally, separate <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a> were trained for each of these tasks and were used in a pipeline fashion where output of one <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is fed as input to another. But it was observed that addressing some of these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> jointly results in better performance. We propose a single, joint neural network based model to carry out all the three tasks of boundary identification, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity type classification</a> and relation type classification. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is referred to as All Word Pairs model (AWP-NN) as it assigns an appropriate label to each word pair in a given sentence for performing end-to-end relation extraction. We also propose to refine output of the AWP-NN model by using inference in Markov Logic Networks (MLN) so that additional <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> can be effectively incorporated. We demonstrate effectiveness of our approach by achieving better end-to-end relation extraction performance than all 4 previous joint modelling approaches, on the standard dataset of ACE 2004.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1078 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1078/>Trust, but Verify ! Better Entity Linking through Automatic Verification</a></strong><br><a href=/people/b/benjamin-heinzerling/>Benjamin Heinzerling</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1078><div class="card-body p-3 small">We introduce automatic verification as a post-processing step for entity linking (EL). The proposed method trusts EL system results collectively, by assuming entity mentions are mostly linked correctly, in order to create a semantic profile of the given text using geospatial and temporal information, as well as fine-grained entity types. This profile is then used to automatically verify each linked mention individually, i.e., to predict whether it has been linked correctly or not. Verification allows leveraging a rich set of global and pairwise features that would be prohibitively expensive for EL systems employing global inference. Evaluation shows consistent improvements across datasets and systems. In particular, when applied to state-of-the-art systems, our method yields an absolute improvement in <a href=https://en.wikipedia.org/wiki/Linker_(computing)>linking</a> performance of up to 1.7 F1 on AIDA / CoNLL&#8217;03 and up to 2.4 F1 on the English TAC KBP 2015 TEDL dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1079/>Named Entity Recognition in the Medical Domain with Constrained CRF Models<span class=acl-fixed-case>CRF</span> Models</a></strong><br><a href=/people/c/charles-jochim/>Charles Jochim</a>
|
<a href=/people/l/lea-deleris/>Léa Deleris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1079><div class="card-body p-3 small">This paper investigates how to improve performance on information extraction tasks by constraining and sequencing CRF-based approaches. We consider two different relation extraction tasks, both from the <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> : <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>dependence relations</a> and <a href=https://en.wikipedia.org/wiki/Probability>probability statements</a>. We explore whether adding <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> can lead to an improvement over standard CRF decoding. Results on our relation extraction tasks are promising, showing significant increases in performance from both (i) adding <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> to post-process the output of a baseline CRF, which captures <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>, and (ii) further allowing flexibility in the application of those <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> by leveraging a binary classifier as a pre-processing step.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1080 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1080/>Learning and Knowledge Transfer with Memory Networks for Machine Comprehension</a></strong><br><a href=/people/m/mohit-yadav/>Mohit Yadav</a>
|
<a href=/people/l/lovekesh-vig/>Lovekesh Vig</a>
|
<a href=/people/g/gautam-shroff/>Gautam Shroff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1080><div class="card-body p-3 small">Enabling machines to read and comprehend <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a> remains an unfulfilled goal for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP research</a>. Recent research efforts on the machine comprehension task have managed to achieve close to ideal performance on <a href=https://en.wikipedia.org/wiki/Simulation>simulated data</a>. However, achieving similar levels of performance on small real world datasets has proved difficult ; major challenges stem from the large vocabulary size, complex grammar, and, the frequent ambiguities in linguistic structure. On the other hand, the requirement of human generated annotations for training, in order to ensure a sufficiently diverse set of questions is prohibitively expensive. Motivated by these practical issues, we propose a novel curriculum inspired training procedure for Memory Networks to improve the performance for machine comprehension with relatively small volumes of training data. Additionally, we explore various <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training regimes</a> for Memory Networks to allow <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> from a closely related domain having larger volumes of <a href=https://en.wikipedia.org/wiki/Data>labelled data</a>. We also suggest the use of a <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> to incorporate the asymmetric nature of <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a>. Our experiments demonstrate improvements on <a href=https://en.wikipedia.org/wiki/Dailymail>Dailymail</a>, <a href=https://en.wikipedia.org/wiki/CNN>CNN</a>, and MCTest datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1081/>If No Media Were Allowed inside the Venue, Was Anybody Allowed?</a></strong><br><a href=/people/z/zahra-sarabi/>Zahra Sarabi</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1081><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> to understand <a href=https://en.wikipedia.org/wiki/Negation>negation</a> in positive terms. Specifically, we extract <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>positive meaning</a> from <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> when the <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation cue</a> syntactically modifies a noun or adjective. Our approach is grounded on generating potential positive interpretations automatically, and then scoring them. Experimental results show that interpretations scored high can be reliably identified.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1082 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1082/>Metaheuristic Approaches to Lexical Substitution and Simplification</a></strong><br><a href=/people/s/sallam-abualhaija/>Sallam Abualhaija</a>
|
<a href=/people/t/tristan-miller/>Tristan Miller</a>
|
<a href=/people/j/judith-eckle-kohler/>Judith Eckle-Kohler</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/k/karl-heinz-zimmermann/>Karl-Heinz Zimmermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1082><div class="card-body p-3 small">In this paper, we propose using metaheuristicsin particular, simulated annealing and the new D-Bees algorithmto solve <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> as an optimization problem within a knowledge-based lexical substitution system. We are the first to perform such an extrinsic evaluation of metaheuristics, for which we use two standard lexical substitution datasets, one <a href=https://en.wikipedia.org/wiki/English_language>English</a> and one <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We find that D-Bees has robust performance for both languages, and performs better than <a href=https://en.wikipedia.org/wiki/Simulated_annealing>simulated annealing</a>, though both achieve good results. Moreover, the D-Beesbased lexical substitution system outperforms state-of-the-art systems on several evaluation metrics. We also show that D-Bees achieves competitive performance in <a href=https://en.wikipedia.org/wiki/Lexical_simplification>lexical simplification</a>, a variant of <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1083/>Paraphrasing Revisited with <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/j/jonathan-mallinson/>Jonathan Mallinson</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1083><div class="card-body p-3 small">Recognizing and generating paraphrases is an important component in many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>. A well-established technique for automatically extracting paraphrases leverages bilingual corpora to find meaning-equivalent phrases in a single language by pivoting over a shared translation in another language. In this paper we revisit bilingual pivoting in the context of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and present a paraphrasing model based purely on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Our model represents <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> in a continuous space, estimates the degree of semantic relatedness between text segments of arbitrary length, and generates candidate <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> for any source input. Experimental results across tasks and datasets show that neural paraphrases outperform those obtained with conventional phrase-based pivoting approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1084/>Multilingual Training of Crosslingual Word Embeddings</a></strong><br><a href=/people/l/long-duong/>Long Duong</a>
|
<a href=/people/h/hiroshi-kanayama/>Hiroshi Kanayama</a>
|
<a href=/people/t/tengfei-ma/>Tengfei Ma</a>
|
<a href=/people/s/steven-bird/>Steven Bird</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1084><div class="card-body p-3 small">Crosslingual word embeddings represent lexical items from different languages using the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, enabling crosslingual transfer. Most prior work constructs <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for a pair of languages, with English on one side. We investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space. In this way, we can exploit and combine strength of many languages. We obtained high performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1085/>Building Lexical Vector Representations from Concept Definitions</a></strong><br><a href=/people/d/danilo-silva-de-carvalho/>Danilo Silva de Carvalho</a>
|
<a href=/people/m/minh-le-nguyen/>Minh Le Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1085><div class="card-body p-3 small">The use of distributional language representations have opened new paths in solving a variety of NLP problems. However, alternative approaches can take advantage of information unavailable through pure <a href=https://en.wikipedia.org/wiki/Statistics>statistical means</a>. This paper presents a method for building vector representations from meaning unit blocks called concept definitions, which are obtained by extracting information from a curated linguistic resource (Wiktionary). The <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> obtained in this way can be compared through conventional cosine similarity and are also interpretable by humans. Evaluation was conducted in semantic similarity and relatedness test sets, with results indicating a performance comparable to other methods based on single linguistic resource extraction. The results also indicate noticeable performance gains when combining distributional similarity scores with the ones obtained using this approach. Additionally, a discussion on the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>&#8217;s shortcomings is provided in the analysis of error cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1086/>ShotgunWSD : An unsupervised algorithm for global word sense disambiguation inspired by <a href=https://en.wikipedia.org/wiki/DNA_sequencing>DNA sequencing</a><span class=acl-fixed-case>S</span>hotgun<span class=acl-fixed-case>WSD</span>: An unsupervised algorithm for global word sense disambiguation inspired by <span class=acl-fixed-case>DNA</span> sequencing</a></strong><br><a href=/people/a/andrei-butnaru/>Andrei Butnaru</a>
|
<a href=/people/r/radu-tudor-ionescu/>Radu Tudor Ionescu</a>
|
<a href=/people/f/florentina-hristea/>Florentina Hristea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1086><div class="card-body p-3 small">In this paper, we present a novel <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised algorithm</a> for word sense disambiguation (WSD) at the document level. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is inspired by a widely-used approach in the field of <a href=https://en.wikipedia.org/wiki/Genetics>genetics</a> for <a href=https://en.wikipedia.org/wiki/Whole_genome_sequencing>whole genome sequencing</a>, known as the Shotgun sequencing technique. The proposed WSD algorithm is based on three main steps. First, a brute-force WSD algorithm is applied to short context windows (up to 10 words) selected from the document in order to generate a short list of likely sense configurations for each window. In the second step, these local sense configurations are assembled into longer composite configurations based on suffix and prefix matching. The resulted configurations are ranked by their length, and the sense of each word is chosen based on a <a href=https://en.wikipedia.org/wiki/Electoral_system>voting scheme</a> that considers only the top k configurations in which the word appears. We compare our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> with other state-of-the-art unsupervised WSD algorithms and demonstrate better performance, sometimes by a very large margin. We also show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can yield better performance than the Most Common Sense (MCS) baseline on one data set. Moreover, our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> has a very small number of parameters, is robust to parameter tuning, and, unlike other bio-inspired methods, it gives a deterministic solution (it does not involve random choices).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1087" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1087/>LanideNN : Multilingual Language Identification on Character Window<span class=acl-fixed-case>L</span>anide<span class=acl-fixed-case>NN</span>: Multilingual Language Identification on Character Window</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1087><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a>, a common first step in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, we want to automatically determine the language of some input text. Monolingual language identification assumes that the given document is written in one language. In multilingual language identification, the document is usually in two or three languages and we just want their names. We aim one step further and propose a method for textual language identification where languages can change arbitrarily and the goal is to identify the spans of each of the languages. Our method is based on Bidirectional Recurrent Neural Networks and it performs well in monolingual and multilingual language identification tasks on six datasets covering 131 languages. The method keeps the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> also for short documents and across domains, so it is ideal for off-the-shelf use without preparation of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1088/>Cross-Lingual Word Embeddings for Low-Resource Language Modeling</a></strong><br><a href=/people/o/oliver-adams/>Oliver Adams</a>
|
<a href=/people/a/adam-makarucha/>Adam Makarucha</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/s/steven-bird/>Steven Bird</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1088><div class="card-body p-3 small">Most languages have no established <a href=https://en.wikipedia.org/wiki/Writing_system>writing system</a> and minimal written records. However, <a href=https://en.wikipedia.org/wiki/Textual_data>textual data</a> is essential for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, and particularly important for training <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to support <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>. Even in cases where text data is missing, there are some languages for which bilingual lexicons are available, since creating lexicons is a fundamental task of documentary linguistics. We investigate the use of such <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> to improve <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> when textual training data is limited to as few as a thousand sentences. The method involves learning cross-lingual word embeddings as a preliminary step in training monolingual language models. Results across a number of languages show that <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are improved by this pre-training. Application to <a href=https://en.wikipedia.org/wiki/Yongning_Na>Yongning Na</a>, a threatened language, highlights challenges in deploying the approach in real low-resource environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1090 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1090/>Psycholinguistic Models of Sentence Processing Improve Sentence Readability Ranking</a></strong><br><a href=/people/d/david-m-howcroft/>David M. Howcroft</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1090><div class="card-body p-3 small">While previous research on <a href=https://en.wikipedia.org/wiki/Readability>readability</a> has typically focused on document-level measures, recent work in areas such as <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> has pointed out the need of sentence-level readability measures. Much of <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> has focused for many years on processing measures that provide difficulty estimates on a word-by-word basis. However, these psycholinguistic measures have not yet been tested on sentence readability ranking tasks. In this paper, we use four psycholinguistic measures : idea density, surprisal, integration cost, and embedding depth to test whether these features are predictive of readability levels. We find that psycholinguistic features significantly improve performance by up to 3 percentage points over a standard document-level readability metric baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1091 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1091/>Web-Scale Language-Independent Cataloging of Noisy Product Listings for E-Commerce<span class=acl-fixed-case>E</span>-Commerce</a></strong><br><a href=/people/p/pradipto-das/>Pradipto Das</a>
|
<a href=/people/y/yandi-xia/>Yandi Xia</a>
|
<a href=/people/a/aaron-levine/>Aaron Levine</a>
|
<a href=/people/g/giuseppe-di-fabbrizio/>Giuseppe Di Fabbrizio</a>
|
<a href=/people/a/ankur-datta/>Ankur Datta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1091><div class="card-body p-3 small">The cataloging of product listings through <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy categorization</a> is a fundamental problem for any <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce marketplace</a>, with applications ranging from personalized search recommendations to <a href=https://en.wikipedia.org/wiki/Query_understanding>query understanding</a>. However, manual and rule based approaches to <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> are not scalable. In this paper, we compare several <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> for categorizing listings in both English and Japanese product catalogs. We show empirically that a combination of words from product titles, navigational breadcrumbs, and <a href=https://en.wikipedia.org/wiki/List_price>list prices</a>, when available, improves results significantly. We outline a novel method using correspondence topic models and a lightweight manual process to reduce <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> from mis-labeled data in the training set. We contrast linear models, gradient boosted trees (GBTs) and convolutional neural networks (CNNs), and show that GBTs and CNNs yield the highest gains in error reduction. Finally, we show GBTs applied in a language-agnostic way on a large-scale Japanese e-commerce dataset have improved <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy categorization</a> performance over current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> based on deep belief network models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1092 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1092/>Recognizing Insufficiently Supported Arguments in Argumentative Essays</a></strong><br><a href=/people/c/christian-stab/>Christian Stab</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1092><div class="card-body p-3 small">In this paper, we propose a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> for assessing the quality of <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>natural language arguments</a>. The premises of a well-reasoned argument should provide enough evidence for accepting or rejecting its claim. Although this criterion, known as <a href=https://en.wikipedia.org/wiki/Necessity_and_sufficiency>sufficiency</a>, is widely adopted in <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation theory</a>, there are no empirical studies on its applicability to real arguments. In this work, we show that human annotators substantially agree on the sufficiency criterion and introduce a novel annotated corpus. Furthermore, we experiment with feature-rich SVMs and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> and achieve 84 % accuracy for automatically identifying insufficiently supported arguments. The final <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> as well as the annotation guideline are freely available for encouraging future research on argument quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1093 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1093/>Distributed Document and Phrase Co-embeddings for Descriptive Clustering</a></strong><br><a href=/people/m/motoki-sato/>Motoki Sato</a>
|
<a href=/people/a/austin-j-brockmeier/>Austin J. Brockmeier</a>
|
<a href=/people/g/georgios-kontonatsios/>Georgios Kontonatsios</a>
|
<a href=/people/t/tingting-mu/>Tingting Mu</a>
|
<a href=/people/j/john-y-goulermas/>John Y. Goulermas</a>
|
<a href=/people/j/junichi-tsujii/>Jun’ichi Tsujii</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1093><div class="card-body p-3 small">Descriptive document clustering aims to automatically discover groups of semantically related documents and to assign a meaningful label to characterise the content of each cluster. In this paper, we present a descriptive clustering approach that employs a distributed representation model, namely the paragraph vector model, to capture semantic similarities between documents and phrases. The proposed method uses a joint representation of phrases and documents (i.e., a co-embedding) to automatically select a <a href=https://en.wikipedia.org/wiki/Linguistic_description>descriptive phrase</a> that best represents each document cluster. We evaluate our method by comparing its performance to an existing state-of-the-art descriptive clustering method that also uses co-embedding but relies on a bag-of-words representation. Results obtained on benchmark datasets demonstrate that the paragraph vector-based method obtains superior performance over the existing approach in both identifying clusters and assigning appropriate descriptive labels to them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1094 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1094/>SMARTies : Sentiment Models for Arabic Target entities<span class=acl-fixed-case>SMART</span>ies: Sentiment Models for <span class=acl-fixed-case>A</span>rabic Target entities</a></strong><br><a href=/people/n/noura-farra/>Noura Farra</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1094><div class="card-body p-3 small">We consider entity-level sentiment analysis in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, a morphologically rich language with increasing resources. We present a <a href=https://en.wikipedia.org/wiki/System>system</a> that is applied to complex posts written in response to Arabic newspaper articles. Our goal is to identify important entity targets within the post along with the polarity expressed about each target. We achieve significant improvements over multiple baselines, demonstrating that the use of specific morphological representations improves the performance of identifying both important targets and their sentiment, and that the use of distributional semantic clusters further boosts performances for these representations, especially when richer linguistic resources are not available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1095 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1095/>Exploring <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> of Spanish tweets<span class=acl-fixed-case>S</span>panish tweets</a></strong><br><a href=/people/i/isabel-segura-bedmar/>Isabel Segura-Bedmar</a>
|
<a href=/people/a/antonio-quiros/>Antonio Quirós</a>
|
<a href=/people/p/paloma-martinez/>Paloma Martínez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1095><div class="card-body p-3 small">Spanish is the third-most used language on the internet, after <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, with a total of 7.7 % (more than 277 million of users) and a huge internet growth of more than 1,400 %. However, most work on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> has been focused on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. This paper describes a deep learning system for Spanish sentiment analysis. To the best of our knowledge, this is the first work that explores the use of a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> to polarity classification of Spanish tweets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1096/>Contextual Bidirectional Long Short-Term Memory Recurrent Neural Network Language Models : A Generative Approach to Sentiment Analysis</a></strong><br><a href=/people/a/amr-mousa/>Amr Mousa</a>
|
<a href=/people/b/bjorn-schuller/>Björn Schuller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1096><div class="card-body p-3 small">Traditional learning-based approaches to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of written text use the concept of bag-of-words or bag-of-n-grams, where a document is viewed as a set of terms or short combinations of terms disregarding grammar rules or <a href=https://en.wikipedia.org/wiki/Word_order>word order</a>. Novel approaches de-emphasize this concept and view the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a sequence classification problem. In this context, <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNNs)</a> have achieved significant success. The idea is to use RNNs as discriminative binary classifiers to predict a positive or negative sentiment label at every word position then perform a type of pooling to get a sentence-level polarity. Here, we investigate a novel generative approach in which a separate <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> is estimated for every sentiment using language models (LMs) based on long short-term memory (LSTM) RNNs. We introduce a novel type of LM using a modified version of bidirectional LSTM (BLSTM) called contextual BLSTM (cBLSTM), where the probability of a word is estimated based on its full left and right contexts. Our approach is compared with a BLSTM binary classifier. Significant improvements are observed in classifying the IMDB movie review dataset. Further improvements are achieved via model combination.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1097/>Large-scale Opinion Relation Extraction with Distantly Supervised Neural Network</a></strong><br><a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/s/shiliang-sun/>Shiliang Sun</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1097><div class="card-body p-3 small">We investigate the task of open domain opinion relation extraction. Different from works on manually labeled corpus, we propose an efficient distantly supervised framework based on <a href=https://en.wikipedia.org/wiki/Pattern_matching>pattern matching</a> and neural network classifiers. The <a href=https://en.wikipedia.org/wiki/Pattern_recognition>patterns</a> are designed to automatically generate training data, and the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a> is design to capture various lexical and syntactic features. The result <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is fast and scalable on large-scale corpus. We test the <a href=https://en.wikipedia.org/wiki/System>system</a> on the <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon online review dataset</a>. The result shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to achieve promising performances without any human annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1098/>Decoding with <a href=https://en.wikipedia.org/wiki/Finite-state_transducer>Finite-State Transducers</a> on GPUs<span class=acl-fixed-case>GPU</span>s</a></strong><br><a href=/people/a/arturo-argueta/>Arturo Argueta</a>
|
<a href=/people/d/david-chiang/>David Chiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1098><div class="card-body p-3 small">Weighted finite automata and transducers (including hidden Markov models and conditional random fields) are widely used in natural language processing (NLP) to perform tasks such as morphological analysis, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, chunking, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, and others. Parallelizing finite state algorithms on graphics processing units (GPUs) would benefit many areas of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Although researchers have implemented GPU versions of basic graph algorithms, no work, to our knowledge, has been done on GPU algorithms for weighted finite automata. We introduce a GPU implementation of the Viterbi and forward-backward algorithm, achieving speedups of up to 4x over our serial implementations running on different computer architectures and 3335x over widely used tools such as OpenFST.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1100 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1100" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1100/>A Multifaceted Evaluation of Neural versus Phrase-Based Machine Translation for 9 Language Directions</a></strong><br><a href=/people/a/antonio-toral/>Antonio Toral</a>
|
<a href=/people/v/victor-m-sanchez-cartagena/>Víctor M. Sánchez-Cartagena</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1100><div class="card-body p-3 small">We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a> and performance across different error categories. We find out that translations produced by neural machine translation systems are considerably different, more fluent and more accurate in terms of <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> compared to those produced by phrase-based systems. Neural machine translation systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/E17-1101.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/E17-1101/>Personalized Machine Translation : Preserving Original Author Traits</a></strong><br><a href=/people/e/ella-rabinovich/>Ella Rabinovich</a>
|
<a href=/people/r/raj-nath-patel/>Raj Nath Patel</a>
|
<a href=/people/s/shachar-mirkin/>Shachar Mirkin</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/s/shuly-wintner/>Shuly Wintner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1101><div class="card-body p-3 small">The language that we produce reflects our personality, and various personal and demographic characteristics can be detected in natural language texts. We focus on one particular <a href=https://en.wikipedia.org/wiki/Trait_theory>personal trait</a> of the author, <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, and study how it is manifested in original texts and in <a href=https://en.wikipedia.org/wiki/Translation>translations</a>. We show that author&#8217;s gender has a powerful, clear signal in originals texts, but this <a href=https://en.wikipedia.org/wiki/Signal_(IPC)>signal</a> is obfuscated in <a href=https://en.wikipedia.org/wiki/Translation>human and machine translation</a>. We then propose simple domain-adaptation techniques that help retain the original gender traits in the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a>, without harming the quality of the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a>, thereby creating more personalized machine translation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1103/>Grouping business news stories based on salience of named entities</a></strong><br><a href=/people/l/llorenc-escoter/>Llorenç Escoter</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/m/mian-du/>Mian Du</a>
|
<a href=/people/a/anisia-katinskaia/>Anisia Katinskaia</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1103><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/News_aggregator>news aggregation systems</a> focused on broad news domains, certain stories may appear in multiple articles. Depending on the relative importance of the story, the number of versions can reach dozens or hundreds within a day. The text in these versions may be nearly identical or quite different. Linking multiple versions of a story into a single group brings several important benefits to the end-userreducing the cognitive load on the reader, as well as signaling the relative importance of the story. We present a grouping algorithm, and explore several vector-based representations of input documents : from a baseline using keywords, to a method using saliencea measure of importance of named entities in the text. We demonstrate that <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> beyond <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> yield substantial improvements, verified on a manually-annotated corpus of business news stories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1104" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1104/>Very Deep Convolutional Networks for Text Classification</a></strong><br><a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/h/holger-schwenk/>Holger Schwenk</a>
|
<a href=/people/l/loic-barrault/>Loïc Barrault</a>
|
<a href=/people/y/yann-lecun/>Yann Lecun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1104><div class="card-body p-3 small">The dominant approach for many NLP tasks are <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>, in particular LSTMs, and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. However, these <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth : using up to 29 convolutional layers, we report improvements over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1105 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1105/>PageRank for Argument Relevance<span class=acl-fixed-case>P</span>age<span class=acl-fixed-case>R</span>ank” for Argument Relevance</a></strong><br><a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a>
|
<a href=/people/y/yamen-ajjour/>Yamen Ajjour</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1105><div class="card-body p-3 small">Future <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> are expected to deliver pro and con arguments in response to queries on controversial topics. While <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> is now in the focus of research, the question of how to retrieve the relevant arguments remains open. This paper proposes a radical model to assess <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> objectively at web scale : the <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> of an argument&#8217;s conclusion is decided by what other arguments reuse it as a premise. We build an argument graph for this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that we analyze with a recursive weighting scheme, adapting key ideas of <a href=https://en.wikipedia.org/wiki/PageRank>PageRank</a>. In experiments on a large ground-truth argument graph, the resulting relevance scores correlate with human average judgments. We outline what natural language challenges must be faced at <a href=https://en.wikipedia.org/wiki/Scalability>web scale</a> in order to stepwise bring argument relevance to <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search engines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1106/>Predicting Counselor Behaviors in Motivational Interviewing Encounters</a></strong><br><a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/k/kenneth-resnicow/>Kenneth Resnicow</a>
|
<a href=/people/s/satinder-singh/>Satinder Singh</a>
|
<a href=/people/l/lawrence-an/>Lawrence An</a>
|
<a href=/people/k/kathy-j-goggin/>Kathy J. Goggin</a>
|
<a href=/people/d/delwyn-catley/>Delwyn Catley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1106><div class="card-body p-3 small">As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors&#8217; language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters ; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors ; and third, we develop an automatic system to predict these behaviors. We introduce a new set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> based on <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntactic patterns</a>, and show that they lead to accuracy figures of up to 90 %, which represent a significant improvement with respect to <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> used in the past.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1107/>Authorship Attribution Using Text Distortion</a></strong><br><a href=/people/e/efstathios-stamatatos/>Efstathios Stamatatos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1107><div class="card-body p-3 small">Authorship attribution is associated with important applications in <a href=https://en.wikipedia.org/wiki/Forensic_science>forensics</a> and humanities research. A crucial point in this field is to quantify the personal style of writing, ideally in a way that is not affected by changes in topic or genre. In this paper, we present a novel method that enhances <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship attribution</a> effectiveness by introducing a text distortion step before extracting <a href=https://en.wikipedia.org/wiki/Stylometry>stylometric measures</a>. The proposed method attempts to mask topic-specific information that is not related to the personal style of authors. Based on experiments on two main tasks in <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship attribution</a>, <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>closed-set attribution</a> and <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship verification</a>, we demonstrate that the proposed approach can enhance existing methods especially under cross-topic conditions, where the training and test corpora do not match in topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1108/>Structured Learning for Temporal Relation Extraction from Clinical Records</a></strong><br><a href=/people/a/artuur-leeuwenberg/>Artuur Leeuwenberg</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1108><div class="card-body p-3 small">We propose a scalable structured learning model that jointly predicts temporal relations between events and temporal expressions (TLINKS), and the relation between these <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a> and the document creation time (DCTR). We employ a structured perceptron, together with integer linear programming constraints for document-level inference during training and prediction to exploit relational properties of temporality, together with global learning of the relations at the document level. Moreover, this study gives insights in the results of integrating constraints for temporal relation extraction when using structured learning and prediction. Our best system outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the art</a> on both the CONTAINS TLINK task, and the DCTR task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1109/>Entity Extraction in Biomedical Corpora : An Approach to Evaluate Word Embedding Features with PSO based Feature Selection<span class=acl-fixed-case>PSO</span> based Feature Selection</a></strong><br><a href=/people/s/shweta-yadav/>Shweta Yadav</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1109><div class="card-body p-3 small">Text mining has drawn significant attention in recent past due to the rapid growth in biomedical and clinical records. Entity extraction is one of the fundamental components for <a href=https://en.wikipedia.org/wiki/Biomedical_text_mining>biomedical text mining</a>. In this paper, we propose a novel approach of <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection</a> for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity extraction</a> that exploits the concept of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and Particle Swarm Optimization (PSO). The system utilizes word embedding features along with several other <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted by studying the properties of the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We obtain an interesting observation that compact word embedding features as determined by PSO are more effective compared to the entire word embedding feature set for entity extraction. The proposed system is evaluated on three benchmark biomedical datasets such as GENIA, GENETAG, and AiMed. The effectiveness of the proposed approach is evident with significant performance gains over the baseline models as well as the other existing systems. We observe improvements of 7.86 %, 5.27 % and 7.25 % <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> points over the baseline models for GENIA, GENETAG, and AiMed dataset respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1110/>Distant Supervision for Relation Extraction beyond the Sentence Boundary</a></strong><br><a href=/people/c/chris-quirk/>Chris Quirk</a>
|
<a href=/people/h/hoifung-poon/>Hoifung Poon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1110><div class="card-body p-3 small">The growing demand for structured knowledge has led to great interest in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, especially in cases with limited supervision. However, existing distance supervision approaches only extract <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross-sentence relation extraction. At the core of our approach is a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph representation</a> that can incorporate both standard dependencies and <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>, thus providing a unifying way to model <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> within and across sentences. We extract features from multiple paths in this <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph</a>, increasing <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and robustness when confronted with <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variation</a> and analysis error. Experiments on an important extraction task for <a href=https://en.wikipedia.org/wiki/Precision_medicine>precision medicine</a> show that our approach can learn an accurate cross-sentence extractor, using only a small existing <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1111/>Noise Mitigation for Neural Entity Typing and <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1111><div class="card-body p-3 small">In this paper, we address two different types of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in information extraction models : noise from distant supervision and noise from pipeline input features. Our target tasks are <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity typing</a> and <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation extraction</a>. For the first noise type, we introduce multi-instance multi-label learning algorithms using neural network models, and apply them to fine-grained entity typing for the first time. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art supervised approach which uses global embeddings of entities. For the second noise type, we propose ways to improve the integration of noisy entity type predictions into <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Our experiments show that probabilistic predictions are more robust than discrete predictions and that joint training of the two tasks performs best.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1113/>Using support vector machines and state-of-the-art <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic alignment</a> to identify cognates in multi-lingual wordlists</a></strong><br><a href=/people/g/gerhard-jager/>Gerhard Jäger</a>
|
<a href=/people/j/johann-mattis-list/>Johann-Mattis List</a>
|
<a href=/people/p/pavel-sofroniev/>Pavel Sofroniev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1113><div class="card-body p-3 small">Most current approaches in phylogenetic linguistics require as input multilingual word lists partitioned into sets of etymologically related words (cognates). Cognate identification is so far done manually by experts, which is time consuming and as of yet only available for a small number of well-studied language families. Automatizing this step will greatly expand the empirical scope of phylogenetic methods in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>, as raw wordlists (in phonetic transcription) are much easier to obtain than wordlists in which cognate words have been fully identified and annotated, even for under-studied languages. A couple of different <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> have been proposed in the past, but they are either disappointing regarding their performance or not applicable to larger datasets. Here we present a new approach that uses support vector machines to unify different state-of-the-art methods for <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic alignment</a> and cognate detection within a single framework. Training and evaluating these method on a typologically broad collection of gold-standard data shows it to be superior to the existing state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1114 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1114/>A Multi-task Approach to Predict Likability of Books</a></strong><br><a href=/people/s/suraj-maharjan/>Suraj Maharjan</a>
|
<a href=/people/j/john-arevalo/>John Arevalo</a>
|
<a href=/people/m/manuel-montes/>Manuel Montes</a>
|
<a href=/people/f/fabio-a-gonzalez/>Fabio A. González</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1114><div class="card-body p-3 small">We investigate the value of <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> and <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a> for predicting successful writing. Similar to previous work, we treat this as a binary classification task and explore new strategies to automatically learn representations from book contents. We evaluate our <a href=https://en.wikipedia.org/wiki/Software_feature>feature set</a> on two different <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> created from Project Gutenberg books. The first presents a novel approach for generating the gold standard labels for the task and the <a href=https://en.wikipedia.org/wiki/Other_(philosophy)>other</a> is based on prior research. Using a combination of hand-crafted and recurrent neural network learned representations in a dual learning setting, we obtain the best performance of 73.50 % weighted F1-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1115/>A Data-Oriented Model of Literary Language</a></strong><br><a href=/people/a/andreas-van-cranenburgh/>Andreas van Cranenburgh</a>
|
<a href=/people/r/rens-bod/>Rens Bod</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1115><div class="card-body p-3 small">We consider the task of predicting how literary a text is, with a gold standard from human ratings. Aside from a standard bigram baseline, we apply rich syntactic tree fragments, mined from the training set, and a series of hand-picked features. Our model is the first to distinguish degrees of highly and less literary novels using a variety of <a href=https://en.wikipedia.org/wiki/Lexicon>lexical and syntactic features</a>, and explains 76.0 % of the variation in literary ratings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1116/>Aye or naw, whit dae ye hink? <a href=https://en.wikipedia.org/wiki/Scottish_independence>Scottish independence</a> and linguistic identity on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a><span class=acl-fixed-case>S</span>cottish independence and linguistic identity on social media</a></strong><br><a href=/people/p/philippa-shoemark/>Philippa Shoemark</a>
|
<a href=/people/d/debnil-sur/>Debnil Sur</a>
|
<a href=/people/l/luke-shrimpton/>Luke Shrimpton</a>
|
<a href=/people/i/iain-murray/>Iain Murray</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1116><div class="card-body p-3 small">Political surveys have indicated a relationship between a sense of Scottish identity and voting decisions in the 2014 <a href=https://en.wikipedia.org/wiki/2014_Scottish_independence_referendum>Scottish Independence Referendum</a>. Identity is often reflected in language use, suggesting the intuitive hypothesis that individuals who support <a href=https://en.wikipedia.org/wiki/Scottish_independence>Scottish independence</a> are more likely to use distinctively Scottish words than those who oppose it. In the first large-scale study of sociolinguistic variation on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in the UK, we identify distinctively Scottish terms in a data-driven way, and find that these <a href=https://en.wikipedia.org/wiki/Terminology>terms</a> are indeed used at a higher rate by users of pro-independence hashtags than by users of anti-independence hashtags. However, we also find that in general people are less likely to use distinctively Scottish words in tweets with referendum-related hashtags than in their general Twitter activity. We attribute this difference to style shifting relative to audience, aligning with previous work showing that Twitter users tend to use fewer local variants when addressing a broader audience.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1118/>Incremental Discontinuous Phrase Structure Parsing with the GAP Transition<span class=acl-fixed-case>GAP</span> Transition</a></strong><br><a href=/people/m/maximin-coavoux/>Maximin Coavoux</a>
|
<a href=/people/b/benoit-crabbe/>Benoît Crabbé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1118><div class="card-body p-3 small">This article introduces a novel transition system for discontinuous lexicalized constituent parsing called SR-GAP. It is an extension of the shift-reduce algorithm with an additional gap transition. Evaluation on two German treebanks shows that SR-GAP outperforms the previous best transition-based discontinuous parser (Maier, 2015) by a large margin (it is notably twice as accurate on the prediction of discontinuous constituents), and is competitive with the state of the art (Fernndez-Gonzlez and Martins, 2015). As a side contribution, we adapt span features (Hall et al., 2014) to discontinuous parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1119" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1119/>Neural Architectures for Fine-grained Entity Type Classification</a></strong><br><a href=/people/s/sonse-shimaoka/>Sonse Shimaoka</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1119><div class="card-body p-3 small">In this work, we investigate several neural network architectures for fine-grained entity type classification and make three key contributions. Despite being a natural comparison and addition, previous work on attentive neural architectures have not considered hand-crafted features and we combine these with learnt features and establish that they complement each other. Additionally, through quantitative analysis we establish that the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> learns to attend over syntactic heads and the phrase containing the mention, both of which are known to be strong hand-crafted features for our task. We introduce parameter sharing between labels through a hierarchical encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained using different data. We demonstrate that the choice of training data has a drastic impact on performance, which decreases by as much as 9.85 % loose micro F1 score for a previously proposed method. Despite this discrepancy, our best model achieves state-of-the-art results with 75.36 % loose micro F1 score on the well-established Figer (GOLD) dataset and we report the best results for models trained using publicly available data for the OntoNotes dataset with 64.93 % loose micro F1 score.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>