<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W18-50.pdf>Proceedings of the 19th Annual <span class=acl-fixed-case>SIG</span>dial Meeting on Discourse and Dialogue</a></h2><p class=lead><a href=/people/k/kazunori-komatani/>Kazunori Komatani</a>,
<a href=/people/d/diane-litman/>Diane Litman</a>,
<a href=/people/k/kai-yu/>Kai Yu</a>,
<a href=/people/a/alexandros-papangelis/>Alex Papangelis</a>,
<a href=/people/l/lawrence-cavedon/>Lawrence Cavedon</a>,
<a href=/people/m/mikio-nakano/>Mikio Nakano</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W18-50</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Melbourne, Australia</dd><dt>Venues:</dt><dd><a href=/venues/sigdial/>SIGDIAL</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigdial/>SIGDIAL</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W18-50>https://aclanthology.org/W18-50</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W18-50.pdf>https://aclanthology.org/W18-50.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W18-50.pdf title="Open PDF of 'Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+19th+Annual+SIGdial+Meeting+on+Discourse+and+Dialogue" title="Search for 'Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5000/>Proceedings of the 19th Annual <span class=acl-fixed-case>SIG</span>dial Meeting on Discourse and Dialogue</a></strong><br><a href=/people/k/kazunori-komatani/>Kazunori Komatani</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a>
|
<a href=/people/a/alexandros-papangelis/>Alex Papangelis</a>
|
<a href=/people/l/lawrence-cavedon/>Lawrence Cavedon</a>
|
<a href=/people/m/mikio-nakano/>Mikio Nakano</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5001 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5001/>Zero-Shot Dialog Generation with Cross-Domain Latent Actions</a></strong><br><a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/m/maxine-eskenazi/>Maxine Eskenazi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5001><div class="card-body p-3 small">This paper introduces zero-shot dialog generation (ZSDG), as a step towards neural dialog systems that can instantly generalize to new situations with minimum data. ZSDG requires an end-to-end generative dialog system to generalize to a new domain for which only a domain description is provided and no training dialogs are available. Then a novel learning framework, Action Matching, is proposed. This <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can learn a cross-domain embedding space that models the semantics of dialog responses which in turn, enables a neural dialog generation model to generalize to new domains. We evaluate our methods on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, a new synthetic dialog dataset, and an existing human-human multi-domain dialog dataset. Experimental results show that our method is able to achieve superior performance in learning dialog models that can rapidly adapt their behavior to new domains and suggests promising future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5002/>Changing the Level of Directness in <a href=https://en.wikipedia.org/wiki/Dialogue>Dialogue</a> using <a href=https://en.wikipedia.org/wiki/Dialogue>Dialogue Vector Models</a> and Recurrent Neural Networks</a></strong><br><a href=/people/l/louisa-pragst/>Louisa Pragst</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5002><div class="card-body p-3 small">In cooperative dialogues, identifying the intent of ones conversation partner and acting accordingly is of great importance. While this endeavour is facilitated by phrasing intentions as directly as possible, we can observe in human-human communication that a number of factors such as <a href=https://en.wikipedia.org/wiki/Social_norm>cultural norms</a> and <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a> may result in expressing one&#8217;s intent indirectly. Therefore, in <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer communication</a> we have to anticipate the possibility of users being indirect and be prepared to interpret their actual meaning. Furthermore, a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> should be able to conform to <a href=https://en.wikipedia.org/wiki/Expectation_(epistemic)>human expectations</a> by adjusting the degree of directness it uses to improve the <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a>. To reach those goals, we propose an approach to differentiate between direct and indirect utterances and find utterances of the opposite characteristic that express the same intent. In this endeavour, we employ dialogue vector models and <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5003/>Modeling Linguistic and Personality Adaptation for Natural Language Generation</a></strong><br><a href=/people/z/zhichao-hu/>Zhichao Hu</a>
|
<a href=/people/j/jean-e-fox-tree/>Jean Fox Tree</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5003><div class="card-body p-3 small">Previous work has shown that conversants adapt to many aspects of their partners&#8217; language. Other work has shown that while every person is unique, they often share general patterns of behavior. Theories of personality aim to explain these shared patterns, and studies have shown that many <a href=https://en.wikipedia.org/wiki/Sensory_cue>linguistic cues</a> are correlated with <a href=https://en.wikipedia.org/wiki/Trait_theory>personality traits</a>. We propose an adaptation measure for adaptive natural language generation for dialogs that integrates the predictions of both <a href=https://en.wikipedia.org/wiki/Personality_psychology>personality theories</a> and adaptation theories, that can be applied as a dialog unfolds, on a turn by turn basis. We show that our measure meets criteria for validity, and that adaptation varies according to corpora and task, speaker, and the set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> used to model it. We also produce fine-grained models according to the dialog segmentation or the speaker, and demonstrate the decaying trend of adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5004/>Estimating User Interest from Open-Domain Dialogue</a></strong><br><a href=/people/m/michimasa-inaba/>Michimasa Inaba</a>
|
<a href=/people/k/kenichi-takahashi/>Kenichi Takahashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5004><div class="card-body p-3 small">Dialogue personalization is an important issue in the field of open-domain chat-oriented dialogue systems. If these <a href=https://en.wikipedia.org/wiki/System>systems</a> could consider their users&#8217; interests, <a href=https://en.wikipedia.org/wiki/User_engagement>user engagement</a> and satisfaction would be greatly improved. This paper proposes a neural network-based method for estimating users&#8217; interests from their utterances in chat dialogues to personalize dialogue systems&#8217; responses. We introduce a method for effectively extracting topics and user interests from utterances and also propose a pre-training approach that increases learning efficiency. Our experimental results indicate that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can estimate user&#8217;s interest more accurately than baseline approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5007/>Neural User Simulation for Corpus-based Policy Optimisation of Spoken Dialogue Systems</a></strong><br><a href=/people/f/florian-kreyssig/>Florian Kreyssig</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5007><div class="card-body p-3 small">User Simulators are one of the major tools that enable offline training of task-oriented dialogue systems. For this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> the Agenda-Based User Simulator (ABUS) is often used. The ABUS is based on hand-crafted rules and its output is in semantic form. Issues arise from both properties such as limited diversity and the inability to interface a text-level belief tracker. This paper introduces the Neural User Simulator (NUS) whose behaviour is learned from a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and which generates <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, hence needing a less labelled dataset than simulators generating a semantic output. In comparison to much of the past work on this topic, which evaluates user simulators on corpus-based metrics, we use the <a href=https://en.wikipedia.org/wiki/National_University_of_Singapore>NUS</a> to train the policy of a reinforcement learning based Spoken Dialogue System. The <a href=https://en.wikipedia.org/wiki/National_University_of_Singapore>NUS</a> is compared to the ABUS by evaluating the <a href=https://en.wikipedia.org/wiki/Policy>policies</a> that were trained using the <a href=https://en.wikipedia.org/wiki/Simulation>simulators</a>. Cross-model evaluation is performed i.e. training on one <a href=https://en.wikipedia.org/wiki/Simulation>simulator</a> and testing on the other. Furthermore, the trained <a href=https://en.wikipedia.org/wiki/Policy>policies</a> are tested on real users. In both evaluation tasks the <a href=https://en.wikipedia.org/wiki/NUS>NUS</a> outperformed the <a href=https://en.wikipedia.org/wiki/ABUS>ABUS</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5008/>Introduction method for <a href=https://en.wikipedia.org/wiki/Argumentative_dialogue>argumentative dialogue</a> using paired question-answering interchange about personality</a></strong><br><a href=/people/k/kazuki-sakai/>Kazuki Sakai</a>
|
<a href=/people/r/ryuichiro-higashinaka/>Ryuichiro Higashinaka</a>
|
<a href=/people/y/yuichiro-yoshikawa/>Yuichiro Yoshikawa</a>
|
<a href=/people/h/hiroshi-ishiguro/>Hiroshi Ishiguro</a>
|
<a href=/people/j/junji-tomita/>Junji Tomita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5008><div class="card-body p-3 small">To provide a better discussion experience in current argumentative dialogue systems, it is necessary for the user to feel motivated to participate, even if the <a href=https://en.wikipedia.org/wiki/System>system</a> already responds appropriately. In this paper, we propose a method that can smoothly introduce <a href=https://en.wikipedia.org/wiki/Argumentative_dialogue>argumentative dialogue</a> by inserting an initial <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>, consisting of question-answer pairs concerning personality. The <a href=https://en.wikipedia.org/wiki/System>system</a> can induce interest of the users prior to agreement or disagreement during the main discourse. By disclosing their interests, the users will feel familiarity and motivation to further engage in the argumentative dialogue and understand the <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s intent. To verify the effectiveness of a question-answer dialogue inserted before the argument, a subjective experiment was conducted using a <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text chat interface</a>. The results suggest that inserting the question-answer dialogue enhances familiarity and naturalness. Notably, the results suggest that women more than men regard the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> as more natural and the argument as deepened, following an exchange concerning personality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5010 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5010/>A Situated Dialogue System for Learning Structural Concepts in Blocks World</a></strong><br><a href=/people/i/ian-perera/>Ian Perera</a>
|
<a href=/people/j/james-allen/>James Allen</a>
|
<a href=/people/c/choh-man-teng/>Choh Man Teng</a>
|
<a href=/people/l/lucian-galescu/>Lucian Galescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5010><div class="card-body p-3 small">We present a modular, end-to-end dialogue system for a situated agent to address a multimodal, natural language dialogue task in which the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> learns complex representations of block structure classes through assertions, demonstrations, and questioning. The concept to learn is provided to the user through a set of positive and negative visual examples, from which the user determines the underlying constraints to be provided to the <a href=https://en.wikipedia.org/wiki/System>system</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> in turn asks questions about demonstrated examples and simulates new examples to check its knowledge and verify the user&#8217;s description is complete. We find that this task is non-trivial for users and generates <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> that is varied yet understood by our deep language understanding architecture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-5011.Attachment.mp4 data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-5011/>Pardon the Interruption : Managing Turn-Taking through Overlap Resolution in Embodied Artificial Agents</a></strong><br><a href=/people/f/felix-gervits/>Felix Gervits</a>
|
<a href=/people/m/matthias-scheutz/>Matthias Scheutz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5011><div class="card-body p-3 small">Speech overlap is a common phenomenon in natural conversation and in task-oriented interactions. As human-robot interaction (HRI) becomes more sophisticated, the need to effectively manage <a href=https://en.wikipedia.org/wiki/Turn-taking>turn-taking</a> and resolve overlap becomes more important. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> for speech overlap resolution in <a href=https://en.wikipedia.org/wiki/Embodied_agent>embodied artificial agents</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> identifies when overlap has occurred and uses timing information, dialogue history, and the agent&#8217;s goals to generate context-appropriate behavior. We implement this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> in a <a href=https://en.wikipedia.org/wiki/Nao_(robot)>Nao robot</a> using the DIARC cognitive robotic architecture. The model is evaluated on a corpus of task-oriented human dialogue, and we find that the <a href=https://en.wikipedia.org/wiki/Robot>robot</a> can replicate many of the most common overlap resolution behaviors found in the human data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5013 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5013/>Turn-Taking Strategies for Human-Robot Peer-Learning Dialogue</a></strong><br><a href=/people/r/ranjini-das/>Ranjini Das</a>
|
<a href=/people/h/heather-pon-barry/>Heather Pon-Barry</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5013><div class="card-body p-3 small">In this paper, we apply the contribution model of grounding to a corpus of human-human peer-mentoring dialogues. From this analysis, we propose effective turn-taking strategies for <a href=https://en.wikipedia.org/wiki/Human&#8211;robot_interaction>human-robot interaction</a> with a teachable robot. Specifically, we focus on (1) how <a href=https://en.wikipedia.org/wiki/Robot>robots</a> can encourage humans to present and (2) how robots can signal that they are going to begin a new presentation. We evaluate the strategies against a corpus of human-robot dialogues and offer three guidelines for teachable robots to follow to achieve more human-like collaborative dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5014 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5014/>Predicting Perceived Age : Both Language Ability and Appearance are Important</a></strong><br><a href=/people/s/sarah-plane/>Sarah Plane</a>
|
<a href=/people/a/ariel-marvasti/>Ariel Marvasti</a>
|
<a href=/people/t/tyler-egan/>Tyler Egan</a>
|
<a href=/people/c/casey-kennington/>Casey Kennington</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5014><div class="card-body p-3 small">When interacting with robots in a situated spoken dialogue setting, human dialogue partners tend to assign anthropomorphic and social characteristics to those <a href=https://en.wikipedia.org/wiki/Robot>robots</a>. In this paper, we explore the age and educational level that human dialogue partners assign to three different robotic systems, including an un-embodied spoken dialogue system. We found that how a robot speaks is as important to human perceptions as the way the robot looks. Using the data from our experiment, we derived prosodic, emotional, and linguistic features from the participants to train and evaluate a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> that predicts perceived intelligence, age, and education level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5015 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5015/>Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog</a></strong><br><a href=/people/j/jiaping-zhang/>Jiaping Zhang</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5015><div class="card-body p-3 small">Creating an intelligent conversational system that understands <a href=https://en.wikipedia.org/wiki/Visual_system>vision</a> and language is one of the ultimate goals in Artificial Intelligence (AI) (Winograd, 1972). Extensive research has focused on vision-to-language generation, however, limited research has touched on combining these two modalities in a goal-driven dialog context. We propose a multimodal hierarchical reinforcement learning framework that dynamically integrates vision and language for task-oriented visual dialog. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> jointly learns the multimodal dialog state representation and the hierarchical dialog policy to improve both dialog task success and efficiency. We also propose a new technique, state adaptation, to integrate <a href=https://en.wikipedia.org/wiki/Context_awareness>context awareness</a> in the dialog state representation. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> and the state adaptation technique in an image guessing game and achieve promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5016 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5016/>Language-Guided Adaptive Perception for Efficient Grounded Communication with Robotic Manipulators in Cluttered Environments</a></strong><br><a href=/people/s/siddharth-patki/>Siddharth Patki</a>
|
<a href=/people/t/thomas-howard/>Thomas Howard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5016><div class="card-body p-3 small">The utility of collaborative manipulators for shared tasks is highly dependent on the speed and accuracy of communication between the human and the robot. The <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time</a> of recently developed probabilistic inference models for situated symbol grounding of natural language instructions depends on the complexity of the representation of the environment in which they reason. As we move towards more complex bi-directional interactions, tasks, and environments, we need intelligent perception models that can selectively infer precise pose, <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, and affordances of the objects when inferring exhaustively detailed world models is inefficient and prohibits real-time interaction with these robots. In this paper we propose a model of language and perception for the problem of adapting the configuration of the robot perception pipeline for tasks where constructing exhaustively detailed models of the environment is inefficient and inconsequential for <a href=https://en.wikipedia.org/wiki/Symbol_grounding>symbol grounding</a>. We present experimental results from a synthetic corpus of natural language instructions for robot manipulation in example environments. The results demonstrate that by adapting <a href=https://en.wikipedia.org/wiki/Perception>perception</a> we get significant gains in terms of <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time</a> for <a href=https://en.wikipedia.org/wiki/Perception>perception</a> and situated symbol grounding of the language instructions without a loss in the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the latter.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5017 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5017/>Unsupervised Counselor Dialogue Clustering for Positive Emotion Elicitation in Neural Dialogue System</a></strong><br><a href=/people/n/nurul-lubis/>Nurul Lubis</a>
|
<a href=/people/s/sakriani-sakti/>Sakriani Sakti</a>
|
<a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5017><div class="card-body p-3 small">Positive emotion elicitation seeks to improve user&#8217;s emotional state through dialogue system interaction, where a chat-based scenario is layered with an implicit goal to address user&#8217;s emotional needs. Standard neural dialogue system approaches still fall short in this situation as they tend to generate only short, generic responses. Learning from expert actions is critical, as these potentially differ from standard dialogue acts. In this paper, we propose using a hierarchical neural network for response generation that is conditioned on 1) expert&#8217;s action, 2) dialogue context, and 3) user emotion, encoded from user input. We construct a corpus of interactions between a counselor and 30 participants following a negative emotional exposure to learn expert actions and responses in a positive emotion elicitation scenario. Instead of relying on the expensive, labor intensive, and often ambiguous human annotations, we unsupervisedly cluster the expert&#8217;s responses and use the resulting labels to train the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a>. Our experiments and evaluation show that the proposed approach yields lower <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> and generates a larger variety of responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5018/>Discovering User Groups for Natural Language Generation</a></strong><br><a href=/people/n/nikos-engonopoulos/>Nikos Engonopoulos</a>
|
<a href=/people/c/christoph-teichmann/>Christoph Teichmann</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5018><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> which predicts how individual users of a dialog system understand and produce utterances based on <a href=https://en.wikipedia.org/wiki/Users&#8217;_group>user groups</a>. In contrast to previous work, these <a href=https://en.wikipedia.org/wiki/User_group>user groups</a> are not specified beforehand, but learned in training. We evaluate on two referring expression (RE) generation tasks ; our experiments show that our model can identify user groups and learn how to most effectively talk to them, and can dynamically assign unseen users to the correct groups as they interact with the system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5019 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5019/>Controlling Personality-Based Stylistic Variation with Neural Natural Language Generators</a></strong><br><a href=/people/s/shereen-oraby/>Shereen Oraby</a>
|
<a href=/people/l/lena-reed/>Lena Reed</a>
|
<a href=/people/s/shubhangi-tandon/>Shubhangi Tandon</a>
|
<a href=/people/s/sharath-t-s/>Sharath T.S.</a>
|
<a href=/people/s/stephanie-lukin/>Stephanie Lukin</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5019><div class="card-body p-3 small">Natural language generators for task-oriented dialogue must effectively realize system dialogue actions and their associated <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. In many applications, it is also desirable for generators to control the style of an utterance. To date, work on task-oriented neural generation has primarily focused on semantic fidelity rather than achieving stylistic goals, while work on <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>style</a> has been done in contexts where it is difficult to measure content preservation. Here we present three different sequence-to-sequence models and carefully test how well they disentangle content and style. We use a statistical generator, Personage, to synthesize a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of over 88,000 restaurant domain utterances whose style varies according to models of personality, giving us total control over both the semantic content and the stylistic variation in the training data. We then vary the amount of explicit stylistic supervision given to the three <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. We show that our most explicit model can simultaneously achieve high fidelity to both semantic and stylistic goals : this model adds a context vector of 36 stylistic parameters as input to the hidden state of the encoder at each time step, showing the benefits of explicit stylistic supervision, even when the amount of training data is large.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5022 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5022/>Cost-Sensitive Active Learning for Dialogue State Tracking</a></strong><br><a href=/people/k/kaige-xie/>Kaige Xie</a>
|
<a href=/people/c/cheng-chang/>Cheng Chang</a>
|
<a href=/people/l/liliang-ren/>Liliang Ren</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5022><div class="card-body p-3 small">Dialogue state tracking (DST), when formulated as a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning problem</a>, relies on <a href=https://en.wikipedia.org/wiki/Data_type>labelled data</a>. Since dialogue state annotation usually requires labelling all turns of a single dialogue and utilizing context information, it is very expensive to annotate all available unlabelled data. In this paper, a novel cost-sensitive active learning framework is proposed based on a set of new dialogue-level query strategies. This is the first attempt to apply <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> for dialogue state tracking. Experiments on DSTC2 show that active learning with mixed data query strategies can effectively achieve the same DST performance with significantly less data annotation compared to traditional training approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5023 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-5023.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5023" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5023/>Discourse Coherence in the Wild : A Dataset, Evaluation and Methods</a></strong><br><a href=/people/a/alice-lai/>Alice Lai</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5023><div class="card-body p-3 small">To date there has been very little work on assessing discourse coherence methods on real-world data. To address this, we present a new corpus of real-world texts (GCDC) as well as the first large-scale evaluation of leading discourse coherence algorithms. We show that neural models, including two that we introduce here (SentAvg and ParSeq), tend to perform best. We analyze these performance differences and discuss patterns we observed in low coherence texts in four domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5024 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5024/>Neural Dialogue Context Online End-of-Turn Detection</a></strong><br><a href=/people/r/ryo-masumura/>Ryo Masumura</a>
|
<a href=/people/t/tomohiro-tanaka/>Tomohiro Tanaka</a>
|
<a href=/people/a/atsushi-ando/>Atsushi Ando</a>
|
<a href=/people/r/ryo-ishii/>Ryo Ishii</a>
|
<a href=/people/r/ryuichiro-higashinaka/>Ryuichiro Higashinaka</a>
|
<a href=/people/y/yushi-aono/>Yushi Aono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5024><div class="card-body p-3 small">This paper proposes a fully neural network based dialogue-context online end-of-turn detection method that can utilize long-range interactive information extracted from both speaker&#8217;s utterances and collocutor&#8217;s utterances. The proposed method combines multiple time-asynchronous long short-term memory recurrent neural networks, which can capture speaker&#8217;s and collocutor&#8217;s multiple sequential features, and their interactions. On the assumption of applying the proposed method to spoken dialogue systems, we introduce speaker&#8217;s acoustic sequential features and collocutor&#8217;s linguistic sequential features, each of which can be extracted in an online manner. Our evaluation confirms the effectiveness of taking dialogue context formed by the speaker&#8217;s utterances and collocutor&#8217;s utterances into consideration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5025 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5025/>Spoken Dialogue for Information Navigation</a></strong><br><a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>
|
<a href=/people/p/panagiotis-papadakos/>Panagiotis Papadakos</a>
|
<a href=/people/y/yannis-stylianou/>Yannis Stylianou</a>
|
<a href=/people/y/yannis-tzitzikas/>Yannis Tzitzikas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5025><div class="card-body p-3 small">Aiming to expand the current research paradigm for training conversational AI agents that can address real-world challenges, we take a step away from traditional slot-filling goal-oriented spoken dialogue systems (SDS) and model the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> in a way that allows users to be more expressive in describing their needs. The goal is to help users make informed decisions rather than being fed matching items. To this end, we describe the Linked-Data SDS (LD-SDS), a system that exploits semantic knowledge bases that connect to <a href=https://en.wikipedia.org/wiki/Linked_data>linked data</a>, and supports complex constraints and preferences. We describe the required changes in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and state tracking, and the need for mined features, and we report the promising results (in terms of semantic errors, effort, etc) of a preliminary evaluation after training two statistical dialogue managers in various conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5026 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5026/>Improving User Impression in Spoken Dialog System with Gradual Speech Form Control</a></strong><br><a href=/people/y/yukiko-kageyama/>Yukiko Kageyama</a>
|
<a href=/people/y/yuya-chiba/>Yuya Chiba</a>
|
<a href=/people/t/takashi-nose/>Takashi Nose</a>
|
<a href=/people/a/akinori-ito/>Akinori Ito</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5026><div class="card-body p-3 small">This paper examines a method to improve the user impression of a spoken dialog system by introducing a mechanism that gradually changes form of utterances every time the user uses the <a href=https://en.wikipedia.org/wiki/System>system</a>. In some <a href=https://en.wikipedia.org/wiki/Language>languages</a>, including <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, the form of utterances changes corresponding to social relationship between the talker and the listener. Thus, this mechanism can be effective to express the system&#8217;s intention to make <a href=https://en.wikipedia.org/wiki/Social_distance>social distance</a> to the user closer ; however, an actual effect of this method is not investigated enough when introduced to the dialog system. In this paper, we conduct dialog experiments and show that controlling the form of system utterances can improve the users&#8217; impression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5027 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5027/>A Bilingual Interactive Human Avatar Dialogue System</a></strong><br><a href=/people/d/dana-abu-ali/>Dana Abu Ali</a>
|
<a href=/people/m/muaz-ahmad/>Muaz Ahmad</a>
|
<a href=/people/h/hayat-al-hassan/>Hayat Al Hassan</a>
|
<a href=/people/p/paula-dozsa/>Paula Dozsa</a>
|
<a href=/people/m/ming-hu/>Ming Hu</a>
|
<a href=/people/j/jose-varias/>Jose Varias</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5027><div class="card-body p-3 small">This demonstration paper presents a bilingual (Arabic-English) interactive human avatar dialogue system. The system is named TOIA (time-offset interaction application), as it simulates <a href=https://en.wikipedia.org/wiki/Face-to-face_interaction>face-to-face conversations</a> between humans using <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>digital human avatars</a> recorded in the past. TOIA is a conversational agent, similar to a chat bot, except that it is based on an actual human being and can be used to preserve and tell stories. The system is designed to allow anybody, simply using a laptop, to create an avatar of themselves, thus facilitating cross-cultural and cross-generational sharing of narratives to wider audiences. The system currently supports monolingual and cross-lingual dialogues in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, but can be extended to other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5029 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5029/>Leveraging Multimodal Dialog Technology for the Design of Automated and Interactive Student Agents for Teacher Training</a></strong><br><a href=/people/d/david-pautler/>David Pautler</a>
|
<a href=/people/v/vikram-ramanarayanan/>Vikram Ramanarayanan</a>
|
<a href=/people/k/kirby-cofino/>Kirby Cofino</a>
|
<a href=/people/p/patrick-l-lange/>Patrick Lange</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5029><div class="card-body p-3 small">We present a paradigm for interactive teacher training that leverages multimodal dialog technology to puppeteer custom-designed embodied conversational agents (ECAs) in student roles. We used the open-source multimodal dialog system HALEF to implement a small-group classroom math discussion involving <a href=https://en.wikipedia.org/wiki/Venn_diagram>Venn diagrams</a> where a human teacher candidate has to interact with two student ECAs whose actions are controlled by the dialog system. Such an automated paradigm has the potential to be extended and scaled to a wide range of interactive simulation scenarios in <a href=https://en.wikipedia.org/wiki/Education>education</a>, <a href=https://en.wikipedia.org/wiki/Medicine>medicine</a>, and <a href=https://en.wikipedia.org/wiki/Business>business</a> where group interaction training is essential.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5032 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-5032.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-5032/>Addressing Objects and Their Relations : The Conversational Entity Dialogue Model</a></strong><br><a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/y/yen-chen-wu/>Yen-Chen Wu</a>
|
<a href=/people/s/steve-young/>Steve Young</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5032><div class="card-body p-3 small">Statistical spoken dialogue systems usually rely on a single- or multi-domain dialogue model that is restricted in its capabilities of modelling complex dialogue structures, e.g., <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>relations</a>. In this work, we propose a novel dialogue model that is centred around entities and is able to model <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> as well as multiple entities of the same type. We demonstrate in a prototype implementation benefits of relation modelling on the dialogue level and show that a trained <a href=https://en.wikipedia.org/wiki/Policy>policy</a> using these <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> outperforms the multi-domain baseline. Furthermore, we show that by modelling the <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> on the dialogue level, the <a href=https://en.wikipedia.org/wiki/System>system</a> is capable of processing <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> present in the user input and even learns to address them in the system response.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5034 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5034/>Fine-Grained Discourse Structures in Continuation Semantics</a></strong><br><a href=/people/t/timothee-bernard/>Timothée Bernard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5034><div class="card-body p-3 small">In this work, we are interested in the computation of logical representations of discourse. We argue that all discourse connectives are <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphors</a> obeying different sets of constraints and show how this view allows one to account for the semantically parenthetical use of attitude verbs and verbs of report (e.g., think, say) and for sequences of <a href=https://en.wikipedia.org/wiki/Conjunction_(grammar)>conjunctions</a> (A CONJ_1 B CONJ_2 C). We implement this proposal in <a href=https://en.wikipedia.org/wiki/Event_(computing)>event semantics</a> using de Groote (2006)&#8217;s dynamic framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5037 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5037/>Identifying Explicit Discourse Connectives in German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/p/peter-bourgonje/>Peter Bourgonje</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5037><div class="card-body p-3 small">We are working on an end-to-end Shallow Discourse Parsing system for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and in this paper focus on the first subtask : the identification of explicit connectives. Starting with the feature set from an English system and a Random Forest classifier, we evaluate our approach on a (relatively small) German annotated corpus, the Potsdam Commentary Corpus. We introduce new <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and experiment with including additional <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> obtained through annotation projection and achieve an <a href=https://en.wikipedia.org/wiki/F-score>f-score</a> of 83.89.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5038 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5038/>Feudal Dialogue Management with Jointly Learned Feature Extractors</a></strong><br><a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/f/florian-kreyssig/>Florian Kreyssig</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/y/yen-chen-wu/>Yen-chen Wu</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5038><div class="card-body p-3 small">Reinforcement learning (RL) is a promising dialogue policy optimisation approach, but traditional RL algorithms fail to scale to large domains. Recently, Feudal Dialogue Management (FDM), has shown to increase the scalability to large domains by decomposing the dialogue management decision into two steps, making use of the domain ontology to abstract the dialogue state in each step. In order to abstract the <a href=https://en.wikipedia.org/wiki/State_space>state space</a>, however, previous work on <a href=https://en.wikipedia.org/wiki/Finite-state_machine>FDM</a> relies on handcrafted feature functions. In this work, we show that these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature functions</a> can be learned jointly with the <a href=https://en.wikipedia.org/wiki/Policy_model>policy model</a> while obtaining similar performance, even outperforming the handcrafted features in several environments and domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5039 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5039/>Variational Cross-domain Natural Language Generation for Spoken Dialogue Systems</a></strong><br><a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/f/florian-kreyssig/>Florian Kreyssig</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/y/yen-chen-wu/>Yen-Chen Wu</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5039><div class="card-body p-3 small">Cross-domain natural language generation (NLG) is still a difficult task within spoken dialogue modelling. Given a <a href=https://en.wikipedia.org/wiki/Semantics>semantic representation</a> provided by the <a href=https://en.wikipedia.org/wiki/Dialogue_manager>dialogue manager</a>, the language generator should generate sentences that convey desired information. Traditional template-based generators can produce sentences with all necessary information, but these sentences are not sufficiently diverse. With RNN-based models, the diversity of the generated sentences can be high, however, in the process some information is lost. In this work, we improve an RNN-based generator by considering latent information at the sentence level during generation using conditional variational auto-encoder architecture. We demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the original RNN-based generator, while yielding highly diverse sentences. In addition, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better when the training data is limited.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5040 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5040/>Coherence Modeling Improves Implicit Discourse Relation Recognition</a></strong><br><a href=/people/n/noriki-nishida/>Noriki Nishida</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5040><div class="card-body p-3 small">The research described in this paper examines how to learn <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic knowledge</a> associated with <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> from unlabeled corpora. We introduce an unsupervised learning method on <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>text coherence</a> that could produce numerical representations that improve implicit discourse relation recognition in a semi-supervised manner. We also empirically examine two variants of coherence modeling : order-oriented and topic-oriented negative sampling, showing that, of the two, topic-oriented negative sampling tends to be more effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5041 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5041/>Adversarial Learning of Task-Oriented Neural Dialog Models</a></strong><br><a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/i/ian-lane/>Ian Lane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5041><div class="card-body p-3 small">In this work, we propose an adversarial learning method for reward estimation in reinforcement learning (RL) based task-oriented dialog models. Most of the current RL based task-oriented dialog systems require the access to a reward signal from either user feedback or user ratings. Such user ratings, however, may not always be consistent or available in practice. Furthermore, online dialog policy learning with RL typically requires a large number of queries to users, suffering from sample efficiency problem. To address these challenges, we propose an adversarial learning method to learn dialog rewards directly from dialog samples. Such rewards are further used to optimize the dialog policy with policy gradient based RL. In the evaluation in a restaurant search domain, we show that the proposed adversarial dialog learning method achieves advanced dialog success rate comparing to strong baseline methods. We further discuss the covariate shift problem in online adversarial dialog learning and show how we can address that with partial access to user feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5042 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5042/>Constructing a Lexicon of English Discourse Connectives<span class=acl-fixed-case>E</span>nglish Discourse Connectives</a></strong><br><a href=/people/d/debopam-das/>Debopam Das</a>
|
<a href=/people/t/tatjana-scheffler/>Tatjana Scheffler</a>
|
<a href=/people/p/peter-bourgonje/>Peter Bourgonje</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5042><div class="card-body p-3 small">We present a new lexicon of English discourse connectives called DiMLex-Eng, built by merging information from two annotated corpora and an additional list of relation signals from the literature. The format follows the German connective lexicon DiMLex, which provides a cross-linguistically applicable XML schema. DiMLex-Eng contains 149 English connectives, and gives information on <a href=https://en.wikipedia.org/wiki/Syntactic_category>syntactic categories</a>, discourse semantics and non-connective uses (if any). We report on the development steps and discuss design decisions encountered in the lexicon expansion phase. The resource is freely available for use in studies of discourse structure and <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational applications</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5044 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-5044.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-5044/>An Analysis of the Effect of Emotional Speech Synthesis on Non-Task-Oriented Dialogue System</a></strong><br><a href=/people/y/yuya-chiba/>Yuya Chiba</a>
|
<a href=/people/t/takashi-nose/>Takashi Nose</a>
|
<a href=/people/t/taketo-kase/>Taketo Kase</a>
|
<a href=/people/m/mai-yamanaka/>Mai Yamanaka</a>
|
<a href=/people/a/akinori-ito/>Akinori Ito</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5044><div class="card-body p-3 small">This paper explores the effect of emotional speech synthesis on a spoken dialogue system when the dialogue is non-task-oriented. Although the use of emotional speech responses have been shown to be effective in a limited domain, e.g., scenario-based and counseling dialogue, the effect is still not clear in the non-task-oriented dialogue such as <a href=https://en.wikipedia.org/wiki/Voice_chat_in_online_gaming>voice chatting</a>. For this purpose, we constructed a simple <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> with example- and rule-based dialogue management. In the <a href=https://en.wikipedia.org/wiki/System>system</a>, two types of emotion labeling with emotion estimation are adopted, i.e., system-driven and user-cooperative emotion labeling. We conducted a dialogue experiment where subjects evaluate the subjective quality of the <a href=https://en.wikipedia.org/wiki/System>system</a> and the dialogue from the multiple aspects such as richness of the dialogue and impression of the agent. We then analyze and discuss the results and show the advantage of using appropriate <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> for the expressive speech responses in the non-task-oriented system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5045 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5045/>Multi-task Learning for Joint Language Understanding and Dialogue State Tracking</a></strong><br><a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a>
|
<a href=/people/r/raghav-gupta/>Raghav Gupta</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5045><div class="card-body p-3 small">This paper presents a novel approach for multi-task learning of language understanding (LU) and dialogue state tracking (DST) in task-oriented dialogue systems. Multi-task training enables the sharing of the neural network layers responsible for encoding the user utterance for both LU and DST and improves performance while reducing the number of network parameters. In our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, DST operates on a set of candidate values for each slot that has been mentioned so far. These candidate sets are generated using LU slot annotations for the current user utterance, dialogue acts corresponding to the preceding system utterance and the dialogue state estimated for the previous turn, enabling DST to handle slots with a large or unbounded set of possible values and deal with slot values not seen during training. Furthermore, to bridge the gap between <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, we investigate the use of scheduled sampling on LU output for the current user utterance as well as the DST output for the preceding turn.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5046 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5046/>Weighting Model Based on <a href=https://en.wikipedia.org/wiki/Group_dynamics>Group Dynamics</a> to Measure Convergence in Multi-party Dialogue</a></strong><br><a href=/people/z/zahra-rahimi/>Zahra Rahimi</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5046><div class="card-body p-3 small">This paper proposes a new weighting method for extending a dyad-level measure of convergence to multi-party dialogues by considering <a href=https://en.wikipedia.org/wiki/Group_dynamics>group dynamics</a> instead of simply <a href=https://en.wikipedia.org/wiki/Average>averaging</a>. Experiments indicate the usefulness of the proposed weighted measure and also show that in general a proper <a href=https://en.wikipedia.org/wiki/Weighting>weighting</a> of the dyad-level measures performs better than non-weighted averaging in multiple tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5047 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5047/>Concept Transfer Learning for Adaptive Language Understanding</a></strong><br><a href=/people/s/su-zhu/>Su Zhu</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5047><div class="card-body p-3 small">Concept definition is important in language understanding (LU) adaptation since literal definition difference can easily lead to data sparsity even if different data sets are actually semantically correlated. To address this issue, in this paper, a novel concept transfer learning approach is proposed. Here, substructures within literal concept definition are investigated to reveal the relationship between <a href=https://en.wikipedia.org/wiki/Concept>concepts</a>. A hierarchical semantic representation for <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> is proposed, where a semantic slot is represented as a composition of atomic concepts. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two tasks : value set mismatch and <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>, and evaluated on two LU benchmarks : ATIS and DSTC 2&3. Thorough empirical studies validate both the efficiency and effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. In particular, we achieve state-of-the-art performance (F-score 96.08 %) on <a href=https://en.wikipedia.org/wiki/Automatic_terminal_information_service>ATIS</a> by only using lexicon features.<i>atomic concepts</i>. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two tasks: value set mismatch and domain adaptation, and evaluated on two LU benchmarks: ATIS and DSTC 2&3. Thorough empirical studies validate both the efficiency and effectiveness of the proposed method. In particular, we achieve state-of-the-art performance (F&#8321;-score 96.08%) on ATIS by only using lexicon features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5048 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5048/>Cogent : A Generic Dialogue System Shell Based on a Collaborative Problem Solving Model<span class=acl-fixed-case>C</span>ogent: A Generic Dialogue System Shell Based on a Collaborative Problem Solving Model</a></strong><br><a href=/people/l/lucian-galescu/>Lucian Galescu</a>
|
<a href=/people/c/choh-man-teng/>Choh Man Teng</a>
|
<a href=/people/j/james-allen/>James Allen</a>
|
<a href=/people/i/ian-perera/>Ian Perera</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5048><div class="card-body p-3 small">The bulk of current research in <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> is focused on fairly simple task models, primarily state-based. Progress on developing <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> for more complex tasks has been limited by the lack generic toolkits to build from. In this paper we report on our development from the ground up of a new dialogue model based on collaborative problem solving. We implemented the model in a dialogue system shell (Cogent) that al-lows developers to plug in problem-solving agents to create dialogue systems in new domains. The Cogent shell has now been used by several independent teams of researchers to develop dialogue systems in different domains, with varied lexicons and interaction style, each with their own problem-solving back-end. We believe this to be the first practical demonstration of the feasibility of a CPS-based dialogue system shell.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5049 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5049/>Identifying Domain Independent Update Intents in Task Based Dialogs</a></strong><br><a href=/people/p/prakhar-biyani/>Prakhar Biyani</a>
|
<a href=/people/c/cem-akkaya/>Cem Akkaya</a>
|
<a href=/people/k/kostas-tsioutsiouliklis/>Kostas Tsioutsiouliklis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5049><div class="card-body p-3 small">One important problem in task-based conversations is that of effectively updating the belief estimates of user-mentioned slot-value pairs. Given a user utterance, the intent of a slot-value pair is captured using dialog acts (DA) expressed in that utterance. However, in certain cases, DA&#8217;s fail to capture the actual update intent of the user. In this paper, we describe such cases and propose a new type of <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic class</a> for <a href=https://en.wikipedia.org/wiki/User_intent>user intents</a>. This new <a href=https://en.wikipedia.org/wiki/Data_type>type</a>, Update Intents (UI), is directly related to the type of update a user intends to perform for a slot-value pair. We define five types of UI&#8217;s, which are independent of the domain of the conversation. We build a multi-class classification model using LSTM&#8217;s to identify the type of <a href=https://en.wikipedia.org/wiki/User_interface>UI</a> in user utterances in the Restaurant and Shopping domains. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve strong <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance in terms of F-1 score.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>