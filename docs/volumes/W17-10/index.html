<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W17-10.pdf>Proceedings of the <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>L</span>ing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</a></h2><p class=lead><a href=/people/g/george-giannakopoulos/>George Giannakopoulos</a>,
<a href=/people/e/elena-lloret/>Elena Lloret</a>,
<a href=/people/j/john-conroy/>John M. Conroy</a>,
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>,
<a href=/people/m/marina-litvak/>Marina Litvak</a>,
<a href=/people/p/peter-a-rankel/>Peter Rankel</a>,
<a href=/people/b/benoit-favre/>Benoit Favre</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W17-10</dd><dt>Month:</dt><dd>April</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Valencia, Spain</dd><dt>Venues:</dt><dd><a href=/venues/multiling/>MultiLing</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W17-10>https://aclanthology.org/W17-10</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/W17-10 title="To the current version of the paper by DOI">10.18653/v1/W17-10</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W17-10.pdf>https://aclanthology.org/W17-10.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W17-10.pdf title="Open PDF of 'Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+MultiLing+2017+Workshop+on+Summarization+and+Summary+Evaluation+Across+Source+Types+and+Genres" title="Search for 'Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1000/>Proceedings of the <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>L</span>ing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</a></strong><br><a href=/people/g/george-giannakopoulos/>George Giannakopoulos</a>
|
<a href=/people/e/elena-lloret/>Elena Lloret</a>
|
<a href=/people/j/john-conroy/>John M. Conroy</a>
|
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>
|
<a href=/people/m/marina-litvak/>Marina Litvak</a>
|
<a href=/people/p/peter-a-rankel/>Peter Rankel</a>
|
<a href=/people/b/benoit-favre/>Benoit Favre</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1002/>Decoupling Encoder and Decoder Networks for Abstractive Document Summarization</a></strong><br><a href=/people/y/ying-xu/>Ying Xu</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1002><div class="card-body p-3 small">Abstractive document summarization seeks to automatically generate a summary for a document, based on some abstract understanding of the original document. State-of-the-art techniques traditionally use attentive encoderdecoder architectures. However, due to the large number of parameters in these <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, they require large training datasets and long training times. In this paper, we propose decoupling the encoder and decoder networks, and training them separately. We encode documents using an unsupervised document encoder, and then feed the document vector to a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network decoder</a>. With this decoupled architecture, we decrease the number of parameters in the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> substantially, and shorten its <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a>. Experiments show that the decoupled model achieves comparable performance with state-of-the-art models for in-domain documents, but less well for out-of-domain documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-1003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-1003/>Centroid-based Text Summarization through Compositionality of Word Embeddings</a></strong><br><a href=/people/g/gaetano-rossiello/>Gaetano Rossiello</a>
|
<a href=/people/p/pierpaolo-basile/>Pierpaolo Basile</a>
|
<a href=/people/g/giovanni-semeraro/>Giovanni Semeraro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1003><div class="card-body p-3 small">The textual similarity is a crucial aspect for many extractive text summarization methods. A bag-of-words representation does not allow to grasp the semantic relationships between concepts when comparing strongly related sentences with no words in common. To overcome this issue, in this paper we propose a centroid-based method for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a> that exploits the compositional capabilities of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. The evaluations on multi-document and multilingual datasets prove the effectiveness of the continuous vector representation of words compared to the <a href=https://en.wikipedia.org/wiki/Bag-of-words_model>bag-of-words model</a>. Despite its simplicity, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves good performance even in comparison to more complex <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>. Our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> is unsupervised and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can be adopted in other summarization tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1004/>Query-based summarization using MDL principle<span class=acl-fixed-case>MDL</span> principle</a></strong><br><a href=/people/m/marina-litvak/>Marina Litvak</a>
|
<a href=/people/n/natalia-vanetik/>Natalia Vanetik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1004><div class="card-body p-3 small">Query-based text summarization is aimed at extracting essential information that answers the query from original text. The answer is presented in a minimal, often predefined, number of words. In this paper we introduce a new unsupervised approach for query-based extractive summarization, based on the minimum description length (MDL) principle that employs Krimp compression algorithm (Vreeken et al., 2011). The key idea of our approach is to select frequent word sets related to a given query that compress document sentences better and therefore describe the document better. A summary is extracted by selecting sentences that best cover query-related frequent word sets. The approach is evaluated based on the DUC 2005 and DUC 2006 datasets which are specifically designed for query-based summarization (DUC, 2005 2006). It competes with the best results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1005/>Word Embedding and Topic Modeling Enhanced Multiple Features for Content Linking and Argument / Sentiment Labeling in Online Forums</a></strong><br><a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/l/liyuan-mao/>Liyuan Mao</a>
|
<a href=/people/m/moye-chen/>Moye Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1005><div class="card-body p-3 small">Multiple grammatical and semantic features are adopted in content linking and argument / sentiment labeling for online forums in this paper. There are mainly two different <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for content linking. First, we utilize the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep feature</a> obtained from Word Embedding Model in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and compute sentence similarity. Second, we use multiple traditional features to locate candidate linking sentences, and then adopt a voting method to obtain the final result. LDA topic modeling is used to mine latent semantic feature and <a href=https://en.wikipedia.org/wiki/K-means_clustering>K-means clustering</a> is implemented for argument labeling, while features from sentiment dictionaries and rule-based sentiment analysis are integrated for sentiment labeling. Experimental results have shown that our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> are valid.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1006/>Ultra-Concise Multi-genre Summarisation of <a href=https://en.wikipedia.org/wiki/Web_2.0>Web2.0</a> : towards Intelligent Content Generation</a></strong><br><a href=/people/e/elena-lloret/>Elena Lloret</a>
|
<a href=/people/e/ester-boldrini/>Ester Boldrini</a>
|
<a href=/people/p/patricio-martinez-barco/>Patricio Martínez-Barco</a>
|
<a href=/people/m/manuel-palomar/>Manuel Palomar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1006><div class="card-body p-3 small">The electronic Word of Mouth has become the most powerful communication channel thanks to the wide usage of the <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a>. Our research proposes an approach towards the production of automatic ultra-concise summaries from multiple <a href=https://en.wikipedia.org/wiki/Web_2.0>Web 2.0 sources</a>. We exploit <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> from reviews and microblogs in different domains, and compile and analyse four types of ultra-concise summaries : a)positive information, b) negative information ; c) both or d) objective information. The appropriateness and usefulness of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is demonstrated by its successful results and great potential in real-life applications, thus meaning a relevant advancement of the state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1007/>Machine Learning Approach to Evaluate MultiLingual Summaries<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>L</span>ingual Summaries</a></strong><br><a href=/people/s/samira-ellouze/>Samira Ellouze</a>
|
<a href=/people/m/maher-jaoua/>Maher Jaoua</a>
|
<a href=/people/l/lamia-hadrich-belguith/>Lamia Hadrich Belguith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1007><div class="card-body p-3 small">The present paper introduces a new MultiLing text summary evaluation method. This method relies on machine learning approach which operates by combining multiple <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to build <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that predict the human score (overall responsiveness) of a new summary. We have tried several single and ensemble learning classifiers to build the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We have experimented our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in summary level evaluation where we evaluate each text summary separately. The correlation between built models and human score is better than the correlation between baselines and <a href=https://en.wikipedia.org/wiki/Score_(game)>manual score</a>.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>