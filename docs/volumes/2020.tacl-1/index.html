<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Transactions of the Association for Computational Linguistics, Volume 8 - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Transactions of the Association for Computational Linguistics, Volume 8</h2><p class=lead><a href=/people/m/mark-johnson/>Mark Johnson</a>,
<a href=/people/b/brian-roark/>Brian Roark</a>,
<a href=/people/a/ani-nenkova/>Ani Nenkova</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.tacl-1</dd><dt>Month:</dt><dd></dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Cambridge, MA</dd><dt>Venue:</dt><dd><a href=/venues/tacl/>TACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>MIT Press</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.tacl-1>https://aclanthology.org/2020.tacl-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Transactions+of+the+Association+for+Computational+Linguistics%2C+Volume+8" title="Search for 'Transactions of the Association for Computational Linguistics, Volume 8' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.tacl-1.0/>Transactions of the Association for Computational Linguistics, Volume 8</a></strong><br><a href=/people/m/mark-johnson/>Mark Johnson</a>
|
<a href=/people/b/brian-roark/>Brian Roark</a>
|
<a href=/people/a/ani-nenkova/>Ani Nenkova</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.tacl-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--tacl-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.tacl-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.tacl-1.2/>AMR-To-Text Generation with Graph Transformer<span class=acl-fixed-case>AMR</span>-To-Text Generation with Graph Transformer</a></strong><br><a href=/people/t/tianming-wang/>Tianming Wang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a>
|
<a href=/people/h/hanqi-jin/>Hanqi Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--tacl-1--2><div class="card-body p-3 small">Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> represent concepts and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> denote relations. The current state-of-the-art methods use graph-to-sequence models ; however, they still can not significantly outperform the previous sequence-to-sequence models or statistical approaches. In this paper, we propose a novel graph-to-sequence model (Graph Transformer) to address this task. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> directly encodes the AMR graphs and learns the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>node representations</a>. A pairwise interaction function is used for computing the semantic relations between the concepts. Moreover, <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> are used for aggregating the information from the incoming and outgoing neighbors, which help the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to capture the semantic information effectively. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural approach</a> by 1.5 BLEU points on LDC2015E86 and 4.8 BLEU points on LDC2017T10 and achieves new state-of-the-art performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.tacl-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--tacl-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.tacl-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.tacl-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.tacl-1.4/>Membership Inference Attacks on Sequence-to-Sequence Models : Is My Data In Your Machine Translation System?<span class=acl-fixed-case>I</span>s My Data In Your Machine Translation System?</a></strong><br><a href=/people/s/sorami-hisamoto/>Sorami Hisamoto</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--tacl-1--4><div class="card-body p-3 small">Data privacy is an important issue for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> as a service providers. We focus on the problem of membership inference attacks : Given a data sample and black-box access to a <a href=https://en.wikipedia.org/wiki/Statistical_model>model&#8217;s API</a>, determine whether the sample existed in the model&#8217;s training data. Our contribution is an investigation of this problem in the context of sequence-to-sequence models, which are important in applications such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Closed-circuit_television>video captioning</a>. We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.tacl-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--tacl-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.tacl-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.tacl-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.tacl-1.5/>SpanBERT : Improving Pre-training by Representing and Predicting Spans<span class=acl-fixed-case>S</span>pan<span class=acl-fixed-case>BERT</span>: Improving Pre-training by Representing and Predicting Spans</a></strong><br><a href=/people/m/mandar-joshi/>Mandar Joshi</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a>
|
<a href=/people/y/yinhan-liu/>Yinhan Liu</a>
|
<a href=/people/d/daniel-s-weld/>Daniel S. Weld</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--tacl-1--5><div class="card-body p-3 small">We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and our better-tuned baselines, with substantial gains on span selection tasks such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. In particular, with the same training data and model size as BERTlarge, our single <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> obtains 94.6 % and 88.7 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6 % F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.tacl-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--tacl-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.tacl-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.tacl-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.tacl-1.7/>A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation</a></strong><br><a href=/people/j/jian-guan/>Jian Guan</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/z/zhihao-zhao/>Zhihao Zhao</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--tacl-1--7><div class="card-body p-3 small">Story generation, namely, generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>, understanding the <a href=https://en.wikipedia.org/wiki/Causality>causal relationships</a>, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we use <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, which combines a discriminative objective to distinguish true and fake stories during <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.tacl-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--tacl-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.tacl-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.tacl-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.tacl-1.8/>Improving Candidate Generation for Low-resource Cross-lingual Entity Linking</a></strong><br><a href=/people/s/shuyan-zhou/>Shuyan Zhou</a>
|
<a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/j/john-wieting/>John Wieting</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--tacl-1--8><div class="card-body p-3 small">Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia pages</a>. Recently, transfer learning methods have been shown to reduce the demand for resources in the low-resource languages by utilizing resources in closely related languages, but the performance still lags far behind their high-resource counterparts. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios. The methods are simple, but effective : We experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9 % in Top-30 gold candidate recall, compared with state-of-the-art baselines. Our improved <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> also yields an average gain of 7.9 % in in-KB accuracy of end-to-end XEL.1</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.tacl-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--tacl-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.tacl-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.tacl-1.11/>Theoretical Limitations of Self-Attention in Neural Sequence Models</a></strong><br><a href=/people/m/michael-hahn/>Michael Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--tacl-1--11><div class="card-body p-3 small">Transformers are emerging as the new workhorse of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, showing great success across tasks. Unlike <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a>, <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a> process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model <a href=https://en.wikipedia.org/wiki/Formal_language>formal languages</a>. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it can not model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>, suggesting that <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.tacl-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--tacl-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.tacl-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.tacl-1.14/>Acoustic-Prosodic and Lexical Cues to Deception and Trust : Deciphering How People Detect Lies</a></strong><br><a href=/people/x/xi-leslie-chen/>Xi (Leslie) Chen</a>
|
<a href=/people/s/sarah-ita-levitan/>Sarah Ita Levitan</a>
|
<a href=/people/m/michelle-levine/>Michelle Levine</a>
|
<a href=/people/m/marko-mandic/>Marko Mandic</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--tacl-1--14><div class="card-body p-3 small">Humans rarely perform better than chance at <a href=https://en.wikipedia.org/wiki/Lie_detection>lie detection</a>. To better understand human perception of deception, we created a game framework, LieCatcher, to collect ratings of perceived deception using a large corpus of deceptive and truthful interviews. We analyzed the acoustic-prosodic and linguistic characteristics of language trusted and mistrusted by raters and compared these to characteristics of actual truthful and deceptive language to understand how perception aligns with reality. With this data we built <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> to automatically distinguish <a href=https://en.wikipedia.org/wiki/Trust_(social_science)>trusted</a> from mistrusted speech, achieving an F1 of 66.1 %. We next evaluated whether the strategies raters said they used to discriminate between truthful and deceptive responses were in fact useful. Our results show that, although several prosodic and lexical features were consistently perceived as trustworthy, they were not reliable cues. Also, the <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> that judges reported using in deception detection were not helpful for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our work sheds light on the nature of trusted language and provides insight into the challenging problem of human deception detection.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>