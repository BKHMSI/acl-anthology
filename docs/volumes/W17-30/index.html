<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the First Workshop on Abusive Language Online - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W17-30.pdf>Proceedings of the First Workshop on Abusive Language Online</a></h2><p class=lead><a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>,
<a href=/people/w/wendy-hui-kyong-chung/>Wendy Hui Kyong Chung</a>,
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>,
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W17-30</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Vancouver, BC, Canada</dd><dt>Venues:</dt><dd><a href=/venues/alw/>ALW</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W17-30>https://aclanthology.org/W17-30</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/W17-30 title="To the current version of the paper by DOI">10.18653/v1/W17-30</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W17-30.pdf>https://aclanthology.org/W17-30.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W17-30.pdf title="Open PDF of 'Proceedings of the First Workshop on Abusive Language Online'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+First+Workshop+on+Abusive+Language+Online" title="Search for 'Proceedings of the First Workshop on Abusive Language Online' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3000/>Proceedings of the First Workshop on Abusive Language Online</a></strong><br><a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/w/wendy-hui-kyong-chung/>Wendy Hui Kyong Chung</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3001/>Dimensions of Abusive Language on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/i/isobelle-clarke/>Isobelle Clarke</a>
|
<a href=/people/j/jack-grieve/>Jack Grieve</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3001><div class="card-body p-3 small">In this paper, we use a new categorical form of multidimensional register analysis to identify the main dimensions of functional linguistic variation in a corpus of abusive language, consisting of racist and sexist Tweets. By analysing the use of a wide variety of parts-of-speech and grammatical constructions, as well as various features related to <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Computer-mediated_communication>computer-mediated communication</a>, we discover three dimensions of linguistic variation in this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, which we interpret as being related to the degree of interactive, antagonistic and attitudinal language exhibited by individual Tweets. We then demonstrate that there is a significant functional difference between racist and sexist Tweets, with sexists Tweets tending to be more interactive and attitudinal than racist Tweets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3002/>Constructive Language in News Comments</a></strong><br><a href=/people/v/varada-kolhatkar/>Varada Kolhatkar</a>
|
<a href=/people/m/maite-taboada/>Maite Taboada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3002><div class="card-body p-3 small">We discuss the characteristics of constructive news comments, and present methods to identify them. First, we define the notion of constructiveness. Second, we annotate a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for <a href=https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_education)>constructiveness</a>. Third, we explore whether available argumentation corpora can be useful to identify constructiveness in news comments. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation corpora</a> achieves a top <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 72.59 % (baseline=49.44 %) on our crowd-annotated test data. Finally, we examine the relation between constructiveness and <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a>. In our crowd-annotated data, 21.42 % of the non-constructive comments and 17.89 % of the constructive comments are toxic, suggesting that non-constructive comments are not much more toxic than constructive comments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3003/>Rephrasing Profanity in Chinese Text<span class=acl-fixed-case>C</span>hinese Text</a></strong><br><a href=/people/h/hui-po-su/>Hui-Po Su</a>
|
<a href=/people/z/zhen-jie-huang/>Zhen-Jie Huang</a>
|
<a href=/people/h/hao-tsung-chang/>Hao-Tsung Chang</a>
|
<a href=/people/c/chuan-jie-lin/>Chuan-Jie Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3003><div class="card-body p-3 small">This paper proposes a <a href=https://en.wikipedia.org/wiki/System>system</a> that can detect and rephrase profanity in <a href=https://en.wikipedia.org/wiki/Written_Chinese>Chinese text</a>. Rather than just masking detected profanity, we want to revise the input sentence by using inoffensive words while keeping their original meanings. 29 of such rephrasing rules were invented after observing sentences on real-word social websites. The overall <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the proposed <a href=https://en.wikipedia.org/wiki/System>system</a> is 85.56 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3004/>Deep Learning for User Comment Moderation</a></strong><br><a href=/people/j/john-pavlopoulos/>John Pavlopoulos</a>
|
<a href=/people/p/prodromos-malakasiotis/>Prodromos Malakasiotis</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3004><div class="card-body p-3 small">Experimenting with a new dataset of 1.6 M user comments from a Greek news portal and existing datasets of EnglishWikipedia comments, we show that an <a href=https://en.wikipedia.org/wiki/Random-access_memory>RNN</a> outperforms the previous state of the art in <a href=https://en.wikipedia.org/wiki/Moderation_system>moderation</a>. A deep, classification-specific attention mechanism improves further the overall performance of the <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>RNN</a>. We also compare against a <a href=https://en.wikipedia.org/wiki/CNN>CNN</a> and a word-list baseline, considering both fully automatic and semi-automatic moderation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3005/>Class-based Prediction Errors to Detect Hate Speech with Out-of-vocabulary Words</a></strong><br><a href=/people/j/joan-serra/>Joan Serrà</a>
|
<a href=/people/i/ilias-leontiadis/>Ilias Leontiadis</a>
|
<a href=/people/d/dimitris-spathis/>Dimitris Spathis</a>
|
<a href=/people/g/gianluca-stringhini/>Gianluca Stringhini</a>
|
<a href=/people/j/jeremy-blackburn/>Jeremy Blackburn</a>
|
<a href=/people/a/athena-vakali/>Athena Vakali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3005><div class="card-body p-3 small">Common approaches to <a href=https://en.wikipedia.org/wiki/Categorization>text categorization</a> essentially rely either on <a href=https://en.wikipedia.org/wiki/N-gram>n-gram counts</a> or on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. This presents important difficulties in highly dynamic or quickly-interacting environments, where the appearance of new words and/or varied misspellings is the norm. A paradigmatic example of this situation is abusive online behavior, with <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> and <a href=https://en.wikipedia.org/wiki/Mass_media>media platforms</a> struggling to effectively combat uncommon or non-blacklisted hate words. To better deal with these issues in those fast-paced environments, we propose using the <a href=https://en.wikipedia.org/wiki/Error_signal>error signal</a> of class-based language models as input to text classification algorithms. In particular, we train a next-character prediction model for any given class and then exploit the error of such class-based models to inform a neural network classifier. This way, we shift from the &#8216;ability to describe&#8217; seen documents to the &#8216;ability to predict&#8217; unseen content. Preliminary studies using out-of-vocabulary splits from abusive tweet data show promising results, outperforming competitive text categorization strategies by 4-11 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3006/>One-step and Two-step Classification for Abusive Language Detection on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/j/ji-ho-park/>Ji Ho Park</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3006><div class="card-body p-3 small">Automatic abusive language detection is a difficult but important task for <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a>. Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> and <a href=https://en.wikipedia.org/wiki/Racism>racism</a>, our approach shows a promising performance of 0.827 <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> by using HybridCNN in one-step and 0.824 <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> by using <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> in two-steps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3008/>Abusive Language Detection on Arabic Social Media<span class=acl-fixed-case>A</span>rabic Social Media</a></strong><br><a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3008><div class="card-body p-3 small">In this paper, we present our work on detecting abusive language on Arabic social media. We extract a list of <a href=https://en.wikipedia.org/wiki/Obscenity>obscene words</a> and <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> using common patterns used in offensive and rude communications. We also classify Twitter users according to whether they use any of these <a href=https://en.wikipedia.org/wiki/Word>words</a> or not in their tweets. We expand the list of obscene words using this classification, and we report results on a newly created dataset of classified Arabic tweets (obscene, offensive, and clean). We make this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> freely available for research, in addition to the list of obscene words and <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a>. We are also publicly releasing a large corpus of classified user comments that were deleted from a popular Arabic news site due to violations the site&#8217;s rules and guidelines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3009 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3009/>Vectors for Counterspeech on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/l/lucas-wright/>Lucas Wright</a>
|
<a href=/people/d/derek-ruths/>Derek Ruths</a>
|
<a href=/people/k/kelly-p-dillon/>Kelly P Dillon</a>
|
<a href=/people/h/haji-mohammad-saleem/>Haji Mohammad Saleem</a>
|
<a href=/people/s/susan-benesch/>Susan Benesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3009><div class="card-body p-3 small">A study of conversations on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> found that some arguments between strangers led to favorable change in discourse and even in <a href=https://en.wikipedia.org/wiki/Attitude_(psychology)>attitudes</a>. The authors propose that such exchanges can be usefully distinguished according to whether individuals or groups take part on each side, since the opportunity for a constructive exchange of views seems to vary accordingly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3010/>Detecting Nastiness in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/n/niloofar-safi-samghabadi/>Niloofar Safi Samghabadi</a>
|
<a href=/people/s/suraj-maharjan/>Suraj Maharjan</a>
|
<a href=/people/a/alan-sprague/>Alan Sprague</a>
|
<a href=/people/r/raquel-diaz-sprague/>Raquel Diaz-Sprague</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3010><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has made it easy for people to connect on a virtually unlimited basis, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> has also opened doors to people who misuse it to undermine, harass, humiliate, threaten and bully others. There is a lack of adequate resources to detect and hinder its occurrence. In this paper, we present our initial NLP approach to detect invective posts as a first step to eventually detect and deter <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>. We crawl data containing <a href=https://en.wikipedia.org/wiki/Profanity>profanities</a> and then determine whether or not it contains invective. Annotations on this <a href=https://en.wikipedia.org/wiki/Data>data</a> are improved iteratively by in-lab annotations and <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We pursue different NLP approaches containing various typical and some newer techniques to distinguish the use of <a href=https://en.wikipedia.org/wiki/Profanity>swear words</a> in a neutral way from those instances in which they are used in an insulting way. We also show that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> not only works for our <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>, but also can be successfully applied to different <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3012/>Understanding Abuse : A Typology of Abusive Language Detection Subtasks</a></strong><br><a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/t/thomas-davidson/>Thomas Davidson</a>
|
<a href=/people/d/dana-warmsley/>Dana Warmsley</a>
|
<a href=/people/i/ingmar-weber/>Ingmar Weber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3012><div class="card-body p-3 small">As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>, and <a href=https://en.wikipedia.org/wiki/Online_abuse>online abuse</a> we propose a typology that captures central similarities and differences between subtasks and discuss the implications of this for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3014 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3014/>Illegal is not a Noun : Linguistic Form for Detection of Pejorative Nominalizations</a></strong><br><a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/m/melissa-robinson/>Melissa Robinson</a>
|
<a href=/people/k/kristy-k-phillips/>Kristy K. Phillips</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3014><div class="card-body p-3 small">This paper focuses on a particular type of abusive language, targeting expressions in which typically neutral adjectives take on pejorative meaning when used as nouns-compare &#8216;gay people&#8217; to &#8216;the gays&#8217;. We first collect and analyze a corpus of hand-curated, expert-annotated pejorative nominalizations for four target adjectives : female, gay, illegal, and poor. We then collect a second corpus of automatically-extracted and POS-tagged, crowd-annotated tweets. For both corpora, we find support for the hypothesis that some <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a>, when nominalized, take on negative meaning. The targeted constructions are non-standard yet widely-used, and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech taggers</a> mistag some nominal forms as <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a>. We implement a tool called NomCatcher to correct these mistaggings, and find that the same tool is effective for identifying new adjectives subject to transformation via <a href=https://en.wikipedia.org/wiki/Nominalization>nominalization</a> into abusive language.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>