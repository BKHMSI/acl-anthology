<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</h2><p class=lead><a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>,
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>,
<a href=/people/b/bing-liu/>Bing Liu</a>,
<a href=/people/e/elnaz-nouri/>Elnaz Nouri</a>,
<a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a>,
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.nlp4convai-1</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/nlp4convai/>NLP4ConvAI</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.nlp4convai-1>https://aclanthology.org/2021.nlp4convai-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+3rd+Workshop+on+Natural+Language+Processing+for+Conversational+AI" title="Search for 'Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.0/>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></strong><br><a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/e/elnaz-nouri/>Elnaz Nouri</a>
|
<a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.6/>Not So Fast, Classifier Accuracy and Entropy Reduction in Incremental Intent Classification</a></strong><br><a href=/people/l/lianna-hrycyk/>Lianna Hrycyk</a>
|
<a href=/people/a/alessandra-zarcone/>Alessandra Zarcone</a>
|
<a href=/people/l/luzian-hahn/>Luzian Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--6><div class="card-body p-3 small">Incremental intent classification requires the assignment of intent labels to partial utterances. However, partial utterances do not necessarily contain enough information to be mapped to the intent class of their complete utterance (correctly and with a certain degree of confidence). Using the final interpretation as the ground truth to measure a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier&#8217;s accuracy</a> during intent classification of partial utterances is thus problematic. We release inCLINC, a dataset of partial and full utterances with human annotations of plausible intent labels for different portions of each utterance, as an upper (human) baseline for incremental intent classification. We analyse the incremental annotations and propose entropy reduction as a measure of human annotators&#8217; convergence on an interpretation (i.e. intent label). We argue that, when the annotators do not converge to one or a few possible interpretations and yet the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> already identifies the final intent class early on, it is a sign of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> that can be ascribed to artefacts in the dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4convai-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.8/>Amendable Generation for Dialogue State Tracking</a></strong><br><a href=/people/x/xin-tian/>Xin Tian</a>
|
<a href=/people/l/liankai-huang/>Liankai Huang</a>
|
<a href=/people/y/yingzhan-lin/>Yingzhan Lin</a>
|
<a href=/people/s/siqi-bao/>Siqi Bao</a>
|
<a href=/people/h/huang-he/>Huang He</a>
|
<a href=/people/y/yunyi-yang/>Yunyi Yang</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/f/fan-wang/>Fan Wang</a>
|
<a href=/people/s/shuqi-sun/>Shuqi Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--8><div class="card-body p-3 small">In task-oriented dialogue systems, recent dialogue state tracking methods tend to perform one-pass generation of the dialogue state based on the previous dialogue state. The mistakes of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> made at the current turn are prone to be carried over to the next turn, causing <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. In this paper, we propose a novel Amendable Generation for Dialogue State Tracking (AG-DST), which contains a two-pass generation process : (1) generating a primitive dialogue state based on the dialogue of the current turn and the previous dialogue state, and (2) amending the primitive dialogue state from the first pass. With the additional amending generation pass, our model is tasked to learn more robust dialogue state tracking by amending the errors that still exist in the primitive dialogue state, which plays the role of reviser in the double-checking process and alleviates unnecessary error propagation. Experimental results show that AG-DST significantly outperforms previous works in two active DST datasets (MultiWOZ 2.2 and WOZ 2.0), achieving new state-of-the-art performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.9/>What Went Wrong? Explaining Overall Dialogue Quality through Utterance-Level Impacts</a></strong><br><a href=/people/j/james-d-finch/>James D. Finch</a>
|
<a href=/people/s/sarah-e-finch/>Sarah E. Finch</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--9><div class="card-body p-3 small">Improving user experience of a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> often requires intensive developer effort to read conversation logs, run statistical analyses, and intuit the relative importance of system shortcomings. This paper presents a novel approach to automated analysis of conversation logs that learns the relationship between user-system interactions and overall dialogue quality. Unlike prior work on utterance-level quality prediction, our approach learns the impact of each interaction from the overall user rating without utterance-level annotation, allowing resultant model conclusions to be derived on the basis of empirical evidence and at low cost. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> identifies <a href=https://en.wikipedia.org/wiki/Interaction>interactions</a> that have a strong correlation with the overall dialogue quality in a chatbot setting. Experiments show that the automated analysis from our model agrees with expert judgments, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.12/>Semi-supervised Intent Discovery with Contrastive Learning</a></strong><br><a href=/people/x/xiang-shen/>Xiang Shen</a>
|
<a href=/people/y/yinge-sun/>Yinge Sun</a>
|
<a href=/people/y/yao-zhang/>Yao Zhang</a>
|
<a href=/people/m/mani-najmabadi/>Mani Najmabadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--12><div class="card-body p-3 small">User intent discovery is a key step in developing a Natural Language Understanding (NLU) module at the core of any modern Conversational AI system. Typically, human experts review a representative sample of user input data to discover new intents, which is subjective, costly, and error-prone. In this work, we aim to assist the NLU developers by presenting a novel method for discovering new intents at scale given a corpus of utterances. Our method utilizes supervised contrastive learning to leverage information from a domain-relevant, already labeled dataset and identifies new intents in the corpus at hand using unsupervised K-means clustering. Our method outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by a large margin up to 2 % and 13 % on two benchmark datasets, measured by clustering accuracy. Furthermore, we apply our method on a large dataset from the travel domain to demonstrate its effectiveness on a real-world use case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.13/>CS-BERT : a pretrained model for customer service dialogues<span class=acl-fixed-case>CS</span>-<span class=acl-fixed-case>BERT</span>: a pretrained model for customer service dialogues</a></strong><br><a href=/people/p/peiyao-wang/>Peiyao Wang</a>
|
<a href=/people/j/joyce-fang/>Joyce Fang</a>
|
<a href=/people/j/julia-reinspach/>Julia Reinspach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--13><div class="card-body p-3 small">Large-scale pretrained transformer models have demonstrated state-of-the-art (SOTA) performance in a variety of NLP tasks. Nowadays, numerous pretrained models are available in different model flavors and different languages, and can be easily adapted to one&#8217;s downstream task. However, only a limited number of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are available for dialogue tasks, and in particular, goal-oriented dialogue tasks. In addition, the available pretrained models are trained on general domain language, creating a mismatch between the pretraining language and the downstream domain launguage. In this contribution, we present CS-BERT, a BERT model pretrained on millions of dialogues in the customer service domain. We evaluate CS-BERT on several downstream customer service dialogue tasks, and demonstrate that our in-domain pretraining is advantageous compared to other pretrained models in both zero-shot experiments as well as in finetuning experiments, especially in a low-resource data setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.14/>PLATO-KAG : Unsupervised Knowledge-Grounded Conversation via Joint Modeling<span class=acl-fixed-case>PLATO</span>-<span class=acl-fixed-case>KAG</span>: Unsupervised Knowledge-Grounded Conversation via Joint Modeling</a></strong><br><a href=/people/x/xinxian-huang/>Xinxian Huang</a>
|
<a href=/people/h/huang-he/>Huang He</a>
|
<a href=/people/s/siqi-bao/>Siqi Bao</a>
|
<a href=/people/f/fan-wang/>Fan Wang</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--14><div class="card-body p-3 small">Large-scale conversation models are turning to leveraging <a href=https://en.wikipedia.org/wiki/Knowledge>external knowledge</a> to improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>factual accuracy</a> in response generation. Considering the infeasibility to annotate the external knowledge for large-scale dialogue corpora, it is desirable to learn the knowledge selection and response generation in an unsupervised manner. In this paper, we propose PLATO-KAG (Knowledge-Augmented Generation), an unsupervised learning approach for end-to-end knowledge-grounded conversation modeling. For each dialogue context, the top-k relevant knowledge elements are selected and then employed in knowledge-grounded response generation. The two components of knowledge selection and response generation are optimized jointly and effectively under a balanced objective. Experimental results on two publicly available datasets validate the superiority of PLATO-KAG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.17/>Personalized Search-based Query Rewrite System for Conversational AI<span class=acl-fixed-case>AI</span></a></strong><br><a href=/people/e/eunah-cho/>Eunah Cho</a>
|
<a href=/people/z/ziyan-jiang/>Ziyan Jiang</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/z/zheng-chen/>Zheng Chen</a>
|
<a href=/people/s/saurabh-gupta/>Saurabh Gupta</a>
|
<a href=/people/x/xing-fan/>Xing Fan</a>
|
<a href=/people/c/chenlei-guo/>Chenlei Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--17><div class="card-body p-3 small">Query rewrite (QR) is an emerging component in conversational AI systems, reducing user defect. User defect is caused by various reasons, such as errors in the spoken dialogue system, users&#8217; slips of the tongue or their abridged language. Many of the user defects stem from personalized factors, such as user&#8217;s speech pattern, <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a>, or preferences. In this work, we propose a personalized search-based QR framework, which focuses on automatic reduction of user defect. We build a personalized index for each user, which encompasses diverse affinity layers to reflect personal preferences for each user in the conversational AI. Our personalized QR system contains retrieval and ranking layers. Supported by user feedback based learning, training our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> does not require hand-annotated data. Experiments on personalized test set showed that our personalized QR system is able to correct systematic and user errors by utilizing phonetic and semantic inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4convai-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.19/>AuGPT : Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models<span class=acl-fixed-case>AuGPT</span>: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models</a></strong><br><a href=/people/j/jonas-kulhanek/>Jonáš Kulhánek</a>
|
<a href=/people/v/vojtech-hudecek/>Vojtěch Hudeček</a>
|
<a href=/people/t/tomas-nekvinda/>Tomáš Nekvinda</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--19><div class="card-body p-3 small">Attention-based pre-trained language models such as GPT-2 brought considerable progress to end-to-end dialogue modelling. However, they also present considerable risks for task-oriented dialogue, such as lack of knowledge grounding or diversity. To address these issues, we introduce modified training objectives for language model finetuning, and we employ massive data augmentation via <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> to increase the diversity of the training data. We further examine the possibilities of combining data from multiples sources to improve performance on the target <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We carefully evaluate our <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>contributions</a> with both <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>human and automatic methods</a>. Our model substantially outperforms the baseline on the MultiWOZ data and shows competitive performance with state of the art in both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.22/>Using Pause Information for More Accurate Entity Recognition</a></strong><br><a href=/people/s/sahas-dendukuri/>Sahas Dendukuri</a>
|
<a href=/people/p/pooja-chitkara/>Pooja Chitkara</a>
|
<a href=/people/j/joel-ruben-antony-moniz/>Joel Ruben Antony Moniz</a>
|
<a href=/people/x/xiao-yang/>Xiao Yang</a>
|
<a href=/people/m/manos-tsagkias/>Manos Tsagkias</a>
|
<a href=/people/s/stephen-pulman/>Stephen Pulman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--22><div class="card-body p-3 small">Entity tags in human-machine dialog are integral to natural language understanding (NLU) tasks in conversational assistants. However, current systems struggle to accurately parse spoken queries with the typical use of text input alone, and often fail to understand the user intent. Previous work in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> has identified a cross-language tendency for longer speech pauses surrounding <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> as compared to <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>. We demonstrate that the linguistic observation on pauses can be used to improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in machine-learnt language understanding tasks. Analysis of pauses in French and English utterances from a commercial voice assistant shows the statistically significant difference in pause duration around multi-token entity span boundaries compared to within entity spans. Additionally, in contrast to text-based NLU, we apply pause duration to enrich contextual embeddings to improve shallow parsing of entities. Results show that our proposed novel embeddings improve the relative error rate by up to 8 % consistently across three domains for <a href=https://en.wikipedia.org/wiki/French_language>French</a>, without any added annotation or alignment costs to the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.24/>Teach Me What to Say and I Will Learn What to Pick : Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models<span class=acl-fixed-case>I</span> Will Learn What to Pick: Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models</a></strong><br><a href=/people/e/ehsan-lotfi/>Ehsan Lotfi</a>
|
<a href=/people/m/maxime-de-bruyn/>Maxime De Bruyn</a>
|
<a href=/people/j/jeska-buhmann/>Jeska Buhmann</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--24><div class="card-body p-3 small">Knowledge Grounded Conversation Models are usually based on a selection / retrieval module and a generation module, trained separately or simultaneously, with or without having access to a &#8216;gold&#8217; knowledge option. With the introduction of large pre-trained generative models, the selection and generation part have become more and more entangled, shifting the focus towards enhancing knowledge incorporation (from multiple sources) instead of trying to pick the best knowledge option. These approaches however depend on knowledge labels and/or a separate dense retriever for their best performance. In this work we study the unsupervised selection abilities of pre-trained generative models (e.g. BART) and show that by adding a score-and-aggregate module between <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>, they are capable of learning to pick the proper knowledge through minimising the language modelling loss (i.e. without having access to knowledge labels). Trained as such, our model-K-Mine-shows competitive selection and generation performance against models that benefit from knowledge labels and/or separate dense retriever.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.25/>Influence of user personality on dialogue task performance : A case study using a rule-based dialogue system</a></strong><br><a href=/people/a/ao-guo/>Ao Guo</a>
|
<a href=/people/a/atsumoto-ohashi/>Atsumoto Ohashi</a>
|
<a href=/people/r/ryu-hirai/>Ryu Hirai</a>
|
<a href=/people/y/yuya-chiba/>Yuya Chiba</a>
|
<a href=/people/y/yuiko-tsunomori/>Yuiko Tsunomori</a>
|
<a href=/people/r/ryuichiro-higashinaka/>Ryuichiro Higashinaka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--25><div class="card-body p-3 small">Endowing a task-oriented dialogue system with adaptiveness to <a href=https://en.wikipedia.org/wiki/User_(computing)>user personality</a> can greatly help improve the performance of a dialogue task. However, such a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> can be practically challenging to implement, because it is unclear how <a href=https://en.wikipedia.org/wiki/User_(computing)>user personality</a> influences dialogue task performance. To explore the relationship between user personality and dialogue task performance, we enrolled participants via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> to first answer specified personality questionnaires and then chat with a dialogue system to accomplish assigned tasks. A rule-based dialogue system on the prevalent Multi-Domain Wizard-of-Oz (MultiWOZ) task was used. A total of 211 participants&#8217; personalities and their 633 dialogues were collected and analyzed. The results revealed that sociable and extroverted people tended to fail the task, whereas neurotic people were more likely to succeed. We extracted <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> related to user dialogue behaviors and performed further analysis to determine which kind of <a href=https://en.wikipedia.org/wiki/Behavior>behavior</a> influences task performance. As a result, we identified that average utterance length and slots per utterance are the key features of dialogue behavior that are highly correlated with both task performance and user personality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4convai-1.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.27/>Towards Zero and Few-shot Knowledge-seeking Turn Detection in Task-orientated Dialogue Systems</a></strong><br><a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/s/shuyang-gao/>Shuyang Gao</a>
|
<a href=/people/s/seokhwan-kim/>Seokhwan Kim</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--27><div class="card-body p-3 small">Most prior work on task-oriented dialogue systems is restricted to supporting domain APIs. However, users may have requests that are out of the scope of these <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APIs</a>. This work focuses on identifying such <a href=https://en.wikipedia.org/wiki/User_(computing)>user requests</a>. Existing methods for this task mainly rely on fine-tuning pre-trained models on large annotated data. We propose a novel method, REDE, based on adaptive representation learning and <a href=https://en.wikipedia.org/wiki/Density_estimation>density estimation</a>. REDE can be applied to zero-shot cases, and quickly learns a high-performing detector with only a few shots by updating less than 3 K parameters. We demonstrate <a href=https://en.wikipedia.org/wiki/Rede>REDE</a>&#8217;s competitive performance on DSTC9 data and our newly collected test set.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>