<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/N19-3.pdf>Proceedings of the 2019 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Student Research Workshop</a></h2><p class=lead><a href=/people/s/sudipta-kar/>Sudipta Kar</a>,
<a href=/people/f/farah-nadeem/>Farah Nadeem</a>,
<a href=/people/l/laura-burdick/>Laura Burdick</a>,
<a href=/people/g/greg-durrett/>Greg Durrett</a>,
<a href=/people/n/na-rae-han/>Na-Rae Han</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>N19-3</dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Minneapolis, Minnesota</dd><dt>Venue:</dt><dd><a href=/venues/naacl/>NAACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/N19-3>https://aclanthology.org/N19-3</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/N19-3.pdf>https://aclanthology.org/N19-3.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/N19-3.pdf title="Open PDF of 'Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2019+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics%3A+Student+Research+Workshop" title="Search for 'Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3000/>Proceedings of the 2019 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Student Research Workshop</a></strong><br><a href=/people/s/sudipta-kar/>Sudipta Kar</a>
|
<a href=/people/f/farah-nadeem/>Farah Nadeem</a>
|
<a href=/people/l/laura-burdick/>Laura Burdick</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/n/na-rae-han/>Na-Rae Han</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3001/>Is It Dish Washer Safe? Automatically Answering Yes / No Questions Using Customer Reviews</a></strong><br><a href=/people/d/daria-dzendzik/>Daria Dzendzik</a>
|
<a href=/people/c/carl-vogel/>Carl Vogel</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3001><div class="card-body p-3 small">It has become commonplace for people to share their opinions about all kinds of products by posting reviews online. It has also become commonplace for potential customers to do research about the quality and limitations of these <a href=https://en.wikipedia.org/wiki/Product_(business)>products</a> by posting questions online. We test the extent to which reviews are useful in <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering</a> by combining two <a href=https://en.wikipedia.org/wiki/Amazon_Web_Services>Amazon datasets</a> and focusing our attention on <a href=https://en.wikipedia.org/wiki/Yes&#8211;no_question>yes / no questions</a>. A manual analysis of 400 cases reveals that the reviews directly contain the answer to the question just over a third of the time. Preliminary reading comprehension experiments with this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> prove inconclusive, with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in the range 50-66 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-3002.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-3002.Note.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347400639 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-3002/>Identifying and Reducing Gender Bias in Word-Level Language Models</a></strong><br><a href=/people/s/shikha-bordia/>Shikha Bordia</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3002><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> exhibit socially problematic biases, which can be propagated or amplified in the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on such <a href=https://en.wikipedia.org/wiki/Data>data</a>. For example, doctor cooccurs more frequently with <a href=https://en.wikipedia.org/wiki/Sex_and_gender_distinction>male pronouns</a> than <a href=https://en.wikipedia.org/wiki/Sex_and_gender_distinction>female pronouns</a>. In this study we (i) propose a metric to measure <a href=https://en.wikipedia.org/wiki/Gender>gender bias</a> ; (ii) measure bias in a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> and the text generated from a recurrent neural network language model trained on the <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> ; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes <a href=https://en.wikipedia.org/wiki/Gender>gender</a> ; (iv) finally, evaluate efficacy of our proposed method on reducing <a href=https://en.wikipedia.org/wiki/Gender>gender bias</a>. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> becomes unstable as the perplexity increases. We replicate this study on three training corporaPenn Treebank, WikiText-2, and CNN / Daily Mailresulting in similar conclusions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3010/>Computational Investigations of Pragmatic Effects in <a href=https://en.wikipedia.org/wiki/Natural_language>Natural Language</a></a></strong><br><a href=/people/j/jad-kabbara/>Jad Kabbara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3010><div class="card-body p-3 small">Semantics and <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatics</a> are two complimentary and intertwined aspects of meaning in language. The <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>former</a> is concerned with the literal (context-free) meaning of words and sentences, the <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>latter</a> focuses on the intended meaning, one that is context-dependent. While NLP research has focused in the past mostly on <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, the goal of this thesis is to develop computational models that leverage this pragmatic knowledge in language that is crucial to performing many NLP tasks correctly. In this proposal, we begin by reviewing the current progress in this thesis, namely, on the tasks of definiteness prediction and adverbial presupposition triggering. Then we discuss the proposed research for the remainder of the thesis which builds on this progress towards the goal of building better and more pragmatically-aware natural language generation and understanding systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3011 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-3011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-3011/>SEDTWik : Segmentation-based Event Detection from <a href=https://en.wikipedia.org/wiki/Twitter>Tweets</a> Using Wikipedia<span class=acl-fixed-case>SEDTW</span>ik: Segmentation-based Event Detection from Tweets Using <span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/k/keval-morabia/>Keval Morabia</a>
|
<a href=/people/n/neti-lalita-bhanu-murthy/>Neti Lalita Bhanu Murthy</a>
|
<a href=/people/a/aruna-malapati/>Aruna Malapati</a>
|
<a href=/people/s/surender-samant/>Surender Samant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3011><div class="card-body p-3 small">Event Detection has been one of the research areas in <a href=https://en.wikipedia.org/wiki/Text_mining>Text Mining</a> that has attracted attention during this decade due to the widespread availability of social media data specifically twitter data. Twitter has become a major source for information about real-world events because of the use of <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> and the small word limit of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> that ensures concise presentation of events. Previous works on event detection from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> are either applicable to detect localized events or breaking news only or miss out on many important events. This paper presents the problems associated with event detection from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> and a tweet-segmentation based system for event detection called SEDTWik, an extension to a previous work, that is able to detect newsworthy events occurring at different locations of the world from a wide range of categories. The main idea is to split each tweet and <a href=https://en.wikipedia.org/wiki/Hashtag>hash-tag</a> into segments, extract bursty segments, cluster them, and summarize them. We evaluated our results on the well-known Events2012 corpus and achieved state-of-the-art results. Keywords : Event detection, <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a>, <a href=https://en.wikipedia.org/wiki/Microblogging>Microblogging</a>, Tweet segmentation, <a href=https://en.wikipedia.org/wiki/Text_mining>Text Mining</a>, <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, <a href=https://en.wikipedia.org/wiki/Hashtag>Hashtag</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3012 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-3012.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355800547 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-3012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-3012/>Multimodal Machine Translation with Embedding Prediction</a></strong><br><a href=/people/t/tosho-hirasawa/>Tosho Hirasawa</a>
|
<a href=/people/h/hayahide-yamagishi/>Hayahide Yamagishi</a>
|
<a href=/people/y/yukio-matsumura/>Yukio Matsumura</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3012><div class="card-body p-3 small">Multimodal machine translation is an attractive application of neural machine translation (NMT). It helps computers to deeply understand <a href=https://en.wikipedia.org/wiki/Visual_system>visual objects</a> and their relations with <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a>. However, multimodal NMT systems suffer from a shortage of available training data, resulting in poor performance for translating rare words. In NMT, pretrained word embeddings have been shown to improve NMT of low-resource domains, and a search-based approach is proposed to address the rare word problem. In this study, we effectively combine these two approaches in the context of multimodal NMT and explore how we can take full advantage of pretrained word embeddings to better translate rare words. We report overall performance improvements of 1.24 METEOR and 2.49 BLEU and achieve an improvement of 7.67 <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> for rare word translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3013/>Deep Learning and Sociophonetics : Automatic Coding of Rhoticity Using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a></a></strong><br><a href=/people/s/sarah-gupta/>Sarah Gupta</a>
|
<a href=/people/a/anthony-dipadova/>Anthony DiPadova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3013><div class="card-body p-3 small">Automated extraction methods are widely available for vowels, but automated methods for coding rhoticity have lagged far behind. R-fulness versus <a href=https://en.wikipedia.org/wiki/R-lessness>r-lessness</a> (in words like park, store, etc.) is a classic and frequently cited variable, but it is still commonly coded by human analysts rather than automated methods. Human-coding requires extensive resources and lacks replicability, making it difficult to compare large datasets across research groups. Can reliable automated methods be developed to aid in coding rhoticity? In this study, we use <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> / Deep Learning, training our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on 208 Boston-area speakers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3014/>Data Augmentation by Data Noising for Open-vocabulary Slots in Spoken Language Understanding</a></strong><br><a href=/people/h/hwa-yeon-kim/>Hwa-Yeon Kim</a>
|
<a href=/people/y/yoon-hyung-roh/>Yoon-Hyung Roh</a>
|
<a href=/people/y/young-gil-kim/>Young-Kil Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3014><div class="card-body p-3 small">One of the main challenges in Spoken Language Understanding (SLU) is dealing with &#8216;open-vocabulary&#8217; slots. Recently, SLU models based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> were proposed, but it is still difficult to recognize the slots of unknown words or &#8216;open-vocabulary&#8217; slots because of the high cost of creating a manually tagged SLU dataset. This paper proposes data noising, which reflects the characteristics of the &#8216;open-vocabulary&#8217; slots, for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. We applied it to an attention based bi-directional recurrent neural network (Liu and Lane, 2016) and experimented with three datasets : Airline Travel Information System (ATIS), Snips, and MIT-Restaurant. We achieved performance improvements of up to 0.57 % and 3.25 in intent prediction (accuracy) and slot filling (f1-score), respectively. Our method is advantageous because <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> does not require additional memory and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can be applied simultaneously with the training process of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3015/>Expectation and Locality Effects in the Prediction of Disfluent Fillers and Repairs in <a href=https://en.wikipedia.org/wiki/English_language>English Speech</a><span class=acl-fixed-case>E</span>nglish Speech</a></strong><br><a href=/people/s/samvit-dammalapati/>Samvit Dammalapati</a>
|
<a href=/people/r/rajakrishnan-rajkumar/>Rajakrishnan Rajkumar</a>
|
<a href=/people/s/sumeet-agarwal/>Sumeet Agarwal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3015><div class="card-body p-3 small">This study examines the role of three influential theories of <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a>, viz., Surprisal Theory, Uniform Information Density (UID) hypothesis and Dependency Locality Theory (DLT), in predicting disfluencies in speech production. To this end, we incorporate features based on lexical surprisal, word duration and DLT integration and storage costs into logistic regression classifiers aimed to predict disfluencies in the Switchboard corpus of English conversational speech. We find that <a href=https://en.wikipedia.org/wiki/Speech_disfluency>disfluencies</a> occur in the face of upcoming difficulties and speakers tend to handle this by lessening <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a> before <a href=https://en.wikipedia.org/wiki/Speech_disfluency>disfluencies</a> occur. Further, we see that reparandums behave differently from disfluent fillers possibly due to the lessening of the <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a> also happening in the word choice of the reparandum, i.e., in the <a href=https://en.wikipedia.org/wiki/Disfluency>disfluency</a> itself. While the UID hypothesis does not seem to play a significant role in disfluency prediction, lexical surprisal and DLT costs do give promising results in explaining <a href=https://en.wikipedia.org/wiki/Language_production>language production</a>. Further, we also find that as a means to lessen <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a> for upcoming difficulties speakers take more time on words preceding disfluencies, making duration a key element in understanding <a href=https://en.wikipedia.org/wiki/Speech_disfluency>disfluencies</a>.<i>viz.</i>, Surprisal Theory, Uniform Information Density (UID) hypothesis and Dependency Locality Theory (DLT), in predicting disfluencies in speech production. To this end, we incorporate features based on lexical surprisal, word duration and DLT integration and storage costs into logistic regression classifiers aimed to predict disfluencies in the Switchboard corpus of English conversational speech. We find that disfluencies occur in the face of upcoming difficulties and speakers tend to handle this by lessening cognitive load before disfluencies occur. Further, we see that reparandums behave differently from disfluent fillers possibly due to the lessening of the cognitive load also happening in the word choice of the reparandum, i.e., in the disfluency itself. While the UID hypothesis does not seem to play a significant role in disfluency prediction, lexical surprisal and DLT costs do give promising results in explaining language production. Further, we also find that as a means to lessen cognitive load for upcoming difficulties speakers take more time on words preceding disfluencies, making duration a key element in understanding disfluencies.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>