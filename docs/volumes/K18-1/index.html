<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 22nd Conference on Computational Natural Language Learning - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/K18-1.pdf>Proceedings of the 22nd Conference on Computational Natural Language Learning</a></h2><p class=lead><a href=/people/a/anna-korhonen/>Anna Korhonen</a>,
<a href=/people/i/ivan-titov/>Ivan Titov</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>K18-1</dd><dt>Month:</dt><dd>October</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Brussels, Belgium</dd><dt>Venue:</dt><dd><a href=/venues/conll/>CoNLL</a></dd><dt>SIG:</dt><dd><a href=/sigs/signll/>SIGNLL</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/K18-1>https://aclanthology.org/K18-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/K18-1.pdf>https://aclanthology.org/K18-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/K18-1.pdf title="Open PDF of 'Proceedings of the 22nd Conference on Computational Natural Language Learning'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+22nd+Conference+on+Computational+Natural+Language+Learning" title="Search for 'Proceedings of the 22nd Conference on Computational Natural Language Learning' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1000/>Proceedings of the 22nd Conference on Computational Natural Language Learning</a></strong><br><a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1001/>Embedded-State Latent Conditional Random Fields for Sequence Labeling</a></strong><br><a href=/people/d/dung-thai/>Dung Thai</a>
|
<a href=/people/s/sree-harsha-ramesh/>Sree Harsha Ramesh</a>
|
<a href=/people/s/shikhar-murty/>Shikhar Murty</a>
|
<a href=/people/l/luke-vilnis/>Luke Vilnis</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1001><div class="card-body p-3 small">Complex textual information extraction tasks are often posed as <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> or <a href=https://en.wikipedia.org/wiki/Shallow_parsing>shallow parsing</a>, where fields are extracted using local labels made consistent through <a href=https://en.wikipedia.org/wiki/Statistical_inference>probabilistic inference</a> in a <a href=https://en.wikipedia.org/wiki/Graphical_model>graphical model</a> with constrained transitions. Recently, it has become common to locally parametrize these models using rich features extracted by <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> (such as LSTM), while enforcing consistent outputs through a simple linear-chain model, representing <a href=https://en.wikipedia.org/wiki/Markov_chain>Markovian dependencies</a> between successive labels. However, the simple <a href=https://en.wikipedia.org/wiki/Graphical_model>graphical model structure</a> belies the often complex <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>non-local constraints</a> between output labels. For example, many fields, such as a first name, can only occur a fixed number of times, or in the presence of other fields. While RNNs have provided increasingly powerful context-aware local features for sequence tagging, they have yet to be integrated with a global graphical model of similar expressivity in the output distribution. Our model goes beyond the linear chain CRF to incorporate multiple hidden states per output label, but parametrizes them parsimoniously with low-rank log-potential scoring matrices, effectively learning an embedding space for hidden states. This augmented latent space of inference variables complements the rich feature representation of the RNN, and allows exact global inference obeying complex, learned non-local output constraints. We experiment with several datasets and show that the model outperforms baseline CRF+RNN models when global output constraints are necessary at inference-time, and explore the interpretable latent structure.<i>shallow parsing</i>, where fields are extracted using local labels made consistent through probabilistic inference in a graphical model with constrained transitions. Recently, it has become common to locally parametrize these models using rich features extracted by recurrent neural networks (such as LSTM), while enforcing consistent outputs through a simple linear-chain model, representing Markovian dependencies between successive labels. However, the simple graphical model structure belies the often complex non-local constraints between output labels. For example, many fields, such as a first name, can only occur a fixed number of times, or in the presence of other fields. While RNNs have provided increasingly powerful context-aware local features for sequence tagging, they have yet to be integrated with a global graphical model of similar expressivity in the output distribution. Our model goes beyond the linear chain CRF to incorporate multiple hidden states per output label, but parametrizes them parsimoniously with low-rank log-potential scoring matrices, effectively learning an embedding space for hidden states. This augmented latent space of inference variables complements the rich feature representation of the RNN, and allows exact global inference obeying complex, learned non-local output constraints. We experiment with several datasets and show that the model outperforms baseline CRF+RNN models when global output constraints are necessary at inference-time, and explore the interpretable latent structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1002/>Continuous Word Embedding Fusion via <a href=https://en.wikipedia.org/wiki/Spectral_decomposition>Spectral Decomposition</a></a></strong><br><a href=/people/t/tianfan-fu/>Tianfan Fu</a>
|
<a href=/people/c/cheng-zhang/>Cheng Zhang</a>
|
<a href=/people/s/stephan-mandt/>Stephan Mandt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1002><div class="card-body p-3 small">Word embeddings have become a mainstream tool in <a href=https://en.wikipedia.org/wiki/Statistical_natural_language_processing>statistical natural language processing</a>. Practitioners often use pre-trained word vectors, which were trained on large generic text corpora, and which are readily available on the web. However, pre-trained word vectors oftentimes lack important words from specific domains. It is therefore often desirable to extend the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> and embed new words into a set of pre-trained word vectors. In this paper, we present an efficient method for including new words from a specialized corpus, containing <a href=https://en.wikipedia.org/wiki/Word_formation>new words</a>, into pre-trained generic word embeddings. We build on the established view of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> as <a href=https://en.wikipedia.org/wiki/Matrix_decomposition>matrix factorizations</a> to present a spectral algorithm for this task. Experiments on several domain-specific corpora with specialized vocabularies demonstrate that our method is able to embed the new words efficiently into the original embedding space. Compared to competing <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a>, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> is faster, parameter-free, and deterministic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1004/>A Trio Neural Model for Dynamic Entity Relatedness Ranking</a></strong><br><a href=/people/t/tu-nguyen/>Tu Nguyen</a>
|
<a href=/people/t/tuan-tran/>Tuan Tran</a>
|
<a href=/people/w/wolfgang-nejdl/>Wolfgang Nejdl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1004><div class="card-body p-3 small">Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity relatedness</a> in a static setting and <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised manner</a>. However, entities in real-world are often involved in many different relationships, consequently entity relations are very dynamic over time. In this work, we propose a neural network-based approach that leverages public attention as <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on <a href=https://en.wikipedia.org/wiki/Data_set>large-scale datasets</a>, we demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves better results than competitive baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1005/>A Unified Neural Network Model for Geolocating Twitter Users<span class=acl-fixed-case>T</span>witter Users</a></strong><br><a href=/people/m/mohammad-ebrahimi/>Mohammad Ebrahimi</a>
|
<a href=/people/e/elaheh-shafieibavani/>Elaheh ShafieiBavani</a>
|
<a href=/people/r/raymond-wong/>Raymond Wong</a>
|
<a href=/people/f/fang-chen/>Fang Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1005><div class="card-body p-3 small">Locations of social media users are important to many applications such as rapid disaster response, <a href=https://en.wikipedia.org/wiki/Targeted_advertising>targeted advertisement</a>, and <a href=https://en.wikipedia.org/wiki/Recommender_system>news recommendation</a>. However, many users do not share their exact <a href=https://en.wikipedia.org/wiki/Geographic_coordinate_system>geographical coordinates</a> due to reasons such as privacy concerns. The lack of explicit location information has motivated a growing body of research in recent years looking at different automatic ways of determining the user&#8217;s primary location. In this paper, we propose a unified user geolocation method which relies on a fusion of neural networks. Our joint model incorporates different types of available information including tweet text, user network, and <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> to predict users&#8217; locations. Moreover, we utilize a bidirectional LSTM network augmented with an attention mechanism to identify the most location indicative words in textual content of tweets. The experiments demonstrate that our approach achieves state-of-the-art performance over two Twitter benchmark geolocation datasets. We also conduct an ablation study to evaluate the contribution of each type of information in user geolocation performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1008/>From Strings to Other Things : Linking the Neighborhood and Transposition Effects in Word Reading</a></strong><br><a href=/people/s/stephan-tulkens/>Stéphan Tulkens</a>
|
<a href=/people/d/dominiek-sandra/>Dominiek Sandra</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1008><div class="card-body p-3 small">We investigate the relation between the transposition and deletion effects in word reading, i.e., the finding that readers can successfully read SLAT as SALT, or WRK as WORK, and the <a href=https://en.wikipedia.org/wiki/Neighbourhood_effect>neighborhood effect</a>. In particular, we investigate whether lexical orthographic neighborhoods take into account transposition and <a href=https://en.wikipedia.org/wiki/Deletion_(linguistics)>deletion</a> in determining neighbors. If this is the case, it is more likely that the <a href=https://en.wikipedia.org/wiki/Neighbourhood_effect>neighborhood effect</a> takes place early during processing, and does not solely rely on similarity of internal representations. We introduce a new neighborhood measure, rd20, which can be used to quantify <a href=https://en.wikipedia.org/wiki/Neighbourhood_effect>neighborhood effects</a> over arbitrary feature spaces. We calculate the rd20 over large sets of words in three languages using various feature sets and show that feature sets that do not allow for transposition or <a href=https://en.wikipedia.org/wiki/Deletion_(linguistics)>deletion</a> explain more variance in Reaction Time (RT) measurements. We also show that the rd20 can be calculated using the hidden state representations of an Multi-Layer Perceptron, and show that these explain less variance than the raw features. We conclude that the <a href=https://en.wikipedia.org/wiki/Neighbourhood_effect>neighborhood effect</a> is unlikely to have a perceptual basis, but is more likely to be the result of items co-activating after recognition. All code is available at :<url>www.github.com/clips/conll2018</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1009/>Global Attention for Name Tagging</a></strong><br><a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/s/spencer-whitehead/>Spencer Whitehead</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1009><div class="card-body p-3 small">Many name tagging approaches use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>local contextual information</a> with much success, but can fail when the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>local context</a> is ambiguous or limited. We present a new <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> to improve <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>name tagging</a> by utilizing local, document-level, and corpus-level contextual information. For each word, we retrieve document-level context from other sentences within the same document and corpus-level context from sentences in other documents. We propose a model that learns to incorporate document-level and corpus-level contextual information alongside local contextual information via document-level and corpus-level attentions, which dynamically weight their respective contextual information and determines the influence of this <a href=https://en.wikipedia.org/wiki/Information>information</a> through gating mechanisms. Experiments on benchmark datasets show the effectiveness of our approach, which achieves state-of-the-art results for <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> on the CoNLL-2002 and CoNLL-2003 datasets. We will make our code and pre-trained models publicly available for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1012/>Uncovering Code-Mixed Challenges : A Framework for Linguistically Driven Question Generation and Neural Based Question Answering</a></strong><br><a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/p/pabitra-lenka/>Pabitra Lenka</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1012><div class="card-body p-3 small">Existing research on question answering (QA) and comprehension reading (RC) are mainly focused on the resource-rich language like <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In recent times, the rapid growth of multi-lingual web content has posed several challenges to the existing <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a>. Code-mixing is one such challenge that makes the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> more complex. In this paper, we propose a linguistically motivated technique for code-mixed question generation (CMQG) and a neural network based architecture for code-mixed question answering (CMQA). For evaluation, we manually create the code-mixed questions for Hindi-English language pair. In order to show the effectiveness of our neural network based CMQA technique, we utilize two benchmark datasets, SQuAD and MMQA. Experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves encouraging performance on CMQG and CMQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1014/>Commonsense Knowledge Base Completion and Generation</a></strong><br><a href=/people/i/itsumi-saito/>Itsumi Saito</a>
|
<a href=/people/k/kyosuke-nishida/>Kyosuke Nishida</a>
|
<a href=/people/h/hisako-asano/>Hisako Asano</a>
|
<a href=/people/j/junji-tomita/>Junji Tomita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1014><div class="card-body p-3 small">This study focuses on acquisition of commonsense knowledge. A previous study proposed a commonsense knowledge base completion (CKB completion) method that predicts a confidence score of for triplet-style knowledge for improving the coverage of CKBs. To improve the accuracy of CKB completion and expand the size of CKBs, we formulate a new commonsense knowledge base generation task (CKB generation) and propose a joint learning method that incorporates both CKB completion and CKB generation. Experimental results show that the joint learning method improved completion accuracy and the generation model created reasonable knowledge. Our generation model could also be used to augment data and improve the accuracy of completion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1015" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1015/>Active Learning for Interactive Neural Machine Translation of Data Streams</a></strong><br><a href=/people/a/alvaro-peris/>Álvaro Peris</a>
|
<a href=/people/f/francisco-casacuberta/>Francisco Casacuberta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1015><div class="card-body p-3 small">We study the application of active learning techniques to the translation of unbounded data streams via interactive neural machine translation. The main idea is to select, from an unbounded stream of source sentences, those worth to be supervised by a human agent. The user will interactively translate those samples. Once validated, these <a href=https://en.wikipedia.org/wiki/Data>data</a> is useful for adapting the neural machine translation model. We propose two novel <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for selecting the samples to be validated. We exploit the information from the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> of a neural machine translation system. Our experiments show that the inclusion of active learning techniques into this <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> allows to reduce the effort required during the process, while increasing the quality of the translation system. Moreover, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> enables to balance the human effort required for achieving a certain translation quality. Moreover, our <a href=https://en.wikipedia.org/wiki/Nervous_system>neural system</a> outperforms classical approaches by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1016/>Churn Intent Detection in Multilingual Chatbot Conversations and <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/c/christian-abbet/>Christian Abbet</a>
|
<a href=/people/m/meryem-mhamdi/>Meryem M’hamdi</a>
|
<a href=/people/a/athanasios-giannakopoulos/>Athanasios Giannakopoulos</a>
|
<a href=/people/r/robert-west/>Robert West</a>
|
<a href=/people/a/andreea-hossmann/>Andreea Hossmann</a>
|
<a href=/people/m/michael-baeriswyl/>Michael Baeriswyl</a>
|
<a href=/people/c/claudiu-musat/>Claudiu Musat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1016><div class="card-body p-3 small">We propose a new method to detect when users express the intent to leave a service, also known as churn. While previous work focuses solely on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, we show that this <a href=https://en.wikipedia.org/wiki/Intention>intent</a> can be detected in chatbot conversations. As companies increasingly rely on <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> they need an overview of potentially churny users. To this end, we crowdsource and publish a dataset of churn intent expressions in chatbot interactions in <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We show that <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on <a href=https://en.wikipedia.org/wiki/Social_media>social media data</a> can detect the same intent in the context of <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>. We introduce a classification architecture that outperforms existing work on churn intent detection in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Moreover, we show that, using bilingual word embeddings, a system trained on combined English and German data outperforms monolingual approaches. As the only existing dataset is in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, we crowdsource and publish a novel dataset of German tweets. We thus underline the universal aspect of the problem, as examples of <a href=https://en.wikipedia.org/wiki/Churn>churn intent</a> in English help us identify <a href=https://en.wikipedia.org/wiki/Churn>churn</a> in German tweets and chatbot conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1020/>Latent Entities Extraction : How to Extract Entities that Do Not Appear in the Text?</a></strong><br><a href=/people/e/eylon-shoshan/>Eylon Shoshan</a>
|
<a href=/people/k/kira-radinsky/>Kira Radinsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1020><div class="card-body p-3 small">Named-entity Recognition (NER) is an important task in the NLP field, and is widely used to solve many challenges. However, in many scenarios, not all of the entities are explicitly mentioned in the text. Sometimes they could be inferred from the context or from other indicative words. Consider the following sentence : CMA can easily hydrolyze into free acetic acid. Although water is not mentioned explicitly, one can infer that <a href=https://en.wikipedia.org/wiki/Hydrogen_peroxide>H2O</a> is an entity involved in the process. In this work, we present the problem of Latent Entities Extraction (LEE). We present several methods for determining whether entities are discussed in a text, even though, potentially, they are not explicitly written. Specifically, we design a neural model that handles extraction of multiple entities jointly. We show that our model, along with multi-task learning approach and a novel task grouping algorithm, reaches high performance in identifying latent entities. Our experiments are conducted on a large <a href=https://en.wikipedia.org/wiki/Data_set>biological dataset</a> from the <a href=https://en.wikipedia.org/wiki/Biochemistry>biochemical field</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains text descriptions of biological processes, and for each <a href=https://en.wikipedia.org/wiki/Process_(engineering)>process</a>, all of the involved entities in the process are labeled, including implicitly mentioned ones. We believe LEE is a task that will significantly improve many NER and subsequent applications and improve text understanding and inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1025/>Multi-Modal Sequence Fusion via Recursive Attention for Emotion Recognition</a></strong><br><a href=/people/r/rory-beard/>Rory Beard</a>
|
<a href=/people/r/ritwik-das/>Ritwik Das</a>
|
<a href=/people/r/raymond-w-m-ng/>Raymond W. M. Ng</a>
|
<a href=/people/p/p-g-keerthana-gopalakrishnan/>P. G. Keerthana Gopalakrishnan</a>
|
<a href=/people/l/luka-eerens/>Luka Eerens</a>
|
<a href=/people/p/pawel-swietojanski/>Pawel Swietojanski</a>
|
<a href=/people/o/ondrej-miksik/>Ondrej Miksik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1025><div class="card-body p-3 small">Natural human communication is nuanced and inherently multi-modal. Humans possess specialised sensoria for processing vocal, visual, and linguistic, and para-linguistic information, but form an intricately fused percept of the multi-modal data stream to provide a holistic representation. Analysis of emotional content in <a href=https://en.wikipedia.org/wiki/Face-to-face_interaction>face-to-face communication</a> is a cognitive task to which humans are particularly attuned, given its sociological importance, and poses a difficult challenge for machine emulation due to the subtlety and expressive variability of cross-modal cues. Inspired by the empirical success of recent so-called End-To-End Memory Networks and related works, we propose an approach based on recursive multi-attention with a shared external memory updated over multiple gated iterations of analysis. We evaluate our model across several large multi-modal datasets and show that global contextualised memory with gated memory update can effectively achieve <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1026/>Using Sparse Semantic Embeddings Learned from Multimodal Text and Image Data to Model Human Conceptual Knowledge</a></strong><br><a href=/people/s/steven-derby/>Steven Derby</a>
|
<a href=/people/p/paul-miller/>Paul Miller</a>
|
<a href=/people/b/brian-murphy/>Brian Murphy</a>
|
<a href=/people/b/barry-devereux/>Barry Devereux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1026><div class="card-body p-3 small">Distributional models provide a convenient way to model <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> using dense embedding spaces derived from <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning algorithms</a>. However, the dimensions of dense embedding spaces are not designed to resemble human semantic knowledge. Moreover, embeddings are often built from a single source of information (typically text data), even though neurocognitive research suggests that <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> is deeply linked to both <a href=https://en.wikipedia.org/wiki/Language>language</a> and perception. In this paper, we combine multimodal information from both text and image-based representations derived from state-of-the-art distributional models to produce sparse, interpretable vectors using Joint Non-Negative Sparse Embedding. Through in-depth analyses comparing these sparse models to human-derived behavioural and neuroimaging data, we demonstrate their ability to predict interpretable linguistic descriptions of human ground-truth semantic knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1031/>Sentence-Level Fluency Evaluation : References Help, But Can Be Spared !</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/s/sascha-rothe/>Sascha Rothe</a>
|
<a href=/people/k/katja-filippova/>Katja Filippova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1031><div class="card-body p-3 small">Motivated by recent findings on the probabilistic modeling of acceptability judgments, we propose syntactic log-odds ratio (SLOR), a normalized language model score, as a metric for referenceless fluency evaluation of natural language generation output at the sentence level. We further introduce WPSLOR, a novel WordPiece-based version, which harnesses a more compact <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. Even though word-overlap metrics like ROUGE are computed with the help of hand-written references, our referenceless methods obtain a significantly higher correlation with human fluency scores on a benchmark dataset of compressed sentences. Finally, we present ROUGE-LM, a reference-based metric which is a natural extension of WPSLOR to the case of available references. We show that ROUGE-LM yields a significantly higher correlation with human judgments than all baseline metrics, including WPSLOR on its own.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1032" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1032/>Predefined Sparseness in Recurrent Sequence Models</a></strong><br><a href=/people/t/thomas-demeester/>Thomas Demeester</a>
|
<a href=/people/j/johannes-deleu/>Johannes Deleu</a>
|
<a href=/people/f/frederic-godin/>Fréderic Godin</a>
|
<a href=/people/c/chris-develder/>Chris Develder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1032><div class="card-body p-3 small">Inducing sparseness while training <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> has been shown to yield <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> with a lower <a href=https://en.wikipedia.org/wiki/Memory_footprint>memory footprint</a> but similar effectiveness to dense models. However, <a href=https://en.wikipedia.org/wiki/Sparseness>sparseness</a> is typically induced starting from a dense model, and thus this advantage does not hold during training. We propose techniques to enforce <a href=https://en.wikipedia.org/wiki/Sparseness>sparseness</a> upfront in recurrent sequence models for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>, to also benefit <a href=https://en.wikipedia.org/wiki/Training>training</a>. First, in <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, we show how to increase hidden state sizes in recurrent layers without increasing the number of parameters, leading to more expressive models. Second, for <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, we show that word embeddings with predefined sparseness lead to similar performance as dense embeddings, at a fraction of the number of trainable parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1033/>Learning to Actively Learn Neural Machine Translation</a></strong><br><a href=/people/m/ming-liu/>Ming Liu</a>
|
<a href=/people/w/wray-buntine/>Wray Buntine</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1033><div class="card-body p-3 small">Traditional active learning (AL) methods for machine translation (MT) rely on <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>heuristics</a>. However, these <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>heuristics</a> are limited when the characteristics of the <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>MT problem</a> change due to e.g. the language pair or the amount of the initial bitext. In this paper, we present a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> to learn sentence selection strategies for neural MT. We train the AL query strategy using a high-resource language-pair based on AL simulations, and then transfer it to the low-resource language-pair of interest. The learned query strategy capitalizes on the shared characteristics between the language pairs to make an effective use of the AL budget. Our experiments on three language-pairs confirms that our method is more effective than strong heuristic-based methods in various conditions, including cold-start and warm-start as well as small and extremely small data conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1034" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1034/>Upcycle Your OCR : Reusing OCRs for Post-OCR Text Correction in Romanised Sanskrit<span class=acl-fixed-case>OCR</span>: Reusing <span class=acl-fixed-case>OCR</span>s for Post-<span class=acl-fixed-case>OCR</span> Text Correction in <span class=acl-fixed-case>R</span>omanised <span class=acl-fixed-case>S</span>anskrit</a></strong><br><a href=/people/a/amrith-krishna/>Amrith Krishna</a>
|
<a href=/people/b/bodhisattwa-p-majumder/>Bodhisattwa P. Majumder</a>
|
<a href=/people/r/rajesh-bhat/>Rajesh Bhat</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1034><div class="card-body p-3 small">We propose a post-OCR text correction approach for digitising texts in <a href=https://en.wikipedia.org/wiki/Romanization_of_Sanskrit>Romanised Sanskrit</a>. Owing to the lack of resources our approach uses <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR models</a> trained for other languages written in Roman. Currently, there exists no dataset available for Romanised Sanskrit OCR. So, we bootstrap a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 430 <a href=https://en.wikipedia.org/wiki/Digital_image>images</a>, scanned in two different settings and their corresponding ground truth. For training, we synthetically generate training images for both the settings. We find that the use of copying mechanism (Gu et al., 2016) yields a percentage increase of 7.69 in Character Recognition Rate (CRR) than the current state of the art model in solving monotone sequence-to-sequence tasks (Schnober et al., 2016). We find that our <a href=https://en.wikipedia.org/wiki/System>system</a> is robust in combating OCR-prone errors, as it obtains a CRR of 87.01 % from an OCR output with CRR of 35.76 % for one of the dataset settings. A human judgement survey performed on the models shows that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> results in predictions which are faster to comprehend and faster to improve for a human than the other <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1039" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1039/>Lessons Learned in Multilingual Grounded Language Learning</a></strong><br><a href=/people/a/akos-kadar/>Ákos Kádár</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a>
|
<a href=/people/m/marc-alexandre-cote/>Marc-Alexandre Côté</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a>
|
<a href=/people/a/afra-alishahi/>Afra Alishahi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1039><div class="card-body p-3 small">Recent work has shown how to learn better visual-semantic embeddings by leveraging image descriptions in more than one language. Here, we investigate in detail which conditions affect the performance of this type of grounded language learning model. We show that multilingual training improves over bilingual training, and that low-resource languages benefit from training with higher-resource languages. We demonstrate that a multilingual model can be trained equally well on either translations or comparable sentence pairs, and that annotating the same set of images in multiple language enables further improvements via an additional caption-caption ranking objective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1040" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1040/>Unsupervised Sentence Compression using Denoising Auto-Encoders</a></strong><br><a href=/people/t/thibault-fevry/>Thibault Févry</a>
|
<a href=/people/j/jason-phang/>Jason Phang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1040><div class="card-body p-3 small">In sentence compression, the task of shortening sentences while retaining the original meaning, <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> tend to be trained on large corpora containing pairs of verbose and compressed sentences. To remove the need for paired corpora, we emulate a summarization task and add noise to extend sentences and train a denoising auto-encoder to recover the original, constructing an end-to-end training regime without the need for any examples of compressed sentences. We conduct a human evaluation of our model on a standard text summarization dataset and show that it performs comparably to a supervised baseline based on <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical correctness</a> and retention of meaning. Despite being exposed to no target data, our <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised models</a> learn to generate imperfect but reasonably readable sentence summaries. Although we underperform supervised models based on ROUGE scores, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are competitive with a supervised baseline based on human evaluation for grammatical correctness and retention of meaning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1042/>Linguistically-Based Deep Unstructured Question Answering</a></strong><br><a href=/people/a/ahmad-aghaebrahimian/>Ahmad Aghaebrahimian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1042><div class="card-body p-3 small">In this paper, we propose a new linguistically-based approach to answering non-factoid open-domain questions from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a>. First, we elaborate on an <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> for textual encoding based on which we introduce a deep end-to-end neural model. This architecture benefits from a bilateral attention mechanism which helps the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to focus on a question and the answer sentence at the same time for phrasal answer extraction. Second, we feed the output of a constituency parser into the model directly and integrate linguistic constituents into the network to help it concentrate on chunks of an answer rather than on its single words for generating more natural output. By optimizing this architecture, we managed to obtain near-to-human-performance results and competitive to a state-of-the-art system on SQuAD and MS-MARCO datasets respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1044/>Challenge or Empower : Revisiting Argumentation Quality in a News Editorial Corpus</a></strong><br><a href=/people/r/roxanne-el-baff/>Roxanne El Baff</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/k/khalid-al-khatib/>Khalid Al-Khatib</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1044><div class="card-body p-3 small">News editorials are said to shape public opinion, which makes them a powerful tool and an important source of political argumentation. However, rarely do <a href=https://en.wikipedia.org/wiki/Editorial>editorials</a> change anyone&#8217;s stance on an issue completely, nor do they tend to argue explicitly (but rather follow a subtle rhetorical strategy). So, what does argumentation quality mean for editorials then? We develop the notion that an effective <a href=https://en.wikipedia.org/wiki/Editorial>editorial</a> challenges readers with opposing stance, and at the same time empowers the arguing skills of readers that share the editorial&#8217;s stance or even challenges both sides. To study <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation quality</a> based on this notion, we introduce a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with 1000 <a href=https://en.wikipedia.org/wiki/Editorial>editorials</a> from the <a href=https://en.wikipedia.org/wiki/The_New_York_Times>New York Times</a>, annotated for their perceived effect along with the annotators&#8217; political orientations. Analyzing the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, we find that annotators with different orientation disagree on the effect significantly. While only 1 % of all editorials changed anyone&#8217;s stance, more than 5 % meet our notion. We conclude that our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> serves as a suitable resource for studying the <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation quality</a> of <a href=https://en.wikipedia.org/wiki/Editorial>news editorials</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1048/>Improving Response Selection in Multi-Turn Dialogue Systems by Incorporating Domain Knowledge</a></strong><br><a href=/people/d/debanjan-chaudhuri/>Debanjan Chaudhuri</a>
|
<a href=/people/a/agustinus-kristiadi/>Agustinus Kristiadi</a>
|
<a href=/people/j/jens-lehmann/>Jens Lehmann</a>
|
<a href=/people/a/asja-fischer/>Asja Fischer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1048><div class="card-body p-3 small">Building systems that can communicate with humans is a core problem in <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>Artificial Intelligence</a>. This work proposes a novel <a href=https://en.wikipedia.org/wiki/Neural_network>neural network architecture</a> for response selection in an end-to-end multi-turn conversational dialogue setting. The <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> applies context level attention and incorporates additional external knowledge provided by descriptions of domain-specific words. It uses a bi-directional Gated Recurrent Unit (GRU) for encoding context and responses and learns to attend over the context words given the latent response representation and vice versa. In addition, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> incorporates external domain specific information using another GRU for encoding the domain keyword descriptions. This allows better representation of domain-specific keywords in responses and hence improves the overall performance. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms all other state-of-the-art methods for response selection in multi-turn conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1049" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1049/>The Lifted Matrix-Space Model for Semantic Composition</a></strong><br><a href=/people/w/woojin-chung/>WooJin Chung</a>
|
<a href=/people/s/sheng-fu-wang/>Sheng-Fu Wang</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1049><div class="card-body p-3 small">Tree-structured neural network architectures for sentence encoding draw inspiration from the approach to semantic composition generally seen in <a href=https://en.wikipedia.org/wiki/Formal_linguistics>formal linguistics</a>, and have shown empirical improvements over comparable sequence models by doing so. Moreover, adding multiplicative interaction terms to the <a href=https://en.wikipedia.org/wiki/Function_composition>composition functions</a> in these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can yield significant further improvements. However, existing compositional approaches that adopt such a powerful <a href=https://en.wikipedia.org/wiki/Function_composition>composition function</a> scale poorly, with parameter counts exploding as model dimension or vocabulary size grows. We introduce the Lifted Matrix-Space model, which uses a global transformation to map vector word embeddings to matrices, which can then be composed via an operation based on matrix-matrix multiplication. Its <a href=https://en.wikipedia.org/wiki/Composition_function>composition function</a> effectively transmits a larger number of activations across layers with relatively few <a href=https://en.wikipedia.org/wiki/Parameter>model parameters</a>. We evaluate our model on the Stanford NLI corpus, the Multi-Genre NLI corpus, and the Stanford Sentiment Treebank and find that it consistently outperforms TreeLSTM (Tai et al., 2015), the previous best known composition function for tree-structured models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1050" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1050/>End-to-End Neural Entity Linking</a></strong><br><a href=/people/n/nikolaos-kolitsas/>Nikolaos Kolitsas</a>
|
<a href=/people/o/octavian-eugen-ganea/>Octavian-Eugen Ganea</a>
|
<a href=/people/t/thomas-hofmann/>Thomas Hofmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1050><div class="card-body p-3 small">Entity Linking (EL) is an essential task for <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic text understanding</a> and <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. Popular methods separately address the Mention Detection (MD) and Entity Disambiguation (ED) stages of EL, without leveraging their mutual dependency. We here propose the first neural end-to-end EL system that jointly discovers and links entities in a text document. The main idea is to consider all possible spans as potential mentions and learn contextual similarity scores over their entity candidates that are useful for both MD and ED decisions. Key components are context-aware mention embeddings, entity embeddings and a probabilistic mention-entity map, without demanding other engineered features. Empirically, we show that our <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end method</a> significantly outperforms popular systems on the Gerbil platform when enough training data is available. Conversely, if testing datasets follow different annotation conventions compared to the training set (e.g. queries/ tweets vs news documents), our ED model coupled with a traditional NER system offers the best or second best EL accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-1052/>Model Transfer with Explicit Knowledge of the Relation between Class Definitions</a></strong><br><a href=/people/h/hiyori-yoshikawa/>Hiyori Yoshikawa</a>
|
<a href=/people/t/tomoya-iwakura/>Tomoya Iwakura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1052><div class="card-body p-3 small">This paper investigates learning methods for multi-class classification using labeled data for the target classification scheme and another labeled data for a similar but different classification scheme (support scheme). We show that if we have prior knowledge about the relation between support and target classification schemes in the form of a class correspondence table, we can use it to improve the model performance further than the simple multi-task learning approach. Instead of learning the individual classification layers for the support and target schemes, the proposed method converts the class label of each example on the support scheme into a set of candidate class labels on the target scheme via the class correspondence table, and then uses the candidate labels to learn the classification layer for the target scheme. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> effectively learns the target schemes especially for the classes that have a tight connection to certain support classes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K18-1054" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K18-1054/>Neural Maximum Subgraph Parsing for Cross-Domain Semantic Dependency Analysis</a></strong><br><a href=/people/y/yufei-chen/>Yufei Chen</a>
|
<a href=/people/s/sheng-huang/>Sheng Huang</a>
|
<a href=/people/f/fang-wang/>Fang Wang</a>
|
<a href=/people/j/junjie-cao/>Junjie Cao</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-1054><div class="card-body p-3 small">We present experiments for cross-domain semantic dependency analysis with a neural Maximum Subgraph parser. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> targets 1-endpoint-crossing, pagenumber-2 graphs which are a good fit to semantic dependency graphs, and utilizes an efficient dynamic programming algorithm for decoding. For disambiguation, the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> associates words with BiLSTM vectors and utilizes these <a href=https://en.wikipedia.org/wiki/Euclidean_vector>vectors</a> to assign scores to candidate dependencies. We conduct experiments on the <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> from SemEval 2015 as well as Chinese CCGBank. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves very competitive results for both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. To improve the parsing performance on cross-domain texts, we propose a data-oriented method to explore the linguistic generality encoded in English Resource Grammar, which is a precisionoriented, hand-crafted HPSG grammar, in an implicit way. Experiments demonstrate the effectiveness of our data-oriented method across a wide range of conditions.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>