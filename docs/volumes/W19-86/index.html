<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 12th International Conference on Natural Language Generation - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the 12th International Conference on Natural Language Generation</h2><p class=lead><a href=/people/k/kees-van-deemter/>Kees van Deemter</a>,
<a href=/people/c/chenghua-lin/>Chenghua Lin</a>,
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W19-86</dd><dt>Month:</dt><dd>October–November</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Tokyo, Japan</dd><dt>Venues:</dt><dd><a href=/venues/inlg/>INLG</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd><a href=/sigs/siggen/>SIGGEN</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W19-86>https://aclanthology.org/W19-86</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+12th+International+Conference+on+Natural+Language+Generation" title="Search for 'Proceedings of the 12th International Conference on Natural Language Generation' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8600/>Proceedings of the 12th International Conference on Natural Language Generation</a></strong><br><a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8605/>Choosing between Long and Short Word Forms in Mandarin<span class=acl-fixed-case>M</span>andarin</a></strong><br><a href=/people/l/lin-li/>Lin Li</a>
|
<a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/d/denis-paperno/>Denis Paperno</a>
|
<a href=/people/j/jingyu-fan/>Jingyu Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8605><div class="card-body p-3 small">Between 80 % and 90 % of all Chinese words have long and short form such as / (lao-hu / hu, tiger) (Duanmu:2013). Consequently, the choice between long and short forms is a key problem for <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a> across <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLG</a>. Following an earlier work on <a href=https://en.wikipedia.org/wiki/Abbreviation>abbreviations</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a> (Mahowald et al, 2013), we bring a probabilistic perspective to these questions, using both a behavioral and a corpus-based approach. We hypothesized that there is a higher probability of choosing short form in supportive context than in neutral context in <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin</a>. Consistent with our prediction, our findings revealed that predictability of contexts makes effect on speakers&#8217; long and short form choice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8609/>Importance of Search and Evaluation Strategies in Neural Dialogue Modeling</a></strong><br><a href=/people/i/ilia-kulikov/>Ilia Kulikov</a>
|
<a href=/people/a/alexander-miller/>Alexander Miller</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8609><div class="card-body p-3 small">We investigate the impact of <a href=https://en.wikipedia.org/wiki/Search_algorithm>search strategies</a> in neural dialogue modeling. We first compare two standard search algorithms, greedy and beam search, as well as our newly proposed iterative beam search which produces a more diverse set of candidate responses. We evaluate these strategies in realistic full conversations with humans and propose a model-based Bayesian calibration to address annotator bias. These conversations are analyzed using two automatic metrics : log-probabilities assigned by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and utterance diversity. Our experiments reveal that better <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithms</a> lead to higher rated conversations. However, finding the optimal selection mechanism to choose from a more diverse set of candidates is still an open question.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-8610" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-8610/>Towards Best Experiment Design for Evaluating Dialogue System Output</a></strong><br><a href=/people/s/sashank-santhanam/>Sashank Santhanam</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8610><div class="card-body p-3 small">To overcome the limitations of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automated metrics</a> (e.g. BLEU, METEOR) for evaluating dialogue systems, researchers typically use <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> to provide convergent evidence. While it has been demonstrated that <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> can suffer from the inconsistency of ratings, extant research has also found that the design of the evaluation task affects the consistency and quality of human judgments. We conduct a between-subjects study to understand the impact of four experiment conditions on human ratings of dialogue system output. In addition to discrete and continuous scale ratings, we also experiment with a novel application of Best-Worst scaling to dialogue evaluation. Through our systematic study with 40 crowdsourced workers in each task, we find that using continuous scales achieves more consistent ratings than Likert scale or ranking-based experiment design. Additionally, we find that factors such as time taken to complete the task and no prior experience of participating in similar studies of rating dialogue system output positively impact <a href=https://en.wikipedia.org/wiki/Consistency>consistency</a> and agreement amongst raters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8611/>A Tree-to-Sequence Model for Neural NLG in Task-Oriented Dialog<span class=acl-fixed-case>NLG</span> in Task-Oriented Dialog</a></strong><br><a href=/people/j/jinfeng-rao/>Jinfeng Rao</a>
|
<a href=/people/k/kartikeya-upasani/>Kartikeya Upasani</a>
|
<a href=/people/a/anusha-balakrishnan/>Anusha Balakrishnan</a>
|
<a href=/people/m/michael-white/>Michael White</a>
|
<a href=/people/a/anuj-kumar/>Anuj Kumar</a>
|
<a href=/people/r/rajen-subba/>Rajen Subba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8611><div class="card-body p-3 small">Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Sequence-to-sequence models on flat meaning representations (MR) have been dominant in this task, for example in the E2E NLG Challenge. Previous work has shown that a tree-structured MR can improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for better discourse-level structuring and sentence-level planning. In this work, we propose a tree-to-sequence model that uses a tree-LSTM encoder to leverage the <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structures</a> in the input MR, and further enhance the decoding by a structure-enhanced attention mechanism. In addition, we explore combining these enhancements with constrained decoding to improve semantic correctness. Our experiments not only show significant improvements over standard seq2seq baselines, but also is more data-efficient and generalizes better to hard scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8615 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8615/>MinWikiSplit : A Sentence Splitting Corpus with Minimal Propositions<span class=acl-fixed-case>M</span>in<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>S</span>plit: A Sentence Splitting Corpus with Minimal Propositions</a></strong><br><a href=/people/c/christina-niklaus/>Christina Niklaus</a>
|
<a href=/people/a/andre-freitas/>André Freitas</a>
|
<a href=/people/s/siegfried-handschuh/>Siegfried Handschuh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8615><div class="card-body p-3 small">We compiled a new sentence splitting corpus that is composed of 203 K pairs of aligned complex source and simplified target sentences. Contrary to previously proposed text simplification corpora, which contain only a small number of split examples, we present a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> where each input sentence is broken down into a set of minimal propositions, i.e. a sequence of sound, self-contained utterances with each of them presenting a minimal semantic unit that can not be further decomposed into meaningful propositions. This <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is useful for developing sentence splitting approaches that learn how to transform sentences with a complex linguistic structure into a fine-grained representation of short sentences that present a simple and more regular structure which is easier to process for downstream applications and thus facilitates and improves their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8621 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8621/>Tell Me More : A Dataset of Visual Scene Description Sequences</a></strong><br><a href=/people/n/nikolai-ilinykh/>Nikolai Ilinykh</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8621><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of what we call image description sequences, which are multi-sentence descriptions of the contents of an image. These descriptions were collected in a pseudo-interactive setting, where the describer was told to describe the given image to a listener who needs to identify the image within a set of images, and who successively asks for more information. As we show, this setup produced nicely structured data that, we think, will be useful for learning models capable of planning and realising such description discourses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8622 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8622.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-8622/>A Closer Look at Recent Results of Verb Selection for Data-to-Text NLG<span class=acl-fixed-case>NLG</span></a></strong><br><a href=/people/g/guanyi-chen/>Guanyi Chen</a>
|
<a href=/people/j/jin-ge-yao/>Jin-Ge Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8622><div class="card-body p-3 small">Automatic natural language generation systems need to use the contextually-appropriate verbs when describing different kinds of facts or events, which has triggered research interest on verb selection for data-to-text generation. In this paper, we discuss a few limitations of the current task settings and the evaluation metrics. We also provide two simple, efficient, interpretable baseline approaches for statistical selection of trend verbs, which give a strong performance on both previously used <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation metrics</a> and our new <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8624 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8624/>BERT for Question Generation<span class=acl-fixed-case>BERT</span> for Question Generation</a></strong><br><a href=/people/y/ying-hong-chan/>Ying-Hong Chan</a>
|
<a href=/people/y/yao-chung-fan/>Yao-Chung Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8624><div class="card-body p-3 small">In this study, we investigate the employment of the pre-trained BERT language model to tackle question generation tasks. We introduce two neural architectures built on top of BERT for question generation tasks. The first one is a straightforward BERT employment, which reveals the defects of directly using BERT for text generation. And, the second one remedies the first one by restructuring the BERT employment into a sequential manner for taking information from previous decoded results. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are trained and evaluated on the question-answering dataset SQuAD. Experiment results show that our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields state-of-the-art performance which advances the BLEU4 score of existing best <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> from 16.85 to 18.91.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8627.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8627 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8627 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8627/>Neural Conversation Model Controllable by Given Dialogue Act Based on Adversarial Learning and Label-aware Objective</a></strong><br><a href=/people/s/seiya-kawano/>Seiya Kawano</a>
|
<a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8627><div class="card-body p-3 small">Building a controllable neural conversation model (NCM) is an important task. In this paper, we focus on controlling the responses of NCMs by using dialogue act labels of responses as conditions. We introduce an adversarial learning framework for the task of generating conditional responses with a new objective to a discriminator, which explicitly distinguishes sentences by using labels. This change strongly encourages the generation of label-conditioned sentences. We compared the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> with some existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for generating conditional responses. The experimental results show that our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> has higher <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a> for dialogue acts even though it has higher or comparable naturalness to existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8632.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8632 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8632 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8632/>Margin Call : an Accessible Web-based Text Viewer with Generated Paragraph Summaries in the Margin</a></strong><br><a href=/people/n/naba-rizvi/>Naba Rizvi</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/l/lidan-wang/>Lidan Wang</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8632><div class="card-body p-3 small">We present <a href=https://en.wikipedia.org/wiki/Margin_Call>Margin Call</a>, a web-based text viewer that automatically generates short summaries for each paragraph of the text and displays the summaries in the margin of the text next to the corresponding paragraph. On the back-end, the summarizer first identifies the most important sentence for each paragraph in the text file uploaded by the user. The selected sentence is then automatically compressed to produce the short summary. The resulting summary is a few words long. The displayed summaries can help the user understand and retrieve information faster from the text, while increasing the retention of information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8634.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8634 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8634 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8634/>Personalized Substitution Ranking for Lexical Simplification</a></strong><br><a href=/people/j/john-s-y-lee/>John Lee</a>
|
<a href=/people/c/chak-yan-yeung/>Chak Yan Yeung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8634><div class="card-body p-3 small">A lexical simplification (LS) system substitutes difficult words in a text with simpler ones to make it easier for the user to understand. In the typical LS pipeline, the Substitution Ranking step determines the best substitution out of a set of candidates. Most current <a href=https://en.wikipedia.org/wiki/System>systems</a> do not consider the user&#8217;s vocabulary proficiency, and always aim for the simplest candidate. This approach may overlook less-simple candidates that the user can understand, and that are semantically closer to the original word. We propose a personalized approach for Substitution Ranking to identify the candidate that is the closest synonym and is non-complex for the user. In experiments on learners of <a href=https://en.wikipedia.org/wiki/English_language>English</a> at different proficiency levels, we show that this approach enhances the semantic faithfulness of the output, at the cost of a relatively small increase in the number of complex words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8635.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8635 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8635 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8635/>Revisiting the Binary Linearization Technique for Surface Realization</a></strong><br><a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8635><div class="card-body p-3 small">End-to-end neural approaches have achieved state-of-the-art performance in many natural language processing (NLP) tasks. Yet, they often lack transparency of the underlying <a href=https://en.wikipedia.org/wiki/Decision-making>decision-making process</a>, hindering error analysis and certain model improvements. In this work, we revisit the binary linearization approach to surface realization, which exhibits more interpretable behavior, but was falling short in terms of prediction accuracy. We show how enriching the training data to better capture word order constraints almost doubles the performance of the <a href=https://en.wikipedia.org/wiki/System>system</a>. We further demonstrate that encoding both local and global prediction contexts yields another considerable performance boost. With the proposed modifications, the <a href=https://en.wikipedia.org/wiki/System>system</a> which ranked low in the latest shared task on multilingual surface realization now achieves best results in five out of ten languages, while being on par with the state-of-the-art approaches in others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8637.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8637 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8637 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-8637" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-8637/>Let’s FACE it. Finnish Poetry Generation with Aesthetics and Framing<span class=acl-fixed-case>FACE</span> it. <span class=acl-fixed-case>F</span>innish Poetry Generation with Aesthetics and Framing</a></strong><br><a href=/people/m/mika-hamalainen/>Mika Hämäläinen</a>
|
<a href=/people/k/khalid-alnajjar/>Khalid Alnajjar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8637><div class="card-body p-3 small">We present a creative poem generator for the morphologically rich Finnish language. Our method falls into the master-apprentice paradigm, where a computationally creative genetic algorithm teaches a BRNN model to generate <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a>. We model several parts of poetic aesthetics in the <a href=https://en.wikipedia.org/wiki/Fitness_function>fitness function</a> of the <a href=https://en.wikipedia.org/wiki/Genetic_algorithm>genetic algorithm</a>, such as sonic features, semantic coherence, <a href=https://en.wikipedia.org/wiki/Imagery>imagery</a> and <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a>. Furthermore, we justify the creativity of our method based on the FACE theory on <a href=https://en.wikipedia.org/wiki/Computational_creativity>computational creativity</a> and take additional care in evaluating our system by automatic metrics for concepts together with human evaluation for <a href=https://en.wikipedia.org/wiki/Aesthetics>aesthetics</a>, <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a> and expressions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8639.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8639 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8639 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-8639" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-8639/>Revisiting Challenges in Data-to-Text Generation with Fact Grounding</a></strong><br><a href=/people/h/hongmin-wang/>Hongmin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8639><div class="card-body p-3 small">Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> where only about 60 % of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50 % more data from the year 2017-19 and enriched input tables, and hope to attract research focuses in this direction. Moreover, we achieve improved <a href=https://en.wikipedia.org/wiki/Fidelity>data fidelity</a> over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8640.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8640 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8640 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8640/>Controlling Contents in Data-to-Document Generation with Human-Designed Topic Labels</a></strong><br><a href=/people/k/kasumi-aoki/>Kasumi Aoki</a>
|
<a href=/people/a/akira-miyazawa/>Akira Miyazawa</a>
|
<a href=/people/t/tatsuya-ishigaki/>Tatsuya Ishigaki</a>
|
<a href=/people/t/tatsuya-aoki/>Tatsuya Aoki</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/k/keiichi-goshima/>Keiichi Goshima</a>
|
<a href=/people/i/ichiro-kobayashi/>Ichiro Kobayashi</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8640><div class="card-body p-3 small">We propose a data-to-document generator that can easily control the contents of output texts based on a neural language model. Conventional data-to-text model is useful when a reader seeks a global summary of data because it has only to describe an important part that has been extracted beforehand. However, because depending on users, it differs what they are interested in, so it is necessary to develop a method to generate various summaries according to users&#8217; interests. We develop a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to generate various summaries and to control their contents by providing the explicit targets for a reference to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> as controllable factors. In the experiments, we used five-minute or one-hour charts of 9 indicators (e.g., Nikkei225), as <a href=https://en.wikipedia.org/wiki/Time_series>time-series data</a>, and daily summaries of Nikkei Quick News as textual data. We conducted comparative experiments using two pieces of information : human-designed topic labels indicating the contents of a sentence and automatically extracted <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> as the referential information for generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8641.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8641 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8641 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8641/>A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation</a></strong><br><a href=/people/y/yuta-hitomi/>Yuta Hitomi</a>
|
<a href=/people/y/yuya-taguchi/>Yuya Taguchi</a>
|
<a href=/people/h/hideaki-tamori/>Hideaki Tamori</a>
|
<a href=/people/k/ko-kikuta/>Ko Kikuta</a>
|
<a href=/people/j/jiro-nishitoba/>Jiro Nishitoba</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8641><div class="card-body p-3 small">Browsing news articles on multiple devices is now possible. The lengths of news article headlines have precise upper bounds, dictated by the size of the display of the relevant device or interface. Therefore, controlling the length of headlines is essential when applying the task of headline generation to <a href=https://en.wikipedia.org/wiki/News_media>news production</a>. However, because there is no corpus of headlines of multiple lengths for a given article, previous research on controlling output length in headline generation has not discussed whether the system outputs could be adequately evaluated without multiple references of different lengths. In this paper, we introduce two <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, which are Japanese News Corpus (JNC) and JApanese MUlti-Length Headline Corpus (JAMUL), to confirm the validity of previous evaluation settings. The JNC provides common supervision data for headline generation. The JAMUL is a large-scale evaluation dataset for <a href=https://en.wikipedia.org/wiki/Headline>headlines</a> of three different lengths composed by professional editors. We report new findings on these corpora ; for example, although the longest length reference summary can appropriately evaluate the existing methods controlling output length, this evaluation setting has several problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8645.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8645 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8645 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8645.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-8645/>Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation</a></strong><br><a href=/people/a/amit-moryossef/>Amit Moryossef</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8645><div class="card-body p-3 small">We follow the step-by-step approach to neural data-to-text generation proposed by Moryossef et al (2019), in which the generation process is divided into a text planning stage followed by a plan realization stage. We suggest four extensions to that framework : (1) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than the original planner ; (2) we incorporate typing hints that improve the model&#8217;s ability to deal with unseen relations and entities ; (3) we introduce a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts ; (4) we incorporate a simple but effective referring expression generation module. These <a href=https://en.wikipedia.org/wiki/Plug-in_(computing)>extensions</a> result in a generation process that is faster, more fluent, and more accurate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8647.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8647 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8647 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8647/>Hotel Scribe : Generating High Variation Hotel Descriptions</a></strong><br><a href=/people/s/saad-mahamood/>Saad Mahamood</a>
|
<a href=/people/m/maciej-zembrzuski/>Maciej Zembrzuski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8647><div class="card-body p-3 small">This paper describes the implementation of the Hotel Scribe system. A commercial Natural Language Generation (NLG) system which generates descriptions of hotels from accommodation metadata with a high level of content and <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variation</a> in English. It has been deployed live by * Anonymised Company Name * for the purpose of improving coverage of accommodation descriptions and for <a href=https://en.wikipedia.org/wiki/Search_engine_optimization>Search Engine Optimisation (SEO)</a>. In this paper, we describe the motivation for building this <a href=https://en.wikipedia.org/wiki/System>system</a>, the challenges faced when dealing with limited metadata, and the implementation used to generate the highly variate accommodation descriptions. Additionally, we evaluate the uniqueness of the texts generated by our <a href=https://en.wikipedia.org/wiki/System>system</a> against comparable human written accommodation description texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8651.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8651 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8651 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-8651" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-8651/>SimpleNLG-DE : Adapting SimpleNLG 4 to German<span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>NLG</span>-<span class=acl-fixed-case>DE</span>: Adapting <span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>NLG</span> 4 to <span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/d/daniel-braun/>Daniel Braun</a>
|
<a href=/people/k/kira-klimt/>Kira Klimt</a>
|
<a href=/people/d/daniela-schneider/>Daniela Schneider</a>
|
<a href=/people/f/florian-matthes/>Florian Matthes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8651><div class="card-body p-3 small">SimpleNLG is a popular open source surface realiser for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. For <a href=https://en.wikipedia.org/wiki/German_language>German</a>, however, the availability of open source and non-domain specific realisers is sparse, partly due to the complexity of the <a href=https://en.wikipedia.org/wiki/German_language>German language</a>. In this paper, we present SimpleNLG-DE, an adaption of SimpleNLG to <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We discuss which parts of the <a href=https://en.wikipedia.org/wiki/German_language>German language</a> have been implemented and how we evaluated our <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> using the TIGER Corpus and newly created <a href=https://en.wikipedia.org/wiki/Data_set>data-sets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8653.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8653 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8653 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8653.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-8653/>Can Neural Image Captioning be Controlled via Forced Attention?</a></strong><br><a href=/people/p/philipp-sadler/>Philipp Sadler</a>
|
<a href=/people/t/tatjana-scheffler/>Tatjana Scheffler</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8653><div class="card-body p-3 small">Learned dynamic weighting of the conditioning signal (attention) has been shown to improve neural language generation in a variety of settings. The <a href=https://en.wikipedia.org/wiki/Weighting>weights</a> applied when generating a particular output sequence have also been viewed as providing a potentially explanatory insight in the internal workings of the <a href=https://en.wikipedia.org/wiki/Electric_generator>generator</a>. In this paper, we reverse the direction of this connection and ask whether through the control of the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> we can control its output. Specifically, we take a standard neural image captioning model that uses <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, and fix the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> to predetermined areas in the image. We evaluate whether the resulting output is more likely to mention the class of the object in that area than the normally generated caption. We introduce three effective methods to control the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> and find that these are producing expected results in up to 27.43 % of the cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8654.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8654 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8654 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8654.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-8654/>Towards a Metric for Automated Conversational Dialogue System Evaluation and Improvement</a></strong><br><a href=/people/j/jan-milan-deriu/>Jan Milan Deriu</a>
|
<a href=/people/m/mark-cieliebak/>Mark Cieliebak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8654><div class="card-body p-3 small">We present AutoJudge, an automated evaluation method for conversational dialogue systems. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> works by first generating <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> based on <a href=https://en.wikipedia.org/wiki/Self-talk>self-talk</a>, i.e. dialogue systems talking to itself. Then, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> uses human ratings on these <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> to train an <a href=https://en.wikipedia.org/wiki/Automated_reasoning>automated judgement model</a>. Our experiments show that AutoJudge correlates well with the human ratings and can be used to automatically evaluate dialogue systems, even in deployed systems. In a second part, we attempt to apply AutoJudge to improve existing <a href=https://en.wikipedia.org/wiki/System>systems</a>. This works well for <a href=https://en.wikipedia.org/wiki/Ranking>re-ranking</a> a set of candidate utterances. However, our experiments show that AutoJudge can not be applied as <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reward</a> for <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, although the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> can distinguish good from bad dialogues. We discuss potential reasons, but state here already that this is still an open question for further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8656.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8656 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8656 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8656/>A Personalized Data-to-Text Support Tool for Cancer Patients</a></strong><br><a href=/people/s/saar-hommes/>Saar Hommes</a>
|
<a href=/people/c/chris-van-der-lee/>Chris van der Lee</a>
|
<a href=/people/f/felix-clouth/>Felix Clouth</a>
|
<a href=/people/j/jeroen-vermunt/>Jeroen Vermunt</a>
|
<a href=/people/x/xander-verbeek/>Xander Verbeek</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8656><div class="card-body p-3 small">In this paper, we present a novel data-to-text system for cancer patients, providing information on quality of life implications after treatment, which can be embedded in the context of shared decision making. Currently, information on quality of life implications is often not discussed, partly because (until recently) data has been lacking. In our work, we rely on a newly developed prediction model, which assigns patients to scenarios. Furthermore, we use data-to-text techniques to explain these scenario-based predictions in personalized and understandable language. We highlight the possibilities of NLG for <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a>, discuss ethical implications and also present the outcomes of a first evaluation with clinicians.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8658.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8658 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8658 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8658/>Using NLG for <a href=https://en.wikipedia.org/wiki/Speech_synthesis>speech synthesis</a> of mathematical sentences<span class=acl-fixed-case>NLG</span> for speech synthesis of mathematical sentences</a></strong><br><a href=/people/a/alessandro-mazzei/>Alessandro Mazzei</a>
|
<a href=/people/m/michele-monticone/>Michele Monticone</a>
|
<a href=/people/c/cristian-bernareggi/>Cristian Bernareggi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8658><div class="card-body p-3 small">People with sight impairments can access to a <a href=https://en.wikipedia.org/wiki/Expression_(mathematics)>mathematical expression</a> by using its LaTeX source. However, this mechanisms have several drawbacks : (1) it assumes the knowledge of the <a href=https://en.wikipedia.org/wiki/LaTeX>LaTeX</a>, (2) it is slow, since <a href=https://en.wikipedia.org/wiki/LaTeX>LaTeX</a> is verbose and (3) it is error-prone since <a href=https://en.wikipedia.org/wiki/LATEX>LATEX</a> is a typographical language. In this paper we study the design of a <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation system</a> for producing a <a href=https://en.wikipedia.org/wiki/Sentence_(mathematical_logic)>mathematical sentence</a>, i.e. a <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>natural language sentence</a> expressing the semantics of a <a href=https://en.wikipedia.org/wiki/Expression_(mathematics)>mathematical expression</a>. Moreover, we describe the main results of a first human based evaluation experiment of the <a href=https://en.wikipedia.org/wiki/System>system</a> for <a href=https://en.wikipedia.org/wiki/Italian_language>Italian language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8661.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8661 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8661 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8661.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-8661/>Towards Generating Math Word Problems from Equations and Topics</a></strong><br><a href=/people/q/qingyu-zhou/>Qingyu Zhou</a>
|
<a href=/people/d/danqing-huang/>Danqing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8661><div class="card-body p-3 small">A math word problem is a narrative with a specific topic that provides clues to the correct equation with numerical quantities and variables therein. In this paper, we focus on the task of <a href=https://en.wikipedia.org/wiki/Word_problem_(mathematics)>generating math word problems</a>. Previous works are mainly template-based with pre-defined rules. We propose a novel <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> to generate <a href=https://en.wikipedia.org/wiki/Word_problem_(mathematics)>math word problems</a> from the given equations and topics. First, we design a <a href=https://en.wikipedia.org/wiki/Nuclear_fusion>fusion mechanism</a> to incorporate the information of both equations and topics. Second, an entity-enforced loss is introduced to ensure the relevance between the generated math problem and the equation. Automatic evaluation results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms the baseline models. In human evaluations, the math word problems generated by our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> are rated as being more relevant (in terms of solvability of the given equations and relevance to topics) and natural (i.e., <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, fluency) than the baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8662.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8662 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8662 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8662/>DisSim : A Discourse-Aware Syntactic Text Simplification Framework for English and German<span class=acl-fixed-case>D</span>is<span class=acl-fixed-case>S</span>im: A Discourse-Aware Syntactic Text Simplification Framework for <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/c/christina-niklaus/>Christina Niklaus</a>
|
<a href=/people/m/matthias-cetto/>Matthias Cetto</a>
|
<a href=/people/a/andre-freitas/>André Freitas</a>
|
<a href=/people/s/siegfried-handschuh/>Siegfried Handschuh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8662><div class="card-body p-3 small">We introduce DisSim, a discourse-aware sentence splitting framework for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a> whose goal is to transform syntactically complex sentences into an intermediate representation that presents a simple and more regular structure which is easier to process for downstream semantic applications. For this purpose, we turn input sentences into a two-layered semantic hierarchy in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them. In that way, we preserve the <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence structure</a> of the input and, hence, its interpretability for downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8663.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8663 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8663 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8663/>Real World Voice Assistant System for Cooking</a></strong><br><a href=/people/t/takahiko-ito/>Takahiko Ito</a>
|
<a href=/people/s/shintaro-inuzuka/>Shintaro Inuzuka</a>
|
<a href=/people/y/yoshiaki-yamada/>Yoshiaki Yamada</a>
|
<a href=/people/j/jun-harashima/>Jun Harashima</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8663><div class="card-body p-3 small">This study presents a voice assistant system to support <a href=https://en.wikipedia.org/wiki/Cooking>cooking</a> by utilizing <a href=https://en.wikipedia.org/wiki/Smart_speaker>smart speakers</a> in Japan. This system not only speaks the procedures written in recipes point by point but also answers the common questions from users for the specified recipes. The <a href=https://en.wikipedia.org/wiki/System>system</a> applies machine comprehension techniques to millions of recipes for answering the common questions in <a href=https://en.wikipedia.org/wiki/Cooking>cooking</a> such as (How should I cook carrots?). Furthermore, numerous <a href=https://en.wikipedia.org/wiki/Machine_learning>machine-learning techniques</a> are applied to generate better responses to users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8665.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8665 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8665 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8665/>Generating Abstractive Summaries with Finetuned Language Models</a></strong><br><a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/z/zachary-ziegler/>Zachary Ziegler</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8665><div class="card-body p-3 small">Neural abstractive document summarization is commonly approached by <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that exhibit a mostly extractive behavior. This behavior is facilitated by a copy-attention which allows <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to copy words from a source document. While models in the mostly extractive news summarization domain benefit from this <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a>, they commonly fail to paraphrase or compress information from the source document. Recent advances in <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer-learning</a> from large pretrained language models give rise to alternative approaches that do not rely on copy-attention and instead learn to generate concise and abstractive summaries. In this paper, as part of the TL;DR challenge, we compare the abstractiveness of summaries from different summarization approaches and show that <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer-learning</a> can be efficiently utilized without any changes to the model architecture. We demonstrate that the approach leads to a higher level of <a href=https://en.wikipedia.org/wiki/Abstraction_(computer_science)>abstraction</a> for a similar performance on the TL;DR challenge tasks, enabling true natural language compression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8669.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8669 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8669 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8669/>Semi-Supervised Neural Text Generation by Joint Learning of Natural Language Generation and Natural Language Understanding Models</a></strong><br><a href=/people/r/raheel-qader/>Raheel Qader</a>
|
<a href=/people/f/francois-portet/>François Portet</a>
|
<a href=/people/c/cyril-labbe/>Cyril Labbé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8669><div class="card-body p-3 small">In Natural Language Generation (NLG), End-to-End (E2E) systems trained through <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> have recently gained a strong interest. Such <a href=https://en.wikipedia.org/wiki/Deep_learning>deep models</a> need a large amount of carefully <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a> to reach satisfactory performance. However, acquiring such <a href=https://en.wikipedia.org/wiki/Data_set_(IBM_mainframe)>datasets</a> for every new NLG application is a tedious and time-consuming task. In this paper, we propose a semi-supervised deep learning scheme that can learn from non-annotated data and annotated data when available. It uses a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>NLG</a> and a Natural Language Understanding (NLU) sequence-to-sequence models which are learned jointly to compensate for the lack of annotation. Experiments on two benchmark datasets show that, with limited amount of annotated data, the method can achieve very competitive results while not using any pre-processing or re-scoring tricks. These findings open the way to the exploitation of non-annotated datasets which is the current bottleneck for the E2E NLG system development to new applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8670.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8670 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8670 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8670/>Neural Generation for <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> : Data and Baselines<span class=acl-fixed-case>C</span>zech: Data and Baselines</a></strong><br><a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/f/filip-jurcicek/>Filip Jurčíček</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8670><div class="card-body p-3 small">We present the first dataset targeted at end-to-end NLG in <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> in the restaurant domain, along with several strong baseline models using the sequence-to-sequence approach. While non-English NLG is under-explored in general, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, as a morphologically rich language, makes the task even harder : Since <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> requires inflecting named entities, delexicalization or copy mechanisms do not work out-of-the-box and lexicalizing the generated outputs is non-trivial. In our experiments, we present two different approaches to this this problem : (1) using a neural language model to select the correct inflected form while lexicalizing, (2) a two-step generation setup : our sequence-to-sequence model generates an interleaved sequence of lemmas and morphological tags, which are then inflected by a morphological generator.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8672.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8672 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8672 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8672.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-8672" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-8672/>A Good Sample is Hard to Find : Noise Injection Sampling and Self-Training for Neural Language Generation Models</a></strong><br><a href=/people/c/chris-kedzie/>Chris Kedzie</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8672><div class="card-body p-3 small">Deep neural networks (DNN) are quickly becoming the de facto standard modeling method for many natural language generation (NLG) tasks. In order for such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to truly be useful, they must be capable of correctly generating utterances for novel meaning representations (MRs) at test time. In practice, even sophisticated DNNs with various forms of semantic control frequently fail to generate utterances faithful to the input MR. In this paper, we propose an architecture agnostic self-training method to sample novel MR / text utterance pairs to augment the original training data. Remarkably, after training on the augmented data, even simple encoder-decoder models with greedy decoding are capable of generating semantically correct utterances that are as good as state-of-the-art outputs in both automatic and human evaluations of quality.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>