<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Computational Linguistics, Volume 47, Issue 4 - December 2021 - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Computational Linguistics, Volume 47, Issue 4 - December 2021</h2><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.cl-4</dd><dt>Month:</dt><dd>December</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Cambridge, MA</dd><dt>Venue:</dt><dd><a href=/venues/cl/>CL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>MIT Press</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.cl-4>https://aclanthology.org/2021.cl-4</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Computational+Linguistics%2C+Volume+47%2C+Issue+4+-+December+2021" title="Search for 'Computational Linguistics, Volume 47, Issue 4 - December 2021' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-4.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-4--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-4.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-4.27/>Abstractive Text Summarization : Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization</a></strong><br><a href=/people/p/panagiotis-kouris/>Panagiotis Kouris</a>
|
<a href=/people/g/georgios-alexandridis/>Georgios Alexandridis</a>
|
<a href=/people/a/andreas-stafylopatis/>Andreas Stafylopatis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-4--27><div class="card-body p-3 small">Abstract Nowadays, most research conducted in the field of abstractive text summarization focuses on neural-based models alone, without considering their combination with knowledge-based approaches that could further enhance their efficiency. In this direction, this work presents a novel framework that combines sequence-to-sequence neural-based text summarization along with structure and semantic-based methodologies. The proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is capable of dealing with the problem of out-of-vocabulary or rare words, improving the performance of the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>. The overall <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is based on a well-defined theoretical model of knowledge-based content generalization and deep learning predictions for generating abstractive summaries. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is composed of three key elements : (i) a pre-processing task, (ii) a machine learning methodology, and (iii) a post-processing task. The pre-processing task is a knowledge-based approach, based on ontological knowledge resources, <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a>, and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, along with content generalization, that transforms ordinary text into a generalized form. A deep learning model of attentive encoder-decoder architecture, which is expanded to enable a coping and coverage mechanism, as well as reinforcement learning and transformer-based architectures, is trained on a generalized version of text-summary pairs, learning to predict summaries in a generalized form. The post-processing task utilizes knowledge resources, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a>, and <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic algorithms</a> based on text similarity methods in order to transform the generalized version of a predicted summary to a final, human-readable form.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-4.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-4--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-4.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cl-4.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cl-4.28/>The (Un)Suitability of Automatic Evaluation Metrics for Text Simplification</a></strong><br><a href=/people/f/fernando-alva-manchego/>Fernando Alva-Manchego</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-4--28><div class="card-body p-3 small">Abstract In order to simplify sentences, several rewriting operations can be performed, such as replacing complex words per simpler synonyms, deleting unnecessary information, and splitting long sentences. Despite this multi-operation nature, evaluation of automatic simplification systems relies on <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that moderately correlate with human judgments on the <a href=https://en.wikipedia.org/wiki/Simplicity>simplicity</a> achieved by executing specific <a href=https://en.wikipedia.org/wiki/Operation_(mathematics)>operations</a> (e.g., <a href=https://en.wikipedia.org/wiki/Simplicity>simplicity gain</a> based on lexical replacements). In this article, we investigate how well existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> can assess sentence-level simplifications where multiple <a href=https://en.wikipedia.org/wiki/Operation_(mathematics)>operations</a> may have been applied and which, therefore, require more general simplicity judgments. For that, we first collect a new and more reliable <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> for evaluating the correlation of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> and human judgments of overall simplicity. Second, we conduct the first meta-evaluation of automatic metrics in Text Simplification, using our new data set (and other existing data) to analyze the variation of the correlation between metrics&#8217; scores and human judgments across three dimensions : the perceived simplicity level, the system type, and the set of references used for computation. We show that these three aspects affect the correlations and, in particular, highlight the limitations of commonly used operation-specific metrics. Finally, based on our findings, we propose a set of recommendations for automatic evaluation of multi-operation simplifications, suggesting which metrics to compute and how to interpret their scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-4.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-4--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-4.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-4.30/>Are Ellipses Important for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>?</a></strong><br><a href=/people/p/payal-khullar/>Payal Khullar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-4--30><div class="card-body p-3 small">Abstract This article describes an experiment to evaluate the impact of different types of ellipses discussed in <a href=https://en.wikipedia.org/wiki/Theoretical_linguistics>theoretical linguistics</a> on Neural Machine Translation (NMT), using English to Hindi / Telugu as source and target languages. Evaluation with manual methods shows that most of the errors made by Google NMT are located in the clause containing the <a href=https://en.wikipedia.org/wiki/Ellipsis>ellipsis</a>, the frequency of such errors is slightly more in <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a> than <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, and the translation adequacy shows improvement when ellipses are reconstructed with their antecedents. These findings not only confirm the importance of <a href=https://en.wikipedia.org/wiki/Ellipsis>ellipses</a> and their resolution for MT, but also hint toward a possible correlation between the translation of discourse devices like <a href=https://en.wikipedia.org/wiki/Ellipsis>ellipses</a> with the morphological incongruity of the source and target. We also observe that not all <a href=https://en.wikipedia.org/wiki/Ellipse>ellipses</a> are translated poorly and benefit from reconstruction, advocating for a disparate treatment of different <a href=https://en.wikipedia.org/wiki/Ellipse>ellipses</a> in MT research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-4.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-4--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-4.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-4.31/>LFG Generation from Acyclic F-Structures is NP-Hard<span class=acl-fixed-case>LFG</span> Generation from Acyclic <span class=acl-fixed-case>F</span>-Structures is <span class=acl-fixed-case>NP</span>-Hard</a></strong><br><a href=/people/j/jurgen-wedekind/>Jürgen Wedekind</a>
|
<a href=/people/r/ronald-m-kaplan/>Ronald M. Kaplan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-4--31><div class="card-body p-3 small">Abstract The universal generation problem for LFG grammars is the problem of determining whether a given <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammar</a> derives any terminal string with a given f-structure. It is known that this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is decidable for acyclic f-structures. In this brief note, we show that for those f-structures the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is nonetheless intractable. This holds even for <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammars</a> that are off-line parsable.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>