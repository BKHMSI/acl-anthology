<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W18-55.pdf>Proceedings of the First Workshop on Fact Extraction and <span class=acl-fixed-case>VER</span>ification (<span class=acl-fixed-case>FEVER</span>)</a></h2><p class=lead><a href=/people/j/james-thorne/>James Thorne</a>,
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>,
<a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>,
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>,
<a href=/people/a/arpit-mittal/>Arpit Mittal</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W18-55</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Brussels, Belgium</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W18-55>https://aclanthology.org/W18-55</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W18-55.pdf>https://aclanthology.org/W18-55.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W18-55.pdf title="Open PDF of 'Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+First+Workshop+on+Fact+Extraction+and+VERification+%28FEVER%29" title="Search for 'Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5500/>Proceedings of the First Workshop on Fact Extraction and <span class=acl-fixed-case>VER</span>ification (<span class=acl-fixed-case>FEVER</span>)</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5501/>The Fact Extraction and VERification (FEVER) Shared Task<span class=acl-fixed-case>VER</span>ification (<span class=acl-fixed-case>FEVER</span>) Shared Task</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5501><div class="card-body p-3 small">We present the results of the first Fact Extraction and VERification (FEVER) Shared Task. The task challenged participants to classify whether human-written factoid claims could be SUPPORTED or REFUTED using evidence retrieved from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We received entries from 23 competing teams, 19 of which scored higher than the previously published baseline. The best performing <a href=https://en.wikipedia.org/wiki/System>system</a> achieved a <a href=https://en.wikipedia.org/wiki/Fever>FEVER score</a> of 64.21 %. In this paper, we present the results of the shared task and a summary of the <a href=https://en.wikipedia.org/wiki/System>systems</a>, highlighting commonalities and innovations among participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5502" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5502/>The Data Challenge in Misinformation Detection : Source Reputation vs. Content Veracity</a></strong><br><a href=/people/f/fatemeh-torabi-asr/>Fatemeh Torabi Asr</a>
|
<a href=/people/m/maite-taboada/>Maite Taboada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5502><div class="card-body p-3 small">Misinformation detection at the level of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>full news articles</a> is a text classification problem. Reliably labeled data in this <a href=https://en.wikipedia.org/wiki/Domain_(mathematical_analysis)>domain</a> is rare. Previous work relied on <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> collected from so-called reputable and suspicious websites and labeled accordingly. We leverage fact-checking websites to collect individually-labeled news articles with regard to the veracity of their content and use this data to test the cross-domain generalization of a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> trained on bigger text collections but labeled according to source reputation. Our results suggest that reputation-based classification is not sufficient for predicting the veracity level of the majority of news articles, and that the system performance on different test datasets depends on topic distribution. Therefore collecting well-balanced and carefully-assessed training data is a priority for developing robust misinformation detection systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5503" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5503/>Crowdsourcing Semantic Label Propagation in Relation Classification</a></strong><br><a href=/people/a/anca-dumitrache/>Anca Dumitrache</a>
|
<a href=/people/l/lora-aroyo/>Lora Aroyo</a>
|
<a href=/people/c/chris-welty/>Chris Welty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5503><div class="card-body p-3 small">Distant supervision is a popular method for performing <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> that is known to produce noisy labels. Most progress in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> has been made with crowdsourced corrections to distant-supervised labels, and there is evidence that indicates still more would be better. In this paper, we explore the problem of propagating human annotation signals gathered for open-domain relation classification through the CrowdTruth methodology for <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>, that captures ambiguity in annotations by measuring inter-annotator disagreement. Our approach propagates annotations to sentences that are similar in a low dimensional embedding space, expanding the number of labels by two orders of magnitude. Our experiments show significant improvement in a sentence-level multi-class relation classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5506/>Joint Modeling for <a href=https://en.wikipedia.org/wiki/Query_expansion>Query Expansion</a> and <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5506><div class="card-body p-3 small">Information extraction about an event can be improved by incorporating <a href=https://en.wikipedia.org/wiki/Empirical_evidence>external evidence</a>. In this study, we propose a joint model for pseudo-relevance feedback based query expansion and <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates an event-specific query to effectively retrieve documents relevant to the event. We demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is comparable or has better performance than the previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in two publicly available datasets. Furthermore, we analyzed the influences of the retrieval effectiveness in our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the <a href=https://en.wikipedia.org/wiki/Information_retrieval>extraction</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5508/>Belittling the Source : Trustworthiness Indicators to Obfuscate Fake News on the Web</a></strong><br><a href=/people/d/diego-esteves/>Diego Esteves</a>
|
<a href=/people/a/aniketh-janardhan-reddy/>Aniketh Janardhan Reddy</a>
|
<a href=/people/p/piyush-chawla/>Piyush Chawla</a>
|
<a href=/people/j/jens-lehmann/>Jens Lehmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5508><div class="card-body p-3 small">With the growth of the <a href=https://en.wikipedia.org/wiki/Internet>internet</a>, the number of fake-news online has been proliferating every year. The consequences of such phenomena are manifold, ranging from lousy decision-making process to bullying and violence episodes. Therefore, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking algorithms</a> became a valuable asset. To this aim, an important step to detect <a href=https://en.wikipedia.org/wiki/Fake_news>fake-news</a> is to have access to a credibility score for a given <a href=https://en.wikipedia.org/wiki/Source_(journalism)>information source</a>. However, most of the widely used Web indicators have either been shutdown to the public (e.g., Google PageRank) or are not free for use (Alexa Rank). Further existing databases are short-manually curated lists of online sources, which do not scale. Finally, most of the research on the topic is theoretical-based or explore <a href=https://en.wikipedia.org/wiki/Confidentiality>confidential data</a> in a restricted simulation environment. In this paper we explore current research, highlight the challenges and propose solutions to tackle the problem of classifying websites into a credibility scale. The proposed model automatically extracts source reputation cues and computes a credibility factor, providing valuable insights which can help in belittling dubious and confirming trustful unknown websites. Experimental results outperform state of the art in the 2-classes and 5-classes setting.<i>fake-news</i> online has been proliferating every year. The consequences of such phenomena are manifold, ranging from lousy decision-making process to bullying and violence episodes. Therefore, fact-checking algorithms became a valuable asset. To this aim, an important step to detect fake-news is to have access to a credibility score for a given information source. However, most of the widely used Web indicators have either been shutdown to the public (e.g., Google PageRank) or are not free for use (Alexa Rank). Further existing databases are short-manually curated lists of online sources, which do not scale. Finally, most of the research on the topic is theoretical-based or explore confidential data in a restricted simulation environment. In this paper we explore current research, highlight the challenges and propose solutions to tackle the problem of classifying websites into a credibility scale. The proposed model automatically extracts source reputation cues and computes a credibility factor, providing valuable insights which can help in belittling dubious and confirming trustful unknown websites. Experimental results outperform state of the art in the 2-classes and 5-classes setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5510/>Stance Detection in Fake News A Combined Feature Representation</a></strong><br><a href=/people/b/bilal-ghanem/>Bilal Ghanem</a>
|
<a href=/people/p/paolo-rosso/>Paolo Rosso</a>
|
<a href=/people/f/francisco-rangel/>Francisco Rangel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5510><div class="card-body p-3 small">With the uncontrolled increasing of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and rumors over the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>, different approaches have been proposed to address the problem. In this paper, we present an approach that combines lexical, word embeddings and <a href=https://en.wikipedia.org/wiki/N-gram>n-gram features</a> to detect the stance in <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. Our approach has been tested on the Fake News Challenge (FNC-1) dataset. Given a news title-article pair, the FNC-1 task aims at determining the relevance of the article and the title. Our proposed approach has achieved an accurate result (59.6 % Macro F1) that is close to the state-of-the-art result with 0.013 difference using a simple <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>feature representation</a>. Furthermore, we have investigated the importance of different <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> in the detection of the classification labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5513" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5513/>Where is Your Evidence : Improving <a href=https://en.wikipedia.org/wiki/Fact-checking>Fact-checking</a> by Justification Modeling</a></strong><br><a href=/people/t/tariq-alhindi/>Tariq Alhindi</a>
|
<a href=/people/s/savvas-petridis/>Savvas Petridis</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5513><div class="card-body p-3 small">Fact-checking is a <a href=https://en.wikipedia.org/wiki/Journalism>journalistic practice</a> that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a>. However, approaches based on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5514/>Affordance Extraction and Inference based on Semantic Role Labeling</a></strong><br><a href=/people/d/daniel-loureiro/>Daniel Loureiro</a>
|
<a href=/people/a/alipio-jorge/>Alípio Jorge</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5514><div class="card-body p-3 small">Common-sense reasoning is becoming increasingly important for the advancement of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. While word embeddings have been very successful, they can not explain which aspects of &#8216;coffee&#8217; and &#8216;tea&#8217; make them similar, or how they could be related to &#8216;shop&#8217;. In this paper, we propose an explicit word representation that builds upon the Distributional Hypothesis to represent meaning from semantic roles, and allow inference of relations from their meshing, as supported by the affordance-based Indexical Hypothesis. We find that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves the state-of-the-art on unsupervised word similarity tasks while allowing for direct inference of new relations from the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5515 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5515/>UCL Machine Reading Group : Four Factor Framework For Fact Finding (HexaF)<span class=acl-fixed-case>UCL</span> Machine Reading Group: Four Factor Framework For Fact Finding (<span class=acl-fixed-case>H</span>exa<span class=acl-fixed-case>F</span>)</a></strong><br><a href=/people/t/takuma-yoneda/>Takuma Yoneda</a>
|
<a href=/people/j/jeff-mitchell/>Jeff Mitchell</a>
|
<a href=/people/j/johannes-welbl/>Johannes Welbl</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5515><div class="card-body p-3 small">In this paper we describe our 2nd place FEVER shared-task system that achieved a FEVER score of 62.52 % on the provisional test set (without additional human evaluation), and 65.41 % on the development set. Our system is a four stage model consisting of <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence retrieval</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a> and aggregation. Retrieval is performed leveraging task-specific features, and then a natural language inference model takes each of the retrieved sentences paired with the claimed fact. The resulting predictions are aggregated across retrieved sentences with a Multi-Layer Perceptron, and re-ranked corresponding to the final prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5516" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5516/>UKP-Athene : Multi-Sentence Textual Entailment for Claim Verification<span class=acl-fixed-case>UKP</span>-Athene: Multi-Sentence Textual Entailment for Claim Verification</a></strong><br><a href=/people/a/andreas-hanselowski/>Andreas Hanselowski</a>
|
<a href=/people/h/hao-zhang/>Hao Zhang</a>
|
<a href=/people/z/zile-li/>Zile Li</a>
|
<a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/b/benjamin-schiller/>Benjamin Schiller</a>
|
<a href=/people/c/claudia-schulz/>Claudia Schulz</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5516><div class="card-body p-3 small">The Fact Extraction and VERification (FEVER) shared task was launched to support the development of systems able to verify claims by extracting supporting or refuting facts from raw text. The shared task organizers provide a large-scale dataset for the consecutive steps involved in <a href=https://en.wikipedia.org/wiki/Verification_and_validation>claim verification</a>, in particular, <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, fact extraction, and <a href=https://en.wikipedia.org/wiki/Statistical_classification>claim classification</a>. In this paper, we present our claim verification pipeline approach, which, according to the preliminary results, scored third in the shared task, out of 23 competing systems. For the <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, we implemented a new entity linking approach. In order to be able to rank candidate facts and classify a claim on the basis of several selected facts, we introduce two extensions to the Enhanced LSTM (ESIM).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5517.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5517 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5517 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5517" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5517/>Team Papelo : Transformer Networks at FEVER<span class=acl-fixed-case>FEVER</span></a></strong><br><a href=/people/c/christopher-malon/>Christopher Malon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5517><div class="card-body p-3 small">We develop a system for the FEVER fact extraction and verification challenge that uses a high precision entailment classifier based on transformer networks pretrained with <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, to classify a broad set of potential evidence. The precision of the <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment classifier</a> allows us to enhance <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> by considering every statement from several articles to decide upon each claim. We include not only the articles best matching the claim text by TFIDF score, but read additional articles whose titles match named entities and capitalized expressions occurring in the claim text. The entailment module evaluates potential evidence one statement at a time, together with the title of the page the evidence came from (providing a hint about possible pronoun antecedents). In preliminary evaluation, the system achieves.5736 FEVER score,.6108 label accuracy, and.6485 evidence F1 on the FEVER shared task test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5519.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5519 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5519 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5519/>SIRIUS-LTG : An Entity Linking Approach to Fact Extraction and Verification<span class=acl-fixed-case>SIRIUS</span>-<span class=acl-fixed-case>LTG</span>: An Entity Linking Approach to Fact Extraction and Verification</a></strong><br><a href=/people/f/farhad-nooralahzadeh/>Farhad Nooralahzadeh</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5519><div class="card-body p-3 small">This article presents the SIRIUS-LTG system for the Fact Extraction and VERification (FEVER) Shared Task. It consists of three components : 1) Wikipedia Page Retrieval : First we extract the entities in the claim, then we find potential Wikipedia URI candidates for each of the entities using a SPARQL query over <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a> 2) Sentence selection : We investigate various <a href=https://en.wikipedia.org/wiki/List_of_Internet_phenomena>techniques</a> i.e. Smooth Inverse Frequency (SIF), Word Mover&#8217;s Distance (WMD), Soft-Cosine Similarity, Cosine similarity with unigram Term Frequency Inverse Document Frequency (TF-IDF) to rank sentences by their similarity to the claim. 3) Textual Entailment : We compare three models for the task of claim classification. We apply a Decomposable Attention (DA) model (Parikh et al., 2016), a Decomposed Graph Entailment (DGE) model (Khot et al., 2018) and a Gradient-Boosted Decision Trees (TalosTree) model (Sean et al., 2017) for this task. The experiments show that the pipeline with simple <a href=https://en.wikipedia.org/wiki/Cosine_similarity>Cosine Similarity</a> using TFIDF in sentence selection along with DA model as labelling model achieves the best results on the development set (F1 evidence : 32.17, label accuracy : 59.61 and FEVER score : 0.3778). Furthermore, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> obtains 30.19, 48.87 and 36.55 in terms of F1 evidence, label accuracy and FEVER score, respectively, on the test set. Our system ranks 15th among 23 participants in the shared task prior to any human-evaluation of the evidence.<i>Wikipedia Page Retrieval</i>: First we extract the entities in the claim, then we find potential Wikipedia URI candidates for each of the entities using a SPARQL query over DBpedia 2) <i>Sentence selection</i>: We investigate various techniques i.e. Smooth Inverse Frequency (SIF), Word Mover&#8217;s Distance (WMD), Soft-Cosine Similarity, Cosine similarity with unigram Term Frequency Inverse Document Frequency (TF-IDF) to rank sentences by their similarity to the claim. 3) <i>Textual Entailment</i>: We compare three models for the task of claim classification. We apply a Decomposable Attention (DA) model (Parikh et al., 2016), a Decomposed Graph Entailment (DGE) model (Khot et al., 2018) and a Gradient-Boosted Decision Trees (TalosTree) model (Sean et al., 2017) for this task. The experiments show that the pipeline with simple Cosine Similarity using TFIDF in sentence selection along with DA model as labelling model achieves the best results on the development set (F1 evidence: 32.17, label accuracy: 59.61 and FEVER score: 0.3778). Furthermore, it obtains 30.19, 48.87 and 36.55 in terms of F1 evidence, label accuracy and FEVER score, respectively, on the test set. Our system ranks 15th among 23 participants in the shared task prior to any human-evaluation of the evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5520 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5520/>Integrating Entity Linking and Evidence Ranking for Fact Extraction and Verification</a></strong><br><a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/takumi-takahashi/>Takumi Takahashi</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5520><div class="card-body p-3 small">We describe here our <a href=https://en.wikipedia.org/wiki/System>system</a> and results on the FEVER shared task. We prepared a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline system</a> which composes of a document selection, a sentence retrieval, and a recognizing textual entailment (RTE) components. A simple entity linking approach with text match is used as the document selection component, this component identifies relevant documents for a given claim by using mentioned entities as clues. The sentence retrieval component selects relevant sentences as candidate evidence from the documents based on <a href=https://en.wikipedia.org/wiki/TF-IDF>TF-IDF</a>. Finally, the RTE component selects evidence sentences by ranking the sentences and classifies the claim simultaneously. The experimental results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved the <a href=https://en.wikipedia.org/wiki/FEVER>FEVER score</a> of 0.4016 and outperformed the official baseline system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5522.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5522 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5522 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5522" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5522/>DeFactoNLP : Fact Verification using Entity Recognition, TFIDF Vector Comparison and Decomposable Attention<span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>F</span>acto<span class=acl-fixed-case>NLP</span>: Fact Verification using Entity Recognition, <span class=acl-fixed-case>TFIDF</span> Vector Comparison and Decomposable Attention</a></strong><br><a href=/people/a/aniketh-janardhan-reddy/>Aniketh Janardhan Reddy</a>
|
<a href=/people/g/gil-rocha/>Gil Rocha</a>
|
<a href=/people/d/diego-esteves/>Diego Esteves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5522><div class="card-body p-3 small">In this paper, we describe DeFactoNLP, the <a href=https://en.wikipedia.org/wiki/System>system</a> we designed for the FEVER 2018 Shared Task. The aim of this task was to conceive a system that can not only automatically assess the veracity of a claim but also retrieve evidence supporting this assessment from Wikipedia. In our approach, the Wikipedia documents whose Term Frequency-Inverse Document Frequency (TFIDF) vectors are most similar to the vector of the claim and those documents whose names are similar to those of the named entities (NEs) mentioned in the claim are identified as the documents which might contain evidence. The sentences in these documents are then supplied to a textual entailment recognition module. This module calculates the probability of each sentence supporting the claim, contradicting the claim or not providing any relevant information to assess the veracity of the claim. Various <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> computed using these <a href=https://en.wikipedia.org/wiki/Probability>probabilities</a> are finally used by a Random Forest classifier to determine the overall truthfulness of the claim. The sentences which support this <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification</a> are returned as evidence. Our approach achieved a 0.4277 evidence F1-score, a 0.5136 label accuracy and a 0.3833 FEVER score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5523/>An End-to-End Multi-task Learning Model for <a href=https://en.wikipedia.org/wiki/Fact-checking>Fact Checking</a></a></strong><br><a href=/people/s/sizhen-li/>Sizhen Li</a>
|
<a href=/people/s/shuai-zhao/>Shuai Zhao</a>
|
<a href=/people/b/bo-cheng/>Bo Cheng</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5523><div class="card-body p-3 small">With huge amount of information generated every day on the web, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a> is an important and challenging task which can help people identify the authenticity of most claims as well as providing evidences selected from knowledge source like <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. Here we decompose this problem into two parts : an entity linking task (retrieving relative Wikipedia pages) and recognizing textual entailment between the claim and selected pages. In this paper, we present an end-to-end multi-task learning with bi-direction attention (EMBA) model to classify the claim as supports, refutes or not enough info with respect to the pages retrieved and detect sentences as evidence at the same time. We conduct experiments on the FEVER (Fact Extraction and VERification) paper test dataset and shared task test dataset, a new public dataset for verification against textual sources. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves comparable performance compared with the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5524 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5524/>Team GESIS Cologne : An all in all sentence-based approach for FEVER<span class=acl-fixed-case>GESIS</span> Cologne: An all in all sentence-based approach for <span class=acl-fixed-case>FEVER</span></a></strong><br><a href=/people/w/wolfgang-otto/>Wolfgang Otto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5524><div class="card-body p-3 small">In this system description of our <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> to participate at the Fever Shared Task, we describe our sentence-based approach. Throughout all steps of our <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a>, we regarded single sentences as our processing unit. In our IR-Component, we searched in the set of all possible Wikipedia introduction sentences without limiting sentences to a fixed number of relevant documents. In the entailment module, we judged every sentence separately and combined the result of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> for the top 5 sentences with the help of an ensemble classifier to make a judgment whether the truth of a statement can be derived from the given claim.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5525 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5525/>Team SWEEPer : Joint Sentence Extraction and Fact Checking with Pointer Networks<span class=acl-fixed-case>SWEEP</span>er: Joint Sentence Extraction and Fact Checking with Pointer Networks</a></strong><br><a href=/people/c/christopher-hidey/>Christopher Hidey</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5525><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> rely on information extracted from unreliable sources. These <a href=https://en.wikipedia.org/wiki/System>systems</a> would thus benefit from knowing whether a statement from an unreliable source is correct. We present experiments on the FEVER (Fact Extraction and VERification) task, a shared task that involves selecting sentences from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and predicting whether a claim is supported by those sentences, refuted, or there is not enough information. Fact checking is a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that benefits from not only asserting or disputing the veracity of a claim but also finding evidence for that position. As these tasks are dependent on each other, an ideal model would consider the veracity of the claim when finding evidence and also find only the evidence that is relevant. We thus jointly model <a href=https://en.wikipedia.org/wiki/Sentence_extraction>sentence extraction</a> and <a href=https://en.wikipedia.org/wiki/Verification_and_validation>verification</a> on the FEVER shared task. Among all participants, we ranked 5th on the <a href=https://en.wikipedia.org/wiki/Blinded_experiment>blind test set</a> (prior to any additional human evaluation of the evidence).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5527 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5527/>Team UMBC-FEVER : Claim verification using Semantic Lexical Resources<span class=acl-fixed-case>UMBC</span>-<span class=acl-fixed-case>FEVER</span> : Claim verification using Semantic Lexical Resources</a></strong><br><a href=/people/a/ankur-padia/>Ankur Padia</a>
|
<a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/t/tim-finin/>Tim Finin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5527><div class="card-body p-3 small">We describe our <a href=https://en.wikipedia.org/wiki/System>system</a> used in the 2018 FEVER shared task. The system employed a frame-based information retrieval approach to select Wikipedia sentences providing evidence and used a <a href=https://en.wikipedia.org/wiki/Multilayer_perceptron>two-layer multilayer perceptron</a> to classify a claim as correct or not. Our submission achieved a score of 0.3966 on the Evidence F1 metric with accuracy of 44.79 %, and FEVER score of 0.2628 F1 points.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>