<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2021.naacl-industry.pdf>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</a></h2><p class=lead><a href=/people/y/young-bum-kim/>Young-bum Kim</a>,
<a href=/people/y/yunyao-li/>Yunyao Li</a>,
<a href=/people/o/owen-rambow/>Owen Rambow</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.naacl-industry</dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venue:</dt><dd><a href=/venues/naacl/>NAACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.naacl-industry>https://aclanthology.org/2021.naacl-industry</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.naacl-industry.pdf>https://aclanthology.org/2021.naacl-industry.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.naacl-industry.pdf title="Open PDF of 'Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2021+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics%3A+Human+Language+Technologies%3A+Industry+Papers" title="Search for 'Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.0/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</a></strong><br><a href=/people/y/young-bum-kim/>Young-bum Kim</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/o/owen-rambow/>Owen Rambow</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.1/>When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages</a></strong><br><a href=/people/s/stojan-trajanovski/>Stojan Trajanovski</a>
|
<a href=/people/c/chad-atalla/>Chad Atalla</a>
|
<a href=/people/k/kunho-kim/>Kunho Kim</a>
|
<a href=/people/v/vipul-agarwal/>Vipul Agarwal</a>
|
<a href=/people/m/milad-shokouhi/>Milad Shokouhi</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--1><div class="card-body p-3 small">Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and <a href=https://en.wikipedia.org/wiki/Microsoft_Outlook>Outlook</a>, finding that contextual signals contribute to performance differently between these scenarios. On <a href=https://en.wikipedia.org/wiki/Email>emails</a>, time context is most beneficial with small relative gains of 2 % over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> yields relative improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> between 9.3 % and 18.6 % across various critical service-oriented text prediction metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-industry.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.10/>Proteno : Text Normalization with Limited Data for Fast Deployment in Text to Speech Systems</a></strong><br><a href=/people/s/shubhi-tyagi/>Shubhi Tyagi</a>
|
<a href=/people/a/antonio-bonafonte/>Antonio Bonafonte</a>
|
<a href=/people/j/jaime-lorenzo-trueba/>Jaime Lorenzo-Trueba</a>
|
<a href=/people/j/javier-latorre/>Javier Latorre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--10><div class="card-body p-3 small">Developing Text Normalization (TN) systems for Text-to-Speech (TTS) on new languages is hard. We propose a novel <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> to facilitate it for multiple languages while using <a href=https://en.wikipedia.org/wiki/Data>data</a> less than 3 % of the size of the data used by the state of the art results on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We treat TN as a sequence classification problem and propose a granular tokenization mechanism that enables the system to learn majority of the classes and their normalizations from the training data itself. This is further combined with minimal precoded linguistic knowledge for other classes. We publish the first results on TN for TTS in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a> and also demonstrate that the performance of the approach is comparable with the previous work done on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. All annotated datasets used for experimentation will be released.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.14/>Autocorrect in the Process of Translation Multi-task Learning Improves Dialogue Machine Translation</a></strong><br><a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/c/chengqi-zhao/>Chengqi Zhao</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--14><div class="card-body p-3 small">Automatic translation of dialogue texts is a much needed demand in many real life scenarios. However, the currently existing <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> delivers unsatisfying results. In this paper, we conduct a deep analysis of a dialogue corpus and summarize three major issues on dialogue translation, including pronoun dropping (), punctuation dropping (), and typos (). In response to these challenges, we propose a joint learning method to identify omission and typo, and utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> to translate dialogue utterances. To properly evaluate the performance, we propose a manually annotated dataset with 1,931 Chinese-English parallel utterances from 300 dialogues as a benchmark testbed for dialogue translation. Our experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> improves translation quality by 3.2 BLEU over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. It also elevates the recovery rate of omitted pronouns from 26.09 % to 47.16 %. We will publish the code and dataset publicly at https://xxx.xx.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-industry.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.16/>Practical Transformer-based Multilingual Text Classification</a></strong><br><a href=/people/c/cindy-wang/>Cindy Wang</a>
|
<a href=/people/m/michele-banko/>Michele Banko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--16><div class="card-body p-3 small">Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on two distinct <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> can improve <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance without the need for additional labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.19/>Graph-based Multilingual Product Retrieval in E-Commerce Search<span class=acl-fixed-case>E</span>-Commerce Search</a></strong><br><a href=/people/h/hanqing-lu/>Hanqing Lu</a>
|
<a href=/people/y/youna-hu/>Youna Hu</a>
|
<a href=/people/t/tong-zhao/>Tong Zhao</a>
|
<a href=/people/t/tony-wu/>Tony Wu</a>
|
<a href=/people/y/yiwei-song/>Yiwei Song</a>
|
<a href=/people/b/bing-yin/>Bing Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--19><div class="card-body p-3 small">Nowadays, with many <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce platforms</a> conducting global business, e-commerce search systems are required to handle product retrieval under multilingual scenarios. Moreover, comparing with maintaining per-country specific e-commerce search systems, having an universal system across countries can further reduce the operational and computational costs, and facilitate business expansion to new countries. In this paper, we introduce an universal end-to-end multilingual retrieval system, and discuss our learnings and technical details when training and deploying the <a href=https://en.wikipedia.org/wiki/System>system</a> to serve billion-scale product retrieval for e-commerce search. In particular, we propose a multilingual graph attention based retrieval network by leveraging recent advances in transformer-based multilingual language models and graph neural network architectures to capture the interactions between search queries and items in e-commerce search. Offline experiments on five countries data show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> outperforms the state-of-the-art baselines by 35 % <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and 25 % mAP on average. Moreover, the proposed model shows significant increase of conversion / revenue in online A / B experiments and has been deployed in production for multiple countries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Industry Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-industry.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.20/>Query2Prod2Vec : Grounded Word Embeddings for <a href=https://en.wikipedia.org/wiki/E-commerce>eCommerce</a><span class=acl-fixed-case>Q</span>uery2<span class=acl-fixed-case>P</span>rod2<span class=acl-fixed-case>V</span>ec: Grounded Word Embeddings for e<span class=acl-fixed-case>C</span>ommerce</a></strong><br><a href=/people/f/federico-bianchi/>Federico Bianchi</a>
|
<a href=/people/j/jacopo-tagliabue/>Jacopo Tagliabue</a>
|
<a href=/people/b/bingqing-yu/>Bingqing Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--20><div class="card-body p-3 small">We present Query2Prod2Vec, a model that grounds lexical representations for product search in product embeddings : in our model, meaning is a mapping between words and a latent space of products in a digital shop. We leverage shopping sessions to learn the underlying space and use merchandising annotations to build lexical analogies for evaluation : our experiments show that our model is more accurate than known techniques from the NLP and IR literature. Finally, we stress the importance of <a href=https://en.wikipedia.org/wiki/Data_efficiency>data efficiency</a> for product search outside of retail giants, and highlight how Query2Prod2Vec fits with practical constraints faced by most practitioners.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.21/>An <a href=https://en.wikipedia.org/wiki/Architecture>Architecture</a> for Accelerated Large-Scale Inference of Transformer-Based Language Models</a></strong><br><a href=/people/a/amir-ganiev/>Amir Ganiev</a>
|
<a href=/people/c/colton-chapin/>Colton Chapin</a>
|
<a href=/people/a/anderson-de-andrade/>Anderson De Andrade</a>
|
<a href=/people/c/chen-liu/>Chen Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--21><div class="card-body p-3 small">This work demonstrates the development process of a machine learning architecture for <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> that can scale to a large volume of requests. We used a BERT model that was fine-tuned for emotion analysis, returning a probability distribution of emotions given a paragraph. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> was deployed as a gRPC service on <a href=https://en.wikipedia.org/wiki/Kubernetes>Kubernetes</a>. Apache Spark was used to perform <a href=https://en.wikipedia.org/wiki/Inference>inference</a> in batches by calling the <a href=https://en.wikipedia.org/wiki/Service_(systems_architecture)>service</a>. We encountered some performance and concurrency challenges and created solutions to achieve <a href=https://en.wikipedia.org/wiki/Time_complexity>faster running time</a>. Starting with 200 successful inference requests per minute, we were able to achieve as high as 18 thousand successful requests per minute with the same batch job resource allocation. As a result, we successfully stored emotion probabilities for 95 million paragraphs within 96 hours.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.24/>Cost-effective Deployment of BERT Models in Serverless Environment<span class=acl-fixed-case>BERT</span> Models in Serverless Environment</a></strong><br><a href=/people/m/marek-suppa/>Marek Suppa</a>
|
<a href=/people/k/katarina-benesova/>Katarína Benešová</a>
|
<a href=/people/a/andrej-svec/>Andrej Švec</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--24><div class="card-body p-3 small">In this study, we demonstrate the viability of deploying BERT-style models to <a href=https://en.wikipedia.org/wiki/AWS_Lambda>AWS Lambda</a> in a production environment. Since the freely available pre-trained models are too large to be deployed in this environment, we utilize knowledge distillation and fine-tune the models on proprietary datasets for two real-world tasks : <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic textual similarity</a>. As a result, we obtain <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that are tuned for a specific domain and deployable in the <a href=https://en.wikipedia.org/wiki/Serverless_computing>serverless environment</a>. The subsequent performance analysis shows that this solution does not only report latency levels acceptable for production use but that it is also a cost-effective alternative to small-to-medium size deployments of BERT models, all without any infrastructure overhead.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.25/>Noise Robust Named Entity Understanding for Voice Assistants</a></strong><br><a href=/people/d/deepak-muralidharan/>Deepak Muralidharan</a>
|
<a href=/people/j/joel-ruben-antony-moniz/>Joel Ruben Antony Moniz</a>
|
<a href=/people/s/sida-gao/>Sida Gao</a>
|
<a href=/people/x/xiao-yang/>Xiao Yang</a>
|
<a href=/people/j/justine-kao/>Justine Kao</a>
|
<a href=/people/s/stephen-pulman/>Stephen Pulman</a>
|
<a href=/people/a/atish-kothari/>Atish Kothari</a>
|
<a href=/people/r/ray-shen/>Ray Shen</a>
|
<a href=/people/y/yinying-pan/>Yinying Pan</a>
|
<a href=/people/v/vivek-kaul/>Vivek Kaul</a>
|
<a href=/people/m/mubarak-seyed-ibrahim/>Mubarak Seyed Ibrahim</a>
|
<a href=/people/g/gang-xiang/>Gang Xiang</a>
|
<a href=/people/n/nan-dun/>Nan Dun</a>
|
<a href=/people/y/yidan-zhou/>Yidan Zhou</a>
|
<a href=/people/a/andy-o/>Andy O</a>
|
<a href=/people/y/yuan-zhang/>Yuan Zhang</a>
|
<a href=/people/p/pooja-chitkara/>Pooja Chitkara</a>
|
<a href=/people/x/xuan-wang/>Xuan Wang</a>
|
<a href=/people/a/alkesh-patel/>Alkesh Patel</a>
|
<a href=/people/k/kushal-tayal/>Kushal Tayal</a>
|
<a href=/people/r/roger-zheng/>Roger Zheng</a>
|
<a href=/people/p/peter-grasch/>Peter Grasch</a>
|
<a href=/people/j/jason-d-williams/>Jason D Williams</a>
|
<a href=/people/l/lin-li/>Lin Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--25><div class="card-body p-3 small">Named Entity Recognition (NER) and Entity Linking (EL) play an essential role in voice assistant interaction, but are challenging due to the special difficulties associated with spoken user queries. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that jointly solves the NER and EL tasks by combining them in a joint reranking module. We show that our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> improves NER accuracy by up to 3.13 % and EL accuracy by up to 3.6 % in <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a>. The <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> used also lead to better accuracies in other natural language understanding tasks, such as domain classification and <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-industry.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.27/>Intent Features for Rich Natural Language Understanding</a></strong><br><a href=/people/b/brian-lester/>Brian Lester</a>
|
<a href=/people/s/sagnik-ray-choudhury/>Sagnik Ray Choudhury</a>
|
<a href=/people/r/rashmi-prasad/>Rashmi Prasad</a>
|
<a href=/people/s/srinivas-bangalore/>Srinivas Bangalore</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--27><div class="card-body p-3 small">Complex natural language understanding modules in dialog systems have a richer understanding of user utterances, and thus are critical in providing a better user experience. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are often created from scratch, for specific clients and use cases and require the annotation of large datasets. This encourages the sharing of annotated data across multiple clients. To facilitate this we introduce the idea of intent features : domain and topic agnostic properties of intents that can be learnt from the syntactic cues only, and hence can be shared. We introduce a new neural network architecture, the Global-Local model, that shows significant improvement over strong baselines for identifying these features in a deployed, multi-intent natural language understanding module, and more generally in a classification setting where a part of an utterance has to be classified utilizing the whole context.<i>intent features</i>: domain and topic agnostic properties of intents that can be learnt from the syntactic cues only, and hence can be shared. We introduce a new neural network architecture, the Global-Local model, that shows significant improvement over strong baselines for identifying these features in a deployed, multi-intent natural language understanding module, and more generally in a classification setting where a part of an utterance has to be classified utilizing the whole context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.33/>Ad Headline Generation using Self-Critical Masked Language Model</a></strong><br><a href=/people/y/yashal-shakti-kanungo/>Yashal Shakti Kanungo</a>
|
<a href=/people/s/sumit-negi/>Sumit Negi</a>
|
<a href=/people/a/aruna-rajan/>Aruna Rajan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--33><div class="card-body p-3 small">For any <a href=https://en.wikipedia.org/wiki/E-commerce>E-commerce website</a> it is a nontrivial problem to build enduring advertisements that attract shoppers. It is hard to pass the creative quality bar of the website, especially at a large scale. We thus propose a programmatic solution to generate product advertising headlines using retail content. We propose a state of the art application of Reinforcement Learning (RL) Policy gradient methods on Transformer (Vaswani et al., 2017) based Masked Language Models (Devlin et al., 2019). Our method creates the advertising headline by jointly conditioning on multiple products that a seller wishes to advertise. We demonstrate that our method outperforms existing Transformer and LSTM + RL methods in overlap metrics and quality audits. We also show that our model generated headlines outperform human submitted headlines in terms of both <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> and creative quality as determined by audits.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.34/>LATEX-Numeric : Language Agnostic Text Attribute Extraction for Numeric Attributes<span class=acl-fixed-case>LATEX</span>-Numeric: Language Agnostic Text Attribute Extraction for Numeric Attributes</a></strong><br><a href=/people/k/kartik-mehta/>Kartik Mehta</a>
|
<a href=/people/i/ioana-oprea/>Ioana Oprea</a>
|
<a href=/people/n/nikhil-rasiwasia/>Nikhil Rasiwasia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--34><div class="card-body p-3 small">In this paper, we present LATEX-Numeric-a high-precision fully-automated scalable framework for extracting E-commerce numeric attributes from unstructured product text like product description. Most of the past work on attribute extraction is not scalable as they rely on manually curated training data, either with or without use of <a href=https://en.wikipedia.org/wiki/Active_learning_(machine_learning)>active learning</a>. We rely on distant supervision for <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data generation</a>, removing dependency on manual labels. One issue with distant supervision is that it leads to incomplete training annotation due to missing attribute values while <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching</a>. We propose a multi-task learning architecture to deal with missing labels in the training data, leading to <a href=https://en.wikipedia.org/wiki/F-number>F1 improvement</a> of 9.2 % for numeric attributes over state-of-the-art single-task architecture. While multi-task architecture benefits both numeric and non-numeric attributes, we present automated techniques to further improve the numeric attributes extraction models. Numeric attributes require a list of units (or aliases) for better matching with distant supervision. We propose an automated algorithm for alias creation using <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a> and <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attribute values</a>, leading to a 20.2 % <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>F1 improvement</a>. Extensive experiments on real world datasets for 20 numeric attributes across 5 product categories and 3 English marketplaces show that LATEX-numeric achieves a high F1-score, without any manual intervention, making it suitable for practical applications. Finally we show that the improvements are language-agnostic and LATEX-Numeric achieves 13.9 % F1 improvement for 3 non-English languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.35/>Training Language Models under Resource Constraints for Adversarial Advertisement Detection</a></strong><br><a href=/people/e/eshwar-shamanna-girishekar/>Eshwar Shamanna Girishekar</a>
|
<a href=/people/s/shiv-surya/>Shiv Surya</a>
|
<a href=/people/n/nishant-nikhil/>Nishant Nikhil</a>
|
<a href=/people/d/dyut-kumar-sil/>Dyut Kumar Sil</a>
|
<a href=/people/s/sumit-negi/>Sumit Negi</a>
|
<a href=/people/a/aruna-rajan/>Aruna Rajan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--35><div class="card-body p-3 small">Advertising on <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce</a> and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning and multi-lingual training can be applied effectively to fine-tune BERT and its variants for text classification tasks in conjunction with different data augmentation strategies. Our extensive experiments on multiple languages show that these techniques detect adversarial ad categories with a substantial gain in precision at high recall threshold over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.39/>Industry Scale Semi-Supervised Learning for Natural Language Understanding</a></strong><br><a href=/people/l/luoxin-chen/>Luoxin Chen</a>
|
<a href=/people/f/francisco-garcia/>Francisco Garcia</a>
|
<a href=/people/v/varun-kumar/>Varun Kumar</a>
|
<a href=/people/h/he-xie/>He Xie</a>
|
<a href=/people/j/jianhua-lu/>Jianhua Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--39><div class="card-body p-3 small">This paper presents a production Semi-Supervised Learning (SSL) pipeline based on the student-teacher framework, which leverages millions of unlabeled examples to improve Natural Language Understanding (NLU) tasks. We investigate two questions related to the use of unlabeled data in production SSL context : 1) how to select samples from a huge unlabeled data pool that are beneficial for SSL training, and 2) how does the selected data affect the performance of different state-of-the-art SSL techniques. We compare four widely used SSL techniques, Pseudo-label (PL), Knowledge Distillation (KD), Virtual Adversarial Training (VAT) and Cross-View Training (CVT) in conjunction with two data selection methods including committee-based selection and submodular optimization based selection. We further examine the benefits and drawbacks of these techniques when applied to intent classification (IC) and named entity recognition (NER) tasks, and provide guidelines specifying when each of these methods might be beneficial to improve large scale NLU systems.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>