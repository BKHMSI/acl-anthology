<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the First Workshop on Neural Machine Translation - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W17-32.pdf>Proceedings of the First Workshop on Neural Machine Translation</a></h2><p class=lead><a href=/people/m/minh-thang-luong/>Thang Luong</a>,
<a href=/people/a/alexandra-birch/>Alexandra Birch</a>,
<a href=/people/g/graham-neubig/>Graham Neubig</a>,
<a href=/people/a/andrew-finch/>Andrew Finch</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W17-32</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Vancouver</dd><dt>Venues:</dt><dd><a href=/venues/ngt/>NGT</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W17-32>https://aclanthology.org/W17-32</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/W17-32 title="To the current version of the paper by DOI">10.18653/v1/W17-32</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W17-32.pdf>https://aclanthology.org/W17-32.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W17-32.pdf title="Open PDF of 'Proceedings of the First Workshop on Neural Machine Translation'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+First+Workshop+on+Neural+Machine+Translation" title="Search for 'Proceedings of the First Workshop on Neural Machine Translation' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3200/>Proceedings of the First Workshop on Neural Machine Translation</a></strong><br><a href=/people/m/minh-thang-luong/>Thang Luong</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3201/>An Empirical Study of Adequate <a href=https://en.wikipedia.org/wiki/Vision_span>Vision Span</a> for Attention-Based Neural Machine Translation</a></strong><br><a href=/people/r/raphael-shu/>Raphael Shu</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3201><div class="card-body p-3 small">Recently, the attention mechanism plays a key role to achieve high performance for Neural Machine Translation models. However, as it computes a <a href=https://en.wikipedia.org/wiki/Score_function>score function</a> for the encoder states in all positions at each decoding step, the attention model greatly increases the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a>. In this paper, we investigate the adequate vision span of attention models in the context of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, by proposing a novel attention framework that is capable of reducing redundant score computation dynamically. The term <a href=https://en.wikipedia.org/wiki/Vision_span>vision span</a>&#8217; means a window of the encoder states considered by the attention model in one step. In our experiments, we found that the average window size of vision span can be reduced by over 50 % with modest loss in accuracy on English-Japanese and German-English translation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3202/>Analyzing Neural MT Search and Model Performance<span class=acl-fixed-case>MT</span> Search and Model Performance</a></strong><br><a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/e/eunah-cho/>Eunah Cho</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3202><div class="card-body p-3 small">In this paper, we offer an in-depth analysis about the modeling and search performance. We address the question if a more complex <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithm</a> is necessary. Furthermore, we investigate the question if more complex <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> which might only be applicable during rescoring are promising. By separating the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> and the <a href=https://en.wikipedia.org/wiki/Mathematical_model>modeling</a> using n-best list reranking, we analyze the influence of both parts of an NMT system independently. By comparing differently performing NMT systems, we show that the better translation is already in the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> of the translation systems with less performance. This results indicate that the current <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithms</a> are sufficient for the <a href=https://en.wikipedia.org/wiki/Network_topology>NMT systems</a>. Furthermore, we could show that even a relatively small n-best list of 50 hypotheses already contain notably better translations.<tex-math>n</tex-math>-best list of 50 hypotheses already contain notably better translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3203/>Stronger Baselines for Trustable Results in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/michael-denkowski/>Michael Denkowski</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3203><div class="card-body p-3 small">Interest in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> has grown rapidly as its effectiveness has been demonstrated across language and data scenarios. New research regularly introduces architectural and algorithmic improvements that lead to significant gains over vanilla NMT implementations. However, these new <a href=https://en.wikipedia.org/wiki/Software_development_process>techniques</a> are rarely evaluated in the context of previously published <a href=https://en.wikipedia.org/wiki/Software_development_process>techniques</a>, specifically those that are widely used in state-of-the-art production and shared-task systems. As a result, it is often difficult to determine whether improvements from research will carry over to <a href=https://en.wikipedia.org/wiki/System>systems</a> deployed for real-world use. In this work, we recommend three specific <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> that are relatively easy to implement and result in much stronger experimental systems. Beyond reporting significantly higher BLEU scores, we conduct an in-depth analysis of where improvements originate and what inherent weaknesses of basic NMT models are being addressed. We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen. This indicates that choosing a strong <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> is crucial for reporting reliable experimental results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3205/>Cost Weighting for Neural Machine Translation Domain Adaptation</a></strong><br><a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/s/samuel-larkin/>Samuel Larkin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3205><div class="card-body p-3 small">In this paper, we propose a new domain adaptation technique for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> called cost weighting, which is appropriate for adaptation scenarios in which a small in-domain data set and a large general-domain data set are available. Cost weighting incorporates a domain classifier into the neural machine translation training algorithm, using features derived from the encoder representation in order to distinguish in-domain from out-of-domain data. Classifier probabilities are used to weight sentences according to their domain similarity when updating the parameters of the neural translation model. We compare cost weighting to two traditional domain adaptation techniques developed for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> : data selection and sub-corpus weighting. Experiments on two large-data tasks show that both the traditional techniques and our novel proposal lead to significant gains, with cost weighting outperforming the traditional methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3206/>Detecting Untranslated Content for Neural Machine Translation</a></strong><br><a href=/people/i/isao-goto/>Isao Goto</a>
|
<a href=/people/h/hideki-tanaka/>Hideki Tanaka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3206><div class="card-body p-3 small">Despite its promise, neural machine translation (NMT) has a serious problem in that source content may be mistakenly left untranslated. The ability to detect untranslated content is important for the practical use of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>. We evaluate two types of <a href=https://en.wikipedia.org/wiki/Probability>probability</a> with which to detect untranslated content : the cumulative attention (ATN) probability and back translation (BT) probability from the target sentence to the source sentence. Experiments on detecting untranslated content in Japanese-English patent translations show that ATN and BT are each more effective than random choice, BT is more effective than ATN, and the combination of the two provides further improvements. We also confirmed the effectiveness of using <a href=https://en.wikipedia.org/wiki/Atrial_natriuretic_peptide>ATN</a> and <a href=https://en.wikipedia.org/wiki/Thiamine_triphosphate>BT</a> to rerank the n-best NMT outputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3207/>Beam Search Strategies for Neural Machine Translation</a></strong><br><a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/y/yaser-al-onaizan/>Yaser Al-Onaizan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3207><div class="card-body p-3 small">The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance on a given <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to-right while keeping a fixed amount of active candidates at each time step. First, this simple <a href=https://en.wikipedia.org/wiki/Search_algorithm>search</a> is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the <a href=https://en.wikipedia.org/wiki/Beam_diameter>beam size</a> until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43 % for the two language pairs <a href=https://en.wikipedia.org/wiki/German_language>German</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a> without losing any translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3209/>Detecting Cross-Lingual Semantic Divergence for Neural Machine Translation</a></strong><br><a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/y/yogarshi-vyas/>Yogarshi Vyas</a>
|
<a href=/people/x/xing-niu/>Xing Niu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3209><div class="card-body p-3 small">Parallel corpora are often not as parallel as one might assume : non-literal translations and noisy translations abound, even in curated corpora routinely used for training and evaluation. We use a cross-lingual textual entailment system to distinguish sentence pairs that are parallel in meaning from those that are not, and show that filtering out divergent examples from training improves translation quality.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>