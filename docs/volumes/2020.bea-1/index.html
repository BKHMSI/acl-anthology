<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2020.bea-1.pdf>Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></h2><p class=lead><a href=/people/j/jill-burstein/>Jill Burstein</a>,
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>,
<a href=/people/c/claudia-leacock/>Claudia Leacock</a>,
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>,
<a href=/people/i/ildiko-pilan/>Ildikó Pilán</a>,
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a>,
<a href=/people/t/torsten-zesch/>Torsten Zesch</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.bea-1</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Seattle, WA, USA → Online</dd><dt>Venues:</dt><dd><a href=/venues/acl/>ACL</a>
| <a href=/venues/bea/>BEA</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigedu/>SIGEDU</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.bea-1>https://aclanthology.org/2020.bea-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.bea-1.pdf>https://aclanthology.org/2020.bea-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.bea-1.pdf title="Open PDF of 'Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Fifteenth+Workshop+on+Innovative+Use+of+NLP+for+Building+Educational+Applications" title="Search for 'Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bea-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bea-1.0/>Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></strong><br><a href=/people/j/jill-burstein/>Jill Burstein</a>
|
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>
|
<a href=/people/c/claudia-leacock/>Claudia Leacock</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/i/ildiko-pilan/>Ildikó Pilán</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bea-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bea-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bea-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bea-1.4/>Complementary Systems for Off-Topic Spoken Response Detection</a></strong><br><a href=/people/v/vatsal-raina/>Vatsal Raina</a>
|
<a href=/people/m/mark-gales/>Mark Gales</a>
|
<a href=/people/k/kate-knill/>Kate Knill</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bea-1--4><div class="card-body p-3 small">Increased demand to learn English for business and education has led to growing interest in automatic spoken language assessment and teaching systems. With this shift to automated approaches it is important that systems reliably assess all aspects of a candidate&#8217;s responses. This paper examines one form of spoken language assessment ; whether the response from the candidate is relevant to the prompt provided. This will be referred to as off-topic spoken response detection. Two forms of previously proposed approaches are examined in this work : the hierarchical attention-based topic model (HATM) ; and the similarity grid model (SGM). The work focuses on the scenario when the prompt, and associated responses, have not been seen in the training data, enabling the <a href=https://en.wikipedia.org/wiki/System>system</a> to be applied to new test scripts without the need to collect data or retrain the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. To improve the performance of the systems for unseen prompts, <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> based on easy data augmentation (EDA) and translation based approaches are applied. Additionally for the HATM, a form of prompt dropout is described. The <a href=https://en.wikipedia.org/wiki/System>systems</a> were evaluated on both seen and unseen prompts from Linguaskill Business and General English tests. For unseen data the performance of the HATM was improved using <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, in contrast to the <a href=https://en.wikipedia.org/wiki/SGM>SGM</a> where no gains were obtained. The two <a href=https://en.wikipedia.org/wiki/Computer_simulation>approaches</a> were found to be complementary to one another, yielding a combined <a href=https://en.wikipedia.org/wiki/F-number>F0.5 score</a> of 0.814 for off-topic response detection where the prompts have not been seen in training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bea-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bea-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bea-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bea-1.5/>CIMA : A Large Open Access Dialogue Dataset for Tutoring<span class=acl-fixed-case>CIMA</span>: A Large Open Access Dialogue Dataset for Tutoring</a></strong><br><a href=/people/k/katherine-stasaski/>Katherine Stasaski</a>
|
<a href=/people/k/kimberly-kao/>Kimberly Kao</a>
|
<a href=/people/m/marti-a-hearst/>Marti A. Hearst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bea-1--5><div class="card-body p-3 small">One-to-one tutoring is often an effective means to help students learn, and recent experiments with neural conversation systems are promising. However, large open datasets of tutoring conversations are lacking. To remedy this, we propose a novel asynchronous method for collecting tutoring dialogue via crowdworkers that is both amenable to the needs of deep learning algorithms and reflective of pedagogical concerns. In this approach, extended conversations are obtained between crowdworkers role-playing as both students and tutors. The CIMA collection, which we make publicly available, is novel in that students are exposed to overlapping grounded concepts between exercises and multiple relevant tutoring responses are collected for the same input. CIMA contains several compelling properties from an educational perspective : student role-players complete exercises in fewer turns during the course of the conversation and tutor players adopt strategies that conform with some educational conversational norms, such as providing hints versus asking questions in appropriate contexts. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> enables a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to be trained to generate the next tutoring utterance in a conversation, conditioned on a provided action strategy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bea-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bea-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bea-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.bea-1.6.Dataset.pdf data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.bea-1.6/>Becoming Linguistically Mature : Modeling English and German Children’s Writing Development Across School Grades<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>G</span>erman Children’s Writing Development Across School Grades</a></strong><br><a href=/people/e/elma-kerz/>Elma Kerz</a>
|
<a href=/people/y/yu-qiao/>Yu Qiao</a>
|
<a href=/people/d/daniel-wiechmann/>Daniel Wiechmann</a>
|
<a href=/people/m/marcus-strobel/>Marcus Ströbel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bea-1--6><div class="card-body p-3 small">In this paper we employ a novel approach to advancing our understanding of the development of writing in English and German children across school grades using classification tasks. The <a href=https://en.wikipedia.org/wiki/Data>data</a> used come from two recently compiled corpora : The English data come from the the GiC corpus (983 school children in second-, sixth-, ninth- and eleventh-grade) and the German data are from the FD-LEX corpus (930 school children in fifth- and ninth-grade). The key to this paper is the combined use of what we refer to as &#8216;complexity contours&#8217;, i.e. series of measurements that capture the progression of linguistic complexity within a text, and Recurrent Neural Network (RNN) classifiers that adequately capture the sequential information in those contours. Our experiments demonstrate that RNN classifiers trained on complexity contours achieve higher classification accuracy than one trained on text-average complexity scores. In a second step, we determine the relative importance of the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from four distinct categories through a Sensitivity-Based Pruning approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bea-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bea-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bea-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bea-1.8/>Can <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> Automatically Score Essay Traits?</a></strong><br><a href=/people/s/sandeep-mathias/>Sandeep Mathias</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bea-1--8><div class="card-body p-3 small">Essay traits are attributes of an essay that can help explain how well written (or badly written) the essay is. Examples of traits include <a href=https://en.wikipedia.org/wiki/Content_(media)>Content</a>, Organization, <a href=https://en.wikipedia.org/wiki/Language>Language</a>, Sentence Fluency, <a href=https://en.wikipedia.org/wiki/Word_choice>Word Choice</a>, etc. A lot of research in the last decade has dealt with automatic holistic essay scoring-where a machine rates an essay and gives a score for the essay. However, writers need feedback, especially if they want to improve their writing-which is why trait-scoring is important. In this paper, we show how a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep-learning based system</a> can outperform feature-based machine learning systems, as well as a string kernel system in scoring <a href=https://en.wikipedia.org/wiki/Essay>essay traits</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bea-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bea-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bea-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bea-1.12/>Applications of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> in Bilingual Language Teaching : An Indonesian-English Case Study<span class=acl-fixed-case>I</span>ndonesian-<span class=acl-fixed-case>E</span>nglish Case Study</a></strong><br><a href=/people/z/zara-maxwelll-smith/>Zara Maxwelll-Smith</a>
|
<a href=/people/s/simon-gonzalez-ochoa/>Simón González Ochoa</a>
|
<a href=/people/b/ben-foley/>Ben Foley</a>
|
<a href=/people/h/hanna-suominen/>Hanna Suominen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bea-1--12><div class="card-body p-3 small">Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this <a href=https://en.wikipedia.org/wiki/Data>data</a> so rich and problematic to classify. In this paper, we set out methodological considerations of using <a href=https://en.wikipedia.org/wiki/Speech_recognition>automated speech recognition</a> to build a <a href=https://en.wikipedia.org/wiki/Speech_corpus>corpus of teacher speech</a> in an Indonesian language classroom. Our preliminary results (64 % word error rate) suggest these tools have the potential to speed <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> in this context. We provide practical examples of our data structure, details of our piloted computer-assisted processes, and fine-grained error analysis. Our study is informed and directed by genuine research questions and discussion in both the education and computational linguistics fields. We highlight some of the benefits and risks of using these emerging <a href=https://en.wikipedia.org/wiki/Technology>technologies</a> to analyze the complex work of <a href=https://en.wikipedia.org/wiki/Language_education>language teachers</a> and in <a href=https://en.wikipedia.org/wiki/Education>education</a> more generally.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bea-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bea-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bea-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bea-1.13/>An empirical investigation of neural methods for content scoring of science explanations</a></strong><br><a href=/people/b/brian-riordan/>Brian Riordan</a>
|
<a href=/people/s/sarah-bichler/>Sarah Bichler</a>
|
<a href=/people/a/allison-bradford/>Allison Bradford</a>
|
<a href=/people/j/jennifer-king-chen/>Jennifer King Chen</a>
|
<a href=/people/k/korah-wiley/>Korah Wiley</a>
|
<a href=/people/l/libby-gerard/>Libby Gerard</a>
|
<a href=/people/m/marcia-c-linn/>Marcia C. Linn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bea-1--13><div class="card-body p-3 small">With the widespread adoption of the Next Generation Science Standards (NGSS), science teachers and online learning environments face the challenge of evaluating students&#8217; integration of different dimensions of <a href=https://en.wikipedia.org/wiki/Science_education>science learning</a>. Recent advances in representation learning in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> have proven effective across many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>, but a rigorous evaluation of the relative merits of these methods for scoring complex constructed response formative assessments has not previously been carried out. We present a detailed empirical investigation of feature-based, recurrent neural network, and pre-trained transformer models on scoring content in real-world formative assessment data. We demonstrate that recent <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural methods</a> can rival or exceed the performance of feature-based methods. We also provide evidence that different classes of neural models take advantage of different learning cues, and pre-trained transformer models may be more robust to spurious, dataset-specific learning cues, better reflecting scoring rubrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bea-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bea-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bea-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.bea-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.bea-1.16/>GECToR Grammatical Error Correction : Tag, Not Rewrite<span class=acl-fixed-case>GECT</span>o<span class=acl-fixed-case>R</span> – Grammatical Error Correction: Tag, Not Rewrite</a></strong><br><a href=/people/k/kostiantyn-omelianchuk/>Kostiantyn Omelianchuk</a>
|
<a href=/people/v/vitaliy-atrasevych/>Vitaliy Atrasevych</a>
|
<a href=/people/a/artem-chernodub/>Artem Chernodub</a>
|
<a href=/people/o/oleksandr-skurzhanskyi/>Oleksandr Skurzhanskyi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bea-1--16><div class="card-body p-3 small">In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages : first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model / ensemble GEC tagger achieves an F_0.5 of 65.3/66.5 on CONLL-2014 (test) and F_0.5 of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bea-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bea-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bea-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.bea-1.17.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.bea-1.17/>Interpreting Neural CWI Classifiers’ Weights as Vocabulary Size<span class=acl-fixed-case>CWI</span> Classifiers’ Weights as Vocabulary Size</a></strong><br><a href=/people/y/yo-ehara/>Yo Ehara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bea-1--17><div class="card-body p-3 small">Complex Word Identification (CWI) is a task for the identification of words that are challenging for <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>second-language learners</a> to read. Even though the use of neural classifiers is now common in CWI, the interpretation of their parameters remains difficult. This paper analyzes neural CWI classifiers and shows that some of their parameters can be interpreted as <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary size</a>. We present a novel formalization of vocabulary size measurement methods that are practiced in the applied linguistics field as a kind of neural classifier. We also contribute to building a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for validating vocabulary testing and readability via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.bea-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--bea-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.bea-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.bea-1.20/>Predicting the Difficulty and Response Time of Multiple Choice Questions Using Transfer Learning</a></strong><br><a href=/people/k/kang-xue/>Kang Xue</a>
|
<a href=/people/v/victoria-yaneva/>Victoria Yaneva</a>
|
<a href=/people/c/christopher-runyon/>Christopher Runyon</a>
|
<a href=/people/p/peter-baldwin/>Peter Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--bea-1--20><div class="card-body p-3 small">This paper investigates whether <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> can improve the prediction of the difficulty and response time parameters for 18,000 multiple-choice questions from a high-stakes medical exam. The type the signal that best predicts difficulty and <a href=https://en.wikipedia.org/wiki/Response_time_(technology)>response time</a> is also explored, both in terms of <a href=https://en.wikipedia.org/wiki/Abstraction_(computer_science)>representation abstraction</a> and item component used as input (e.g., whole item, answer options only, etc.). The results indicate that, for our sample, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> can improve the prediction of item difficulty when <a href=https://en.wikipedia.org/wiki/Mental_chronometry>response time</a> is used as an auxiliary task but not the other way around. In addition, difficulty was best predicted using signal from the item stem (the description of the clinical case), while all parts of the item were important for predicting the response time.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>