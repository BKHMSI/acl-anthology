<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 13th International Conference on Computational Semantics - Long Papers - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W19-04.pdf>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</a></h2><p class=lead><a href=/people/s/simon-dobnik/>Simon Dobnik</a>,
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>,
<a href=/people/v/vera-demberg/>Vera Demberg</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W19-04</dd><dt>Month:</dt><dd>May</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Gothenburg, Sweden</dd><dt>Venues:</dt><dd><a href=/venues/iwcs/>IWCS</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigsem/>SIGSEM</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W19-04>https://aclanthology.org/W19-04</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W19-04.pdf>https://aclanthology.org/W19-04.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W19-04.pdf title="Open PDF of 'Proceedings of the 13th International Conference on Computational Semantics - Long Papers'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+13th+International+Conference+on+Computational+Semantics+-+Long+Papers" title="Search for 'Proceedings of the 13th International Conference on Computational Semantics - Long Papers' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0400/>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</a></strong><br><a href=/people/s/simon-dobnik/>Simon Dobnik</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0401/>Projecting Temporal Properties, Events and Actions</a></strong><br><a href=/people/t/tim-fernando/>Tim Fernando</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0401><div class="card-body p-3 small">Temporal notions based on a finite set A of properties are represented in strings, on which <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projections</a> are defined that vary the granularity A. The structure of properties in A is elaborated to describe statives, events and actions, subject to a distinction in meaning (advocated by Levin and Rappaport Hovav) between what the lexicon prescribes and what a context of use supplies. The <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projections</a> proposed are deployed as labels for records and record types amenable to <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite-state methods</a>.<i>A</i> of properties are represented in strings, on which projections are defined that vary the granularity <i>A</i>. The structure of properties in <i>A</i> is elaborated to describe statives, events and actions, subject to a distinction in meaning (advocated by Levin and Rappaport Hovav) between what the lexicon prescribes and what a context of use supplies. The projections proposed are deployed as labels for records and record types amenable to finite-state methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0402/>A Type-coherent, Expressive Representation as an Initial Step to <a href=https://en.wikipedia.org/wiki/Language_understanding>Language Understanding</a></a></strong><br><a href=/people/g/gene-louis-kim/>Gene Louis Kim</a>
|
<a href=/people/l/lenhart-schubert/>Lenhart Schubert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0402><div class="card-body p-3 small">A growing interest in tasks involving <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> by the NLP community has led to the need for effective <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> and <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. Modern NLP systems use semantic representations that do not quite fulfill the nuanced needs for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> : adequately modeling <a href=https://en.wikipedia.org/wiki/Semantics>language semantics</a>, enabling general inferences, and being accurately recoverable. This document describes underspecified logical forms (ULF) for Episodic Logic (EL), which is an initial form for a semantic representation that balances these needs. ULFs fully resolve the semantic type structure while leaving issues such as quantifier scope, <a href=https://en.wikipedia.org/wiki/Word_sense>word sense</a>, and <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a> unresolved ; they provide a starting point for further resolution into EL, and enable certain structural inferences without further resolution. This document also presents preliminary results of creating a hand-annotated corpus of ULFs for the purpose of training a precise ULF parser, showing a three-person pairwise interannotator agreement of 0.88 on confident annotations. We hypothesize that a divide-and-conquer approach to <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> starting with derivation of ULFs will lead to <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analyses</a> that do justice to subtle aspects of <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>linguistic meaning</a>, and will enable construction of more accurate semantic parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0405/>An Improved Approach for Semantic Graph Composition with CCG<span class=acl-fixed-case>CCG</span></a></strong><br><a href=/people/a/austin-blodgett/>Austin Blodgett</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0405><div class="card-body p-3 small">This paper builds on previous work using Combinatory Categorial Grammar (CCG) to derive a transparent syntax-semantics interface for Abstract Meaning Representation (AMR) parsing. We define new <a href=https://en.wikipedia.org/wiki/Semantics_(computer_science)>semantics</a> for the CCG combinators that is better suited to deriving AMR graphs. In particular, we define relation-wise alternatives for the application and composition combinators : these require that the two constituents being combined overlap in one AMR relation. We also provide a new <a href=https://en.wikipedia.org/wiki/Semantics_(computer_science)>semantics</a> for type raising, which is necessary for certain <a href=https://en.wikipedia.org/wiki/Constructor_(object-oriented_programming)>constructions</a>. Using these mechanisms, we suggest an analysis of eventive nouns, which present a challenge for deriving AMR graphs. Our theoretical analysis will facilitate future work on robust and transparent AMR parsing using CCG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0407/>Towards a Compositional Analysis of German Light Verb Constructions (LVCs) Combining Lexicalized Tree Adjoining Grammar (LTAG) with Frame Semantics<span class=acl-fixed-case>G</span>erman Light Verb Constructions (<span class=acl-fixed-case>LVC</span>s) Combining <span class=acl-fixed-case>L</span>exicalized <span class=acl-fixed-case>T</span>ree <span class=acl-fixed-case>A</span>djoining <span class=acl-fixed-case>G</span>rammar (<span class=acl-fixed-case>LTAG</span>) with Frame Semantics</a></strong><br><a href=/people/j/jens-fleischhauer/>Jens Fleischhauer</a>
|
<a href=/people/t/thomas-gamerschlag/>Thomas Gamerschlag</a>
|
<a href=/people/l/laura-kallmeyer/>Laura Kallmeyer</a>
|
<a href=/people/s/simon-petitjean/>Simon Petitjean</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0407><div class="card-body p-3 small">Complex predicates formed of a semantically &#8216;light&#8217; verbal head and a noun or verb which contributes the major part of the meaning are frequently referred to as &#8216;light verb constructions&#8217; (LVCs). In the paper, we present a case study of LVCs with the German posture verb stehen &#8216;stand&#8217;. In our account, we model the syntactic as well as semantic composition of such LVCs by combining Lexicalized Tree Adjoining Grammar (LTAG) with <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>frames</a>. Starting from the analysis of the literal uses of posture verbs, we show how the meaning components of the literal uses are systematically exploited in the interpretation of stehen-LVCs. The paper constitutes an important step towards a compositional and computational analysis of LVCs. We show that LTAG allows us to separate constructional from lexical meaning components and that frames enable elegant generalizations over event types and related constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0408/>Words are Vectors, Dependencies are Matrices : Learning Word Embeddings from Dependency Graphs</a></strong><br><a href=/people/p/paula-czarnowska/>Paula Czarnowska</a>
|
<a href=/people/g/guy-emerson/>Guy Emerson</a>
|
<a href=/people/a/ann-copestake/>Ann Copestake</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0408><div class="card-body p-3 small">Distributional Semantic Models (DSMs) construct vector representations of word meanings based on their contexts. Typically, the contexts of a word are defined as its closest neighbours, but they can also be retrieved from its syntactic dependency relations. In this work, we propose a new dependency-based DSM. The novelty of our model lies in associating an independent meaning representation, a <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a>, with each dependency-label. This allows <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to capture specifics of the relations between words and contexts, leading to good performance on both intrinsic and extrinsic evaluation tasks. In addition to that, our model has an inherent ability to represent dependency chains as products of matrices which provides a straightforward way of handling further contexts of a word.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-0409" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-0409/>Temporal and Aspectual Entailment</a></strong><br><a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/s/sander-bijl-de-vroe/>Sander Bijl de Vroe</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0409><div class="card-body p-3 small">Inferences regarding Jane&#8217;s arrival in London from <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predications</a> such as Jane is going to London or Jane has gone to London depend on <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and aspect of the predications. Tense determines the temporal location of the predication in the past, present or future of the time of utterance. The aspectual auxiliaries on the other hand specify the internal constituency of the event, i.e. whether the event of going to London is completed and whether its consequences hold at that time or not. While <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a> are among the most important factors for determining <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>, there has been very little work to show whether modern <a href=https://en.wikipedia.org/wiki/Embedding>embedding models</a> capture these semantic concepts. In this paper we propose a novel entailment dataset and analyse the ability of contextualised word representations to perform <a href=https://en.wikipedia.org/wiki/Inference>inference</a> on <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predications</a> across aspectual types and tenses. We show that they encode a substantial amount of information relating to <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>, but fail to consistently model inferences that require reasoning with these semantic properties.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0412/>Aligning Open IE Relations and KB Relations using a Siamese Network Based on Word Embedding<span class=acl-fixed-case>IE</span> Relations and <span class=acl-fixed-case>KB</span> Relations using a <span class=acl-fixed-case>S</span>iamese Network Based on Word Embedding</a></strong><br><a href=/people/r/rifki-afina-putri/>Rifki Afina Putri</a>
|
<a href=/people/g/giwon-hong/>Giwon Hong</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0412><div class="card-body p-3 small">Open Information Extraction (Open IE) aims at generating entity-relation-entity triples from a large amount of text, aiming at capturing key semantics of the text. Given a triple, the <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a> expresses the type of semantic relation between the entities. Although relations from an Open IE system are more extensible than those used in a traditional Information Extraction system and a Knowledge Base (KB) such as Knowledge Graphs, the former lacks in semantics ; an Open IE relation is simply a sequence of words, whereas a KB relation has a predefined meaning. As a way to provide a meaning to an Open IE relation, we attempt to align it with one of the predefined set of relations used in a KB. Our approach is to use a <a href=https://en.wikipedia.org/wiki/Siamese_network>Siamese network</a> that compares two sequences of word embeddings representing an Open IE relation and a predefined KB relation. In order to make the approach practical, we automatically generate a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training dataset</a> using a distant supervision approach instead of relying on a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>hand-labeled dataset</a>. Our experiment shows that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can capture the <a href=https://en.wikipedia.org/wiki/Relational_semantics>relational semantics</a> better than the recent <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0414 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-0414" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-0414/>The Effect of Context on Metaphor Paraphrase Aptness Judgments</a></strong><br><a href=/people/y/yuri-bizzoni/>Yuri Bizzoni</a>
|
<a href=/people/s/shalom-lappin/>Shalom Lappin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0414><div class="card-body p-3 small">We conduct two experiments to study the effect of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> on metaphor paraphrase aptness judgments. The first is an AMT crowd source task in which speakers rank metaphor-paraphrase candidate sentence pairs in short document contexts for paraphrase aptness. In the second we train a composite DNN to predict these human judgments, first in binary classifier mode, and then as gradient ratings. We found that for both mean human judgments and our DNN&#8217;s predictions, adding document context compresses the aptness scores towards the center of the scale, raising low out-of-context ratings and decreasing high out-of-context scores. We offer a provisional explanation for this compression effect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0415/>Predicting Word Concreteness and Imagery</a></strong><br><a href=/people/j/jean-charbonnier/>Jean Charbonnier</a>
|
<a href=/people/c/christian-wartena/>Christian Wartena</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0415><div class="card-body p-3 small">Concreteness of words has been studied extensively in psycholinguistic literature. A number of <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> have been created with average values for perceived concreteness of words. We show that we can train a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> on these data, using <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and morphological features, that can predict these concreteness values with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We evaluate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on 7 publicly available datasets. Only for a few small subsets of these datasets prediction of concreteness values are found in the literature. Our results clearly outperform the reported results for these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0416/>Learning to Explicitate Connectives with Seq2Seq Network for Implicit Discourse Relation Classification<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq Network for Implicit Discourse Relation Classification</a></strong><br><a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0416><div class="card-body p-3 small">Implicit discourse relation classification is one of the most difficult steps in discourse parsing. The difficulty stems from the fact that the coherence relation must be inferred based on the content of the discourse relational arguments. Therefore, an effective encoding of the relational arguments is of crucial importance. We here propose a new model for implicit discourse relation classification, which consists of a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>, and a sequence-to-sequence model which is trained to generate a representation of the discourse relational arguments by trying to predict the relational arguments including a suitable implicit connective. Training is possible because such implicit connectives have been annotated as part of the PDTB corpus. Along with a memory network, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> could generate more refined representations for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. And on the now standard 11-way classification, our method outperforms the previous state of the art systems on the PDTB benchmark on multiple settings including cross validation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-0421" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-0421/>Using Multi-Sense Vector Embeddings for Reverse Dictionaries</a></strong><br><a href=/people/m/michael-a-hedderich/>Michael A. Hedderich</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0421><div class="card-body p-3 small">Popular word embedding methods such as <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically can not serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of <a href=https://en.wikipedia.org/wiki/Reverse_dictionary>reverse dictionaries</a>. We propose a <a href=https://en.wikipedia.org/wiki/Scientific_technique>technique</a> to easily integrate them into an existing <a href=https://en.wikipedia.org/wiki/Neural_network>neural network architecture</a> using an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0425 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0425/>Frame Identification as Categorization : Exemplars vs Prototypes in Embeddingland</a></strong><br><a href=/people/j/jennifer-sikos/>Jennifer Sikos</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0425><div class="card-body p-3 small">Categorization is a central capability of <a href=https://en.wikipedia.org/wiki/Cognition>human cognition</a>, and a number of <a href=https://en.wikipedia.org/wiki/Theory>theories</a> have been developed to account for properties of <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a>. Even though many tasks in <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> also involve <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> of some kind, theories of <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> do not play a major role in contemporary research in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. This paper follows the idea that embedding-based models of semantics lend themselves well to being formulated in terms of classical categorization theories. The benefit is a space of model families that enables (a) the formulation of hypotheses about the impact of major design decisions, and (b) a transparent assessment of these decisions. We instantiate this idea on the task of frame-semantic frame identification. We define four models that cross two <a href=https://en.wikipedia.org/wiki/Design_of_experiments>design variables</a> : (a) the choice of <a href=https://en.wikipedia.org/wiki/Design_of_experiments>prototype vs. exemplar categorization</a>, corresponding to different degrees of <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> applied to the input ; and (b) the presence vs. absence of a <a href=https://en.wikipedia.org/wiki/Design_of_experiments>fine-tuning step</a>, corresponding to <a href=https://en.wikipedia.org/wiki/Design_of_experiments>generic vs. task-adaptive categorization</a>. We find that for frame identification, <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and task-adaptive categorization both yield substantial benefits. Our prototype-based, fine-tuned model, which combines the best choices for these variables, establishes a new state of the art in frame identification.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>