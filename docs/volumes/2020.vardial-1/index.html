<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects</h2><p class=lead><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>,
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>,
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>,
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>,
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.vardial-1</dd><dt>Month:</dt><dd>December</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Barcelona, Spain (Online)</dd><dt>Venues:</dt><dd><a href=/venues/coling/>COLING</a>
| <a href=/venues/vardial/>VarDial</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>International Committee on Computational Linguistics (ICCL)</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.vardial-1>https://aclanthology.org/2020.vardial-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+7th+Workshop+on+NLP+for+Similar+Languages%2C+Varieties+and+Dialects" title="Search for 'Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.0/>Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.2/>ASR for Non-standardised Languages with Dialectal Variation : the case of <a href=https://en.wikipedia.org/wiki/Swiss_German>Swiss German</a><span class=acl-fixed-case>ASR</span> for Non-standardised Languages with Dialectal Variation: the case of <span class=acl-fixed-case>S</span>wiss <span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/i/iuliia-nigmatulina/>Iuliia Nigmatulina</a>
|
<a href=/people/t/tannon-kew/>Tannon Kew</a>
|
<a href=/people/t/tanja-samardzic/>Tanja Samardzic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--2><div class="card-body p-3 small">Strong regional variation, together with the lack of standard <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a>, makes Swiss German automatic speech recognition (ASR) particularly difficult in a multi-dialectal setting. This paper focuses on one of the many challenges, namely, the choice of the output text to represent non-standardised Swiss German. We investigate two potential options : a) dialectal writing approximate phonemic transcriptions that provide close correspondence between grapheme labels and the acoustic signal but are highly inconsistent and b) normalised writing transcriptions resembling standard <a href=https://en.wikipedia.org/wiki/German_language>German</a> that are relatively consistent but distant from the acoustic signal. To find out which <a href=https://en.wikipedia.org/wiki/Writing>writing</a> facilitates Swiss German ASR, we build several systems using the <a href=https://en.wikipedia.org/wiki/Kaldi_(software)>Kaldi toolkit</a> and a dataset covering 14 regional varieties. A formal comparison shows that the system trained on the <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalised transcriptions</a> achieves better results in word error rate (WER) (29.39 %) but underperforms at the <a href=https://en.wikipedia.org/wiki/Character_(computing)>character level</a>, suggesting <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>dialectal transcriptions</a> offer a viable solution for downstream applications where <a href=https://en.wikipedia.org/wiki/Dialect>dialectal differences</a> are important. To better assess word-level performance for dialectal transcriptions, we use a flexible WER measure (FlexWER). When evaluated with this <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, the system trained on <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>dialectal transcriptions</a> outperforms that trained on the normalised writing. Besides establishing a benchmark for Swiss German multi-dialectal ASR, our findings can be helpful in designing ASR systems for other languages without standard <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.4/>Machine-oriented NMT Adaptation for Zero-shot NLP tasks : Comparing the Usefulness of Close and Distant Languages<span class=acl-fixed-case>NMT</span> Adaptation for Zero-shot <span class=acl-fixed-case>NLP</span> tasks: Comparing the Usefulness of Close and Distant Languages</a></strong><br><a href=/people/a/amirhossein-tebbifakhr/>Amirhossein Tebbifakhr</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--4><div class="card-body p-3 small">Neural Machine Translation (NMT) models are typically trained by considering humans as end-users and maximizing human-oriented objectives. However, in some scenarios, their output is consumed by automatic NLP components rather than by humans. In these scenarios, translations&#8217; quality is measured in terms of their fitness for purpose (i.e. maximizing performance of external NLP tools) rather than in terms of standard human fluency / adequacy criteria. Recently, <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning techniques</a> exploiting the feedback from downstream NLP tools have been proposed for machine-oriented NMT adaptation. In this work, we tackle the problem in a multilingual setting where a single NMT model translates from multiple languages for downstream automatic processing in the target language. Knowledge sharing across close and distant languages allows to apply our machine-oriented approach in the zero-shot setting where no labeled data for the test language is seen at training time. Moreover, we incorporate multi-lingual BERT in the source side of our NMT system to benefit from the knowledge embedded in this model. Our experiments show coherent performance gains, for different language directions over both i) generic NMT models (trained for human consumption), and ii) fine-tuned multilingual BERT. This gain for zero-shot language directions (e.g. SpanishEnglish) is higher when the models are fine-tuned on a closely-related source language (Italian) than a distant one (German).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.5/>Character Alignment in Morphologically Complex Translation Sets for Related Languages</a></strong><br><a href=/people/m/michael-gasser/>Michael Gasser</a>
|
<a href=/people/b/binyam-ephrem-seyoum/>Binyam Ephrem Seyoum</a>
|
<a href=/people/n/nazareth-amlesom-kifle/>Nazareth Amlesom Kifle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--5><div class="card-body p-3 small">For languages with complex morphology, word-to-word translation is a task with various potential applications, for example, in <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Language_education>language instruction</a>, and dictionary creation, as well as in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In this paper, we confine ourselves to the subtask of <a href=https://en.wikipedia.org/wiki/Character_alignment>character alignment</a> for the particular case of <a href=https://en.wikipedia.org/wiki/Language_family>families of related languages</a> with very few resources for most or all members. There are many such <a href=https://en.wikipedia.org/wiki/Language_family>families</a> ; we focus on the subgroup of Semitic languages spoken in <a href=https://en.wikipedia.org/wiki/Ethiopia>Ethiopia</a> and <a href=https://en.wikipedia.org/wiki/Eritrea>Eritrea</a>. We begin with an adaptation of the familiar <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment algorithms</a> behind <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>, modifying them as appropriate for our task. We show how <a href=https://en.wikipedia.org/wiki/Character_alignment>character alignment</a> can reveal morphological, phonological, and orthographic correspondences among related languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.6/>Bilingual Lexicon Induction across Orthographically-distinct Under-Resourced Dravidian Languages<span class=acl-fixed-case>D</span>ravidian Languages</a></strong><br><a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/n/navaneethan-rajasekaran/>Navaneethan Rajasekaran</a>
|
<a href=/people/m/mihael-arcan/>Mihael Arcan</a>
|
<a href=/people/k/kevin-mcguinness/>Kevin McGuinness</a>
|
<a href=/people/n/noel-e-oconnor/>Noel E. O’Connor</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--6><div class="card-body p-3 small">Bilingual lexicons are a vital tool for under-resourced languages and recent state-of-the-art approaches to this leverage pretrained monolingual word embeddings using supervised or semi-supervised approaches. However, these approaches require cross-lingual information such as seed dictionaries to train the model and find a <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a> between the word embedding spaces. Especially in the case of low-resourced languages, seed dictionaries are not readily available, and as such, these methods produce extremely weak results on these <a href=https://en.wikipedia.org/wiki/Programming_language>languages</a>. In this work, we focus on the <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a>, namely <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, and <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, which are even more challenging as they are written in unique scripts. To take advantage of <a href=https://en.wikipedia.org/wiki/Orthography>orthographic information</a> and <a href=https://en.wikipedia.org/wiki/Cognate>cognates</a> in these <a href=https://en.wikipedia.org/wiki/Language>languages</a>, we bring the related languages into a single script. Previous approaches have used linguistically sub-optimal measures such as the Levenshtein edit distance to detect cognates, whereby we demonstrate that the longest common sub-sequence is linguistically more sound and improves the performance of bilingual lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these <a href=https://en.wikipedia.org/wiki/Language>languages</a> many times, making bilingual lexicon induction approaches feasible for such under-resourced languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.9/>Recycling and Comparing Morphological Annotation Models for Armenian Diachronic-Variational Corpus Processing<span class=acl-fixed-case>A</span>rmenian Diachronic-Variational Corpus Processing</a></strong><br><a href=/people/c/chahan-vidal-gorene/>Chahan Vidal-Gorène</a>
|
<a href=/people/v/victoria-khurshudyan/>Victoria Khurshudyan</a>
|
<a href=/people/a/anaid-donabedian-demopoulos/>Anaïd Donabédian-Demopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--9><div class="card-body p-3 small">Armenian is a language with significant variation and unevenly distributed NLP resources for different varieties. An attempt is made to process an <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNN model</a> for morphological annotation on the basis of different Armenian data (provided or not with morphologically annotated corpora), and to compare the annotation results of <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNN</a> and rule-based models. Different tests were carried out to evaluate the reuse of an unspecialized model of lemmatization and POS-tagging for under-resourced language varieties. The research focused on three dialects and further extended to <a href=https://en.wikipedia.org/wiki/Western_Armenian>Western Armenian</a> with a <a href=https://en.wikipedia.org/wiki/Mean>mean accuracy</a> of 94,00 % in <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> and 97,02 % in POS-tagging, as well as a possible reusability of models to cover different other <a href=https://en.wikipedia.org/wiki/Armenian_language>Armenian varieties</a>. Interestingly, the comparison of an RNN model trained on <a href=https://en.wikipedia.org/wiki/Eastern_Armenian>Eastern Armenian</a> with the <a href=https://en.wikipedia.org/wiki/Eastern_Armenian>Eastern Armenian National Corpus rule-based model</a> applied to <a href=https://en.wikipedia.org/wiki/Western_Armenian>Western Armenian</a> showed an enhancement of 19 % in parsing. This <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> covers 88,79 % of a short heterogeneous dataset in <a href=https://en.wikipedia.org/wiki/Western_Armenian>Western Armenian</a>, and could be a baseline for a massive corpus annotation in that standard. It is argued that an RNN-based model can be a valid alternative to a rule-based one giving consideration to such factors as time-consumption, <a href=https://en.wikipedia.org/wiki/Reusability>reusability</a> for different varieties of a target language and significant qualitative results in morphological annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.16/>Uralic Language Identification (ULI) 2020 shared task dataset and the Wanca 2017 corpora<span class=acl-fixed-case>ULI</span>) 2020 shared task dataset and the Wanca 2017 corpora</a></strong><br><a href=/people/t/tommi-jauhiainen/>Tommi Jauhiainen</a>
|
<a href=/people/h/heidi-jauhiainen/>Heidi Jauhiainen</a>
|
<a href=/people/n/niko-partanen/>Niko Partanen</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--16><div class="card-body p-3 small">This article introduces the Wanca 2017 web corpora from which the sentences written in minor Uralic languages were collected for the test set of the Uralic Language Identification (ULI) 2020 shared task. We describe the ULI shared task and how the <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> was constructed using the Wanca 2017 corpora and texts in different languages from the Leipzig corpora collection. We also provide the results of a baseline language identification experiment conducted using the ULI 2020 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.19/>HeLju@VarDial 2020 : Social Media Variety Geolocation with BERT Models<span class=acl-fixed-case>H</span>e<span class=acl-fixed-case>L</span>ju@<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2020: Social Media Variety Geolocation with <span class=acl-fixed-case>BERT</span> Models</a></strong><br><a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--19><div class="card-body p-3 small">This paper describes the Helsinki-Ljubljana contribution to the VarDial shared task on social media variety geolocation. Our solutions are based on the BERT Transformer models, the constrained versions of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> reaching 1st place in two subtasks and 3rd place in one subtask, while our unconstrained models outperform all the constrained systems by a large margin. We show in our analyses that Transformer-based models outperform traditional <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> by far, and that improvements obtained by pre-training <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> on large quantities of (mostly standard) text are significant, but not drastic, with single-language models also outperforming multilingual models. Our manual analysis shows that two types of signals are the most crucial for a (mis)prediction : <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> and <a href=https://en.wikipedia.org/wiki/Dialect>dialectal features</a>, both of which are handled well by our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.21/>Experiments in Language Variety Geolocation and Dialect Identification</a></strong><br><a href=/people/t/tommi-jauhiainen/>Tommi Jauhiainen</a>
|
<a href=/people/h/heidi-jauhiainen/>Heidi Jauhiainen</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--21><div class="card-body p-3 small">In this paper we describe the systems we used when participating in the VarDial Evaluation Campaign organized as part of the 7th workshop on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for similar languages, varieties and dialects. The shared tasks we participated in were the second edition of the Romanian Dialect Identification (RDI) and the first edition of the Social Media Variety Geolocation (SMG). The submissions of our SUKI team used generative language models based on <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Naive Bayes</a> and character n-grams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.22/>Exploring the Power of Romanian BERT for Dialect Identification<span class=acl-fixed-case>R</span>omanian <span class=acl-fixed-case>BERT</span> for Dialect Identification</a></strong><br><a href=/people/g/george-eduard-zaharia/>George-Eduard Zaharia</a>
|
<a href=/people/a/andrei-marius-avram/>Andrei-Marius Avram</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a>
|
<a href=/people/t/traian-rebedea/>Traian Rebedea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--22><div class="card-body p-3 small">Dialect identification represents a key aspect for improving a series of tasks, for example, <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a>, considering that the location of the speaker can greatly influence the attitude towards a subject. In this work, we describe the systems developed by our team for VarDial 2020 : Romanian Dialect Identification, a task specifically created for challenging participants to solve the previously mentioned issue. More specifically, we introduce a series of neural systems based on Transformers, that combine a BERT model exclusively pre-trained on the <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian language</a> with techniques such as <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a> or character-level embeddings. By using these approaches, we were able to obtain a 0.6475 macro F1 score on the test dataset, thus allowing us to be ranked 5th out of 8 participant teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--vardial-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.vardial-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.27/>Geolocation of Tweets with a BiLSTM Regression Model<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> Regression Model</a></strong><br><a href=/people/p/piyush-mishra/>Piyush Mishra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--vardial-1--27><div class="card-body p-3 small">Identifying a user&#8217;s location can be useful for <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation systems</a>, <a href=https://en.wikipedia.org/wiki/Demography>demographic analyses</a>, and <a href=https://en.wikipedia.org/wiki/Emergency_management>disaster outbreak monitoring</a>. Although <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> allows users to voluntarily reveal their location, such information is n&#8217;t universally available. Analyzing a tweet can provide a general estimation of a tweet location while giving insight into the dialect of the user and other <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>linguistic markers</a>. Such <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic attributes</a> can be used to provide a regional approximation of tweet origins. In this paper, we present a neural regression model that can identify the linguistic intricacies of a tweet to predict the location of the user. The final <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> identifies the <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> embedded in the tweet and predicts the location of the tweet.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>