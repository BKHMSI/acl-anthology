<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of The Third Workshop on Representation Learning for NLP - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W18-30.pdf>Proceedings of The Third Workshop on Representation Learning for <span class=acl-fixed-case>NLP</span></a></h2><p class=lead><a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>,
<a href=/people/k/kris-cao/>Kris Cao</a>,
<a href=/people/h/he-he/>He He</a>,
<a href=/people/f/felix-hill/>Felix Hill</a>,
<a href=/people/s/spandana-gella/>Spandana Gella</a>,
<a href=/people/j/jamie-kiros/>Jamie Kiros</a>,
<a href=/people/h/hongyuan-mei/>Hongyuan Mei</a>,
<a href=/people/d/dipendra-misra/>Dipendra Misra</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W18-30</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Melbourne, Australia</dd><dt>Venues:</dt><dd><a href=/venues/acl/>ACL</a>
| <a href=/venues/repl4nlp/>RepL4NLP</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigrep/>SIGREP</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W18-30>https://aclanthology.org/W18-30</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W18-30.pdf>https://aclanthology.org/W18-30.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W18-30.pdf title="Open PDF of 'Proceedings of The Third Workshop on Representation Learning for NLP'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+The+Third+Workshop+on+Representation+Learning+for+NLP" title="Search for 'Proceedings of The Third Workshop on Representation Learning for NLP' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3000/>Proceedings of The Third Workshop on Representation Learning for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/k/kris-cao/>Kris Cao</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/f/felix-hill/>Felix Hill</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/j/jamie-kiros/>Jamie Kiros</a>
|
<a href=/people/h/hongyuan-mei/>Hongyuan Mei</a>
|
<a href=/people/d/dipendra-misra/>Dipendra Misra</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3001/>Corpus Specificity in LSA and <a href=https://en.wikipedia.org/wiki/Word2vec>Word2vec</a> : The Role of Out-of-Domain Documents<span class=acl-fixed-case>LSA</span> and Word2vec: The Role of Out-of-Domain Documents</a></strong><br><a href=/people/e/edgar-altszyler/>Edgar Altszyler</a>
|
<a href=/people/m/mariano-sigman/>Mariano Sigman</a>
|
<a href=/people/d/diego-fernandez-slezak/>Diego Fernández Slezak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3001><div class="card-body p-3 small">Despite the popularity of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, the precise way by which they acquire semantic relations between words remain unclear. In the present article, we investigate whether LSA and word2vec capacity to identify relevant semantic relations increases with corpus size. One intuitive hypothesis is that the capacity to identify relevant associations should increase as the amount of data increases. However, if <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus size</a> grows in topics which are not specific to the domain of interest, <a href=https://en.wikipedia.org/wiki/Signal-to-noise_ratio>signal to noise ratio</a> may weaken. Here we investigate the effect of corpus specificity and size in <a href=https://en.wikipedia.org/wiki/Word_embedding>word-embeddings</a>, and for this, we study two ways for progressive elimination of documents : the elimination of random documents vs. the elimination of documents unrelated to a specific task. We show that <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> can take advantage of all the documents, obtaining its best performance when it is trained with the whole corpus. On the contrary, the specialization (removal of out-of-domain documents) of the training corpus, accompanied by a decrease of <a href=https://en.wikipedia.org/wiki/Dimensionality>dimensionality</a>, can increase LSA word-representation quality while speeding up the processing time. From a cognitive-modeling point of view, we point out that LSA&#8217;s word-knowledge acquisitions may not be efficiently exploiting higher-order co-occurrences and global relations, whereas <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> does.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3002/>Hierarchical Convolutional Attention Networks for Text Classification</a></strong><br><a href=/people/s/shang-gao/>Shang Gao</a>
|
<a href=/people/a/arvind-ramanathan/>Arvind Ramanathan</a>
|
<a href=/people/g/georgia-tourassi/>Georgia Tourassi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3002><div class="card-body p-3 small">Recent work in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has demonstrated that self-attention mechanisms can be used in place of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> to increase <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training speed</a> without sacrificing <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>model accuracy</a>. We propose combining this approach with the benefits of convolutional filters and a hierarchical structure to create a document classification model that is both highly accurate and fast to train we name our method Hierarchical Convolutional Attention Networks. We demonstrate the effectiveness of this <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> by surpassing the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on several classification tasks while being twice as fast to train.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3003/>Extrofitting : Enriching Word Representation and its Vector Space with Semantic Lexicons<span class=acl-fixed-case>E</span>xtrofitting: Enriching Word Representation and its Vector Space with Semantic Lexicons</a></strong><br><a href=/people/h/hwiyeol-jo/>Hwiyeol Jo</a>
|
<a href=/people/s/stanley-jungkyu-choi/>Stanley Jungkyu Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3003><div class="card-body p-3 small">We propose post-processing method for enriching not only word representation but also its <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> using semantic lexicons, which we call extrofitting. The method consists of 3 steps as follows : (i) Expanding 1 or more <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>dimension(s)</a> on all the word vectors, filling with their representative value. (ii) Transferring semantic knowledge by averaging each representative values of synonyms and filling them in the expanded dimension(s). These two steps make representations of the synonyms close together. (iii) Projecting the <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> using Linear Discriminant Analysis, which eliminates the expanded dimension(s) with semantic knowledge. When experimenting with GloVe, we find that our method outperforms Faruqui&#8217;s retrofitting on some of word similarity task. We also report further analysis on our method in respect to word vector dimensions, <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary size</a> as well as other well-known pretrained word vectors (e.g., Word2Vec, Fasttext).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3005/>Text Completion using Context-Integrated Dependency Parsing</a></strong><br><a href=/people/a/amr-rekaby-salama/>Amr Rekaby Salama</a>
|
<a href=/people/o/ozge-alacam/>Özge Alaçam</a>
|
<a href=/people/w/wolfgang-menzel/>Wolfgang Menzel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3005><div class="card-body p-3 small">Incomplete linguistic input, i.e. due to a noisy environment, is one of the challenges that a successful <a href=https://en.wikipedia.org/wiki/Communication_system>communication system</a> has to deal with. In this paper, we study text completion with a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> composed of sentences with gaps where a successful completion can not be achieved through a uni-modal (language-based) approach. We present a solution based on a context-integrating dependency parser incorporating an additional non-linguistic modality. An incompleteness in one <a href=https://en.wikipedia.org/wiki/Communication_channel>channel</a> is compensated by information from another one and the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> learns the association between the two modalities from a multiple level knowledge representation. We examined several model variations by adjusting the degree of influence of different modalities in the <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a> on possible filler words and their exact reference to a non-linguistic context element. Our model is able to fill the gap with 95.4 % word and 95.2 % exact reference accuracy hence the successful prediction can be achieved not only on the word level (such as mug) but also with respect to the correct identification of its context reference (such as mug 2 among several mug instances).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3006/>Quantum-Inspired Complex Word Embedding</a></strong><br><a href=/people/q/qiuchi-li/>Qiuchi Li</a>
|
<a href=/people/s/sagar-uprety/>Sagar Uprety</a>
|
<a href=/people/b/benyou-wang/>Benyou Wang</a>
|
<a href=/people/d/dawei-song/>Dawei Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3006><div class="card-body p-3 small">A challenging task for word embeddings is to capture the emergent meaning or polarity of a combination of individual words. For example, existing approaches in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> will assign high probabilities to the words Penguin and Fly if they frequently co-occur, but it fails to capture the fact that they occur in an opposite sense-Penguins do not fly. We hypothesize that humans do not associate a single polarity or sentiment to each word. The word contributes to the overall polarity of a combination of words depending upon which other words it is combined with. This is analogous to the behavior of microscopic particles which exist in all possible states at the same time and interfere with each other to give rise to new states depending upon their relative phases. We make use of the Hilbert Space representation of such particles in <a href=https://en.wikipedia.org/wiki/Quantum_mechanics>Quantum Mechanics</a> where we subscribe a relative phase to each word, which is a <a href=https://en.wikipedia.org/wiki/Complex_number>complex number</a>, and investigate two such quantum inspired models to derive the meaning of a combination of words. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve better performances than state-of-the-art non-quantum models on binary sentence classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3007/>Natural Language Inference with Definition Embedding Considering Context On the Fly</a></strong><br><a href=/people/k/kosuke-nishida/>Kosuke Nishida</a>
|
<a href=/people/k/kyosuke-nishida/>Kyosuke Nishida</a>
|
<a href=/people/h/hisako-asano/>Hisako Asano</a>
|
<a href=/people/j/junji-tomita/>Junji Tomita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3007><div class="card-body p-3 small">Natural language inference (NLI) is one of the most important tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. In this study, we propose a novel method using word dictionaries, which are pairs of a word and its definition, as external knowledge. Our neural definition embedding mechanism encodes input sentences with the definitions of each word of the sentences on the fly. It can encode the definition of words considering the context of input sentences by using an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. We evaluated our method using <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> as a dictionary and confirmed that our method performed better than baseline models when using the full or a subset of 100d GloVe as word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3008/>Comparison of Representations of Named Entities for Document Classification</a></strong><br><a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3008><div class="card-body p-3 small">We explore representations for multi-word names in text classification tasks, on Reuters (RCV1) topic and sector classification. We find that : the best way to treat <a href=https://en.wikipedia.org/wiki/Name>names</a> is to split them into tokens and use each token as a separate feature ; NEs have more impact on sector classification than topic classification ; replacing NEs with entity types is not an effective strategy ; representing tokens by different embeddings for proper names vs. common nouns does not improve results. We highlight the improvements over state-of-the-art results that our <a href=https://en.wikipedia.org/wiki/Computer_simulation>CNN models</a> yield.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3011/>A Hybrid Learning Scheme for Chinese Word Embedding<span class=acl-fixed-case>C</span>hinese Word Embedding</a></strong><br><a href=/people/w/wenfan-chen/>Wenfan Chen</a>
|
<a href=/people/w/weiguo-sheng/>Weiguo Sheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3011><div class="card-body p-3 small">To improve <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, subword information has been widely employed in state-of-the-art methods. These <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can be classified to either compositional or predictive models. In this paper, we propose a hybrid learning scheme, which integrates compositional and predictive model for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. Such a scheme can take advantage of both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, thus effectively learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. The proposed scheme has been applied to learn <a href=https://en.wikipedia.org/wiki/Linguistic_description>word representation</a> on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Our results show that the proposed scheme can significantly improve the performance of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> in terms of <a href=https://en.wikipedia.org/wiki/Analogy>analogical reasoning</a> and is robust to the size of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3012.Notes.pdf data-toggle=tooltip data-placement=top title=Notes><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3012/>Unsupervised Random Walk Sentence Embeddings : A Strong but Simple Baseline</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3012><div class="card-body p-3 small">Using a <a href=https://en.wikipedia.org/wiki/Random_walk_model>random walk model</a> of <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a>, Arora et al. (2017) proposed a strong baseline for computing sentence embeddings : take a weighted average of word embeddings and modify with SVD. This simple <a href=https://en.wikipedia.org/wiki/Methodology>method</a> even outperforms far more complex approaches such as <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> on textual similarity tasks. In this paper, we first show that word vector length has a confounding effect on the probability of a sentence being generated in Arora et al.&#8217;s model. We propose a <a href=https://en.wikipedia.org/wiki/Random_walk_model>random walk model</a> that is robust to this confound, where the probability of word generation is inversely related to the angular distance between the word and sentence embeddings. Our <a href=https://en.wikipedia.org/wiki/Stiffness>approach</a> beats Arora et al.&#8217;s by up to 44.4 % on textual similarity tasks and is competitive with state-of-the-art methods. Unlike Arora et al.&#8217;s method, ours requires no hyperparameter tuning, which means it can be used when there is no labelled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3013/>Evaluating Word Embeddings in Multi-label Classification Using Fine-Grained Name Typing</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3013><div class="card-body p-3 small">Embedding models typically associate each word with a single real-valued vector, representing its different properties. Evaluation methods, therefore, need to analyze the accuracy and completeness of these properties in <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. This requires fine-grained analysis of embedding subspaces. Multi-label classification is an appropriate way to do so. We propose a new evaluation method for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> based on <a href=https://en.wikipedia.org/wiki/Multi-label_classification>multi-label classification</a> given a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. The task we use is fine-grained name typing : given a large corpus, find all types that a name can refer to based on the name embedding. Given the scale of entities in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, we can build <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for this task that are complementary to the current embedding evaluation datasets in : they are very large, contain fine-grained classes, and allow the direct evaluation of embeddings without confounding factors like sentence context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3015/>Exploiting Common Characters in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> to Learn Cross-Lingual Word Embeddings via Matrix Factorization<span class=acl-fixed-case>C</span>hinese and <span class=acl-fixed-case>J</span>apanese to Learn Cross-Lingual Word Embeddings via Matrix Factorization</a></strong><br><a href=/people/j/jilei-wang/>Jilei Wang</a>
|
<a href=/people/s/shiying-luo/>Shiying Luo</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/t/tao-dai/>Tao Dai</a>
|
<a href=/people/s/shu-tao-xia/>Shu-Tao Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3015><div class="card-body p-3 small">Learning vector space representation of words (i.e., word embeddings) has recently attracted wide research interests, and has been extended to cross-lingual scenario. Currently most cross-lingual word embedding learning models are based on sentence alignment, which inevitably introduces much noise. In this paper, we show in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, the acquisition of semantic relation among words can benefit from the large number of common characters shared by both languages ; inspired by this unique feature, we design a method named CJC targeting to generate cross-lingual context of words. We combine CJC with GloVe based on matrix factorization, and then propose an integrated model named CJ-Glo. Taking two sentence-aligned models and CJ-BOC (also exploits common characters but is based on CBOW) as baseline algorithms, we compare them with CJ-Glo on a series of NLP tasks including cross-lingual synonym, word analogy and sentence alignment. The result indicates CJ-Glo achieves the best performance among these methods, and is more stable in cross-lingual tasks ; moreover, compared with CJ-BOC, CJ-Glo is less sensitive to the alteration of parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3018/>Injecting Lexical Contrast into Word Vectors by Guiding Vector Space Specialisation</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3018><div class="card-body p-3 small">Word vector space specialisation models offer a portable, light-weight approach to fine-tuning arbitrary distributional vector spaces to discern between <a href=https://en.wikipedia.org/wiki/Synonym>synonymy</a> and <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonymy</a>. Their effectiveness is drawn from <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>external linguistic constraints</a> that specify the exact <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical relation</a> between words. In this work, we show that a careful selection of the <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>external constraints</a> can steer and improve the <a href=https://en.wikipedia.org/wiki/Specialization_(functional)>specialisation</a>. By simply selecting appropriate constraints, we report state-of-the-art results on a suite of tasks with well-defined benchmarks where modeling lexical contrast is crucial : 1) true semantic similarity, with highest reported scores on SimLex-999 and SimVerb-3500 to date ; 2) detecting antonyms ; and 3) distinguishing antonyms from synonyms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3019 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3019.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3019/>Characters or <a href=https://en.wikipedia.org/wiki/Morphemes>Morphemes</a> : How to Represent Words?</a></strong><br><a href=/people/a/ahmet-ustun/>Ahmet Üstün</a>
|
<a href=/people/m/murathan-kurfali/>Murathan Kurfalı</a>
|
<a href=/people/b/burcu-can/>Burcu Can</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3019><div class="card-body p-3 small">In this paper, we investigate the effects of using subword information in <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. We argue that using syntactic subword units effects the quality of the word representations positively. We introduce a morpheme-based model and compare it against to word-based, character-based, and character n-gram level models. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> takes a list of candidate segmentations of a word and learns the representation of the word based on different segmentations that are weighted by an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. We performed experiments on <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> as a morphologically rich language and <a href=https://en.wikipedia.org/wiki/English_language>English</a> with a comparably poorer morphology. The results show that morpheme-based models are better at learning word representations of morphologically complex languages compared to character-based and character n-gram level models since the morphemes help to incorporate more syntactic knowledge in learning, that makes morpheme-based models better at syntactic tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3020 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3020/>Learning Hierarchical Structures On-The-Fly with a Recurrent-Recursive Model for Sequences</a></strong><br><a href=/people/a/athul-paul-jacob/>Athul Paul Jacob</a>
|
<a href=/people/z/zhouhan-lin/>Zhouhan Lin</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/y/yoshua-bengio/>Yoshua Bengio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3020><div class="card-body p-3 small">We propose a <a href=https://en.wikipedia.org/wiki/Hierarchical_model>hierarchical model</a> for sequential data that learns a <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> on-the-fly, i.e. while reading the sequence. In the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> adapts its structure and reuses recurrent weights in a recursive manner. This creates adaptive skip-connections that ease the learning of long-term dependencies. The <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a> can either be inferred without <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervision</a> through <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, or learned in a supervised manner. We provide preliminary experiments in a novel Math Expression Evaluation (MEE) task, which is created to have a hierarchical tree structure that can be used to study the effectiveness of our model. Additionally, we test our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a well-known propositional logic and language modelling tasks. Experimental results have shown the potential of our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3021/>Limitations of Cross-Lingual Learning from Image Search</a></strong><br><a href=/people/m/mareike-hartmann/>Mareike Hartmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3021><div class="card-body p-3 small">Cross-lingual representation learning is an important step in making <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> scale to all the world&#8217;s languages. Previous work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. However, that work focused (almost exclusively) on the translation of nouns only. Here, we investigate whether the meaning of other parts-of-speech (POS), in particular <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>, can be learned in the same way. Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3022 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3022/>Learning Semantic Textual Similarity from Conversations</a></strong><br><a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/s/steve-yuan/>Steve Yuan</a>
|
<a href=/people/d/daniel-cer/>Daniel Cer</a>
|
<a href=/people/s/sheng-yi-kong/>Sheng-yi Kong</a>
|
<a href=/people/n/noah-constant/>Noah Constant</a>
|
<a href=/people/p/petr-pilar/>Petr Pilar</a>
|
<a href=/people/h/heming-ge/>Heming Ge</a>
|
<a href=/people/y/yun-hsuan-sung/>Yun-Hsuan Sung</a>
|
<a href=/people/b/brian-strope/>Brian Strope</a>
|
<a href=/people/r/ray-kurzweil/>Ray Kurzweil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3022><div class="card-body p-3 small">We present a novel approach to learn <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> for sentence-level semantic similarity using conversational data. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> trains an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised model</a> to predict conversational responses. The resulting <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> perform well on the Semantic Textual Similarity (STS) Benchmark and SemEval 2017&#8217;s Community Question Answering (CQA) question similarity subtask. Performance is further improved by introducing multitask training, combining conversational response prediction and natural language inference. Extensive experiments show the proposed model achieves the best performance among all neural models on the STS Benchmark and is competitive with the state-of-the-art feature engineered and mixed systems for both tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3023 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3023/>Multilingual Seq2seq Training with Similarity Loss for Cross-Lingual Document Classification</a></strong><br><a href=/people/k/katherine-yu/>Katherine Yu</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/b/barlas-oguz/>Barlas Oguz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3023><div class="card-body p-3 small">In this paper we continue experiments where neural machine translation training is used to produce joint cross-lingual fixed-dimensional sentence embeddings. In this framework we introduce a simple method of adding a <a href=https://en.wikipedia.org/wiki/Loss_function>loss</a> to the <a href=https://en.wikipedia.org/wiki/Loss_function>learning objective</a> which penalizes distance between representations of bilingually aligned sentences. We evaluate cross-lingual transfer using two approaches, cross-lingual similarity search on an aligned corpus (Europarl) and cross-lingual document classification on a recently published benchmark Reuters corpus, and we find the similarity loss significantly improves performance on both. Furthermore, we notice that while our Reuters results are very competitive, our English results are not as competitive, showing room for improvement in the current cross-lingual state-of-the-art. Our results are based on a set of 6 <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3024 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3024/>LSTMs Exploit Linguistic Attributes of Data<span class=acl-fixed-case>LSTM</span>s Exploit Linguistic Attributes of Data</a></strong><br><a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/c/chenhao-tan/>Chenhao Tan</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3024><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> have found success in a variety of natural language processing applications, they are general models of sequential data. We investigate how the properties of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language data</a> affect an LSTM&#8217;s ability to learn a nonlinguistic task : recalling elements from its input. We find that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on natural language data are able to recall tokens from much longer sequences than <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on non-language sequential data. Furthermore, we show that the LSTM learns to solve the memorization task by explicitly using a subset of its <a href=https://en.wikipedia.org/wiki/Neuron>neurons</a> to count timesteps in the input. We hypothesize that the patterns and structure in natural language data enable LSTMs to learn by providing approximate ways of reducing loss, but understanding the effect of different <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> on the learnability of LSTMs remains an open question.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3026 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-3026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-3026/>Jointly Embedding Entities and Text with Distant Supervision</a></strong><br><a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/a/albert-m-lai/>Albert M Lai</a>
|
<a href=/people/e/eric-fosler-lussier/>Eric Fosler-Lussier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3026><div class="card-body p-3 small">Learning representations for knowledge base entities and concepts is becoming increasingly important for NLP applications. However, recent entity embedding methods have relied on structured resources that are expensive to create for new domains and corpora. We present a distantly-supervised method for jointly learning embeddings of entities and text from an unnanotated corpus, using only a list of <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> between <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> and surface forms. We learn embeddings from open-domain and biomedical corpora, and compare against prior methods that rely on human-annotated text or large knowledge graph structure. Our embeddings capture entity similarity and relatedness better than prior work, both in existing biomedical datasets and a new Wikipedia-based dataset that we release to the community. Results on analogy completion and entity sense disambiguation indicate that <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> and <a href=https://en.wikipedia.org/wiki/Word>words</a> capture complementary information that can be effectively combined for downstream use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3027 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3027/>A Sequence-to-Sequence Model for Semantic Role Labeling</a></strong><br><a href=/people/a/angel-daza/>Angel Daza</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3027><div class="card-body p-3 small">We explore a novel approach for Semantic Role Labeling (SRL) by casting it as a sequence-to-sequence process. We employ an attention-based model enriched with a copying mechanism to ensure faithful regeneration of the input sequence, while enabling interleaved generation of argument role labels. We apply this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a monolingual setting, performing PropBank SRL on English language data. The constrained sequence generation set-up enforced with the copying mechanism allows us to analyze the performance and special properties of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on manually labeled data and benchmarking against state-of-the-art sequence labeling models. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to solve the SRL argument labeling task on English data, yet further structural decoding constraints will need to be added to make the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> truly competitive. Our work represents the first step towards more advanced, generative SRL labeling setups.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>