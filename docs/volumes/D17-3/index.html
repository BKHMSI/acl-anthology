<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</h2><p class=lead><a href=/people/a/alexandra-birch/>Alexandra Birch</a>,
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>D17-3</dd><dt>Month:</dt><dd>September</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Copenhagen, Denmark</dd><dt>Venue:</dt><dd><a href=/venues/emnlp/>EMNLP</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigdat/>SIGDAT</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/D17-3>https://aclanthology.org/D17-3</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2017+Conference+on+Empirical+Methods+in+Natural+Language+Processing%3A+Tutorial+Abstracts" title="Search for 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3001/>Acquisition, Representation and Usage of Conceptual Hierarchies</a></strong><br><a href=/people/m/marius-pasca/>Marius Pasca</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3001><div class="card-body p-3 small">Through subsumption and <a href=https://en.wikipedia.org/wiki/Instantiation_principle>instantiation</a>, individual instances (artificial intelligence, the spotted pig) otherwise spanning a wide range of domains can be brought together and organized under conceptual hierarchies. The hierarchies connect more specific concepts (computer science subfields, gastropubs) to more general concepts (academic disciplines, restaurants) through IsA relations. Explicit or implicit properties applicable to, and defining, more general concepts are inherited by their more specific concepts, down to the instances connected to the lower parts of the <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchies</a>. Subsumption represents a crisp, universally-applicable principle towards consistently representing IsA relations in any knowledge resource. Yet knowledge resources often exhibit significant differences in their scope, representation choices and intended usage, to cause significant differences in their expected usage and impact on various <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. This tutorial examines the theoretical foundations of subsumption, and its practical embodiment through IsA relations compiled manually or extracted automatically. It addresses IsA relations from their formal definition ; through practical choices made in their representation within the larger and more widely-used of the available knowledge resources ; to their automatic acquisition from document repositories, as opposed to their manual compilation by human contributors ; to their impact in <a href=https://en.wikipedia.org/wiki/Textual_analysis>text analysis</a> and <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>. As <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> move away from returning a set of links and closer to returning results that more directly answer queries, IsA relations play an increasingly important role towards a better understanding of documents and queries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3002/>Computational Sarcasm</a></strong><br><a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/a/aditya-joshi/>Aditya Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3002><div class="card-body p-3 small">Sarcasm is a form of <a href=https://en.wikipedia.org/wiki/Irony>verbal irony</a> that is intended to express contempt or ridicule. Motivated by challenges posed by sarcastic text to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, computational approaches to <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> have witnessed a growing interest at NLP forums in the past decade. Computational sarcasm refers to automatic approaches pertaining to <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>. The tutorial will provide a bird&#8217;s-eye view of the research in computational sarcasm for text, while focusing on significant milestones. The tutorial begins with linguistic theories of sarcasm, with a focus on incongruity : a useful notion that underlies <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> and other forms of <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a>. Since the most significant work in computational sarcasm is sarcasm detection : predicting whether a given piece of text is sarcastic or not, sarcasm detection forms the focus hereafter. We begin our discussion on sarcasm detection with <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, touching on strategies, challenges and nature of datasets. Then, we describe algorithms for sarcasm detection : rule-based (where a specific evidence of sarcasm is utilised as a rule), statistical classifier-based (where features are designed for a statistical classifier), a topic model-based technique, and deep learning-based algorithms for sarcasm detection. In case of each of these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>, we refer to our work on sarcasm detection and share our learnings. Since information beyond the text to be classified, contextual information is useful for sarcasm detection, we then describe approaches that use such information through conversational context or author-specific context. We then follow it by novel areas in computational sarcasm such as sarcasm generation, sarcasm v / s irony classification, etc. We then summarise the tutorial and describe future directions based on errors reported in past work. The tutorial will end with a demonstration of our work on sarcasm detection. This tutorial will be of interest to researchers investigating computational sarcasm and related areas such as <a href=https://en.wikipedia.org/wiki/Computational_humour>computational humour</a>, figurative language understanding, emotion and sentiment sentiment analysis, etc. The tutorial is motivated by our continually evolving survey paper of sarcasm detection, that is available on arXiv at : Joshi, Aditya, Pushpak Bhattacharyya, and Mark James Carman. Automatic Sarcasm Detection : A Survey. arXiv preprint arXiv:1602.03426 (2016).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3003/>Graph-based Text Representations: Boosting Text Mining, <span class=acl-fixed-case>NLP</span> and Information Retrieval with Graphs</a></strong><br><a href=/people/f/fragkiskos-d-malliaros/>Fragkiskos D. Malliaros</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3003><div class="card-body p-3 small">Graphs or networks have been widely used as modeling tools in Natural Language Processing (NLP), Text Mining (TM) and Information Retrieval (IR). Traditionally, the unigram bag-of-words representation is applied; that way, a document is represented as a multiset of its terms, disregarding dependencies between the terms. Although several variants and extensions of this modeling approach have been proposed (e.g., the n-gram model), the main weakness comes from the underlying term independence assumption. The order of the terms within a document is completely disregarded and any relationship between terms is not taken into account in the final task (e.g., text categorization). Nevertheless, as the heterogeneity of text collections is increasing (especially with respect to document length and vocabulary), the research community has started exploring different document representations aiming to capture more fine-grained contexts of co-occurrence between different terms, challenging the well-established unigram bag-of-words model. To this direction, graphs constitute a well-developed model that has been adopted for text representation. The goal of this tutorial is to offer a comprehensive presentation of recent methods that rely on graph-based text representations to deal with various tasks in NLP and IR. We will describe basic as well as novel graph theoretic concepts and we will examine how they can be applied in a wide range of text-related application domains.\n\nAll the material associated to the tutorial will be available at: http://fragkiskosm.github.io/projects/graph_text_tutorial</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-3004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3004/>Semantic Role Labeling</a></strong><br><a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3004><div class="card-body p-3 small">This tutorial describes semantic role labelling (SRL), the task of mapping text to shallow semantic representations of eventualities and their participants. The tutorial introduces the SRL task and discusses recent research directions related to the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The audience of this tutorial will learn about the linguistic background and motivation for semantic roles, and also about a range of <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> for this task, from early approaches to the current state-of-the-art. We will further discuss recently proposed variations to the traditional SRL task, including topics such as semantic proto-role labeling. We also cover techniques for reducing required annotation effort, such as methods exploiting unlabeled corpora (semi-supervised and unsupervised techniques), model adaptation across languages and domains, and methods for crowdsourcing semantic role annotation (e.g., question-answer driven SRL). Methods based on different machine learning paradigms, including <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, generative Bayesian models, graph-based algorithms and bootstrapping style techniques. Beyond sentence-level SRL, we discuss work that involves semantic roles in discourse. In particular, we cover <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> and <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> related to the task of identifying implicit roles and linking them to discourse antecedents. We introduce different approaches to this task from the literature, including models based on <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, centering, and selectional preferences. We also review how new insights gained through them can be useful for the traditional SRL task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3005/>Memory Augmented Neural Networks for Natural Language Processing</a></strong><br><a href=/people/c/caglar-gulcehre/>Caglar Gulcehre</a>
|
<a href=/people/s/sarath-chandar/>Sarath Chandar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3005><div class="card-body p-3 small">Designing of general-purpose learning algorithms is a long-standing goal of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a>. A general purpose AI agent should be able to have a memory that it can store and retrieve information from. Despite the success of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> in particular with the introduction of LSTMs and GRUs to this area, there are still a set of complex tasks that can be challenging for conventional <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Those tasks often require a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to be equipped with an explicit, <a href=https://en.wikipedia.org/wiki/External_memory>external memory</a> in which a larger, potentially unbounded, set of facts need to be stored. They include but are not limited to, <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>, <a href=https://en.wikipedia.org/wiki/Planning>planning</a>, episodic question-answering and learning compact algorithms. Recently two promising approaches based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to this type of tasks have been proposed : Memory Networks and Neural Turing Machines. In this tutorial, we will give an overview of this new paradigm of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> with <a href=https://en.wikipedia.org/wiki/Memory>memory</a>. We will present a unified architecture for Memory Augmented Neural Networks (MANN) and discuss the ways in which one can address the external memory and hence read / write from it. Then we will introduce <a href=https://en.wikipedia.org/wiki/Neural_Turing_machine>Neural Turing Machines</a> and Memory Networks as specific instantiations of this general architecture. In the second half of the tutorial, we will focus on recent advances in MANN which focus on the following questions : How can we read / write from an extremely large memory in a scalable way? How can we design efficient non-linear addressing schemes? How can we do efficient <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a> using large scale memory and an <a href=https://en.wikipedia.org/wiki/Episodic_memory>episodic memory</a>? The answer to any one of these questions introduces a variant of MANN. We will conclude the tutorial with several open challenges in MANN and its applications to NLP.We will introduce several <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> of MANN in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> throughout the tutorial. Few examples include <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, visual question answering, and <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>. For updated information and material, please refer to our tutorial website : https://sites.google.com/view/mann-emnlp2017/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3007/>Cross-Lingual Word Representations : Induction and Evaluation</a></strong><br><a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3007><div class="card-body p-3 small">In recent past, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> as a field has seen tremendous utility of distributional word vector representations as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> in downstream tasks. The fact that these <a href=https://en.wikipedia.org/wiki/Word_vector>word vectors</a> can be trained on unlabeled monolingual corpora of a language makes them an inexpensive resource in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. With the increasing use of monolingual word vectors, there is a need for word vectors that can be used as efficiently across multiple languages as monolingually. Therefore, learning bilingual and multilingual word embeddings / vectors is currently an important research topic. These vectors offer an elegant and language-pair independent way to represent content across different languages. This tutorial aims to bring NLP researchers up to speed with the current techniques in cross-lingual word representation learning. We will first discuss how to induce cross-lingual word representations (covering both bilingual and multilingual ones) from various data types and resources (e.g., parallel data, comparable data, non-aligned monolingual data in different languages, dictionaries and theasuri, or, even, images, eye-tracking data). We will then discuss how to evaluate such <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a>, intrinsically and extrinsically. We will introduce researchers to state-of-the-art methods for constructing cross-lingual word representations and discuss their applicability in a broad range of downstream NLP applications. We will deliver a detailed survey of the current methods, discuss best training and evaluation practices and use-cases, and provide links to publicly available implementations, datasets, and pre-trained models.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>