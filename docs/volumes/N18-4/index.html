<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/N18-4.pdf>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Student Research Workshop</a></h2><p class=lead><a href=/people/s/silvio-cordeiro/>Silvio Ricardo Cordeiro</a>,
<a href=/people/s/shereen-oraby/>Shereen Oraby</a>,
<a href=/people/u/umashanthi-pavalanathan/>Umashanthi Pavalanathan</a>,
<a href=/people/k/kyeongmin-rim/>Kyeongmin Rim</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>N18-4</dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>New Orleans, Louisiana, USA</dd><dt>Venue:</dt><dd><a href=/venues/naacl/>NAACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/N18-4>https://aclanthology.org/N18-4</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/N18-4 title="To the current version of the paper by DOI">10.18653/v1/N18-4</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/N18-4.pdf>https://aclanthology.org/N18-4.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/N18-4.pdf title="Open PDF of 'Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2018+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics%3A+Student+Research+Workshop" title="Search for 'Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4000/>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Student Research Workshop</a></strong><br><a href=/people/s/silvio-cordeiro/>Silvio Ricardo Cordeiro</a>
|
<a href=/people/s/shereen-oraby/>Shereen Oraby</a>
|
<a href=/people/u/umashanthi-pavalanathan/>Umashanthi Pavalanathan</a>
|
<a href=/people/k/kyeongmin-rim/>Kyeongmin Rim</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4001 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277631295 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-4001/>Alignment, Acceptance, and Rejection of Group Identities in Online Political Discourse</a></strong><br><a href=/people/h/hagyeong-shin/>Hagyeong Shin</a>
|
<a href=/people/g/gabriel-doyle/>Gabriel Doyle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4001><div class="card-body p-3 small">Conversation is a joint social process, with participants cooperating to exchange information. This process is helped along through linguistic alignment : participants&#8217; adoption of each other&#8217;s word use. This alignment is robust, appearing many settings, and is nearly always positive. We create an alignment model for examining alignment in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter conversations</a> across antagonistic groups. This model finds that some word categories, specifically <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> used to establish <a href=https://en.wikipedia.org/wiki/Identity_(social_science)>group identity</a> and <a href=https://en.wikipedia.org/wiki/Group_cohesiveness>common ground</a>, are negatively aligned. This negative alignment is observed despite other categories, which are less related to the <a href=https://en.wikipedia.org/wiki/Group_dynamics>group dynamics</a>, showing the standard positive alignment. This suggests that alignment is strongly biased toward cooperative alignment, but that different linguistic features can show substantially different behaviors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4002/>Combining Abstractness and Language-specific Theoretical Indicators for Detecting Non-Literal Usage of Estonian Particle Verbs<span class=acl-fixed-case>E</span>stonian Particle Verbs</a></strong><br><a href=/people/e/eleri-aedmaa/>Eleri Aedmaa</a>
|
<a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4002><div class="card-body p-3 small">This paper presents two novel <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and a random-forest classifier to automatically predict <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>literal vs. non-literal language usage</a> for a highly frequent type of <a href=https://en.wikipedia.org/wiki/Interlingue>multi-word expression</a> in a low-resource language, i.e., <a href=https://en.wikipedia.org/wiki/Estonian_language>Estonian</a>. We demonstrate the value of language-specific indicators induced from theoretical linguistic research, which outperform a high majority baseline when combined with language-independent features of non-literal language (such as abstractness).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4003/>Verb Alternations and Their Impact on Frame Induction</a></strong><br><a href=/people/e/esther-seyffarth/>Esther Seyffarth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4003><div class="card-body p-3 small">Frame induction is the automatic creation of frame-semantic resources similar to <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> or <a href=https://en.wikipedia.org/wiki/PropBank>PropBank</a>, which map lexical units of a language to frame representations of each lexical unit&#8217;s semantics. For verbs, these <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> usually include a specification of their argument slots and of the selectional restrictions that apply to each slot. Verbs that participate in diathesis alternations have different syntactic realizations whose <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> are closely related, but not identical. We discuss the influence that such alternations have on frame induction, compare several possible frame structures for verbs in the <a href=https://en.wikipedia.org/wiki/Causative_alternation>causative alternation</a>, and propose a systematic analysis of alternating verbs that encodes their similarities as well as their differences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4005/>Towards Qualitative Word Embeddings Evaluation : Measuring Neighbors Variation</a></strong><br><a href=/people/b/benedicte-pierrejean/>Bénédicte Pierrejean</a>
|
<a href=/people/l/ludovic-tanguy/>Ludovic Tanguy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4005><div class="card-body p-3 small">We propose a method to study the variation lying between different word embeddings models trained with different parameters. We explore the variation between <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with only one varying parameter by observing the distributional neighbors variation and show how changing only one parameter can have a massive impact on a given <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a>. We show that the <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>variation</a> is not affecting all words of the <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> equally. Variation is influenced by parameters such as setting a parameter to its minimum or maximum value but it also depends on the corpus intrinsic features such as the frequency of a word. We identify semantic classes of words remaining stable across the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained and specific words having high variation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4006/>A Deeper Look into Dependency-Based Word Embeddings</a></strong><br><a href=/people/s/sean-macavaney/>Sean MacAvaney</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4006><div class="card-body p-3 small">We investigate the effect of various dependency-based word embeddings on distinguishing between functional and domain similarity, word similarity rankings, and two downstream tasks in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Variations include word embeddings trained using context windows from Stanford and Universal dependencies at several levels of enhancement (ranging from unlabeled, to Enhanced++ dependencies). Results are compared to basic linear contexts and evaluated on several <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We found that embeddings trained with Universal and Stanford dependency contexts excel at different tasks, and that enhanced dependencies often improve performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4007/>Learning Word Embeddings for Data Sparse and Sentiment Rich Data Sets</a></strong><br><a href=/people/p/prathusha-kameswara-sarma/>Prathusha Kameswara Sarma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4007><div class="card-body p-3 small">This research proposal describes two <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that are aimed at learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for data sparse and sentiment rich data sets. The goal is to use <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> adapted for domain specific data sets in downstream applications such as sentiment classification. The first approach learns word embeddings in a supervised fashion via SWESA (Supervised Word Embeddings for Sentiment Analysis), an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on data sets that are of modest size. SWESA leverages document labels to jointly learn polarity-aware word embeddings and a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to classify unseen documents. In the second approach domain adapted (DA) word embeddings are learned by exploiting the specificity of domain specific data sets and the breadth of generic word embeddings. The new embeddings are formed by aligning corresponding word vectors using Canonical Correlation Analysis (CCA) or the related nonlinear Kernel CCA. Experimental results on binary sentiment classification tasks using both approaches for standard <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> are presented.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4010 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276463184 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-4010/>End-to-End Learning of Task-Oriented Dialogs</a></strong><br><a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/i/ian-lane/>Ian Lane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4010><div class="card-body p-3 small">In this thesis proposal, we address the limitations of conventional pipeline design of task-oriented dialog systems and propose end-to-end learning solutions. We design neural network based dialog system that is able to robustly track dialog state, interface with knowledge bases, and incorporate structured query results into system responses to successfully complete task-oriented dialog. In learning such neural network based dialog systems, we propose hybrid offline training and online interactive learning methods. We introduce a multi-task learning method in pre-training the dialog agent in a supervised manner using task-oriented dialog corpora. The supervised training agent can further be improved via interacting with users and learning online from user demonstration and feedback with imitation and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. In addressing the sample efficiency issue with online policy learning, we further propose a method by combining the learning-from-user and learning-from-simulation approaches to improve the online interactive learning efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4014/>Japanese Predicate Conjugation for Neural Machine Translation<span class=acl-fixed-case>J</span>apanese Predicate Conjugation for Neural Machine Translation</a></strong><br><a href=/people/m/michiki-kurosawa/>Michiki Kurosawa</a>
|
<a href=/people/y/yukio-matsumura/>Yukio Matsumura</a>
|
<a href=/people/h/hayahide-yamagishi/>Hayahide Yamagishi</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4014><div class="card-body p-3 small">Neural machine translation (NMT) has a drawback in that can generate only high-frequency words owing to the computational costs of the <a href=https://en.wikipedia.org/wiki/Softmax_function>softmax function</a> in the output layer. In Japanese-English NMT, Japanese predicate conjugation causes an increase in <a href=https://en.wikipedia.org/wiki/Japanese_vocabulary>vocabulary size</a>. For example, one verb can have as many as 19 surface varieties. In this research, we focus on <a href=https://en.wikipedia.org/wiki/Grammatical_conjugation>predicate conjugation</a> for compressing the vocabulary size in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. The vocabulary list is filled with the various forms of verbs. We propose methods using predicate conjugation information without discarding <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a>. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can generate low-frequency words and deal with unknown words. Two methods were considered to introduce conjugation information : the first considers it as a token (conjugation token) and the second considers it as an embedded vector (conjugation feature). The results using these methods demonstrate that the vocabulary size can be compressed by approximately 86.1 % (Tanaka corpus) and the NMT models can output the words not in the training data set. Furthermore, BLEU scores improved by 0.91 points in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese-to-English translation</a>, and 0.32 points in <a href=https://en.wikipedia.org/wiki/Japanese_language>English-to-Japanese translation</a> with <a href=https://en.wikipedia.org/wiki/ASPEC>ASPEC</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4015 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4015/>Metric for Automatic Machine Translation Evaluation based on Universal Sentence Representations</a></strong><br><a href=/people/h/hiroki-shimanaka/>Hiroki Shimanaka</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4015><div class="card-body p-3 small">Sentence representations can capture a wide range of information that can not be captured by local features based on character or word N-grams. This paper examines the usefulness of universal sentence representations for evaluating the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Al-though it is difficult to train sentence representations using small-scale translation datasets with manual evaluation, sentence representations trained from large-scale data in other tasks can improve the automatic evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Experimental results of the WMT-2016 dataset show that the proposed method achieves state-of-the-art performance with sentence representation features only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4016 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4016/>Neural Machine Translation for Low Resource Languages using Bilingual Lexicon Induced from Comparable Corpora</a></strong><br><a href=/people/s/sree-harsha-ramesh/>Sree Harsha Ramesh</a>
|
<a href=/people/k/krishna-prasad-sankaranarayanan/>Krishna Prasad Sankaranarayanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4016><div class="card-body p-3 small">Resources for the non-English languages are scarce and this paper addresses this problem in the context of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, by automatically extracting parallel sentence pairs from the multilingual articles available on the Internet. In this paper, we have used an end-to-end Siamese bidirectional recurrent neural network to generate parallel sentences from comparable multilingual articles in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. Subsequently, we have showed that using the harvested dataset improved BLEU scores on both NMT and phrase-based SMT systems for the low-resource language pairs : EnglishHindi and EnglishTamil, when compared to training exclusively on the limited bilingual corpora collected for these language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4017 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4017/>Training a Ranking Function for Open-Domain Question Answering</a></strong><br><a href=/people/p/phu-mon-htut/>Phu Mon Htut</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4017><div class="card-body p-3 small">In recent years, there have been amazing advances in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> for <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a>. In <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a>, the machine reader has to extract the answer from the given ground truth paragraph. Recently, the state-of-the-art machine reading models achieve human level performance in SQuAD which is a reading comprehension-style question answering (QA) task. The success of <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a> has inspired researchers to combine <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a> with <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a> to tackle open-domain QA. However, these systems perform poorly compared to reading comprehension-style QA because it is difficult to retrieve the pieces of paragraphs that contain the answer to the question. In this study, we propose two neural network rankers that assign scores to different passages based on their likelihood of containing the answer to a given question. Additionally, we analyze the relative importance of <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> and word level relevance matching in open-domain QA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-4019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-4019 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-4019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-4019/>Sensing and Learning Human Annotators Engaged in Narrative Sensemaking<span class=acl-fixed-case>S</span>ensing and Learning Human Annotators Engaged in Narrative Sensemaking</a></strong><br><a href=/people/m/mckenna-tornblad/>McKenna Tornblad</a>
|
<a href=/people/l/luke-lapresi/>Luke Lapresi</a>
|
<a href=/people/c/christopher-homan/>Christopher Homan</a>
|
<a href=/people/r/raymond-ptucha/>Raymond Ptucha</a>
|
<a href=/people/c/cecilia-ovesdotter-alm/>Cecilia Ovesdotter Alm</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-4019><div class="card-body p-3 small">While labor issues and <a href=https://en.wikipedia.org/wiki/Quality_assurance>quality assurance</a> in <a href=https://en.wikipedia.org/wiki/Crowdwork>crowdwork</a> are increasingly studied, how annotators make sense of texts and how they are personally impacted by doing so are not. We study these questions via a narrative-sorting annotation task, where carefully selected (by sequentiality, topic, emotional content, and length) collections of tweets serve as examples of everyday storytelling. As readers process these narratives, we measure their <a href=https://en.wikipedia.org/wiki/Facial_expression>facial expressions</a>, <a href=https://en.wikipedia.org/wiki/Electrodermal_activity>galvanic skin response</a>, and self-reported reactions. From the perspective of annotator well-being, a reassuring outcome was that the sorting task did not cause a measurable <a href=https://en.wikipedia.org/wiki/Fight-or-flight_response>stress response</a>, however readers reacted to <a href=https://en.wikipedia.org/wiki/Humour>humor</a>. In terms of <a href=https://en.wikipedia.org/wiki/Sensemaking>sensemaking</a>, readers were more confident when sorting sequential, target-topical, and highly emotional tweets. As <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> becomes more common, this research sheds light onto the perceptive capabilities and emotional impact of human readers.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>