<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2021.mtsummit-at4ssl.pdf>Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL)</a></h2><p class=lead><a href=/people/d/dimitar-shterionov/>Dimitar Shterionov</a>
<span class=text-muted>(Editor)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.mtsummit-at4ssl</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Virtual</dd><dt>Venue:</dt><dd><a href=/venues/mtsummit/>MTSummit</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Machine Translation in the Americas</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.mtsummit-at4ssl>https://aclanthology.org/2021.mtsummit-at4ssl</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.mtsummit-at4ssl.pdf>https://aclanthology.org/2021.mtsummit-at4ssl.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.mtsummit-at4ssl.pdf title="Open PDF of 'Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+1st+International+Workshop+on+Automatic+Translation+for+Signed+and+Spoken+Languages+%28AT4SSL%29" title="Search for 'Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-at4ssl.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-at4ssl.0/>Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL)</a></strong><br><a href=/people/d/dimitar-shterionov/>Dimitar Shterionov</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-at4ssl.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-at4ssl--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-at4ssl.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-at4ssl.3/>Sign and Search : Sign Search Functionality for Sign Language Lexica</a></strong><br><a href=/people/m/manolis-fragkiadakis/>Manolis Fragkiadakis</a>
|
<a href=/people/p/peter-van-der-putten/>Peter van der Putten</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-at4ssl--3><div class="card-body p-3 small">Sign language lexica are a useful resource for researchers and people learning <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a>. Current implementations allow a user to search a sign either by its gloss or by selecting its primary features such as <a href=https://en.wikipedia.org/wiki/Handshape>handshape</a> and <a href=https://en.wikipedia.org/wiki/Location>location</a>. This study focuses on exploring a reverse search functionality where a user can sign a query sign in front of a webcam and retrieve a set of matching signs. By extracting different body joints combinations (upper body, dominant hand&#8217;s arm and wrist) using the pose estimation framework OpenPose, we compare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance metrics between 20 query signs, each performed by eight participants on a 1200 sign lexicon. The results show that UMAP and DTW can predict a matching sign with an 80 % and 71 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> respectively at the top-20 retrieved signs using the movement of the dominant hand arm. Using DTW and adding more sign instances from other participants in the lexicon, the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> can be raised to 90 % at the top-10 ranking. Our results suggest that our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> can be used with no training in any sign language lexicon regardless of its size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-at4ssl.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-at4ssl--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-at4ssl.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-at4ssl.4/>The Myth of Signing Avatars</a></strong><br><a href=/people/j/john-c-mcdonald/>John C. McDonald</a>
|
<a href=/people/r/rosalee-wolfe/>Rosalee Wolfe</a>
|
<a href=/people/e/eleni-efthimiou/>Eleni Efthimiou</a>
|
<a href=/people/e/evita-fontinea/>Evita Fontinea</a>
|
<a href=/people/f/frankie-picron/>Frankie Picron</a>
|
<a href=/people/d/davy-van-landuyt/>Davy Van Landuyt</a>
|
<a href=/people/t/tina-sioen/>Tina Sioen</a>
|
<a href=/people/a/annelies-braffort/>Annelies Braffort</a>
|
<a href=/people/m/michael-filhol/>Michael Filhol</a>
|
<a href=/people/s/sarah-ebling/>Sarah Ebling</a>
|
<a href=/people/t/thomas-hanke/>Thomas Hanke</a>
|
<a href=/people/v/verena-krausneker/>Verena Krausneker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-at4ssl--4><div class="card-body p-3 small">Development of <a href=https://en.wikipedia.org/wiki/Machine_translation>automatic translation</a> between signed and spoken languages has lagged behind the development of <a href=https://en.wikipedia.org/wiki/Machine_translation>automatic translation</a> between <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken languages</a>, but it is a common misperception that extending machine translation techniques to include signed languages should be a straightforward process. A contributing factor is the lack of an acceptable method for displaying <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a> apart from <a href=https://en.wikipedia.org/wiki/Language_interpretation>interpreters</a> on video. This position paper examines the challenges of displaying a <a href=https://en.wikipedia.org/wiki/Sign_language>signed language</a> as a target in <a href=https://en.wikipedia.org/wiki/Automatic_translation>automatic translation</a>, analyses the underlying causes and suggests strategies to develop display technologies that are acceptable to sign language communities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-at4ssl.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-at4ssl--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-at4ssl.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-at4ssl.5/>AVASAG : A German Sign Language Translation System for Public Services (short paper)<span class=acl-fixed-case>AVASAG</span>: A <span class=acl-fixed-case>G</span>erman <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage Translation System for Public Services (short paper)</a></strong><br><a href=/people/f/fabrizio-nunnari/>Fabrizio Nunnari</a>
|
<a href=/people/j/judith-bauerdiek/>Judith Bauerdiek</a>
|
<a href=/people/l/lucas-bernhard/>Lucas Bernhard</a>
|
<a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a>
|
<a href=/people/c/corinna-jager/>Corinna Jäger</a>
|
<a href=/people/a/amelie-unger/>Amelie Unger</a>
|
<a href=/people/k/kristoffer-waldow/>Kristoffer Waldow</a>
|
<a href=/people/s/sonja-wecker/>Sonja Wecker</a>
|
<a href=/people/e/elisabeth-andre/>Elisabeth André</a>
|
<a href=/people/s/stephan-busemann/>Stephan Busemann</a>
|
<a href=/people/c/christian-dold/>Christian Dold</a>
|
<a href=/people/a/arnulph-fuhrmann/>Arnulph Fuhrmann</a>
|
<a href=/people/p/patrick-gebhard/>Patrick Gebhard</a>
|
<a href=/people/y/yasser-hamidullah/>Yasser Hamidullah</a>
|
<a href=/people/m/marcel-hauck/>Marcel Hauck</a>
|
<a href=/people/y/yvonne-kossel/>Yvonne Kossel</a>
|
<a href=/people/m/martin-misiak/>Martin Misiak</a>
|
<a href=/people/d/dieter-wallach/>Dieter Wallach</a>
|
<a href=/people/a/alexander-stricker/>Alexander Stricker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-at4ssl--5><div class="card-body p-3 small">This paper presents an overview of AVASAG ; an ongoing applied-research project developing a text-to-sign-language translation system for public services. We describe the scientific innovation points (geometry-based SL-description, 3D animation and video corpus, simplified annotation scheme, motion capture strategy) and the overall translation pipeline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-at4ssl.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-at4ssl--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-at4ssl.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-at4ssl.7/>Approaching Sign Language Gloss Translation as a Low-Resource Machine Translation Task</a></strong><br><a href=/people/x/xuan-zhang/>Xuan Zhang</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-at4ssl--7><div class="card-body p-3 small">A cascaded Sign Language Translation system first maps sign videos to gloss annotations and then translates <a href=https://en.wikipedia.org/wiki/Gloss_(annotation)>glosses</a> into a <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken languages</a>. This work focuses on the second-stage gloss translation component, which is challenging due to the scarcity of publicly available parallel data. We approach <a href=https://en.wikipedia.org/wiki/Gloss_(annotation)>gloss translation</a> as a low-resource machine translation task and investigate two popular methods for improving translation quality : hyperparameter search and backtranslation. We discuss the potentials and pitfalls of these methods based on experiments on the RWTH-PHOENIX-Weather 2014 T dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-at4ssl.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-at4ssl--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-at4ssl.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-at4ssl.8/>Automatic generation of a 3D sign language avatar on <a href=https://en.wikipedia.org/wiki/Augmented_reality>AR glasses</a> given 2D videos of human signers<span class=acl-fixed-case>D</span> sign language avatar on <span class=acl-fixed-case>AR</span> glasses given 2<span class=acl-fixed-case>D</span> videos of human signers</a></strong><br><a href=/people/l/lan-thao-nguyen/>Lan Thao Nguyen</a>
|
<a href=/people/f/florian-schicktanz/>Florian Schicktanz</a>
|
<a href=/people/a/aeneas-stankowski/>Aeneas Stankowski</a>
|
<a href=/people/e/eleftherios-avramidis/>Eleftherios Avramidis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-at4ssl--8><div class="card-body p-3 small">In this paper we present a prototypical implementation of a <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> that allows the automatic generation of a German Sign Language avatar from <a href=https://en.wikipedia.org/wiki/2D_computer_graphics>2D video material</a>. The presentation is accompanied by the source code. We record <a href=https://en.wikipedia.org/wiki/List_of_human_positions>human pose movements</a> during signing with <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision models</a>. The joint coordinates of hands and arms are imported as <a href=https://en.wikipedia.org/wiki/Landmark>landmarks</a> to control the skeleton of our avatar. From the anatomically independent landmarks, we create another <a href=https://en.wikipedia.org/wiki/Skeleton>skeleton</a> based on the avatar&#8217;s skeletal bone architecture to calculate the bone rotation data. This <a href=https://en.wikipedia.org/wiki/Data>data</a> is then used to control our <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>human 3D avatar</a>. The <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>avatar</a> is displayed on <a href=https://en.wikipedia.org/wiki/Augmented_reality>AR glasses</a> and can be placed virtually in the room, in a way that it can be perceived simultaneously to the verbal speaker. In further work <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is aimed to be enhanced with <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation methods</a> for serving as a <a href=https://en.wikipedia.org/wiki/Language_interpretation>sign language interpreter</a>. The <a href=https://en.wikipedia.org/wiki/Prototype>prototype</a> has been shown to people of the deaf and hard-of-hearing community for assessing its comprehensibility. Problems emerged with the transferred hand rotations, hand gestures were hard to recognize on the <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>avatar</a> due to deformations like twisted finger meshes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-at4ssl.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-at4ssl--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-at4ssl.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-at4ssl.11/>Defining meaningful units. Challenges in sign segmentation and segment-meaning mapping (short paper)</a></strong><br><a href=/people/m/mirella-de-sisto/>Mirella De Sisto</a>
|
<a href=/people/d/dimitar-shterionov/>Dimitar Shterionov</a>
|
<a href=/people/i/irene-murtagh/>Irene Murtagh</a>
|
<a href=/people/m/myriam-vermeerbergen/>Myriam Vermeerbergen</a>
|
<a href=/people/l/lorraine-leeson/>Lorraine Leeson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-at4ssl--11><div class="card-body p-3 small">This paper addresses the tasks of sign segmentation and segment-meaning mapping in the context of sign language (SL) recognition. It aims to give an overview of the linguistic properties of SL, such as <a href=https://en.wikipedia.org/wiki/Coarticulation>coarticulation</a> and <a href=https://en.wikipedia.org/wiki/Simultaneity>simultaneity</a>, which make these tasks complex. A better understanding of SL structure is the necessary ground for the design and development of SL recognition and segmentation methodologies, which are fundamental for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> of these languages. Based on this preliminary exploration, a proposal for mapping segments to meaning in the form of an agglomerate of lexical and non-lexical information is introduced.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>