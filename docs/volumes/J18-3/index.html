<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Computational Linguistics, Volume 44, Issue 3 - September 2018 - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Computational Linguistics, Volume 44, Issue 3 - September 2018</h2><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>J18-3</dd><dt>Month:</dt><dd>September</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Cambridge, MA</dd><dt>Venue:</dt><dd><a href=/venues/cl/>CL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>MIT Press</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/J18-3>https://aclanthology.org/J18-3</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Computational+Linguistics%2C+Volume+44%2C+Issue+3+-+September+2018" title="Search for 'Computational Linguistics, Volume 44, Issue 3 - September 2018' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3000/>Computational Linguistics, Volume 44, Issue 3 - September 2018</a></strong><br></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3002/>A Structured Review of the Validity of BLEU<span class=acl-fixed-case>BLEU</span></a></strong><br><a href=/people/e/ehud-reiter/>Ehud Reiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-3002><div class="card-body p-3 small">The BLEU metric has been widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for over 15 years to evaluate <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>, especially in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. I present a structured review of the evidence on whether <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> is a valid evaluation techniquein other words, whether <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> correlate with real-world utility and user-satisfaction of NLP systems ; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3003/>Native Language Identification With Classifier Stacking and Ensembles</a></strong><br><a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/m/mark-dras/>Mark Dras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-3003><div class="card-body p-3 small">Ensemble methods using multiple <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> have proven to be among the most successful approaches for the task of Native Language Identification (NLI), achieving the current state of the art. However, a systematic examination of ensemble methods for NLI has yet to be conducted. Additionally, deeper <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble architectures</a> such as classifier stacking have not been closely evaluated. We present a set of experiments using three ensemble-based models, testing each with multiple configurations and algorithms. This includes a rigorous application of meta-classification models for NLI, achieving state-of-the-art results on several large data sets, evaluated in both intra-corpus and cross-corpus modes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3005/>Using <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> for Granularities of Tokenization</a></strong><br><a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-3005><div class="card-body p-3 small">Depending on downstream applications, it is advisable to extend the notion of <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a> from low-level character-based token boundary detection to identification of meaningful and useful language units. This entails both identifying units composed of several single words that form a several single words that form a, as well as splitting single-word compounds into their meaningful parts. In this article, we introduce unsupervised and knowledge-free methods for these two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. The main novelty of our research is based on the fact that methods are primarily based on distributional similarity, of which we use two flavors : a sparse count-based and a dense neural-based distributional semantic model. First, we introduce DRUID, which is a method for detecting MWEs. The evaluation on MWE-annotated data sets in two languages and newly extracted evaluation data sets for 32 languages shows that DRUID compares favorably over previous methods not utilizing distributional information. Second, we present SECOS, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for decompounding close compounds. In an evaluation of four dedicated decompounding data sets across four languages and on data sets extracted from <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a> for 14 languages, we demonstrate the superiority of our approach over unsupervised baselines, sometimes even matching the performance of previous language-specific and supervised methods. In a final experiment, we show how both decompounding and MWE information can be used in <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>. Here, we obtain the best results when combining <a href=https://en.wikipedia.org/wiki/Word>word information</a> with MWEs and the <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound parts</a> in a bag-of-words retrieval set-up.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3006/>Feature-Based Decipherment for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/i/iftekhar-naim/>Iftekhar Naim</a>
|
<a href=/people/p/parker-riley/>Parker Riley</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-3006><div class="card-body p-3 small">Orthographic similarities across languages provide a strong signal for unsupervised probabilistic transduction (decipherment) for closely related language pairs. The existing decipherment models, however, are not well suited for exploiting these orthographic similarities. We propose a <a href=https://en.wikipedia.org/wiki/Log-linear_model>log-linear model</a> with <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive for the proposed <a href=https://en.wikipedia.org/wiki/Log-linear_model>log-linear model</a>. To address this challenge, we perform <a href=https://en.wikipedia.org/wiki/Approximate_inference>approximate inference</a> via Markov chain Monte Carlo sampling and <a href=https://en.wikipedia.org/wiki/Contrastive_divergence>contrastive divergence</a>. Our results show that the proposed <a href=https://en.wikipedia.org/wiki/Log-linear_model>log-linear model</a> with <a href=https://en.wikipedia.org/wiki/Contrastive_divergence>contrastive divergence</a> outperforms the existing generative decipherment models by exploiting the orthographic features. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> both scales to large vocabularies and preserves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in low- and no-resource contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3007/>Survey : <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>Anaphora</a> With Non-nominal Antecedents in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>Computational Linguistics</a> : a Survey<span class=acl-fixed-case>S</span>urvey: Anaphora With Non-nominal Antecedents in Computational Linguistics: a <span class=acl-fixed-case>S</span>urvey</a></strong><br><a href=/people/v/varada-kolhatkar/>Varada Kolhatkar</a>
|
<a href=/people/a/adam-roussel/>Adam Roussel</a>
|
<a href=/people/s/stefanie-dipper/>Stefanie Dipper</a>
|
<a href=/people/h/heike-zinsmeister/>Heike Zinsmeister</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-3007><div class="card-body p-3 small">This article provides an extensive overview of the literature related to the phenomenon of non-nominal-antecedent anaphora (also known as abstract anaphora or discourse deixis), a type of <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a> in which an <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphor</a> like that refers to an antecedent (marked in boldface) that is syntactically non-nominal, such as the first sentence in It&#8217;s way too hot here. That&#8217;s why I&#8217;m moving to Alaska. Annotating and automatically resolving these cases of <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a> is interesting in its own right because of the complexities involved in identifying non-nominal antecedents, which typically represent abstract objects such as <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a>, facts, and propositions. There is also practical value in the resolution of non-nominal-antecedent anaphora, as this would help computational systems in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, as well as, conceivably, any other task dependent on some measure of text understanding. Most of the existing approaches to anaphora annotation and resolution focus on nominal-antecedent anaphora, classifying many of the cases where the antecedents are syntactically non-nominal as non-anaphoric. There has been some work done on this topic, but it remains scattered and difficult to collect and assess. With this article, we hope to bring together and synthesize work done in disparate contexts up to now in order to identify fundamental problems and draw conclusions from an overarching perspective. Having a good picture of the current state of the art in this field can help researchers direct their efforts to where they are most necessary. Because of the great variety of theoretical approaches that have been brought to bear on the problem, there is an equally diverse array of terminologies that are used to describe it, so we will provide an overview and discussion of these <a href=https://en.wikipedia.org/wiki/Terminology>terminologies</a>. We also describe the linguistic properties of non-nominal-antecedent anaphora, examine previous annotation efforts that have addressed this topic, and present the computational approaches that aim at resolving non-nominal-antecedent anaphora automatically. We close with a review of the remaining open questions in this area and some of our recommendations for future research.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>