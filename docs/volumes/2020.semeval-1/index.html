<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Fourteenth Workshop on Semantic Evaluation - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the Fourteenth Workshop on Semantic Evaluation</h2><p class=lead><a href=/people/a/aurelie-herbelot/>Aurelie Herbelot</a>,
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a>,
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>,
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>,
<a href=/people/j/jonathan-may/>Jonathan May</a>,
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.semeval-1</dd><dt>Month:</dt><dd>December</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Barcelona (online)</dd><dt>Venues:</dt><dd><a href=/venues/coling/>COLING</a>
| <a href=/venues/semeval/>SemEval</a></dd><dt>SIGs:</dt><dd><a href=/sigs/siglex/>SIGLEX</a>
|
<a href=/sigs/sigsem/>SIGSEM</a></dd><dt>Publisher:</dt><dd>International Committee for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.semeval-1>https://aclanthology.org/2020.semeval-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Fourteenth+Workshop+on+Semantic+Evaluation" title="Search for 'Proceedings of the Fourteenth Workshop on Semantic Evaluation' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.0/>Proceedings of the Fourteenth Workshop on Semantic Evaluation</a></strong><br><a href=/people/a/aurelie-herbelot/>Aurelie Herbelot</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.6/>Discovery Team at SemEval-2020 Task 1 : Context-sensitive Embeddings Not Always Better than Static for Semantic Change Detection<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: Context-sensitive Embeddings Not Always Better than Static for Semantic Change Detection</a></strong><br><a href=/people/m/matej-martinc/>Matej Martinc</a>
|
<a href=/people/s/syrielle-montariol/>Syrielle Montariol</a>
|
<a href=/people/e/elaine-zosa/>Elaine Zosa</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--6><div class="card-body p-3 small">This paper describes the approaches used by the Discovery Team to solve SemEval-2020 Task 1-Unsupervised Lexical Semantic Change Detection. The proposed method is based on clustering of BERT contextual embeddings, followed by a comparison of cluster distributions across time. The best results were obtained by an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> of this method and static Word2Vec embeddings. According to the official results, our approach proved the best for <a href=https://en.wikipedia.org/wiki/Latin>Latin</a> in Subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.7/>GM-CTSC at SemEval-2020 Task 1 : Gaussian Mixtures Cross Temporal Similarity Clustering<span class=acl-fixed-case>GM</span>-<span class=acl-fixed-case>CTSC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: <span class=acl-fixed-case>G</span>aussian Mixtures Cross Temporal Similarity Clustering</a></strong><br><a href=/people/p/pierluigi-cassotti/>Pierluigi Cassotti</a>
|
<a href=/people/a/annalina-caputo/>Annalina Caputo</a>
|
<a href=/people/m/marco-polignano/>Marco Polignano</a>
|
<a href=/people/p/pierpaolo-basile/>Pierpaolo Basile</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--7><div class="card-body p-3 small">This paper describes the system proposed by the Random team for SemEval-2020 Task 1 : Unsupervised Lexical Semantic Change Detection. We focus our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> on the <a href=https://en.wikipedia.org/wiki/Detection_theory>detection problem</a>. Given the semantics of words captured by temporal word embeddings in different time periods, we investigate the use of <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> to detect when the target word has gained or lost senses. To this end, we define a new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> based on Gaussian Mixture Models to cluster the target similarities computed over the two periods. We compare the proposed approach with a number of <a href=https://en.wikipedia.org/wiki/Similarity_(geometry)>similarity-based thresholds</a>. We found that, although the performance of the detection methods varies across the word embedding algorithms, the combination of Gaussian Mixture with Temporal Referencing resulted in our best system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.10/>RIJP at SemEval-2020 Task 1 : Gaussian-based Embeddings for Semantic Change Detection<span class=acl-fixed-case>RIJP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: <span class=acl-fixed-case>G</span>aussian-based Embeddings for Semantic Change Detection</a></strong><br><a href=/people/r/ran-iwamoto/>Ran Iwamoto</a>
|
<a href=/people/m/masahiro-yukawa/>Masahiro Yukawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--10><div class="card-body p-3 small">This paper describes the model proposed and submitted by our RIJP team to SemEval 2020 Task1 : Unsupervised Lexical Semantic Change Detection. In the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, words are represented by <a href=https://en.wikipedia.org/wiki/Normal_distribution>Gaussian distributions</a>. For Subtask 1, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved average scores of 0.51 and 0.70 in the evaluation and post-evaluation processes, respectively. The higher score in the post-evaluation process than that in the evaluation process was achieved owing to appropriate parameter tuning. The results indicate that the proposed Gaussian-based embedding model is able to express <a href=https://en.wikipedia.org/wiki/Semantic_shift>semantic shifts</a> while having a low computational</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.14/>UiO-UvA at SemEval-2020 Task 1 : Contextualised Embeddings for Lexical Semantic Change Detection<span class=acl-fixed-case>U</span>i<span class=acl-fixed-case>O</span>-<span class=acl-fixed-case>U</span>v<span class=acl-fixed-case>A</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: Contextualised Embeddings for Lexical Semantic Change Detection</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/m/mario-giulianelli/>Mario Giulianelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--14><div class="card-body p-3 small">We apply contextualised word embeddings to lexical semantic change detection in the SemEval-2020 Shared Task 1. This paper focuses on Subtask 2, ranking words by the degree of their semantic drift over time. We analyse the performance of two contextualising architectures (BERT and ELMo) and three change detection algorithms. We find that the most effective algorithms rely on the cosine similarity between averaged token embeddings and the pairwise distances between token embeddings. They outperform strong baselines by a large margin (in the post-evaluation phase, we have the best Subtask 2 submission for SemEval-2020 Task 1), but interestingly, the choice of a particular <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> depends on the distribution of gold scores in the test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.15/>BMEAUT at SemEval-2020 Task 2 : <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Entailment</a> with Semantic Graphs<span class=acl-fixed-case>BMEAUT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 2: Lexical Entailment with Semantic Graphs</a></strong><br><a href=/people/a/adam-kovacs/>Ádám Kovács</a>
|
<a href=/people/k/kinga-gemes/>Kinga Gémes</a>
|
<a href=/people/a/andras-kornai/>Andras Kornai</a>
|
<a href=/people/g/gabor-recski/>Gábor Recski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--15><div class="card-body p-3 small">In this paper we present a novel rule-based, language independent method for determining lexical entailment relations using semantic representations built from Wiktionary definitions. Combined with a simple WordNet-based method our system achieves top scores on the English and Italian datasets of the Semeval-2020 task Predicting Multilingual and Cross-lingual (graded) Lexical Entailment (Glava et al., 2020). A detailed error analysis of our output uncovers future di- rections for improving both the semantic parsing method and the inference process on semantic graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.16/>BRUMS at SemEval-2020 Task 3 : Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity<span class=acl-fixed-case>BRUMS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 3: Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity</a></strong><br><a href=/people/h/hansi-hettiarachchi/>Hansi Hettiarachchi</a>
|
<a href=/people/t/tharindu-ranasinghe/>Tharindu Ranasinghe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--16><div class="card-body p-3 small">This paper presents the team BRUMS submission to SemEval-2020 Task 3 : Graded Word Similarity in Context. The system utilises state-of-the-art contextualised word embeddings, which have some task-specific adaptations, including stacked embeddings and average embeddings. Overall, the <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> achieves good evaluation scores across all the languages, while maintaining simplicity. Following the final rankings, our approach is ranked within the top 5 solutions of each language while preserving the 1st position of Finnish subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.19/>UZH at SemEval-2020 Task 3 : Combining BERT with WordNet Sense Embeddings to Predict Graded Word Similarity Changes<span class=acl-fixed-case>UZH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 3: Combining <span class=acl-fixed-case>BERT</span> with <span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Sense Embeddings to Predict Graded Word Similarity Changes</a></strong><br><a href=/people/l/li-tang/>Li Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--19><div class="card-body p-3 small">CoSimLex is a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that can be used to evaluate the ability of context-dependent word embed- dings for modeling subtle, graded changes of meaning, as perceived by humans during reading. At SemEval-2020, task 3, subtask 1 is about predicting the (graded) effect of context in word similarity, using CoSimLex to quantify such a change of <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> for a pair of words, from one context to another. Here, a meaning shift is composed of two aspects, a) discrete changes observed between different word senses, and b) more subtle changes of meaning representation that are not captured in those discrete changes. Therefore, this SemEval task was designed to allow the evaluation of <a href=https://en.wikipedia.org/wiki/System>systems</a> that can deal with a mix of both situations of <a href=https://en.wikipedia.org/wiki/Semantic_shift>semantic shift</a>, as they occur in the human perception of meaning. The described system was developed to improve the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>BERT baseline</a> provided with the task, by reducing distortions in the BERT semantic space, compared to the human semantic space. To this end, complementarity between 768- and 1024-dimensional BERT embeddings, and average word sense vectors were used. With this <a href=https://en.wikipedia.org/wiki/System>system</a>, after some fine-tuning, the baseline performance of 0.705 (uncentered Pearson correlation with human semantic shift data from 27 annotators) was enhanced by more than 6 %, to 0.7645. We hope that this work can make a contribution to further our understanding of the semantic vector space of human perception, as it can be modeled with context-dependent word embeddings in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.23/>DCC-Uchile at SemEval-2020 Task 1 : Temporal Referencing Word Embeddings<span class=acl-fixed-case>DCC</span>-Uchile at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: Temporal Referencing Word Embeddings</a></strong><br><a href=/people/f/frank-d-zamora-reina/>Frank D. Zamora-Reina</a>
|
<a href=/people/f/felipe-bravo-marquez/>Felipe Bravo-Marquez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--23><div class="card-body p-3 small">We present a system for the task of unsupervised lexical change detection : given a target word and two corpora spanning different periods of time, automatically detects whether the word has lost or gained senses from one corpus to another. Our system employs the temporal referencing method to obtain compatible representations of target words in different periods of time. This is done by concatenating corpora of different periods and performing a temporal referencing of target words i.e., treating occurrences of target words in different periods as two independent tokens. Afterwards, we train <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> on the joint corpus and compare the referenced vectors of each target word using <a href=https://en.wikipedia.org/wiki/Cosine>cosine similarity</a>. Our submission was ranked 7th among 34 teams for subtask 1, obtaining an average accuracy of 0.637, only 0.050 points behind the first ranked system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.26/>SST-BERT at SemEval-2020 Task 1 : Semantic Shift Tracing by Clustering in BERT-based Embedding Spaces<span class=acl-fixed-case>SST</span>-<span class=acl-fixed-case>BERT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: Semantic Shift Tracing by Clustering in <span class=acl-fixed-case>BERT</span>-based Embedding Spaces</a></strong><br><a href=/people/v/vani-kanjirangat/>Vani Kanjirangat</a>
|
<a href=/people/s/sandra-mitrovic/>Sandra Mitrovic</a>
|
<a href=/people/a/alessandro-antonucci/>Alessandro Antonucci</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--26><div class="card-body p-3 small">Lexical semantic change detection (also known as semantic shift tracing) is a task of identifying words that have changed their meaning over time. Unsupervised semantic shift tracing, focal point of SemEval2020, is particularly challenging. Given the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setup</a>, in this work, we propose to identify <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a> among different occurrences of each target word, considering these as representatives of different word meanings. As such, disagreements in obtained clusters naturally allow to quantify the level of <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic shift</a> per each target word in four target languages. To leverage this idea, <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> is performed on contextualized (BERT-based) embeddings of word occurrences. The obtained results show that our approach performs well both measured separately (per language) and overall, where we surpass all provided SemEval baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.27/>TemporalTeller at SemEval-2020 Task 1 : Unsupervised Lexical Semantic Change Detection with Temporal Referencing<span class=acl-fixed-case>T</span>emporal<span class=acl-fixed-case>T</span>eller at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Referencing</a></strong><br><a href=/people/j/jinan-zhou/>Jinan Zhou</a>
|
<a href=/people/j/jiaxin-li/>Jiaxin Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--27><div class="card-body p-3 small">This paper describes our TemporalTeller system for SemEval Task 1 : Unsupervised Lexical Semantic Change Detection. We develop a unified framework for the common semantic change detection pipelines including preprocessing, learning word embeddings, calculating vector distances and determining threshold. We also propose Gamma Quantile Threshold to distinguish between changed and stable words. Based on our system, we conduct a comprehensive comparison among BERT, <a href=https://en.wikipedia.org/wiki/Skip-gram>Skip-gram</a>, Temporal Referencing and alignment-based methods. Evaluation results show that <a href=https://en.wikipedia.org/wiki/Skip-gram>Skip-gram</a> with Temporal Referencing achieves the best performance of 66.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>classification accuracy</a> and 51.8 % Spearman&#8217;s Ranking Correlation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.35/>Ferryman at SemEval-2020 Task 3 : Bert with TFIDF-Weighting for Predicting the Effect of Context in Word Similarity<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 3: Bert with <span class=acl-fixed-case>TFIDF</span>-Weighting for Predicting the Effect of Context in Word Similarity</a></strong><br><a href=/people/w/weilong-chen/>Weilong Chen</a>
|
<a href=/people/x/xin-yuan/>Xin Yuan</a>
|
<a href=/people/s/sai-zhang/>Sai Zhang</a>
|
<a href=/people/j/jiehui-wu/>Jiehui Wu</a>
|
<a href=/people/y/yanru-zhang/>Yanru Zhang</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--35><div class="card-body p-3 small">Word similarity is widely used in machine learning applications like <a href=https://en.wikipedia.org/wiki/Web_search_engine>searching engine</a> and <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation</a>. Measuring the changing meaning of the same word between two different sentences is not only a way to handle complex features in word usage (such as sentence syntax and semantics), but also an important method for different word polysemy modeling. In this paper, we present the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> proposed by team Ferryman. Our system is based on the Bidirectional Encoder Representations from Transformers (BERT) model combined with term frequency-inverse document frequency (TF-IDF), applying the method on the provided datasets called CoSimLex, which covers four different languages including <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovene</a>, and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>. Our team Ferryman wins the the first position for English task and the second position for <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> in the subtask 1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.37/>JUSTMasters at SemEval-2020 Task 3 : Multilingual Deep Learning Model to Predict the Effect of Context in Word Similarity<span class=acl-fixed-case>JUSTM</span>asters at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 3: Multilingual Deep Learning Model to Predict the Effect of Context in Word Similarity</a></strong><br><a href=/people/n/nour-al-khdour/>Nour Al-khdour</a>
|
<a href=/people/m/mutaz-bni-younes/>Mutaz Bni Younes</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a>
|
<a href=/people/m/mohammad-al-smadi/>Mohammad AL-Smadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--37><div class="card-body p-3 small">There is a growing research interest in studying word similarity. Without a doubt, two similar words in a context may considered different in another context. Therefore, this paper investigates the effect of the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> in word similarity. The SemEval-2020 workshop has provided a shared task (Task 3 : Predicting the (Graded) Effect of Context in Word Similarity). In this task, the organizers provided unlabeled datasets for four languages, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> and <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovenian</a>. Our team, JUSTMasters, has participated in this competition in the two subtasks : A and B. Our approach has used a weighted average ensembling method for different pretrained embeddings techniques for each of the four languages. Our proposed model outperformed the baseline models in both subtasks and acheived the best result for subtask 2 in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, with score 0.725 and 0.68 respectively. We have been ranked the sixth for subtask 1, with scores for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, and <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovenian</a> as follows : 0.738, 0.44, 0.546, 0.512.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.38/>Will_Go at SemEval-2020 Task 3 : An Accurate Model for Predicting the (Graded) Effect of Context in Word Similarity Based on BERT<span class=acl-fixed-case>W</span>ill_<span class=acl-fixed-case>G</span>o at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 3: An Accurate Model for Predicting the (Graded) Effect of Context in Word Similarity Based on <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/w/wei-bao/>Wei Bao</a>
|
<a href=/people/h/hongshu-che/>Hongshu Che</a>
|
<a href=/people/j/jiandong-zhang/>Jiandong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--38><div class="card-body p-3 small">Natural Language Processing (NLP) has been widely used in the <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a> in recent years. Our paper mainly discusses a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to analyze the effect that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> has on human perception of similar words, which is the third task of SemEval 2020. We apply several methods in calculating the distance between two embedding vector generated by Bidirectional Encoder Representation from Transformer (BERT). Our team will go won the 1st place in Finnish language track of subtask1, the second place in English track of subtask1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.41/>SemEval-2020 Task 6 : Definition Extraction from Free Text with the DEFT Corpus<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 6: Definition Extraction from Free Text with the <span class=acl-fixed-case>DEFT</span> Corpus</a></strong><br><a href=/people/s/sasha-spala/>Sasha Spala</a>
|
<a href=/people/n/nicholas-miller/>Nicholas Miller</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/c/carl-dockhorn/>Carl Dockhorn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--41><div class="card-body p-3 small">Research on definition extraction has been conducted for well over a decade, largely with significant constraints on the type of definitions considered. In this work, we present DeftEval, a SemEval shared task in which participants must extract definitions from free text using a term-definition pair corpus that reflects the complex reality of definitions in natural language. Definitions and glosses in free text often appear without explicit indicators, across sentences boundaries, or in an otherwise complex linguistic manner. DeftEval involved 3 distinct subtasks : 1) Sentence classification, 2) <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, and 3) relation extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.42/>IIE-NLP-NUT at SemEval-2020 Task 4 : Guiding PLM with Prompt Template Reconstruction Strategy for ComVE<span class=acl-fixed-case>IIE</span>-<span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>NUT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Guiding <span class=acl-fixed-case>PLM</span> with Prompt Template Reconstruction Strategy for <span class=acl-fixed-case>C</span>om<span class=acl-fixed-case>VE</span></a></strong><br><a href=/people/l/luxi-xing/>Luxi Xing</a>
|
<a href=/people/y/yuqiang-xie/>Yuqiang Xie</a>
|
<a href=/people/y/yue-hu/>Yue Hu</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--42><div class="card-body p-3 small">This paper introduces our systems for the first two subtasks of SemEval Task4 : Commonsense Validation and Explanation. To clarify the intention for judgment and inject contrastive information for selection, we propose the input reconstruction strategy with prompt templates. Specifically, we formalize the <a href=https://en.wikipedia.org/wiki/Question_answering>subtasks</a> into the multiple-choice question answering format and construct the input with the prompt templates, then, the final prediction of question answering is considered as the result of subtasks. Experimental results show that our approaches achieve significant performance compared with the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a>. Our approaches secure the third rank on both <a href=https://en.wikipedia.org/wiki/Standard_score>official test sets</a> of the first two subtasks with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 96.4 and an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 94.3 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.46" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.46/>BUT-FIT at SemEval-2020 Task 4 : Multilingual Commonsense<span class=acl-fixed-case>BUT</span>-<span class=acl-fixed-case>FIT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Multilingual Commonsense</a></strong><br><a href=/people/j/josef-jon/>Josef Jon</a>
|
<a href=/people/m/martin-fajcik/>Martin Fajcik</a>
|
<a href=/people/m/martin-docekal/>Martin Docekal</a>
|
<a href=/people/p/pavel-smrz/>Pavel Smrz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--46><div class="card-body p-3 small">We participated in all three subtasks. In subtasks A and B, our submissions are based on pretrained language representation models (namely ALBERT) and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. We experimented with solving the task for another language, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, by means of multilingual models and machine translated dataset, or translated model inputs. We show that with a strong machine translation system, our <a href=https://en.wikipedia.org/wiki/System>system</a> can be used in another language with a small <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy loss</a>. In subtask C, our submission, which is based on pretrained sequence-to-sequence model (BART), ranked 1st in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score ranking</a>, however, we show that the correlation between <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and human evaluation, in which our submission ended up 4th, is low. We analyse the metrics used in the evaluation and we propose an additional score based on model from subtask B, which correlates well with our manual ranking, as well as reranking method based on the same principle. We performed an error and dataset analysis for all subtasks and we present our findings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.49/>Masked Reasoner at SemEval-2020 Task 4 : Fine-Tuning RoBERTa for Commonsense Reasoning<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Fine-Tuning <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a for Commonsense Reasoning</a></strong><br><a href=/people/d/daming-lu/>Daming Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--49><div class="card-body p-3 small">This paper describes the masked reasoner system that participated in SemEval-2020 Task 4 : Commonsense Validation and Explanation. The <a href=https://en.wikipedia.org/wiki/System>system</a> participated in the subtask B.We proposes a novel <a href=https://en.wikipedia.org/wiki/Methodology>method</a> to fine-tune RoBERTa by masking the most important word in the statement. We believe that the confidence of the system in recovering that word is positively correlated to the score the masked language model gives to the current statement-explanation pair. We evaluate the importance of each word using InferSent and do the masked fine-tuning on RoBERTa. Then we use the fine-tuned model to predict the most plausible explanation. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is fast in training and achieved 73.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.52/>UoR at SemEval-2020 Task 4 : Pre-trained Sentence Transformer Models for Commonsense Validation and Explanation<span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>R</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Pre-trained Sentence Transformer Models for Commonsense Validation and Explanation</a></strong><br><a href=/people/t/thanet-markchom/>Thanet Markchom</a>
|
<a href=/people/b/bhuvana-dhruva/>Bhuvana Dhruva</a>
|
<a href=/people/c/chandresh-pravin/>Chandresh Pravin</a>
|
<a href=/people/h/huizhi-liang/>Huizhi Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--52><div class="card-body p-3 small">SemEval Task 4 Commonsense Validation and Explanation Challenge is to validate whether a system can differentiate natural language statements that make sense from those that do not make sense. Two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>subtasks</a>, A and B, are focused in this work, i.e., detecting against-common-sense statements and selecting explanations of why they are false from the given options. Intuitively, commonsense validation requires additional knowledge beyond the given statements. Therefore, we propose a system utilising pre-trained sentence transformer models based on BERT, RoBERTa and DistillBERT architectures to embed the statements before classification. According to the results, these embeddings can improve the performance of the typical MLP and LSTM classifiers as downstream models of both subtasks compared to regular tokenised statements. These embedded statements are shown to comprise additional information from external resources which help validate common sense in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.53" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.53/>BUT-FIT at SemEval-2020 Task 5 : Automatic Detection of Counterfactual Statements with Deep Pre-trained Language Representation Models<span class=acl-fixed-case>BUT</span>-<span class=acl-fixed-case>FIT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: Automatic Detection of Counterfactual Statements with Deep Pre-trained Language Representation Models</a></strong><br><a href=/people/m/martin-fajcik/>Martin Fajcik</a>
|
<a href=/people/j/josef-jon/>Josef Jon</a>
|
<a href=/people/m/martin-docekal/>Martin Docekal</a>
|
<a href=/people/p/pavel-smrz/>Pavel Smrz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--53><div class="card-body p-3 small">This paper describes BUT-FIT&#8217;s submission at SemEval-2020 Task 5 : Modelling Causal Reasoning in Language : Detecting Counterfactuals. The challenge focused on detecting whether a given statement contains a counterfactual (Subtask 1) and extracting both antecedent and consequent parts of the counterfactual from the text (Subtask 2). We experimented with various state-of-the-art language representation models (LRMs). We found RoBERTa LRM to perform the best in both subtasks. We achieved the first place in both exact match and F1 for Subtask 2 and ranked second for Subtask 1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.58/>ACNLP at SemEval-2020 Task 6 : A Supervised Approach for Definition Extraction<span class=acl-fixed-case>ACNLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 6: A Supervised Approach for Definition Extraction</a></strong><br><a href=/people/f/fabien-caspani/>Fabien Caspani</a>
|
<a href=/people/p/pirashanth-ratnamogan/>Pirashanth Ratnamogan</a>
|
<a href=/people/m/mathis-linger/>Mathis Linger</a>
|
<a href=/people/m/mhamed-hajaiej/>Mhamed Hajaiej</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--58><div class="card-body p-3 small">We describe our contribution to two of the subtasks of SemEval 2020 Task 6, DeftEval : Extracting term-definition pairs in free text. The system for Subtask 1 : Sentence Classification is based on a transformer architecture where we use <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to fine-tune a pretrained model on the downstream task, and the one for Subtask 3 : Relation Classification uses a Random Forest classifier with handcrafted dedicated features. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> respectively achieve 0.830 and 0.994 <a href=https://en.wikipedia.org/wiki/Standard_score>F1-scores</a> on the <a href=https://en.wikipedia.org/wiki/Standard_score>official test set</a>, and we believe that the insights derived from our study are potentially relevant to help advance the research on definition extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.60/>CN-HIT-IT.NLP at SemEval-2020 Task 4 : Enhanced Language Representation with Multiple Knowledge Triples<span class=acl-fixed-case>CN</span>-<span class=acl-fixed-case>HIT</span>-<span class=acl-fixed-case>IT</span>.<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Enhanced Language Representation with Multiple Knowledge Triples</a></strong><br><a href=/people/y/yice-zhang/>Yice Zhang</a>
|
<a href=/people/j/jiaxuan-lin/>Jiaxuan Lin</a>
|
<a href=/people/y/yang-fan/>Yang Fan</a>
|
<a href=/people/p/peng-jin/>Peng Jin</a>
|
<a href=/people/y/yuanchao-liu/>Yuanchao Liu</a>
|
<a href=/people/b/bingquan-liu/>Bingquan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--60><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> that participated in the SemEval-2020 task 4 : Commonsense Validation and Explanation. For this task, it is obvious that external knowledge, such as <a href=https://en.wikipedia.org/wiki/Knowledge_graph>Knowledge graph</a>, can help the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> understand commonsense in natural language statements. But how to select the right triples for statements remains unsolved, so how to reduce the interference of irrelevant triples on <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance is a research focus. This paper adopt a modified K-BERT as the language encoder, to enhance language representation through triples from <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. Experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is better than <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> without external knowledge, and is slightly better than the original K-BERT. We got an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy score</a> of 0.97 in subtaskA, ranking 1/45, and got an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy score</a> of 0.948, ranking 2/35.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.62/>CS-NLP Team at SemEval-2020 Task 4 : Evaluation of State-of-the-art NLP Deep Learning Architectures on Commonsense Reasoning Task<span class=acl-fixed-case>CS</span>-<span class=acl-fixed-case>NLP</span> Team at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Evaluation of State-of-the-art <span class=acl-fixed-case>NLP</span> Deep Learning Architectures on Commonsense Reasoning Task</a></strong><br><a href=/people/s/sirwe-saeedi/>Sirwe Saeedi</a>
|
<a href=/people/a/aliakbar-panahi/>Aliakbar Panahi</a>
|
<a href=/people/s/seyran-saeedi/>Seyran Saeedi</a>
|
<a href=/people/a/alvis-c-fong/>Alvis C Fong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--62><div class="card-body p-3 small">In this paper, we investigate a commonsense inference task that unifies <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>. We describe our attempt at SemEval-2020 Task 4 competition : Commonsense Validation and Explanation (ComVE) challenge. We discuss several state-of-the-art deep learning architectures for this challenge. Our system uses prepared labeled textual datasets that were manually curated for three different natural language inference subtasks. The goal of the first subtask is to test whether a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can distinguish between natural language statements that make sense and those that do not make sense. We compare the performance of several <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and fine-tuned classifiers. Then, we propose a method inspired by <a href=https://en.wikipedia.org/wiki/Question_answering>question / answering tasks</a> to treat a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification problem</a> as a <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple choice question task</a> to boost the performance of our experimental results (96.06 %), which is significantly better than the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. For the second subtask, which is to select the reason why a statement does not make sense, we stand within the first six teams (93.7 %) among 27 participants with very competitive results. Our result for last subtask of generating reason against the nonsense statement shows many potentials for future researches as we applied the most powerful generative model of language (GPT-2) with 6.1732 BLEU score among first four teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.65/>JBNU at SemEval-2020 Task 4 : BERT and UniLM for Commonsense Validation and Explanation<span class=acl-fixed-case>JBNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: <span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>LM</span> for Commonsense Validation and Explanation</a></strong><br><a href=/people/s/seung-hoon-na/>Seung-Hoon Na</a>
|
<a href=/people/j/jong-hyeon-lee/>Jong-Hyeon Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--65><div class="card-body p-3 small">This paper presents our contributions to the SemEval-2020 Task 4 Commonsense Validation and Explanation (ComVE) and includes the experimental results of the two Subtasks B and C of the SemEval-2020 Task 4. Our systems rely on pre-trained language models, i.e., BERT (including its variants) and UniLM, and rank 10th and 7th among 27 and 17 systems on Subtasks B and C, respectively. We analyze the commonsense ability of the existing pretrained language models by testing them on the SemEval-2020 Task 4 ComVE dataset, specifically for Subtasks B and C, the explanation subtasks with multi-choice and sentence generation, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.67" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.67/>KaLM at SemEval-2020 Task 4 : Knowledge-aware Language Models for Comprehension and Generation<span class=acl-fixed-case>K</span>a<span class=acl-fixed-case>LM</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Knowledge-aware Language Models for Comprehension and Generation</a></strong><br><a href=/people/j/jiajing-wan/>Jiajing Wan</a>
|
<a href=/people/x/xinting-huang/>Xinting Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--67><div class="card-body p-3 small">This paper presents our <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> in SemEval 2020 Task 4 : Commonsense Validation and <a href=https://en.wikipedia.org/wiki/Explanation>Explanation</a>. We propose a novel way to search for evidence and choose the different large-scale pre-trained models as the backbone for three subtasks. The results show that our evidence-searching approach improves <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance on commonsense explanation task. Our <a href=https://en.wikipedia.org/wiki/Team>team</a> ranks 2nd in subtask C according to human evaluation score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.70/>LMVE at SemEval-2020 Task 4 : Commonsense Validation and Explanation Using Pretraining Language Model<span class=acl-fixed-case>LMVE</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Commonsense Validation and Explanation Using Pretraining Language Model</a></strong><br><a href=/people/s/shilei-liu/>Shilei Liu</a>
|
<a href=/people/y/yu-guo/>Yu Guo</a>
|
<a href=/people/b/bochao-li/>BoChao Li</a>
|
<a href=/people/f/feiliang-ren/>Feiliang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--70><div class="card-body p-3 small">This paper introduces our <a href=https://en.wikipedia.org/wiki/System>system</a> for commonsense validation and <a href=https://en.wikipedia.org/wiki/Explanation>explanation</a>. For Sen-Making task, we use a novel pretraining language model based architecture to pick out one of the two given statements that is againstcommon sense. For Explanation task, we use a hint sentence mechanism to improve the performance greatly. In addition, we propose a subtask level transfer learning to share information between subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.73/>SSN-NLP at SemEval-2020 Task 4 : Text Classification and Generation on Common Sense Context Using Neural Networks<span class=acl-fixed-case>SSN</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Text Classification and Generation on Common Sense Context Using Neural Networks</a></strong><br><a href=/people/r/rishivardhan-k/>Rishivardhan K.</a>
|
<a href=/people/k/kayalvizhi-s/>Kayalvizhi S</a>
|
<a href=/people/t/thenmozhi-d/>Thenmozhi D.</a>
|
<a href=/people/r/raghav-r/>Raghav R.</a>
|
<a href=/people/k/kshitij-sharma/>Kshitij Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--73><div class="card-body p-3 small">Common sense validation deals with testing whether a <a href=https://en.wikipedia.org/wiki/System>system</a> can differentiate natural language statements that make sense from those that do not make sense. This paper describes the our approach to solve this challenge. For common sense validation with <a href=https://en.wikipedia.org/wiki/Multiple_choice>multi choice</a>, we propose a stacking based approach to classify sentences that are more favourable in terms of common sense to the particular statement. We have used majority voting classifier methodology amongst three models such as Bidirectional Encoder Representations from Transformers (BERT), Micro Text Classification (Micro TC) and XLNet. For sentence generation, we used Neural Machine Translation (NMT) model to generate explanatory sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.77/>UAICS at SemEval-2020 Task 4 : Using a Bidirectional Transformer for Task a<span class=acl-fixed-case>UAICS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: Using a Bidirectional Transformer for Task a</a></strong><br><a href=/people/c/ciprian-gabriel-cusmuliuc/>Ciprian-Gabriel Cusmuliuc</a>
|
<a href=/people/l/lucia-georgiana-coca/>Lucia-Georgiana Coca</a>
|
<a href=/people/a/adrian-iftene/>Adrian Iftene</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--77><div class="card-body p-3 small">Commonsense Validation and Explanation has been a difficult task for machines since the dawn of <a href=https://en.wikipedia.org/wiki/Computing>computing</a>. Although very trivial to humans it poses a high <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> for machines due to the necessity of <a href=https://en.wikipedia.org/wiki/Inference>inference</a> over a pre-existing knowledge base. In order to try and solve this problem the SemEval 2020 Task 4-Commonsense Validation and Explanation (ComVE) aims to evaluate systems capable of multiple stages of ComVE. The challenge includes 3 <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> (A, B and C), each with it&#8217;s own requirements. Our team participated only in task A which required selecting the statement that made the least sense. We choose to use a bidirectional transformer in order to solve the challenge, this paper presents the details of our method, runs and result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.79/>Warren at SemEval-2020 Task 4 : ALBERT and Multi-Task Learning for Commonsense Validation<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 4: <span class=acl-fixed-case>ALBERT</span> and Multi-Task Learning for Commonsense Validation</a></strong><br><a href=/people/y/yuhang-wu/>Yuhang Wu</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--79><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> in subtask A of SemEval 2020 Shared Task 4. We propose a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning model</a> based on MTL(Multi-Task Learning) to enhance the prediction ability of commonsense validation. The experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms the single-task text classification model. We combine MTL and ALBERT pretrain model to achieve an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.904 and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is ranked 16th on the final leader board of the competition among the 45 teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.83/>ETHAN at SemEval-2020 Task 5 : Modelling Causal Reasoning in Language Using Neuro-symbolic Cloud Computing<span class=acl-fixed-case>ETHAN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: Modelling Causal Reasoning in Language Using Neuro-symbolic Cloud Computing</a></strong><br><a href=/people/l/len-yabloko/>Len Yabloko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--83><div class="card-body p-3 small">I present ETHAN : Experimental Testing of Hybrid AI Node implemented entirely on free cloud computing infrastructure. The ultimate goal of this research is to create modular reusable hybrid neuro-symbolic architecture for <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>Artificial Intelligence</a>. As a test case I model natural language comprehension of causal relations from open domain text corpus that combines semi-supervised language model (Huggingface Transformers) with constituency and dependency parsers (Allen Institute for Artificial Intelligence.)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.84/>Ferryman as SemEval-2020 Task 5 : Optimized BERT for Detecting Counterfactuals<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: Optimized <span class=acl-fixed-case>BERT</span> for Detecting Counterfactuals</a></strong><br><a href=/people/w/weilong-chen/>Weilong Chen</a>
|
<a href=/people/y/yan-zhuang/>Yan Zhuang</a>
|
<a href=/people/p/peng-wang/>Peng Wang</a>
|
<a href=/people/f/feng-hong/>Feng Hong</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/y/yanru-zhang/>Yanru Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--84><div class="card-body p-3 small">The main purpose of this article is to state the effect of using different <a href=https://en.wikipedia.org/wiki/Scientific_method>methods</a> and <a href=https://en.wikipedia.org/wiki/Scientific_modelling>models</a> for <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactual determination</a> and detection of causal knowledge. Nowadays, counterfactual reasoning has been widely used in various fields. In the realm of natural language process(NLP), counterfactual reasoning has huge potential to improve the correctness of a sentence. In the shared Task 5 of detecting counterfactual in SemEval 2020, we pre-process the officially given dataset according to case conversion, extract stem and abbreviation replacement. We use last-5 bidirectional encoder representation from bidirectional encoder representation from transformer (BERT)and term frequencyinverse document frequency (TF-IDF) vectorizer for counterfactual detection. Meanwhile, multi-sample dropout and <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>cross validation</a> are used to improve versatility and prevent problems such as poor generosity caused by <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. Finally, our team Ferryman ranked the 8th place in the sub-task 1 of this competition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.86/>Lee at SemEval-2020 Task 5 : ALBERT Model Based on the Maximum Ensemble Strategy and Different Data Sampling Methods for Detecting Counterfactual Statements<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: <span class=acl-fixed-case>ALBERT</span> Model Based on the Maximum Ensemble Strategy and Different Data Sampling Methods for Detecting Counterfactual Statements</a></strong><br><a href=/people/j/junyi-li/>Junyi Li</a>
|
<a href=/people/y/yuhang-wu/>Yuhang Wu</a>
|
<a href=/people/b/bin-wang/>Bin Wang</a>
|
<a href=/people/h/haiyan-ding/>Haiyan Ding</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--86><div class="card-body p-3 small">This article describes the system submitted to SemEval 2020 Task 5 : Modelling Causal Reasoning in Language : Detecting Counterfactuals. In this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we only participate in the subtask A which is detecting counterfactual statements. In order to solve this sub-task, first of all, because of the problem of data balance, we use the undersampling and oversampling methods to process the data set. Second, we used the ALBERT model and the maximum ensemble method based on the ALBERT model. Our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> achieved a F1 score of 0.85 in subtask A.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.87/>NLU-Co at SemEval-2020 Task 5 : NLU / SVM Based Model Apply Tocharacterise and Extract Counterfactual Items on Raw Data<span class=acl-fixed-case>NLU</span>-Co at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: <span class=acl-fixed-case>NLU</span>/<span class=acl-fixed-case>SVM</span> Based Model Apply Tocharacterise and Extract Counterfactual Items on Raw Data</a></strong><br><a href=/people/e/elvis-mboning-tchiaze/>Elvis Mboning Tchiaze</a>
|
<a href=/people/d/damien-nouvel/>Damien Nouvel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--87><div class="card-body p-3 small">In this article, we try to solve the problem of classification of counterfactual statements and extraction of antecedents / consequences in raw data, by mobilizing on one hand Support vector machine (SVMs) and on the other hand Natural Language Understanding (NLU) infrastructures available on the market for conversational agents. Our experiments allowed us to test different <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipelines</a> of two known <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> (Snips NLU and Rasa NLU). The results obtained show that a Rasa NLU pipeline, built with a well-preprocessed dataset and tuned algorithms, allows to model accurately the structure of a <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactual event</a>, in order to facilitate the identification and the extraction of its components.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.89/>YNU-oxz at SemEval-2020 Task 5 : Detecting Counterfactuals Based on Ordered Neurons LSTM and Hierarchical Attention Network<span class=acl-fixed-case>YNU</span>-oxz at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 5: Detecting Counterfactuals Based on Ordered Neurons <span class=acl-fixed-case>LSTM</span> and Hierarchical Attention Network</a></strong><br><a href=/people/x/xiaozhi-ou/>Xiaozhi Ou</a>
|
<a href=/people/s/shengyan-liu/>Shengyan Liu</a>
|
<a href=/people/h/hongling-li/>Hongling Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--89><div class="card-body p-3 small">This paper describes the system and results of our team&#8217;s participation in SemEval-2020 Task5 : Modelling Causal Reasoning in Language : Detecting Counterfactuals, which aims to simulate counterfactual semantics and reasoning in natural language. This <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> contains two subtasks : Subtask1Detecting counterfactual statements and Subtask2Detecting antecedent and consequence. We only participated in Subtask1, aiming to determine whether a given sentence is counterfactual. In order to solve this task, we proposed a system based on Ordered Neurons LSTM (ON-LSTM) with Hierarchical Attention Network (HAN) and used Pooling operation for dimensionality reduction. Finally, we used the K-fold approach as the ensemble method. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 0.7040 in Subtask1 (Ranked 16/27).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.90/>BERTatDE at SemEval-2020 Task 6 : Extracting Term-definition Pairs in Free Text Using Pre-trained Model<span class=acl-fixed-case>BERT</span>at<span class=acl-fixed-case>DE</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 6: Extracting Term-definition Pairs in Free Text Using Pre-trained Model</a></strong><br><a href=/people/h/huihui-zhang/>Huihui Zhang</a>
|
<a href=/people/f/feiliang-ren/>Feiliang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--90><div class="card-body p-3 small">Definition extraction is an important task in Nature Language Processing, and it is used to identify the terms and definitions related to terms. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> contains sentence classification task (i.e., classify whether it contains definition) and sequence labeling task (i.e., find the boundary of terms and definitions). The paper describes our system BERTatDE1 in sentence classification task (subtask 1) and sequence labeling task (subtask 2) in the definition extraction (SemEval-2020 Task 6). We use BERT to solve the multi-domain problems including the uncertainty of term boundary that is, different areas have different ways to definite the domain related terms. We use <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, BiLSTM and <a href=https://en.wikipedia.org/wiki/Attention>attention</a> in subtask 1 and our best result achieved 79.71 % in F1 and the eighteenth place in subtask 1. For the subtask 2, we use BERT, BiLSTM and CRF to <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, and achieve 40.73 % in Macro-averaged F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.92" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.92/>Defx at SemEval-2020 Task 6 : Joint Extraction of Concepts and Relations for Definition Extraction<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 6: Joint Extraction of Concepts and Relations for Definition Extraction</a></strong><br><a href=/people/m/marc-hubner/>Marc Hübner</a>
|
<a href=/people/c/christoph-alt/>Christoph Alt</a>
|
<a href=/people/r/robert-schwarzenberg/>Robert Schwarzenberg</a>
|
<a href=/people/l/leonhard-hennig/>Leonhard Hennig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--92><div class="card-body p-3 small">Definition Extraction systems are a valuable knowledge source for both humans and algorithms. In this paper we describe our submissions to the DeftEval shared task (SemEval-2020 Task 6), which is evaluated on an English textbook corpus. We provide a detailed explanation of our <a href=https://en.wikipedia.org/wiki/System>system</a> for the joint extraction of definition concepts and the relations among them. Furthermore we provide an ablation study of our model variations and describe the results of an error analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.97.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--97 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.97 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.97" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.97/>UPB at SemEval-2020 Task 6 : Pretrained Language Models for Definition Extraction<span class=acl-fixed-case>UPB</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 6: Pretrained Language Models for Definition Extraction</a></strong><br><a href=/people/a/andrei-marius-avram/>Andrei-Marius Avram</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a>
|
<a href=/people/c/costin-chiru/>Costin Chiru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--97><div class="card-body p-3 small">This work presents our contribution in the context of the 6th task of SemEval-2020 : Extracting Definitions from Free Text in Textbooks (DeftEval). This competition consists of three subtasks with different levels of granularity : (1) classification of sentences as definitional or non-definitional, (2) labeling of definitional sentences, and (3) relation classification. We use various pretrained language models (i.e., BERT, XLNet, RoBERTa, SciBERT, and ALBERT) to solve each of the three subtasks of the competition. Specifically, for each language model variant, we experiment by both freezing its weights and fine-tuning them. We also explore a multi-task architecture that was trained to jointly predict the outputs for the second and the third subtasks. Our best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> evaluated on the DeftEval dataset obtains the 32nd place for the first subtask and the 37th place for the second subtask. The code is available for further research at :<url>https://github.com/avramandrei/DeftEval</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.104/>Buhscitu at SemEval-2020 Task 7 : Assessing Humour in Edited News Headlines Using Hand-Crafted Features and Online Knowledge Bases<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Assessing Humour in Edited News Headlines Using Hand-Crafted Features and Online Knowledge Bases</a></strong><br><a href=/people/k/kristian-norgaard-jensen/>Kristian Nørgaard Jensen</a>
|
<a href=/people/n/nicolaj-filrup-rasmussen/>Nicolaj Filrup Rasmussen</a>
|
<a href=/people/t/thai-wang/>Thai Wang</a>
|
<a href=/people/m/marco-placenti/>Marco Placenti</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--104><div class="card-body p-3 small">This paper describes a system that aims at assessing humour intensity in edited news headlines as part of the 7th task of SemEval-2020 on Humor, <a href=https://en.wikipedia.org/wiki/Emphasis_(typography)>Emphasis</a> and <a href=https://en.wikipedia.org/wiki/Sentimentality>Sentiment</a>. Various factors need to be accounted for in order to assess the <a href=https://en.wikipedia.org/wiki/Funniness>funniness</a> of an edited headline. We propose an architecture that uses hand-crafted features, <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> and a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to understand <a href=https://en.wikipedia.org/wiki/Humour>humour</a>, and combines them in a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms two <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. In general, automatic humour assessment remains a difficult task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--105 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.105/>Hasyarasa at SemEval-2020 Task 7 : Quantifying Humor as Departure from Expectedness<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Quantifying Humor as Departure from Expectedness</a></strong><br><a href=/people/r/ravi-theja-desetty/>Ravi Theja Desetty</a>
|
<a href=/people/r/ranit-chatterjee/>Ranit Chatterjee</a>
|
<a href=/people/s/smita-ghaisas/>Smita Ghaisas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--105><div class="card-body p-3 small">This paper describes our system submission Hasyarasa for the SemEval-2020 Task-7 : Assessing Humor in Edited News Headlines. This <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> has two subtasks. The goal of Subtask 1 is to predict the mean funniness of the edited headline given the original and the edited headline. In Subtask 2, given two edits on the original headline, the goal is to predict the funnier of the two. We observed that the departure from expected state/ actions of situations/ individuals is the cause of <a href=https://en.wikipedia.org/wiki/Humour>humor</a> in the edited headlines. We propose two novel features : Contextual Semantic Distance and Contextual Neighborhood Distance to estimate this departure and thus capture the contextual absurdity and hence the <a href=https://en.wikipedia.org/wiki/Humour>humor</a> in the edited headlines. We have used these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> together with a Bi-LSTM Attention based model and have achieved 0.53310 RMSE for Subtask 1 and 60.19 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for Subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.110/>YNU-HPCC at SemEval-2020 Task 7 : Using an Ensemble BiGRU Model to Evaluate the Humor of Edited News Titles<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Using an Ensemble <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>GRU</span> Model to Evaluate the Humor of Edited News Titles</a></strong><br><a href=/people/j/joseph-tomasulo/>Joseph Tomasulo</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--110><div class="card-body p-3 small">This paper describes an ensemble model designed for Semeval-2020 Task 7. The task is based on the Humicroedit dataset that is comprised of news titles and one-word substitutions designed to make them humorous. We use <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, <a href=https://en.wikipedia.org/wiki/FastText>FastText</a>, Elmo, and Word2Vec to encode these titles then pass them to a bidirectional gated recurrent unit (BiGRU) with attention. Finally, we used <a href=https://en.wikipedia.org/wiki/XGBoost>XGBoost</a> on the concatenation of the results of the different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to make predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--113 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.113/>NLP_UIOWA at SemEval-2020 Task 8 : You’re Not the Only One Cursed with Knowledge-Multi Branch Model Memotion Analysis<span class=acl-fixed-case>NLP</span>_<span class=acl-fixed-case>UIOWA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: You’re Not the Only One Cursed with Knowledge - Multi Branch Model Memotion Analysis</a></strong><br><a href=/people/i/ingroj-shrestha/>Ingroj Shrestha</a>
|
<a href=/people/j/jonathan-rusert/>Jonathan Rusert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--113><div class="card-body p-3 small">We propose hybrid models (HybridE and HybridW) for meme analysis (SemEval 2020 Task 8), which involves sentiment classification (Subtask A), humor classification (Subtask B), and scale of semantic classes (Subtask C). The hybrid model consists of BLSTM and CNN for text and image processing respectively. HybridE provides equal weight to BLSTM and CNN performance, while HybridW provides weightage based on the performance of BLSTM and CNN on a validation set. The performances (macro F1) of our hybrid model on Subtask A are 0.329 (HybridE), 0.328 (HybridW), on Subtask B are 0.507 (HybridE), 0.512 (HybridW), and on Subtask C are 0.309 (HybridE), 0.311 (HybridW).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--117 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.117" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.117/>CS-Embed at SemEval-2020 Task 9 : The Effectiveness of Code-switched Word Embeddings for Sentiment Analysis<span class=acl-fixed-case>CS</span>-Embed at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: The Effectiveness of Code-switched Word Embeddings for Sentiment Analysis</a></strong><br><a href=/people/f/frances-adriana-laureano-de-leon/>Frances Adriana Laureano De Leon</a>
|
<a href=/people/f/florimond-gueniat/>Florimond Guéniat</a>
|
<a href=/people/h/harish-tayyar-madabushi/>Harish Tayyar Madabushi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--117><div class="card-body p-3 small">The growing popularity and applications of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of social media posts has naturally led to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis of posts</a> written in multiple languages, a practice known as <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>. While recent research into code-switched posts has focused on the use of multilingual word embeddings, these embeddings were not trained on code-switched data. In this work, we present <a href=https://en.wikipedia.org/wiki/Word_embedding>word-embeddings</a> trained on <a href=https://en.wikipedia.org/wiki/Code-switching>code-switched tweets</a>, specifically those that make use of <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and English, known as <a href=https://en.wikipedia.org/wiki/Spanglish>Spanglish</a>. We explore the embedding space to discover how they capture the meanings of words in both languages. We test the effectiveness of these embeddings by participating in SemEval 2020 Task 9 : <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> on Code-Mixed Social Media Text. We utilised them to train a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment classifier</a> that achieves an F-1 score of 0.722. This is higher than the <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>baseline</a> for the <a href=https://en.wikipedia.org/wiki/Surveying>competition</a> of 0.656, with our team (codalab username francesita) ranking 14 out of 29 participating teams, beating the <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>baseline</a>.<i>Sentiment Analysis on Code-Mixed Social Media Text</i>. We utilised them to train a sentiment classifier that achieves an F-1 score of 0.722. This is higher than the baseline for the competition of 0.656, with our team (codalab username francesita) ranking 14 out of 29 participating teams, beating the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.118/>FII-UAIC at SemEval-2020 Task 9 : Sentiment Analysis for Code-Mixed Social Media Text Using CNN<span class=acl-fixed-case>FII</span>-<span class=acl-fixed-case>UAIC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text Using <span class=acl-fixed-case>CNN</span></a></strong><br><a href=/people/l/lavinia-aparaschivei/>Lavinia Aparaschivei</a>
|
<a href=/people/a/andrei-palihovici/>Andrei Palihovici</a>
|
<a href=/people/d/daniela-gifu/>Daniela Gîfu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--118><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> for Code-Mixed Social Media Text task at the SemEval 2020 competition focuses on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in code-mixed social media text, specifically, on the combination of English with Spanish (Spanglish) and Hindi (Hinglish). In this paper, we present a system able to classify <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, from Spanish and English languages, into positive, negative and neutral. Firstly, we built a classifier able to provide corresponding <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment labels</a>. Besides the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment labels</a>, we provide the <a href=https://en.wikipedia.org/wiki/Linguistic_description>language labels</a> at the <a href=https://en.wikipedia.org/wiki/Linguistic_description>word level</a>. Secondly, we generate a word-level representation, using Convolutional Neural Network (CNN) architecture. Our solution indicates promising results for the Sentimix Spanglish-English task (0.744), the team, Lavinia_Ap, occupied the 9th place. However, for the Sentimix Hindi-English task (0.324) the results have to be improved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--123 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.123/>NLP-CIC at SemEval-2020 Task 9 : Analysing Sentiment in Code-switching Language Using a Simple Deep-learning Classifier<span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>CIC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Analysing Sentiment in Code-switching Language Using a Simple Deep-learning Classifier</a></strong><br><a href=/people/j/jason-angel/>Jason Angel</a>
|
<a href=/people/s/segun-taofeek-aroyehun/>Segun Taofeek Aroyehun</a>
|
<a href=/people/a/antonio-tamayo/>Antonio Tamayo</a>
|
<a href=/people/a/alexander-gelbukh/>Alexander Gelbukh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--123><div class="card-body p-3 small">Code-switching is a phenomenon in which two or more languages are used in the same message. Nowadays, it is quite common to find messages with languages mixed in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This phenomenon presents a challenge for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. In this paper, we use a standard <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network model</a> to predict the sentiment of tweets in a blend of Spanish and English languages. Our simple approach achieved a F1-score of 0:71 on test set on the competition. We analyze our best model capabilities and perform <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> to expose important difficulties for classifying sentiment in a code-switching setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.124/>Palomino-Ochoa at SemEval-2020 Task 9 : Robust System Based on Transformer for Code-Mixed Sentiment Classification<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Robust System Based on Transformer for Code-Mixed Sentiment Classification</a></strong><br><a href=/people/d/daniel-palomino/>Daniel Palomino</a>
|
<a href=/people/j/jose-ochoa-luna/>José Ochoa-Luna</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--124><div class="card-body p-3 small">We present a transfer learning system to perform a mixed Spanish-English sentiment classification task. Our proposal uses the state-of-the-art language model BERT and embed it within a ULMFiT transfer learning pipeline. This combination allows us to predict the polarity detection of code-mixed (English-Spanish) tweets. Thus, among 29 submitted systems, our approach (referred to as dplominop) is ranked 4th on the Sentimix Spanglish test set of SemEval 2020 Task 9. In fact, our <a href=https://en.wikipedia.org/wiki/System>system</a> yields the weighted-F1 score value of 0.755 which can be easily reproduced the source code and implementation details are made available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.125/>ULD@NUIG at SemEval-2020 Task 9 : Generative Morphemes with an Attention Model for Sentiment Analysis in Code-Mixed Text<span class=acl-fixed-case>ULD</span>@<span class=acl-fixed-case>NUIG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Generative Morphemes with an Attention Model for Sentiment Analysis in Code-Mixed Text</a></strong><br><a href=/people/k/koustava-goswami/>Koustava Goswami</a>
|
<a href=/people/p/priya-rani/>Priya Rani</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/t/theodorus-fransen/>Theodorus Fransen</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--125><div class="card-body p-3 small">Code mixing is a common phenomena in <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual societies</a> where people switch from one language to another for various reasons. Recent advances in public communication over different <a href=https://en.wikipedia.org/wiki/Social_media>social media sites</a> have led to an increase in the frequency of code-mixed usage in <a href=https://en.wikipedia.org/wiki/Written_language>written language</a>. In this paper, we present the Generative Morphemes with Attention (GenMA) Model sentiment analysis system contributed to SemEval 2020 Task 9 SentiMix. The system aims to predict the sentiments of the given English-Hindi code-mixed tweets without using word-level language tags instead inferring this automatically using a <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological model</a>. The system is based on a novel deep neural network (DNN) architecture, which has outperformed the baseline F1-score on the test data-set as well as the validation data-set. Our results can be found under the user name koustava on the Sentimix Hindi English page.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.129/>ECNU at SemEval-2020 Task 7 : Assessing Humor in Edited News Headlines Using BiLSTM with Attention<span class=acl-fixed-case>ECNU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Assessing Humor in Edited News Headlines Using <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> with Attention</a></strong><br><a href=/people/t/tiantian-zhang/>Tiantian Zhang</a>
|
<a href=/people/z/zhixuan-chen/>Zhixuan Chen</a>
|
<a href=/people/m/man-lan/>Man Lan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--129><div class="card-body p-3 small">In this paper we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to SemEval 2020 Task 7 : Assessing Humor in Edited News Headlines. We participated in all subtasks, in which the main goal is to predict the mean funniness of the edited headline given the original and the edited headline. Our system involves two similar sub-networks, which generate <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representations</a> for the original and edited headlines respectively. And then we do a subtract operation of the outputs from two sub-networks to predict the funniness of the edited headline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.130/>ELMo-NB at SemEval-2020 Task 7 : Assessing Sense of Humor in EditedNews Headlines Using ELMo and NB<span class=acl-fixed-case>ELM</span>o-<span class=acl-fixed-case>NB</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Assessing Sense of Humor in <span class=acl-fixed-case>E</span>dited<span class=acl-fixed-case>N</span>ews Headlines Using <span class=acl-fixed-case>ELM</span>o and <span class=acl-fixed-case>NB</span></a></strong><br><a href=/people/e/enas-khwaileh/>Enas Khwaileh</a>
|
<a href=/people/m/muntaha-a-al-asad/>Muntaha A. Al-As’ad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--130><div class="card-body p-3 small">Our approach is constructed to improve on a couple of aspects ; preprocessing with an emphasis on humor sense detection, using embeddings from state-of-the-art language model(Elmo), and ensembling the results came up with using machine learning model Na ve Bayes(NB) with a deep learning pre-trained models. Elmo-NB participation has scored (0.5642) on the competition leader board, where results were measured by Root Mean Squared Error (RMSE).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--131 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.131/>Ferryman at SemEval-2020 Task 7 : Ensemble Model for Assessing Humor in Edited News Headlines<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Ensemble Model for Assessing Humor in Edited News Headlines</a></strong><br><a href=/people/w/weilong-chen/>Weilong Chen</a>
|
<a href=/people/j/jipeng-li/>Jipeng Li</a>
|
<a href=/people/c/chenghao-huang/>Chenghao Huang</a>
|
<a href=/people/w/wei-bai/>Wei Bai</a>
|
<a href=/people/y/yanru-zhang/>Yanru Zhang</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--131><div class="card-body p-3 small">Natural language processing (NLP) has been applied to various <a href=https://en.wikipedia.org/wiki/Field_(computer_science)>fields</a> including <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. In the shared task of assessing the funniness of edited news headlines, which is a part of the SemEval 2020 competition, we preprocess datasets by replacing abbreviation, stemming words, then merge three models including Light Gradient Boosting Machine (LightGBM), Long Short-Term Memory (LSTM), and Bidirectional Encoder Representation from Transformer (BERT) by taking the average to perform the best. Our team Ferryman wins the 9th place in Sub-task 1 of Task 7-Regression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.132/>Funny3 at SemEval-2020 Task 7 : Humor Detection of Edited Headlines with LSTM and TFIDF Neural Network System<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Humor Detection of Edited Headlines with <span class=acl-fixed-case>LSTM</span> and <span class=acl-fixed-case>TFIDF</span> Neural Network System</a></strong><br><a href=/people/x/xuefeng-luo/>Xuefeng Luo</a>
|
<a href=/people/k/kuan-tang/>Kuan Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--132><div class="card-body p-3 small">This paper presents a neural network system where we participate in the first task of SemEval-2020 shared task 7 Assessing the Funniness of Edited News Headlines. Our target is to create to <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> that can predict the funniness of edited headlines. We build our model using a combination of LSTM and TF-IDF, then a feed-forward neural network. The <a href=https://en.wikipedia.org/wiki/System>system</a> manages to slightly improve RSME scores regarding our mean score baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.133/>HumorAAC at SemEval-2020 Task 7 : Assessing the Funniness of Edited News Headlines through Regression and Trump Mentions<span class=acl-fixed-case>H</span>umor<span class=acl-fixed-case>AAC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Assessing the Funniness of Edited News Headlines through Regression and Trump Mentions</a></strong><br><a href=/people/a/anna-katharina-dick/>Anna-Katharina Dick</a>
|
<a href=/people/c/charlotte-weirich/>Charlotte Weirich</a>
|
<a href=/people/a/alla-kutkina/>Alla Kutkina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--133><div class="card-body p-3 small">In this paper we describe our contribution to the Semeval-2020 Humor Assessment task. We essentially use three different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that are passed into a <a href=https://en.wikipedia.org/wiki/Ridge_regression>ridge regression</a> to determine a funniness score for an edited news headline : statistical, count-based features, semantic features and contextual information. For deciding which one of two given edited headlines is funnier, we additionally use <a href=https://en.wikipedia.org/wiki/Score_(statistics)>scoring information</a> and <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>. Our work was mostly concentrated on investigating <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, rather than improving <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> based on pre-trained language models. The resulting <a href=https://en.wikipedia.org/wiki/System>system</a> is task-specific, lightweight and performs above the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>majority baseline</a>. Our experiments indicate that <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> related to socio-cultural context, in our case mentions of Donald Trump, generally perform better than context-independent features like headline length.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.136/>MLEngineer at SemEval-2020 Task 7 : BERT-Flair Based Humor Detection Model (BFHumor)<span class=acl-fixed-case>MLE</span>ngineer at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: <span class=acl-fixed-case>BERT</span>-Flair Based Humor Detection Model (<span class=acl-fixed-case>BFH</span>umor)</a></strong><br><a href=/people/f/fara-shatnawi/>Fara Shatnawi</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a>
|
<a href=/people/m/mahmoud-hammad/>Mahmoud Hammad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--136><div class="card-body p-3 small">Task 7, Assessing the Funniness of Edited News Headlines, in the International Workshop SemEval2020 introduces two sub-tasks to predict the funniness values of edited news headlines from the Reddit website. This paper proposes the BFHumor model of the MLEngineer team that participates in both sub-tasks in this competition. The BFHumor&#8217;s model is defined as a BERT-Flair based humor detection model that is a combination of different pre-trained models with various Natural Language Processing (NLP) techniques. The Bidirectional Encoder Representations from Transformers (BERT) regressor is considered the primary pre-trained model in our approach, whereas Flair is the main NLP library. It is worth mentioning that the BFHumor model has been ranked 4th in sub-task1 with a root mean square error (RMSE) value of 0.51966, and it is 0.02 away from the first ranked model. Also, the team is ranked 12th in the sub-task2 with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.62291, which is 0.05 away from the top-ranked model. Our results indicate that the BFHumor model is one of the top models for detecting humor in the text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.140/>UTFPR at SemEval-2020 Task 7 : Using Co-occurrence Frequencies to Capture Unexpectedness<span class=acl-fixed-case>UTFPR</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Using Co-occurrence Frequencies to Capture Unexpectedness</a></strong><br><a href=/people/g/gustavo-paetzold/>Gustavo Henrique Paetzold</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--140><div class="card-body p-3 small">We describe the UTFPR system for SemEval-2020&#8217;s Task 7 : Assessing Humor in Edited News Headlines. Ours is a minimalist unsupervised system that uses word co-occurrence frequencies from large corpora to capture unexpectedness as a mean to capture funniness. Our <a href=https://en.wikipedia.org/wiki/System>system</a> placed 22nd on the shared task&#8217;s Task 2. We found that our approach requires more text than we used to perform reliably, and that unexpectedness alone is not sufficient to gauge funniness for humorous content that targets a diverse target audience.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.141/>WUY at SemEval-2020 Task 7 : Combining BERT and Naive Bayes-SVM for Humor Assessment in Edited News Headlines<span class=acl-fixed-case>WUY</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 7: Combining <span class=acl-fixed-case>BERT</span> and Naive <span class=acl-fixed-case>B</span>ayes-<span class=acl-fixed-case>SVM</span> for Humor Assessment in Edited News Headlines</a></strong><br><a href=/people/c/cheng-zhang/>Cheng Zhang</a>
|
<a href=/people/h/hayato-yamana/>Hayato Yamana</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--141><div class="card-body p-3 small">This paper describes our participation in SemEval 2020 Task 7 on assessment of humor in edited news headlines, which includes two subtasks, estimating the humor of micro-editd news headlines (subtask A) and predicting the more humorous of the two edited headlines (subtask B). To address these tasks, we propose two <a href=https://en.wikipedia.org/wiki/System>systems</a>. The first system adopts a regression-based fine-tuned single-sequence bidirectional encoder representations from transformers (BERT) model with easy data augmentation (EDA), called BERT+EDA. The second system adopts a hybrid of a regression-based fine-tuned sequence-pair BERT model and a combined <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Naive Bayes</a> and support vector machine (SVM) model estimated on term frequencyinverse document frequency (TFIDF) features, called BERT+NB-SVM. In this case, no additional training datasets were used, and the BERT+NB-SVM model outperformed BERT+EDA. The official root-mean-square deviation (RMSE) score for subtask A is 0.57369 and ranks 31st out of 48, whereas the best RMSE of BERT+NB-SVM is 0.52429, ranking 7th. For subtask B, we simply use a sequence-pair BERT model, the official accuracy of which is 0.53196 and ranks 25th out of 32.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--144 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.144/>BERT at SemEval-2020 Task 8 : Using BERT to Analyse Meme Emotions<span class=acl-fixed-case>BERT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: Using <span class=acl-fixed-case>BERT</span> to Analyse Meme Emotions</a></strong><br><a href=/people/a/adithya-avvaru/>Adithya Avvaru</a>
|
<a href=/people/s/sanath-vobilisetty/>Sanath Vobilisetty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--144><div class="card-body p-3 small">Sentiment analysis, being one of the most sought after research problems within Natural Language Processing (NLP) researchers. The range of problems being addressed by <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> is increasing. Till now, most of the research focuses on predicting sentiment, or sentiment categories like <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, <a href=https://en.wikipedia.org/wiki/Humour>humor</a>, offense and motivation on text data. But, there is very limited research that is focusing on predicting or analyzing the sentiment of <a href=https://en.wikipedia.org/wiki/Internet_meme>internet memes</a>. We try to address this problem as part of Task 8 of SemEval 2020 : Memotion Analysis. We have participated in all the three tasks under Memotion Analysis. Our system built using state-of-the-art Transformer-based pre-trained Bidirectional Encoder Representations from Transformers (BERT) performed better compared to baseline models for the two tasks A and C and performed close to the baseline model for task B. In this paper, we present the data used, steps used by us for data cleaning and preparation, the fine-tuning process for BERT based model and finally predict the sentiment or sentiment categories. We found that the sequence models like Long Short Term Memory(LSTM) and its variants performed below par in predicting the sentiments. We also performed a comparative analysis with other Transformer based models like DistilBERT and XLNet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--146 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.146/>CSECU_KDE_MA at SemEval-2020 Task 8 : A Neural Attention Model for Memotion Analysis<span class=acl-fixed-case>CSECU</span>_<span class=acl-fixed-case>KDE</span>_<span class=acl-fixed-case>MA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: A Neural Attention Model for Memotion Analysis</a></strong><br><a href=/people/a/abu-nowshed-chy/>Abu Nowshed Chy</a>
|
<a href=/people/u/umme-aymun-siddiqua/>Umme Aymun Siddiqua</a>
|
<a href=/people/m/masaki-aono/>Masaki Aono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--146><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Meme>meme</a> is a pictorial representation of an idea or theme. In the age of emerging volume of <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>, <a href=https://en.wikipedia.org/wiki/Meme>memes</a> are spreading rapidly from person to person and becoming a trending ways of opinion expression. However, due to the multimodal characteristics of meme contents, detecting and analyzing the underlying emotion of a <a href=https://en.wikipedia.org/wiki/Meme>meme</a> is a formidable task. In this paper, we present our approach for detecting the emotion of a <a href=https://en.wikipedia.org/wiki/Meme>meme</a> defined in the SemEval-2020 Task 8. Our team CSECU_KDE_MA employs an attention-based neural network model to tackle the problem. Upon extracting the text contents from a <a href=https://en.wikipedia.org/wiki/Meme>meme</a> using an optical character reader (OCR), we represent it using the distributed representation of words. Next, we perform the <a href=https://en.wikipedia.org/wiki/Convolution>convolution</a> based on multiple kernel sizes to obtain the higher-level feature sequences. The <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature sequences</a> are then fed into the attentive time-distributed bidirectional LSTM model to learn the long-term dependencies effectively. Experimental results show that our proposed neural model obtained competitive performance among the participants&#8217; systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--149 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.149/>Hitachi at SemEval-2020 Task 8 : Simple but Effective Modality Ensemble for Meme Emotion Recognition<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: Simple but Effective Modality Ensemble for Meme Emotion Recognition</a></strong><br><a href=/people/t/terufumi-morishita/>Terufumi Morishita</a>
|
<a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/s/shota-horiguchi/>Shota Horiguchi</a>
|
<a href=/people/h/hiroaki-ozaki/>Hiroaki Ozaki</a>
|
<a href=/people/t/toshinori-miyoshi/>Toshinori Miyoshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--149><div class="card-body p-3 small">Users of <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking services</a> often share their emotions via multi-modal content, usually <a href=https://en.wikipedia.org/wiki/Image>images</a> paired with text embedded in them. SemEval-2020 task 8, Memotion Analysis, aims at automatically recognizing these <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> of so-called <a href=https://en.wikipedia.org/wiki/Internet_meme>internet memes</a>. In this paper, we propose a simple but effective Modality Ensemble that incorporates visual and textual deep-learning models, which are independently trained, rather than providing a single multi-modal joint network. To this end, we first fine-tune four pre-trained <a href=https://en.wikipedia.org/wiki/Visual_system>visual models</a> (i.e., Inception-ResNet, PolyNet, SENet, and PNASNet) and four textual models (i.e., <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, GPT-2, Transformer-XL, and XLNet). Then, we fuse their predictions with <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble methods</a> to effectively capture cross-modal correlations. The experiments performed on dev-set show that both visual and textual features aided each other, especially in subtask-C, and consequently, our system ranked 2nd on subtask-C.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--154 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.154/>Memebusters at SemEval-2020 Task 8 : Feature Fusion Model for Sentiment Analysis on Memes Using Transfer Learning<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: Feature Fusion Model for Sentiment Analysis on Memes Using Transfer Learning</a></strong><br><a href=/people/m/mayukh-sharma/>Mayukh Sharma</a>
|
<a href=/people/i/ilanthenral-kandasamy/>Ilanthenral Kandasamy</a>
|
<a href=/people/w/w-b-vasantha/>W.b. Vasantha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--154><div class="card-body p-3 small">In this paper, we describe our deep learning system used for SemEval 2020 Task 8 : Memotion analysis. We participated in all the subtasks i.e Subtask A : Sentiment classification, Subtask B : Humor classification, and Subtask C : Scales of semantic classes. Similar multimodal architecture was used for each subtask. The proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> makes use of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for images and text feature extraction. The extracted <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are then fused together using stacked bidirectional Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) model with attention mechanism for final predictions. We also propose a single model for predicting semantic classes (Subtask B) as well as their scales (Subtask C) by branching the final output of the post LSTM dense layers. Our model was ranked 5 in Subtask B and ranked 8 in Subtask C and performed nicely in Subtask A on the leader board. Our system makes use of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> and fusion of image and text features for predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.157/>SIS@IIITH at SemEval-2020 Task 8 : An Overview of Simple Text Classification Methods for Meme Analysis<span class=acl-fixed-case>SIS</span>@<span class=acl-fixed-case>IIITH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: An Overview of Simple Text Classification Methods for Meme Analysis</a></strong><br><a href=/people/s/sravani-boinepelli/>Sravani Boinepelli</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--157><div class="card-body p-3 small">Memes are steadily taking over the feeds of the public on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. There is always the threat of malicious users on the internet posting offensive content, even through <a href=https://en.wikipedia.org/wiki/Meme>memes</a>. Hence, the automatic detection of offensive images / memes is imperative along with detection of offensive text. However, this is a much more complex task as it involves both <a href=https://en.wikipedia.org/wiki/Sensory_cue>visual cues</a> as well as <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>cultural / context knowledge</a>. This paper describes our approach to the task of SemEval-2020 Task 8 : Memotion Analysis. We chose to participate only in Task A which dealt with Sentiment Classification, which we formulated as a text classification problem. Through our experiments, we explored multiple training models to evaluate the performance of simple text classification algorithms on the raw text obtained after running <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR</a> on meme images. Our submitted <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 72.69 % and exceeded the existing baseline&#8217;s Macro F1 score by 8 % on the official test dataset. Apart from describing our official submission, we shall elucidate how different <a href=https://en.wikipedia.org/wiki/Statistical_model>classification models</a> respond to this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--159 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.159/>UoR at SemEval-2020 Task 8 : Gaussian Mixture Modelling (GMM) Based Sampling Approach for Multi-modal Memotion Analysis<span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>R</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 8: <span class=acl-fixed-case>G</span>aussian Mixture Modelling (<span class=acl-fixed-case>GMM</span>) Based Sampling Approach for Multi-modal Memotion Analysis</a></strong><br><a href=/people/z/zehao-liu/>Zehao Liu</a>
|
<a href=/people/e/emmanuel-osei-brefo/>Emmanuel Osei-Brefo</a>
|
<a href=/people/s/siyuan-chen/>Siyuan Chen</a>
|
<a href=/people/h/huizhi-liang/>Huizhi Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--159><div class="card-body p-3 small">Memes are widely used on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. They usually contain multi-modal information such as <a href=https://en.wikipedia.org/wiki/Image>images</a> and <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a>, serving as valuable data sources to analyse opinions and sentiment orientations of online communities. The provided memes data often face an imbalanced data problem, that is, some classes or labelled sentiment categories significantly outnumber other classes. This often results in difficulty in applying machine learning techniques where balanced labelled input data are required. In this paper, a Gaussian Mixture Model sampling method is proposed to tackle the problem of <a href=https://en.wikipedia.org/wiki/Social_class>class imbalance</a> for the memes sentiment classification task. To utilise both text and image data, a multi-modal CNN-LSTM model is proposed to jointly learn latent features for positive, negative and neutral category predictions. The experiments show that the re-sampling model can slightly improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the trial data of sub-task A of Task 8. The multi-modal CNN-LSTM model can achieve macro F1 score 0.329 on the test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.162/>BAKSA at SemEval-2020 Task 9 : Bolstering CNN with Self-Attention for Sentiment Analysis of Code Mixed Text<span class=acl-fixed-case>BAKSA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Bolstering <span class=acl-fixed-case>CNN</span> with Self-Attention for Sentiment Analysis of Code Mixed Text</a></strong><br><a href=/people/a/ayush-kumar/>Ayush Kumar</a>
|
<a href=/people/h/harsh-agarwal/>Harsh Agarwal</a>
|
<a href=/people/k/keshav-bansal/>Keshav Bansal</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--162><div class="card-body p-3 small">Sentiment Analysis of code-mixed text has diversified applications in <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a> ranging from tagging user reviews to identifying social or political sentiments of a sub-population. In this paper, we present an ensemble architecture of convolutional neural net (CNN) and self-attention based LSTM for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of code-mixed tweets. While the CNN component helps in the classification of positive and negative tweets, the self-attention based LSTM, helps in the classification of neutral tweets, because of its ability to identify correct <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> among multiple <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment bearing units</a>. We achieved F1 scores of 0.707 (ranked 5th) and 0.725 (ranked 13th) on Hindi-English (Hinglish) and Spanish-English (Spanglish) datasets, respectively. The submissions for Hinglish and Spanglish tasks were made under the usernames ayushk and harsh_6 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.164/>Deep Learning Brasil-NLP at SemEval-2020 Task 9 : Sentiment Analysis of Code-Mixed Tweets Using Ensemble of Language Models<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Sentiment Analysis of Code-Mixed Tweets Using Ensemble of Language Models</a></strong><br><a href=/people/m/manoel-verissimo-dos-santos-neto/>Manoel Veríssimo dos Santos Neto</a>
|
<a href=/people/a/ayrton-amaral/>Ayrton Amaral</a>
|
<a href=/people/n/nadia-silva/>Nádia Silva</a>
|
<a href=/people/a/anderson-da-silva-soares/>Anderson da Silva Soares</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--164><div class="card-body p-3 small">In this paper, we describe a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to predict <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> in code-mixed tweets (hindi-english). Our team called verissimo.manoel in CodaLab developed an approach based on an ensemble of four models (MultiFiT, BERT, ALBERT, and XLNET). The final <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification algorithm</a> was an ensemble of some predictions of all softmax values from these four <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. This <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> was used and evaluated in the context of the SemEval 2020 challenge (task 9), and our <a href=https://en.wikipedia.org/wiki/System>system</a> got 72.7 % on the F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.170/>IUST at SemEval-2020 Task 9 : Sentiment Analysis for Code-Mixed Social Media Text Using Deep Neural Networks and Linear Baselines<span class=acl-fixed-case>IUST</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text Using Deep Neural Networks and Linear Baselines</a></strong><br><a href=/people/s/soroush-javdan/>Soroush Javdan</a>
|
<a href=/people/t/taha-shangipour-ataei/>Taha Shangipour ataei</a>
|
<a href=/people/b/behrouz-minaei-bidgoli/>Behrouz Minaei-Bidgoli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--170><div class="card-body p-3 small">Sentiment Analysis is a well-studied field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. However, the rapid growth of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and noisy content within them poses significant challenges in addressing this problem with well-established methods and tools. One of these challenges is <a href=https://en.wikipedia.org/wiki/Code_mixing>code-mixing</a>, which means using different languages to convey thoughts in social media texts. Our group, with the name of IUST(username : TAHA), participated at the SemEval-2020 shared task 9 on Sentiment Analysis for Code-Mixed Social Media Text, and we have attempted to develop a system to predict the sentiment of a given code-mixed tweet. We used different preprocessing techniques and proposed to use different methods that vary from NBSVM to more complicated deep neural network models. Our best performing method obtains an F1 score of 0.751 for the Spanish-English sub-task and 0.706 over the Hindi-English sub-task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--174 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.174/>MeisterMorxrc at SemEval-2020 Task 9 : Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets<span class=acl-fixed-case>M</span>eister<span class=acl-fixed-case>M</span>orxrc at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets</a></strong><br><a href=/people/q/qi-wu/>Qi Wu</a>
|
<a href=/people/p/peng-wang/>Peng Wang</a>
|
<a href=/people/c/chenghao-huang/>Chenghao Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--174><div class="card-body p-3 small">Natural language processing (NLP) has been applied to various <a href=https://en.wikipedia.org/wiki/Field_(computer_science)>fields</a> including <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. In the shared task of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of code-mixed tweets, which is a part of the SemEval-2020 competition, we preprocess datasets by replacing <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> and deleting uncommon characters and so on, and then fine-tune the Bidirectional Encoder Representation from Transformers(BERT) to perform the best. After exhausting top3 submissions, Our team MeisterMorxrc achieves an averaged F1 score of 0.730 in this task, and and our codalab username is MeisterMorxrc</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.181/>WESSA at SemEval-2020 Task 9 : Code-Mixed Sentiment Analysis Using Transformers<span class=acl-fixed-case>WESSA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Code-Mixed Sentiment Analysis Using Transformers</a></strong><br><a href=/people/a/ahmed-sultan/>Ahmed Sultan</a>
|
<a href=/people/m/mahmoud-salim/>Mahmoud Salim</a>
|
<a href=/people/a/amina-gaber/>Amina Gaber</a>
|
<a href=/people/i/islam-el-hosary/>Islam El Hosary</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--181><div class="card-body p-3 small">In this paper, we describe our system submitted for SemEval 2020 Task 9, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> for Code-Mixed Social Media Text alongside other experiments. Our best performing system is a Transfer Learning-based model that fine-tunes XLM-RoBERTa, a transformer-based multilingual masked language model, on monolingual English and Spanish data and Spanish-English code-mixed data. Our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms the official task baseline by achieving a 70.1 % average F1-Score on the official leaderboard using the test set. For later submissions, our <a href=https://en.wikipedia.org/wiki/System>system</a> manages to achieve a 75.9 % average F1-Score on the test set using CodaLab username ahmed0sultan.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--183 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.183/>Zyy1510 Team at SemEval-2020 Task 9 : Sentiment Analysis for Code-Mixed Social Media Text with Sub-word Level Representations<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text with Sub-word Level Representations</a></strong><br><a href=/people/y/yueying-zhu/>Yueying Zhu</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a>
|
<a href=/people/h/hongling-li/>Hongling Li</a>
|
<a href=/people/k/kunjie-dong/>Kunjie Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--183><div class="card-body p-3 small">This paper reports the zyy1510 team&#8217;s work in the International Workshop on Semantic Evaluation (SemEval-2020) shared task on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment analysis</a> for Code-Mixed (Hindi-English, English-Spanish) Social Media Text. The purpose of this task is to determine the polarity of the text, dividing it into one of the three labels positive, negative and neutral. To achieve this goal, we propose an ensemble model of word n-grams-based Multinomial Naive Bayes (MNB) and sub-word level representations in LSTM (Sub-word LSTM) to identify the sentiments of code-mixed data of Hindi-English and English-Spanish. This <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble model</a> combines the advantage of rich sequential patterns and the intermediate features after convolution from the LSTM model, and the polarity of keywords from the MNB model to obtain the final sentiment score. We have tested our system on Hindi-English and English-Spanish code-mixed social media data sets released for the task. Our model achieves the F1 score of 0.647 in the Hindi-English task and 0.682 in the English-Spanish task, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--188 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.188/>SemEval-2020 Task 12 : Multilingual Offensive Language Identification in Social Media (OffensEval 2020)<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Multilingual Offensive Language Identification in Social Media (<span class=acl-fixed-case>O</span>ffens<span class=acl-fixed-case>E</span>val 2020)</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/s/sara-rosenthal/>Sara Rosenthal</a>
|
<a href=/people/p/pepa-atanasova/>Pepa Atanasova</a>
|
<a href=/people/g/georgi-karadzhov/>Georgi Karadzhov</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/l/leon-derczynski/>Leon Derczynski</a>
|
<a href=/people/z/zeses-pitenis/>Zeses Pitenis</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--188><div class="card-body p-3 small">We present the results and the main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020). The task included three subtasks corresponding to the hierarchical taxonomy of the OLID schema from OffensEval-2019, and it was offered in five languages : <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>. OffensEval-2020 was one of the most popular tasks at SemEval-2020, attracting a large number of participants across all subtasks and languages : a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.189/>Galileo at SemEval-2020 Task 12 : Multi-lingual Learning for Offensive Language Identification Using Pre-trained Language Models<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Multi-lingual Learning for Offensive Language Identification Using Pre-trained Language Models</a></strong><br><a href=/people/s/shuohuan-wang/>Shuohuan Wang</a>
|
<a href=/people/j/jiaxiang-liu/>Jiaxiang Liu</a>
|
<a href=/people/x/xuan-ouyang/>Xuan Ouyang</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--189><div class="card-body p-3 small">This paper describes <a href=https://en.wikipedia.org/wiki/Galileo_(spacecraft)>Galileo</a>&#8217;s performance in SemEval-2020 Task 12 on detecting and categorizing <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. For Offensive Language Identification, we proposed a multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive language categorization, we proposed a knowledge distillation method trained on soft labels generated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A-Offensive Language Identification, we ranked first in terms of average F1 scores in all languages. We are also the only team which ranked among the top three across all languages. We also took the first place in Sub-task B-Automatic Categorization of Offense Types and Sub-task C-Offence Target Identification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.191/>Aschern at SemEval-2020 Task 11 : It Takes Three to Tango : RoBERTa, CRF, and Transfer Learning<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: It Takes Three to Tango: <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a, <span class=acl-fixed-case>CRF</span>, and Transfer Learning</a></strong><br><a href=/people/a/anton-chernyavskiy/>Anton Chernyavskiy</a>
|
<a href=/people/d/dmitry-ilvovsky/>Dmitry Ilvovsky</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--191><div class="card-body p-3 small">We describe our <a href=https://en.wikipedia.org/wiki/System>system</a> for SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. We developed ensemble models using RoBERTa-based neural architectures, additional CRF layers, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> between the two subtasks, and advanced post-processing to handle the multi-label nature of the task, the consistency between nested spans, repetitions, and labels from similar spans in training. We achieved sizable improvements over baseline fine-tuned RoBERTa models, and the official evaluation ranked our system 3rd (almost tied with the 2nd) out of 36 teams on the span identification subtask with an F1 score of 0.491, and 2nd (almost tied with the 1st) out of 31 teams on the technique classification subtask with an F1 score of 0.62.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--198 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.198/>AdelaideCyC at SemEval-2020 Task 12 : Ensemble of Classifiers for Offensive Language Detection in Social Media<span class=acl-fixed-case>A</span>delaide<span class=acl-fixed-case>C</span>y<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Ensemble of Classifiers for Offensive Language Detection in Social Media</a></strong><br><a href=/people/m/mahen-herath/>Mahen Herath</a>
|
<a href=/people/t/thushari-atapattu/>Thushari Atapattu</a>
|
<a href=/people/h/hoang-anh-dung/>Hoang Anh Dung</a>
|
<a href=/people/c/christoph-treude/>Christoph Treude</a>
|
<a href=/people/k/katrina-falkner/>Katrina Falkner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--198><div class="card-body p-3 small">This paper describes the systems our team (AdelaideCyC) has developed for SemEval Task 12 (OffensEval 2020) to detect offensive language in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. The challenge focuses on three subtasks offensive language identification (subtask A), offense type identification (subtask B), and offense target identification (subtask C). Our team has participated in all the three subtasks. We have developed machine learning and deep learning-based ensembles of models. We have achieved F1-scores of 0.906, 0.552, and 0.623 in subtask A, B, and C respectively. While our performance scores are promising for subtask A, the results demonstrate that subtask B and C still remain challenging to classify.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--202 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.202/>GruPaTo at SemEval-2020 Task 12 : Retraining mBERT on Social Media and Fine-tuned Offensive Language Models<span class=acl-fixed-case>G</span>ru<span class=acl-fixed-case>P</span>a<span class=acl-fixed-case>T</span>o at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Retraining m<span class=acl-fixed-case>BERT</span> on Social Media and Fine-tuned Offensive Language Models</a></strong><br><a href=/people/d/davide-colla/>Davide Colla</a>
|
<a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/j/jelena-mitrovic/>Jelena Mitrović</a>
|
<a href=/people/m/michael-granitzer/>Michael Granitzer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--202><div class="card-body p-3 small">We introduce an approach to multilingual Offensive Language Detection based on the mBERT transformer model. We download extra training data from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>, and use it to re-train the model. We then fine-tuned the model on the provided training data and, in some configurations, implement transfer learning approach exploiting the typological relatedness between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>. Our systems obtained good results across the three languages (.9036 for EN,.7619 for DA, and.7789 for TR).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.203/>GUIR at SemEval-2020 Task 12 : Domain-Tuned Contextualized Models for Offensive Language Detection<span class=acl-fixed-case>GUIR</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Domain-Tuned Contextualized Models for Offensive Language Detection</a></strong><br><a href=/people/s/sajad-sotudeh/>Sajad Sotudeh</a>
|
<a href=/people/t/tong-xiang/>Tong Xiang</a>
|
<a href=/people/h/hao-ren-yao/>Hao-Ren Yao</a>
|
<a href=/people/s/sean-macavaney/>Sean MacAvaney</a>
|
<a href=/people/e/eugene-yang/>Eugene Yang</a>
|
<a href=/people/n/nazli-goharian/>Nazli Goharian</a>
|
<a href=/people/o/ophir-frieder/>Ophir Frieder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--203><div class="card-body p-3 small">Offensive language detection is an important and challenging task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. We present our submissions to the OffensEval 2020 shared task, which includes three English sub-tasks : identifying the presence of offensive language (Sub-task A), identifying the presence of target in offensive language (Sub-task B), and identifying the categories of the target (Sub-task C). Our experiments explore using a domain-tuned contextualized language model (namely, BERT) for this task. We also experiment with different <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> and configurations (e.g., a multi-view SVM) stacked upon BERT models for specific <a href=https://en.wikipedia.org/wiki/Subroutine>sub-tasks</a>. Our submissions achieve <a href=https://en.wikipedia.org/wiki/F-number>F1 scores</a> of 91.7 % in <a href=https://en.wikipedia.org/wiki/Task_(computing)>Sub-task</a> A, 66.5 % in Sub-task B, and 63.2 % in Sub-task C. We perform an ablation study which reveals that domain tuning considerably improves the classification performance. Furthermore, error analysis shows common misclassification errors made by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and outlines research directions for future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.204/>IIITG-ADBU at SemEval-2020 Task 12 : Comparison of BERT and BiLSTM in Detecting Offensive Language<span class=acl-fixed-case>IIITG</span>-<span class=acl-fixed-case>ADBU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Comparison of <span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> in Detecting Offensive Language</a></strong><br><a href=/people/a/arup-baruah/>Arup Baruah</a>
|
<a href=/people/k/kaushik-das/>Kaushik Das</a>
|
<a href=/people/f/ferdous-barbhuiya/>Ferdous Barbhuiya</a>
|
<a href=/people/k/kuntal-dey/>Kuntal Dey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--204><div class="card-body p-3 small">Task 12 of SemEval 2020 consisted of 3 subtasks, namely offensive language identification (Subtask A), categorization of offense type (Subtask B), and offense target identification (Subtask C). This paper presents the results our <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> obtained for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a> in the 3 subtasks. The <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> used by us were BERT and BiLSTM. On the test set, our BERT classifier obtained macro F1 score of 0.90707 for subtask A, and 0.65279 for subtask B. The BiLSTM classifier obtained macro F1 score of 0.57565 for subtask C. The paper also performs an analysis of the errors made by our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. We conjecture that the presence of few misleading instances in the dataset is affecting the performance of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. Our analysis also discusses the need of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>temporal context</a> and <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> to determine the offensiveness of few comments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.208/>NUIG at SemEval-2020 Task 12 : Pseudo Labelling for Offensive Content Classification<span class=acl-fixed-case>NUIG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Pseudo Labelling for Offensive Content Classification</a></strong><br><a href=/people/s/shardul-suryawanshi/>Shardul Suryawanshi</a>
|
<a href=/people/m/mihael-arcan/>Mihael Arcan</a>
|
<a href=/people/p/paul-buitelaar/>Paul Buitelaar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--208><div class="card-body p-3 small">This work addresses the classification problem defined by sub-task A (English only) of the OffensEval 2020 challenge. We used a semi-supervised approach to classify given tweets into an offensive (OFF) or not-offensive (NOT) class. As the OffensEval 2020 dataset is loosely labelled with confidence scores given by <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised models</a>, we used last year&#8217;s offensive language identification dataset (OLID) to label the OffensEval 2020 dataset. Our approach uses a pseudo-labelling method to annotate the current <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We trained four text classifiers on the OLID dataset and the classifier with the highest macro-averaged F1-score has been used to pseudo label the OffensEval 2020 dataset. The same model which performed best amongst four text classifiers on <a href=https://en.wikipedia.org/wiki/Online_analytical_processing>OLID dataset</a> has been trained on the combined dataset of <a href=https://en.wikipedia.org/wiki/Online_analytical_processing>OLID</a> and pseudo labelled OffensEval 2020. We evaluated the classifiers with precision, recall and macro-averaged F1-score as the primary evaluation metric on the OLID and OffensEval 2020 datasets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details :.<url>http://creativecommons.org/licenses/by/4.0/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.213/>UHH-LT at SemEval-2020 Task 12 : Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection<span class=acl-fixed-case>UHH</span>-<span class=acl-fixed-case>LT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection</a></strong><br><a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--213><div class="card-body p-3 small">Fine-tuning of pre-trained transformer networks such as BERT yield state-of-the-art results for text classification tasks. Typically, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is performed on <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>task-specific training datasets</a> in a supervised manner. One can also fine-tune in unsupervised manner beforehand by further pre-training the masked language modeling (MLM) task. Hereby, in-domain data for <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised MLM</a> resembling the actual classification target dataset allows for domain adaptation of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. In this paper, we compare current pre-trained transformer networks with and without MLM fine-tuning on their performance for offensive language detection. Our MLM fine-tuned RoBERTa-based classifier officially ranks 1st in the SemEval 2020 Shared Task 12 for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. Further experiments with the ALBERT model even surpass this result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--214 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.214/>EL-BERT at SemEval-2020 Task 10 : A Multi-Embedding Ensemble Based Approach for Emphasis Selection in Visual Media<span class=acl-fixed-case>EL</span>-<span class=acl-fixed-case>BERT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 10: A Multi-Embedding Ensemble Based Approach for Emphasis Selection in Visual Media</a></strong><br><a href=/people/c/chandresh-kanani/>Chandresh Kanani</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--214><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Media_(communication)>visual media</a>, text emphasis is the strengthening of words in a text to convey the intent of the author. Text emphasis in visual media is generally done by using different colors, backgrounds, or font for the text ; it helps in conveying the actual meaning of the message to the readers. Emphasis selection is the task of choosing candidate words for <a href=https://en.wikipedia.org/wiki/Emphasis_(typography)>emphasis</a>, it helps in automatically designing <a href=https://en.wikipedia.org/wiki/Poster>posters</a> and other media contents with <a href=https://en.wikipedia.org/wiki/Writing>written text</a>. If we consider only the text and do not know the intent, then there can be multiple valid emphasis selections. We propose the use of <a href=https://en.wikipedia.org/wiki/Musical_ensemble>ensembles</a> for emphasis selection to improve over single emphasis selection models. We show that the use of multi-embedding helps in enhancing the results for base models. To show the efficacy of proposed approach we have also done a comparison of our results with state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--218 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.218/>LAST at SemEval-2020 Task 10 : Finding Tokens to Emphasise in Short Written Texts with Precomputed Embedding Models and LightGBM<span class=acl-fixed-case>LAST</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 10: Finding Tokens to Emphasise in Short Written Texts with Precomputed Embedding Models and <span class=acl-fixed-case>L</span>ight<span class=acl-fixed-case>GBM</span></a></strong><br><a href=/people/y/yves-bestgen/>Yves Bestgen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--218><div class="card-body p-3 small">To select tokens to be emphasised in short texts, a system mainly based on precomputed embedding models, such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and ELMo, and LightGBM is proposed. Its performance is low. Additional analyzes suggest that its effectiveness is poor at predicting the highest emphasis scores while they are the most important for the challenge and that it is very sensitive to the specific instances provided during learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.220/>Randomseed19 at SemEval-2020 Task 10 : Emphasis Selection for Written Text in Visual Media<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 10: Emphasis Selection for Written Text in Visual Media</a></strong><br><a href=/people/a/aleksandr-shatilov/>Aleksandr Shatilov</a>
|
<a href=/people/d/denis-gordeev/>Denis Gordeev</a>
|
<a href=/people/a/alexey-rey/>Alexey Rey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--220><div class="card-body p-3 small">This paper describes our approach to emphasis selection for <a href=https://en.wikipedia.org/wiki/Writing>written text</a> in <a href=https://en.wikipedia.org/wiki/Visual_media>visual media</a> as a solution for SemEval 2020 Task 10. We used an ensemble of several different Transformer-based models and cast the task as a sequence labeling problem with two tags : &#8216;I&#8217; as &#8216;emphasized&#8217; and &#8216;O&#8217; as &#8216;non-emphasized&#8217; for each token in the text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.224/>YNU-HPCC at SemEval-2020 Task 10 : Using a Multi-granularity Ordinal Classification of the BiLSTM Model for Emphasis Selection<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 10: Using a Multi-granularity Ordinal Classification of the <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> Model for Emphasis Selection</a></strong><br><a href=/people/d/dawei-liao/>Dawei Liao</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--224><div class="card-body p-3 small">In this study, we propose a multi-granularity ordinal classification method to address the problem of emphasis selection. In detail, the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> is learned from Embeddings from Language Model (ELMO) to extract feature vector representation. Then, the ordinal classifica-tions are implemented on four different multi-granularities to approximate the continuous em-phasize values. Comparative experiments were conducted to compare the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with baseline in which the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is transformed to label distribution problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.229/>JUST at SemEval-2020 Task 11 : Detecting Propaganda Techniques Using BERT Pre-trained Model<span class=acl-fixed-case>JUST</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Detecting Propaganda Techniques Using <span class=acl-fixed-case>BERT</span> Pre-trained Model</a></strong><br><a href=/people/o/ola-altiti/>Ola Altiti</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a>
|
<a href=/people/r/rasha-obiedat/>Rasha Obiedat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--229><div class="card-body p-3 small">This paper presents the submission to semeval-2020 task 11, Detection of Propaganda Techniques in News Articles. Knowing that there are two subtasks in this competition, we have participated in the Technique Classification subtask (TC), which aims to identify the propaganda techniques used in a specific propaganda span. We have used and implemented various <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to detect <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>. Our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is based on BERT uncased pre-trained language model as it has achieved state-of-the-art performance on multiple NLP benchmarks. The performance results of our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> have scored 0.55307 F1-Score, which outperforms the baseline model provided by the organizers with 0.2519 F1-Score, and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is 0.07 away from the best performing team. Compared to other participating systems, our submission is ranked 15th out of 31 participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--232 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.232/>NLFIIT at SemEval-2020 Task 11 : Neural Network Architectures for Detection of Propaganda Techniques in News Articles<span class=acl-fixed-case>NLFIIT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Neural Network Architectures for Detection of Propaganda Techniques in News Articles</a></strong><br><a href=/people/m/matej-martinkovic/>Matej Martinkovic</a>
|
<a href=/people/s/samuel-pecar/>Samuel Pecar</a>
|
<a href=/people/m/marian-simko/>Marian Simko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--232><div class="card-body p-3 small">Since <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> became more common technique in <a href=https://en.wikipedia.org/wiki/News>news</a>, it is very important to look for possibilities of its automatic detection. In this paper, we present neural model architecture submitted to the SemEval-2020 Task 11 competition : Detection of Propaganda Techniques in News Articles. We participated in both subtasks, propaganda span identification and propaganda technique classification. Our model utilizes recurrent Bi-LSTM layers with pre-trained word representations and also takes advantage of self-attention mechanism. Our model managed to achieve score 0.405 F1 for subtask 1 and 0.553 F1 for subtask 2 on test set resulting in 17th and 16th place in subtask 1 and subtask 2, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--233 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.233/>PsuedoProp at SemEval-2020 Task 11 : Propaganda Span Detection Using BERT-CRF and Ensemble Sentence Level Classifier<span class=acl-fixed-case>P</span>suedo<span class=acl-fixed-case>P</span>rop at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Propaganda Span Detection Using <span class=acl-fixed-case>BERT</span>-<span class=acl-fixed-case>CRF</span> and Ensemble Sentence Level Classifier</a></strong><br><a href=/people/a/aniruddha-chauhan/>Aniruddha Chauhan</a>
|
<a href=/people/h/harshita-diddee/>Harshita Diddee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--233><div class="card-body p-3 small">This paper explains our teams&#8217; submission to the Shared Task of Fine-Grained Propaganda Detection in which we propose a sequential BERT-CRF based Span Identification model where the fine-grained detection is carried out only on the articles that are flagged as containing propaganda by an ensemble SLC model. We propose this setup bearing in mind the practicality of this approach in identifying propaganda spans in the exponentially increasing content base where the fine-tuned analysis of the entire data repository may not be the optimal choice due to its massive computational resource requirements. We present our analysis on different <a href=https://en.wikipedia.org/wiki/Electoral_system>voting ensembles</a> for the SLC model. Our <a href=https://en.wikipedia.org/wiki/System>system</a> ranks 14th on the test set and 22nd on the development set and with an F1 score of 0.41 and 0.39 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.234/>SkoltechNLP at SemEval-2020 Task 11 : Exploring Unsupervised Text Augmentation for Propaganda Detection<span class=acl-fixed-case>S</span>koltech<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Exploring Unsupervised Text Augmentation for Propaganda Detection</a></strong><br><a href=/people/d/daryna-dementieva/>Daryna Dementieva</a>
|
<a href=/people/i/igor-markov/>Igor Markov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--234><div class="card-body p-3 small">This paper presents a solution for the Span Identification (SI) task in the Detection of Propaganda Techniques in News Articles competition at SemEval-2020. The goal of the SI task is to identify specific fragments of each article which contain the use of at least one propaganda technique. This is a binary sequence tagging task. We tested several approaches finally selecting a fine-tuned BERT model as our <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a>. Our main contribution is an investigation of several unsupervised data augmentation techniques based on distributional semantics expanding the original small training dataset as applied to this BERT-based sequence tagger. We explore various expansion strategies and show that they can substantially shift the balance between <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>, while maintaining comparable levels of the F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.237/>syrapropa at SemEval-2020 Task 11 : BERT-based Models Design for Propagandistic Technique and Span Detection<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: <span class=acl-fixed-case>BERT</span>-based Models Design for Propagandistic Technique and Span Detection</a></strong><br><a href=/people/j/jinfen-li/>Jinfen Li</a>
|
<a href=/people/l/lu-xiao/>Lu Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--237><div class="card-body p-3 small">This paper describes the BERT-based models proposed for two subtasks in SemEval-2020 Task 11 : Detection of Propaganda Techniques in News Articles. We first build the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for Span Identification (SI) based on SpanBERT, and facilitate the detection by a deeper <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and a sentence-level representation. We then develop a hybrid model for the Technique Classification (TC). The hybrid model is composed of three submodels including two BERT models with different training methods, and a feature-based Logistic Regression model. We endeavor to deal with imbalanced dataset by adjusting <a href=https://en.wikipedia.org/wiki/Cost_function>cost function</a>. We are in the seventh place in SI subtask (0.4711 of F1-measure), and in the third place in TC subtask (0.6783 of F1-measure) on the development set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.238/>Team DiSaster at SemEval-2020 Task 11 : Combining BERT and Hand-crafted Features for Identifying Propaganda Techniques in News<span class=acl-fixed-case>D</span>i<span class=acl-fixed-case>S</span>aster at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Combining <span class=acl-fixed-case>BERT</span> and Hand-crafted Features for Identifying Propaganda Techniques in News</a></strong><br><a href=/people/a/anders-kaas/>Anders Kaas</a>
|
<a href=/people/v/viktor-torp-thomsen/>Viktor Torp Thomsen</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--238><div class="card-body p-3 small">The identification of communication techniques in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> such as <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> is important, as such techniques can influence the opinions of large numbers of people. Most work so far focused on the <a href=https://en.wikipedia.org/wiki/Identification_(biology)>identification</a> at the news article level. Recently, a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and shared task has been proposed for the identification of propaganda techniques at the finer-grained span level. This paper describes our system submission to the subtask of technique classification (TC) for the SemEval 2020 shared task on detection of propaganda techniques in news articles. We propose a method of combining neural BERT representations with hand-crafted features via stacked generalization. Our model has the added advantage that it combines the power of contextual representations from BERT with simple span-based and article-based global features. We present an ablation study which shows that even though BERT representations are very powerful also for this task, BERT still benefits from being combined with carefully designed task-specific features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--240 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.240/>TTUI at SemEval-2020 Task 11 : Propaganda Detection with Transfer Learning and Ensembles<span class=acl-fixed-case>TTUI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Propaganda Detection with Transfer Learning and Ensembles</a></strong><br><a href=/people/m/moonsung-kim/>Moonsung Kim</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--240><div class="card-body p-3 small">In this paper, we describe our approaches and <a href=https://en.wikipedia.org/wiki/System>systems</a> for the SemEval-2020 Task 11 on propaganda technique detection. We fine-tuned BERT and RoBERTa pre-trained models then merged them with an average ensemble. We conducted several experiments for input representations dealing with long texts and preserving context as well as for the imbalanced class problem. Our system ranked 20th out of 36 teams with 0.398 F1 in the SI task and 14th out of 31 teams with 0.556 F1 in the TC task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--241 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.241/>UAIC1860 at SemEval-2020 Task 11 : Detection of Propaganda Techniques in News Articles<span class=acl-fixed-case>UAIC</span>1860 at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Detection of Propaganda Techniques in News Articles</a></strong><br><a href=/people/v/vlad-ermurachi/>Vlad Ermurachi</a>
|
<a href=/people/d/daniela-gifu/>Daniela Gifu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--241><div class="card-body p-3 small">The Detection of Propaganda Techniques in News Articles task at the SemEval 2020 competition focuses on detecting and classifying <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>, pervasive in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news article</a>. In this paper, we present a system able to evaluate on sentence level, three traditional text representation techniques for these study goals, using : tf*idf, word and character n-grams. Firstly, we built a binary classifier able to provide corresponding propaganda labels, propaganda or non-propaganda. Secondly, we build a multilabel multiclass model to identify applied <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.242/>UMSIForeseer at SemEval-2020 Task 11 : Propaganda Detection by Fine-Tuning BERT with Resampling and Ensemble Learning<span class=acl-fixed-case>UMSIF</span>oreseer at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Propaganda Detection by Fine-Tuning <span class=acl-fixed-case>BERT</span> with Resampling and Ensemble Learning</a></strong><br><a href=/people/y/yunzhe-jiang/>Yunzhe Jiang</a>
|
<a href=/people/c/cristina-garbacea/>Cristina Garbacea</a>
|
<a href=/people/q/qiaozhu-mei/>Qiaozhu Mei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--242><div class="card-body p-3 small">We describe our participation at the SemEval 2020 Detection of Propaganda Techniques in News Articles-Techniques Classification (TC) task, designed to categorize textual fragments into one of the 14 given propaganda techniques. Our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> leverages pre-trained BERT models. We present our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> implementations, evaluation results and analysis of these results. We also investigate the potential of combining <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> with <a href=https://en.wikipedia.org/wiki/Sample-rate_conversion>resampling</a> and ensemble learning methods to deal with data imbalance and improve performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--243 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.243/>UNTLing at SemEval-2020 Task 11 : Detection of Propaganda Techniques in English News Articles<span class=acl-fixed-case>UNTL</span>ing at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 11: Detection of Propaganda Techniques in <span class=acl-fixed-case>E</span>nglish News Articles</a></strong><br><a href=/people/m/maia-petee/>Maia Petee</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--243><div class="card-body p-3 small">Our system for the PropEval task explores the ability of semantic features to detect and label propagandistic rhetorical techniques in English news articles. For Subtask 2, labeling identified propagandistic fragments with one of fourteen technique labels, our system attains a micro-averaged F1 of 0.40 ; in this paper, we take a detailed look at the fourteen labels and how well our semantically-focused model detects each of them. We also propose strategies to fill the gaps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.250.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--250 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.250 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.250/>Amsqr at SemEval-2020 Task 12 : Offensive Language Detection Using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> and Anti-adversarial Features<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Offensive Language Detection Using Neural Networks and Anti-adversarial Features</a></strong><br><a href=/people/a/alejandro-mosquera/>Alejandro Mosquera</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--250><div class="card-body p-3 small">This paper describes a method and <a href=https://en.wikipedia.org/wiki/System>system</a> to solve the problem of detecting offensive language in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> using anti-adversarial features. Our submission to the SemEval-2020 task 12 challenge was generated by an stacked ensemble of neural networks fine-tuned on the OLID dataset and additional external sources. For Task-A (English), text normalisation filters were applied at both graphical and lexical level. The normalisation step effectively mitigates not only the natural presence of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>lexical variants</a> but also intentional attempts to bypass moderation by introducing out of vocabulary words. Our approach provides strong F1 scores for both 2020 (0.9134) and 2019 (0.8258) challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--258 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.258/>Hitachi at SemEval-2020 Task 12 : Offensive Language Identification with Noisy Labels Using Statistical Sampling and Post-Processing<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Offensive Language Identification with Noisy Labels Using Statistical Sampling and Post-Processing</a></strong><br><a href=/people/m/manikandan-ravikiran/>Manikandan Ravikiran</a>
|
<a href=/people/a/amin-ekant-muljibhai/>Amin Ekant Muljibhai</a>
|
<a href=/people/t/toshinori-miyoshi/>Toshinori Miyoshi</a>
|
<a href=/people/h/hiroaki-ozaki/>Hiroaki Ozaki</a>
|
<a href=/people/y/yuta-koreeda/>Yuta Koreeda</a>
|
<a href=/people/s/sakata-masayuki/>Sakata Masayuki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--258><div class="card-body p-3 small">In this paper, we present our participation in SemEval-2020 Task-12 Subtask-A (English Language) which focuses on offensive language identification from noisy labels. To this end, we developed a hybrid system with the BERT classifier trained with tweets selected using Statistical Sampling Algorithm (SA) and Post-Processed (PP) using an offensive wordlist. Our developed system achieved 34th position with Macro-averaged F1-score (Macro-F1) of 0.90913 over both offensive and non-offensive classes. We further show comprehensive results and <a href=https://en.wikipedia.org/wiki/Error_analysis_(linguistics)>error analysis</a> to assist future research in offensive language identification with noisy labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--263 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.263/>IR3218-UI at SemEval-2020 Task 12 : Emoji Effects on Offensive Language IdentifiCation<span class=acl-fixed-case>IR</span>3218-<span class=acl-fixed-case>UI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Emoji Effects on Offensive Language <span class=acl-fixed-case>I</span>dentifi<span class=acl-fixed-case>C</span>ation</a></strong><br><a href=/people/s/sandy-kurniawan/>Sandy Kurniawan</a>
|
<a href=/people/i/indra-budi/>Indra Budi</a>
|
<a href=/people/m/muhammad-okky-ibrohim/>Muhammad Okky Ibrohim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--263><div class="card-body p-3 small">In this paper, we present our approach and the results of our participation in OffensEval 2020. There are three sub-tasks in OffensEval 2020 namely offensive language identification (sub-task A), automatic categorization of offense types (sub-task B), and offense target identification (sub-task C). We participated in sub-task A of English OffensEval 2020. Our approach emphasizes on how the <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> affects offensive language identification. Our model used LSTM combined with GloVe pre-trained word vectors to identify offensive language on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. The best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtained macro F1-score of 0.88428.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--266 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.266/>JCT at SemEval-2020 Task 12 : Offensive Language Detection in Tweets Using Preprocessing Methods, Character and Word N-grams<span class=acl-fixed-case>JCT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Offensive Language Detection in Tweets Using Preprocessing Methods, Character and Word N-grams</a></strong><br><a href=/people/m/moshe-uzan/>Moshe Uzan</a>
|
<a href=/people/y/yaakov-hacohen-kerner/>Yaakov HaCohen-Kerner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--266><div class="card-body p-3 small">In this paper, we describe our submissions to SemEval-2020 contest. We tackled subtask 12-Multilingual Offensive Language Identification in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a>. We developed different models for four languages : <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>. We applied three supervised machine learning methods using various combinations of character and word n-gram features. In addition, we applied various combinations of basic preprocessing methods. Our best submission was a model we built for offensive language identification in <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> using <a href=https://en.wikipedia.org/wiki/Random_Forest>Random Forest</a>. This <a href=https://en.wikipedia.org/wiki/Model_(person)>model</a> was ranked at the 6 position out of 39 submissions. Our result is lower by only 0.0025 than the result of the team that won the 4 place using entirely non-neural methods. Our experiments indicate that char ngram features are more helpful than word ngram features. This phenomenon probably occurs because <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> are more characterized by <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> than by words, <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> are short, and contain various special sequences of characters, e.g., <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a>, <a href=https://en.wikipedia.org/wiki/Shortcut_(computing)>shortcuts</a>, slang words, and typos.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--273 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.273/>Lee at SemEval-2020 Task 12 : A BERT Model Based on the Maximum Self-ensemble Strategy for Identifying Offensive Language<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: A <span class=acl-fixed-case>BERT</span> Model Based on the Maximum Self-ensemble Strategy for Identifying Offensive Language</a></strong><br><a href=/people/j/junyi-li/>Junyi Li</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a>
|
<a href=/people/z/zichen-zhang/>Zichen Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--273><div class="card-body p-3 small">This article describes the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to SemEval 2020 Task 12 : OffensEval 2020. This task aims to identify and classify offensive languages in different languages on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We only participate in the English part of subtask A, which aims to identify offensive languages in English. To solve this task, we propose a BERT model system based on the transform mechanism, and use the maximum self-ensemble to improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved a macro F1 score of 0.913(ranked 13/82) in subtask A.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.274.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--274 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.274 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.274/>LIIR at SemEval-2020 Task 12 : A Cross-Lingual Augmentation Approach for Multilingual Offensive Language Identification<span class=acl-fixed-case>LIIR</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: A Cross-Lingual Augmentation Approach for Multilingual Offensive Language Identification</a></strong><br><a href=/people/e/erfan-ghadery/>Erfan Ghadery</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--274><div class="card-body p-3 small">This paper presents our system entitled &#8216;LIIR&#8217; for SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval 2). We have participated in sub-task A for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish languages</a>. We adapt and fine-tune the <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and Multilingual Bert models made available by Google AI for English and non-English languages respectively. For the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, we use a combination of two fine-tuned BERT models. For other languages we propose a cross-lingual augmentation approach in order to enrich training data and we use Multilingual BERT to obtain sentence representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.283/>SalamNET at SemEval-2020 Task 12 : Deep Learning Approach for Arabic Offensive Language Detection<span class=acl-fixed-case>S</span>alam<span class=acl-fixed-case>NET</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Deep Learning Approach for <span class=acl-fixed-case>A</span>rabic Offensive Language Detection</a></strong><br><a href=/people/f/fatemah-husain/>Fatemah Husain</a>
|
<a href=/people/j/jooyeon-lee/>Jooyeon Lee</a>
|
<a href=/people/s/sam-henry/>Sam Henry</a>
|
<a href=/people/o/ozlem-uzuner/>Ozlem Uzuner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--283><div class="card-body p-3 small">This paper describes SalamNET, an Arabic offensive language detection system that has been submitted to SemEval 2020 shared task 12 : Multilingual Offensive Language Identification in Social Media. Our approach focuses on applying multiple deep learning models and conducting in depth error analysis of results to provide system implications for future development considerations. To pursue our goal, a Recurrent Neural Network (RNN), a Gated Recurrent Unit (GRU), and Long-Short Term Memory (LSTM) models with different design architectures have been developed and evaluated. The SalamNET, a Bi-directional Gated Recurrent Unit (Bi-GRU) based model, reports a macro-F1 score of 0.83 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--285 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.285/>Sonal.kumari at SemEval-2020 Task 12 : Social Media Multilingual Offensive Text Identification and Categorization Using Neural Network Models<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Social Media Multilingual Offensive Text Identification and Categorization Using Neural Network Models</a></strong><br><a href=/people/s/sonal-kumari/>Sonal Kumari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--285><div class="card-body p-3 small">In this paper, we present our approaches and results for SemEval-2020 Task 12, Multilingual Offensive Language Identification in Social Media (OffensEval 2020). The OffensEval 2020 had three subtasks : A) Identifying the tweets to be offensive (OFF) or non-offensive (NOT) for Arabic, Danish, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, Greek, and Turkish languages, B) Detecting if the offensive tweet is targeted (TIN) or untargeted (UNT) for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, and C) Categorizing the offensive targeted tweets into three classes, namely : individual (IND), Group (GRP), or Other (OTH) for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. We participate in all the subtasks A, B, and C. In our solution, first we use the pre-trained BERT model for all <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>subtasks</a>, A, B, and C and then we apply the BiLSTM model with attention mechanism (Attn-BiLSTM) for the same. Our result demonstrates that the pre-trained model is not giving good results for all types of languages and is compute and memory intensive whereas the Attn-BiLSTM model is fast and gives good accuracy with fewer resources. The Attn-BiLSTM model is giving better accuracy for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> and <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a> where the pre-trained model is not able to capture the complete context of these languages due to lower vocab-size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--287 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.287/>SSN_NLP_MLRG at SemEval-2020 Task 12 : Offensive Language Identification in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a> Using BERT and Machine Learning Approach<span class=acl-fixed-case>SSN</span>_<span class=acl-fixed-case>NLP</span>_<span class=acl-fixed-case>MLRG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Offensive Language Identification in <span class=acl-fixed-case>E</span>nglish, <span class=acl-fixed-case>D</span>anish, <span class=acl-fixed-case>G</span>reek Using <span class=acl-fixed-case>BERT</span> and Machine Learning Approach</a></strong><br><a href=/people/a/a-kalaivani/>A Kalaivani</a>
|
<a href=/people/t/thenmozhi-d/>Thenmozhi D.</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--287><div class="card-body p-3 small">Offensive language identification is to detect the hurtful tweets, <a href=https://en.wikipedia.org/wiki/Pejorative>derogatory comments</a>, swear words on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. As an emerging growth of social media communication, offensive language detection has received more attention in the last years ; we focus to perform the task on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> and <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>. We have investigated which can be effect more on pre-trained models BERT (Bidirectional Encoder Representation from Transformer) and Machine Learning Approaches. Our investigation shows the difference performance between the three languages and to identify the best performance is evaluated by the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification algorithms</a>. In the shared task SemEval-2020, our team SSN_NLP_MLRG submitted for three languages that are Subtasks A, B, C in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, Subtask A in <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> and Subtask A in <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>. Our team SSN_NLP_MLRG obtained the <a href=https://en.wikipedia.org/wiki/F1_score>F1 Scores</a> as 0.90, 0.61, 0.52 for the Subtasks A, B, C in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, 0.56 for the Subtask A in <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> and 0.67 for the Subtask A in Greek respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--289 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.semeval-1.289" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.289/>TAC at SemEval-2020 Task 12 : Ensembling Approach for Multilingual Offensive Language Identification in Social Media<span class=acl-fixed-case>TAC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Ensembling Approach for Multilingual Offensive Language Identification in Social Media</a></strong><br><a href=/people/t/talha-anwar/>Talha Anwar</a>
|
<a href=/people/o/omer-baig/>Omer Baig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--289><div class="card-body p-3 small">Usage of <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is getting more common these days, and there is a need of a mechanism to detect it and control it. This paper deals with offensive language detection in five different languages ; <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a> and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>. We presented an almost similar <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble pipeline</a> comprised of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning and deep learning models</a> for all five languages. Three <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and four <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> were used in the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a>. In the OffensEval-2020 competition our model achieved <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 0.85, 0.74, 0.68, 0.81, and 0.9 for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>, <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, Greek and English language tasks respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.semeval-1.295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--semeval-1--295 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.semeval-1.295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.semeval-1.295/>UoB at SemEval-2020 Task 12 : Boosting BERT with Corpus Level Information<span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>B</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 12: Boosting <span class=acl-fixed-case>BERT</span> with Corpus Level Information</a></strong><br><a href=/people/w/wah-meng-lim/>Wah Meng Lim</a>
|
<a href=/people/h/harish-tayyar-madabushi/>Harish Tayyar Madabushi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--semeval-1--295><div class="card-body p-3 small">Pre-trained language model word representation, such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, have been extremely successful in several Natural Language Processing tasks significantly improving on the state-of-the-art. This can largely be attributed to their ability to better capture <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> contained within a sentence. Several tasks, however, can benefit from information available at a corpus level, such as Term Frequency-Inverse Document Frequency (TF-IDF). In this work we test the effectiveness of integrating this <a href=https://en.wikipedia.org/wiki/Information>information</a> with BERT on the task of identifying abuse on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and show that integrating this <a href=https://en.wikipedia.org/wiki/Information>information</a> with BERT does indeed significantly improve performance. We participate in Sub-Task A (abuse detection) wherein we achieve a score within two points of the top performing team and in Sub-Task B (target detection) wherein we are ranked 4 of the 44 participating teams.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>