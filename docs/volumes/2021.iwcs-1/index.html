<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 14th International Conference on Computational Semantics (IWCS) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2021.iwcs-1.pdf>Proceedings of the 14th International Conference on Computational Semantics (IWCS)</a></h2><p class=lead><a href=/people/s/sina-zarriess/>Sina Zarrieß</a>,
<a href=/people/j/johan-bos/>Johan Bos</a>,
<a href=/people/r/rik-van-noord/>Rik van Noord</a>,
<a href=/people/l/lasha-abzianidze/>Lasha Abzianidze</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.iwcs-1</dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Groningen, The Netherlands (online)</dd><dt>Venue:</dt><dd><a href=/venues/iwcs/>IWCS</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigsem/>SIGSEM</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.iwcs-1>https://aclanthology.org/2021.iwcs-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.iwcs-1.pdf>https://aclanthology.org/2021.iwcs-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.iwcs-1.pdf title="Open PDF of 'Proceedings of the 14th International Conference on Computational Semantics (IWCS)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+14th+International+Conference+on+Computational+Semantics+%28IWCS%29" title="Search for 'Proceedings of the 14th International Conference on Computational Semantics (IWCS)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwcs-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwcs-1.0/>Proceedings of the 14th International Conference on Computational Semantics (IWCS)</a></strong><br><a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a>
|
<a href=/people/r/rik-van-noord/>Rik van Noord</a>
|
<a href=/people/l/lasha-abzianidze/>Lasha Abzianidze</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwcs-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwcs-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwcs-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.iwcs-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.iwcs-1.1/>Switching Contexts : Transportability Measures for NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/g/guy-marshall/>Guy Marshall</a>
|
<a href=/people/m/mokanarangan-thayaparan/>Mokanarangan Thayaparan</a>
|
<a href=/people/p/philip-osborne/>Philip Osborne</a>
|
<a href=/people/a/andre-freitas/>André Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwcs-1--1><div class="card-body p-3 small">This paper explores the topic of transportability, as a sub-area of generalisability. By proposing the utilisation of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> based on well-established statistics, we are able to estimate the change in performance of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP models</a> in new contexts. Defining a new measure for transportability may allow for better estimation of NLP system performance in new domains, and is crucial when assessing the performance of NLP systems in new tasks and domains. Through several instances of increasing <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a>, we demonstrate how lightweight domain similarity measures can be used as estimators for the transportability in NLP applications. The proposed transportability measures are evaluated in the context of <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> and Natural Language Inference tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwcs-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwcs-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwcs-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwcs-1.2/>Applied Temporal Analysis : A Complete Run of the FraCaS Test Suite<span class=acl-fixed-case>F</span>ra<span class=acl-fixed-case>C</span>a<span class=acl-fixed-case>S</span> Test Suite</a></strong><br><a href=/people/j/jean-philippe-bernardy/>Jean-Philippe Bernardy</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwcs-1--2><div class="card-body p-3 small">In this paper, we propose an implementation of temporal semantics that translates <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>syntax trees</a> to <a href=https://en.wikipedia.org/wiki/Well-formed_formula>logical formulas</a>, suitable for consumption by the <a href=https://en.wikipedia.org/wiki/Coq>Coq proof assistant</a>. The analysis supports a wide range of phenomena including : temporal references, temporal adverbs, aspectual classes and progressives. The new semantics are built on top of a previous system handling all sections of the FraCaS test suite except the temporal reference section, and we obtain an accuracy of 81 percent overall and 73 percent for the problems explicitly marked as related to temporal reference. To the best of our knowledge, this is the best performance of a <a href=https://en.wikipedia.org/wiki/Formal_system>logical system</a> on the whole of the FraCaS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwcs-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwcs-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwcs-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.iwcs-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.iwcs-1.7/>Critical Thinking for <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a></a></strong><br><a href=/people/g/gregor-betz/>Gregor Betz</a>
|
<a href=/people/c/christian-voigt/>Christian Voigt</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwcs-1--7><div class="card-body p-3 small">This paper takes a first step towards a critical thinking curriculum for neural auto-regressive language models. We introduce a synthetic corpus of deductively valid arguments, and generate artificial argumentative texts to train CRiPT : a <a href=https://en.wikipedia.org/wiki/Critical_thinking>critical thinking</a> intermediarily pre-trained transformer based on GPT-2. Significant transfer learning effects can be observed : Trained on three simple core schemes, CRiPT accurately completes conclusions of different, and more complex types of arguments, too. CRiPT generalizes the core argument schemes in a correct way. Moreover, we obtain consistent and promising results for NLU benchmarks. In particular, CRiPT&#8217;s zero-shot accuracy on the GLUE diagnostics exceeds GPT-2&#8217;s performance by 15 percentage points. The findings suggest that intermediary pre-training on texts that exemplify basic reasoning abilities (such as typically covered in critical thinking textbooks) might help <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to acquire a broad range of reasoning skills. The synthetic argumentative texts presented in this paper are a promising starting point for building such a critical thinking curriculum for <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwcs-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwcs-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwcs-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.iwcs-1.12.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment>
<i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.iwcs-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.iwcs-1.12/>Monotonicity Marking from Universal Dependency Trees<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Trees</a></strong><br><a href=/people/z/zeming-chen/>Zeming Chen</a>
|
<a href=/people/q/qiyue-gao/>Qiyue Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwcs-1--12><div class="card-body p-3 small">Dependency parsing is a tool widely used in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. However, there is hardly any work that connects dependency parsing to <a href=https://en.wikipedia.org/wiki/Monotonic_function>monotonicity</a>, which is an essential part of logic and linguistic semantics. In this paper, we present a system that automatically annotates <a href=https://en.wikipedia.org/wiki/Monotonic_function>monotonicity information</a> based on Universal Dependency parse trees. Our system utilizes surface-level monotonicity facts about <a href=https://en.wikipedia.org/wiki/Quantifier_(logic)>quantifiers</a>, <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical items</a>, and token-level polarity information. We compared our <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s performance with existing systems in the literature, including NatLog and ccg2mono, on a small evaluation dataset. Results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms NatLog and ccg2mono.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwcs-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwcs-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwcs-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.iwcs-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.iwcs-1.13/>Is that really a question? Going beyond factoid questions in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/a/aikaterini-lida-kalouli/>Aikaterini-Lida Kalouli</a>
|
<a href=/people/r/rebecca-kehlbeck/>Rebecca Kehlbeck</a>
|
<a href=/people/r/rita-sevastjanova/>Rita Sevastjanova</a>
|
<a href=/people/o/oliver-deussen/>Oliver Deussen</a>
|
<a href=/people/d/daniel-keim/>Daniel Keim</a>
|
<a href=/people/m/miriam-butt/>Miriam Butt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwcs-1--13><div class="card-body p-3 small">Research in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has mainly focused on factoid questions, with the goal of finding quick and reliable ways of matching a query to an answer. However, human discourse involves more than that : <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> contains non-canonical questions deployed to achieve specific communicative goals. In this paper, we investigate this under-studied aspect of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> by introducing a targeted <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, creating an appropriate <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and providing baseline models of diverse nature. With this, we are also able to generate useful insights on the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and open the way for future research in this direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwcs-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwcs-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwcs-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwcs-1.15/>Breeding Fillmore’s Chickens and Hatching the Eggs : Recombining Frames and Roles in Frame-Semantic Parsing<span class=acl-fixed-case>F</span>illmore’s Chickens and Hatching the Eggs: Recombining Frames and Roles in Frame-Semantic Parsing</a></strong><br><a href=/people/g/gosse-minnema/>Gosse Minnema</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwcs-1--15><div class="card-body p-3 small">Frame-semantic parsers traditionally predict <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a>, <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>frames</a>, and semantic roles in a fixed order. This paper explores the &#8216;chicken-or-egg&#8217; problem of interdependencies between these components theoretically and practically. We introduce a flexible BERT-based sequence labeling architecture that allows for predicting frames and roles independently from each other or combining them in several ways. Our results show that our setups can approximate more complex traditional <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>&#8217; performance, while allowing for a clearer view of the interdependencies between the pipeline&#8217;s components, and of how frame and role prediction models make different use of BERT&#8217;s layers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwcs-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwcs-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwcs-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwcs-1.16/>Large-scale text pre-training helps with dialogue act recognition, but not without <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a></a></strong><br><a href=/people/b/bill-noble/>Bill Noble</a>
|
<a href=/people/v/vladislav-maraev/>Vladislav Maraev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwcs-1--16><div class="card-body p-3 small">We use dialogue act recognition (DAR) to investigate how well BERT represents utterances in dialogue, and how fine-tuning and large-scale pre-training contribute to its performance. We find that while both the standard BERT pre-training and pretraining on dialogue-like data are useful, task-specific fine-tuning is essential for good performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwcs-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwcs-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwcs-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.iwcs-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.iwcs-1.22/>Variation in framing as a function of temporal reporting distance</a></strong><br><a href=/people/l/levi-remijnse/>Levi Remijnse</a>
|
<a href=/people/m/marten-postma/>Marten Postma</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwcs-1--22><div class="card-body p-3 small">In this paper, we measure variation in <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a> as a function of <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>foregrounding</a> and <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>backgrounding</a> in a co-referential corpus with a range of temporal distance. In one type of experiment, frame-annotated corpora grouped under event types were contrasted, resulting in a ranking of frames with typicality rates. In contrasting between publication dates, a different ranking of frames emerged for documents that are close to or far from the event instance. In the second type of analysis, we trained a diagnostic classifier with frame occurrences in order to let it differentiate documents based on their temporal distance class (close to or far from the event instance). The <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> performs above chance and outperforms <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> with words.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>