<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/N18-2.pdf>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></h2><p class=lead><a href=/people/m/marilyn-walker/>Marilyn Walker</a>,
<a href=/people/h/heng-ji/>Heng Ji</a>,
<a href=/people/a/amanda-stent/>Amanda Stent</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>N18-2</dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>New Orleans, Louisiana</dd><dt>Venue:</dt><dd><a href=/venues/naacl/>NAACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/N18-2>https://aclanthology.org/N18-2</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/N18-2 title="To the current version of the paper by DOI">10.18653/v1/N18-2</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/N18-2.pdf>https://aclanthology.org/N18-2.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/N18-2.pdf title="Open PDF of 'Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2018+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics%3A+Human+Language+Technologies%2C+Volume+2+%28Short+Papers%29" title="Search for 'Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2000/>Proceedings of the 2018 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></strong><br><a href=/people/m/marilyn-walker/>Marilyn Walker</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/a/amanda-stent/>Amanda Stent</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2003.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2003/>Gender Bias in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a> : Evaluation and Debiasing Methods</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2003><div class="card-body p-3 small">In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for co-reference resolution focused on <a href=https://en.wikipedia.org/wiki/Sexism>gender bias</a>, WinoBias. Our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a>. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2005/>Is Something Better than Nothing? Automatically Predicting Stance-based Arguments Using <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Small Labelled Dataset</a></strong><br><a href=/people/p/pavithra-rajendran/>Pavithra Rajendran</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a>
|
<a href=/people/s/simon-parsons/>Simon Parsons</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2005><div class="card-body p-3 small">Online reviews have become a popular portal among customers making decisions about purchasing products. A number of corpora of reviews have been widely investigated in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> in general, and, in particular, in <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>. This is a subset of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> that deals with extracting arguments and the relations among them from user-based content. A major problem faced by <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> research is the lack of human-annotated data. In this paper, we investigate the use of weakly supervised and semi-supervised methods for automatically annotating data, and thus providing large annotated datasets. We do this by building on previous work that explores the classification of opinions present in reviews based whether the stance is expressed explicitly or implicitly. In the work described here, we automatically annotate stance as implicit or explicit and our results show that the datasets we generate, although noisy, can be used to learn better models for implicit / explicit opinion classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2006/>Multi-Task Learning for Argumentation Mining in Low-Resource Settings</a></strong><br><a href=/people/c/claudia-schulz/>Claudia Schulz</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/t/tobias-kahse/>Tobias Kahse</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2006><div class="card-body p-3 small">We investigate whether and where <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning (MTL)</a> can improve performance on NLP problems related to argumentation mining (AM), in particular argument component identification. Our results show that MTL performs particularly well (and better than single-task learning) when little training data is available for the main task, a common scenario in AM. Our findings challenge previous assumptions that conceptualizations across AM datasets are divergent and that MTL is difficult for semantic or higher-level tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2007 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2007/>Neural Models for Reasoning over Multiple Mentions Using Coreference</a></strong><br><a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/q/qiao-jin/>Qiao Jin</a>
|
<a href=/people/z/zhilin-yang/>Zhilin Yang</a>
|
<a href=/people/w/william-cohen/>William Cohen</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2007><div class="card-body p-3 small">Many problems in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> require aggregating information from multiple mentions of the same entity which may be far apart in the text. Existing Recurrent Neural Network (RNN) layers are biased towards short-term dependencies and hence not suited to such tasks. We present a recurrent layer which is instead biased towards coreferent dependencies. The layer uses coreference annotations extracted from an external system to connect entity mentions belonging to the same cluster. Incorporating this layer into a state-of-the-art reading comprehension model improves performance on three datasets Wikihop, LAMBADA and the bAbi AI tasks with large gains when training data is scarce.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2010/>Natural Language Generation by Hierarchical Decoding with Linguistic Patterns</a></strong><br><a href=/people/s/shang-yu-su/>Shang-Yu Su</a>
|
<a href=/people/k/kai-ling-lo/>Kai-Ling Lo</a>
|
<a href=/people/y/yi-ting-yeh/>Yi-Ting Yeh</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2010><div class="card-body p-3 small">Natural language generation (NLG) is a critical component in spoken dialogue systems. Classic NLG can be divided into two phases : (1) sentence planning : deciding on the overall <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structure</a>, (2) surface realization : determining specific word forms and flattening the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structure</a> into a string. Many simple NLG models are based on recurrent neural networks (RNN) and sequence-to-sequence (seq2seq) model, which basically contains a encoder-decoder structure ; these NLG models generate sentences from scratch by jointly optimizing sentence planning and surface realization using a simple cross entropy loss training criterion. However, the simple encoder-decoder architecture usually suffers from generating complex and long sentences, because the decoder has to learn all grammar and diction knowledge. This paper introduces a hierarchical decoding NLG model based on linguistic patterns in different levels, and shows that the proposed method outperforms the traditional one with a smaller model size. Furthermore, the design of the hierarchical decoding is flexible and easily-extendible in various NLG systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2012/>RankME : Reliable Human Ratings for Natural Language Generation<span class=acl-fixed-case>R</span>ank<span class=acl-fixed-case>ME</span>: Reliable Human Ratings for Natural Language Generation</a></strong><br><a href=/people/j/jekaterina-novikova/>Jekaterina Novikova</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/v/verena-rieser/>Verena Rieser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2012><div class="card-body p-3 small">Human evaluation for natural language generation (NLG) often suffers from inconsistent user ratings. While previous research tends to attribute this problem to individual user preferences, we show that the quality of human judgements can also be improved by experimental design. We present a novel rank-based magnitude estimation method (RankME), which combines the use of continuous scales and relative assessments. We show that RankME significantly improves the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> and consistency of human ratings compared to traditional evaluation methods. In addition, we show that it is possible to evaluate NLG systems according to multiple, distinct criteria, which is important for error analysis. Finally, we demonstrate that RankME, in combination with Bayesian estimation of system quality, is a cost-effective alternative for ranking multiple NLG systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2013 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2013/>Sentence Simplification with Memory-Augmented Neural Networks</a></strong><br><a href=/people/t/tu-vu/>Tu Vu</a>
|
<a href=/people/b/baotian-hu/>Baotian Hu</a>
|
<a href=/people/t/tsendsuren-munkhdalai/>Tsendsuren Munkhdalai</a>
|
<a href=/people/h/hong-yu/>Hong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2013><div class="card-body p-3 small">Sentence simplification aims to simplify the content and structure of complex sentences, and thus make them easier to interpret for human readers, and easier to process for downstream NLP applications. Recent advances in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> have paved the way for novel approaches to the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In this paper, we adapt an <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> with augmented memory capacities called Neural Semantic Encoders (Munkhdalai and Yu, 2017) for sentence simplification. Our experiments demonstrate the effectiveness of our approach on different simplification datasets, both in terms of automatic evaluation measures and human judgments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2014/>A Corpus of Non-Native Written English Annotated for Metaphor<span class=acl-fixed-case>E</span>nglish Annotated for Metaphor</a></strong><br><a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a>
|
<a href=/people/c/chee-wee-leong/>Chee Wee (Ben) Leong</a>
|
<a href=/people/m/michael-flor/>Michael Flor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2014><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 240 argumentative essays written by non-native speakers of English annotated for <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a>. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is made publicly available. We provide benchmark performance of state-of-the-art systems on this new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, and explore the relationship between writing proficiency and <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor use</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2016.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2016/>An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols</a></strong><br><a href=/people/c/chaitanya-kulkarni/>Chaitanya Kulkarni</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/r/raghu-machiraju/>Raghu Machiraju</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2016><div class="card-body p-3 small">We describe an effort to annotate a corpus of natural language instructions consisting of 622 wet lab protocols to facilitate automatic or semi-automatic conversion of protocols into a machine-readable format and benefit biological research. Experimental results demonstrate the utility of our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for developing machine learning approaches to shallow semantic parsing of instructional texts. We make our annotated Wet Lab Protocol Corpus available to the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2017/>Annotation Artifacts in Natural Language Inference Data</a></strong><br><a href=/people/s/suchin-gururangan/>Suchin Gururangan</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2017><div class="card-body p-3 small">Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this <a href=https://en.wikipedia.org/wiki/Protocol_(science)>protocol</a> leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67 % of <a href=https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism>SNLI</a> (Bowman et. al, 2015) and 53 % of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> and <a href=https://en.wikipedia.org/wiki/Vagueness>vagueness</a> are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2018/>Humor Recognition Using <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a></a></strong><br><a href=/people/p/peng-yu-chen/>Peng-Yu Chen</a>
|
<a href=/people/v/von-wun-soo/>Von-Wun Soo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2018><div class="card-body p-3 small">Humor is an essential but most fascinating element in personal communication. How to build <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> to discover the structures of humor, recognize <a href=https://en.wikipedia.org/wiki/Humour>humor</a> and even generate <a href=https://en.wikipedia.org/wiki/Humour>humor</a> remains a challenge and there have been yet few attempts on it. In this paper, we construct and collect four <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with distinct joke types in both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and conduct learning experiments on humor recognition. We implement a Convolutional Neural Network (CNN) with extensive filter size, number and Highway Networks to increase the depth of networks. Results show that our model outperforms in recognition of different types of <a href=https://en.wikipedia.org/wiki/Humour>humor</a> with benchmarks collected in both English and Chinese languages on <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> in comparison to previous works.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2020.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2020/>Reference-less Measure of Faithfulness for Grammatical Error Correction</a></strong><br><a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2020><div class="card-body p-3 small">We propose USim, a semantic measure for Grammatical Error Correction (that measures the semantic faithfulness of the output to the source, thereby complementing existing reference-less measures (RLMs) for measuring the output&#8217;s grammaticality. USim operates by comparing the semantic symbolic structure of the source and the correction, without relying on manually-curated references. Our experiments establish the validity of USim, by showing that the semantic structures can be consistently applied to ungrammatical text, that valid corrections obtain a high USim similarity score to the source, and that invalid corrections obtain a lower score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2024/>Analogies in Complex Verb Meaning Shifts : the Effect of Affect in Semantic Similarity Models</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2024><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> to detect and distinguish analogies in meaning shifts between German base and complex verbs. In contrast to <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus-based studies</a>, a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> demonstrates that regular shifts represent the smallest class. Classification experiments relying on a standard similarity model successfully distinguish between four types of shifts, with verb classes boosting the performance, and affective features for abstractness, <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> and <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> representing the most salient indicators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2025/>Character-Based Neural Networks for Sentence Pair Modeling</a></strong><br><a href=/people/w/wuwei-lan/>Wuwei Lan</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2025><div class="card-body p-3 small">Sentence pair modeling is critical for many NLP tasks, such as paraphrase identification, <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic textual similarity</a>, and natural language inference. Most state-of-the-art neural models for these tasks rely on pretrained word embedding and compose sentence-level semantics in varied ways ; however, few works have attempted to verify whether we really need pretrained embeddings in these tasks. In this paper, we study how effective subword-level (character and character n-gram) representations are in sentence pair modeling. Though it is well-known that subword models are effective in tasks with single sentence input, including <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, they have not been systematically studied in sentence pair modeling tasks where the semantic and string similarities between texts matter. Our experiments show that subword models without any pretrained word embedding can achieve new state-of-the-art results on two social media datasets and competitive results on news data for paraphrase identification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2027 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2027.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2027/>Diachronic Usage Relatedness (DURel): A Framework for the Annotation of Lexical Semantic Change<span class=acl-fixed-case>DUR</span>el): A Framework for the Annotation of Lexical Semantic Change</a></strong><br><a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/s/stefanie-eckmann/>Stefanie Eckmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2027><div class="card-body p-3 small">We propose a framework that extends synchronic polysemy annotation to diachronic changes in lexical meaning, to counteract the lack of resources for evaluating computational models of lexical semantic change. Our framework exploits an intuitive notion of semantic relatedness, and distinguishes between innovative and reductive meaning changes with high inter-annotator agreement. The resulting <a href=https://en.wikipedia.org/wiki/Test_(assessment)>test set</a> for <a href=https://en.wikipedia.org/wiki/German_language>German</a> comprises ratings from five annotators for the relatedness of 1,320 use pairs across 22 target words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2029.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2029.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2029" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2029/>Discriminating between Lexico-Semantic Relations with the Specialization Tensor Model</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2029><div class="card-body p-3 small">We present a simple and effective feed-forward neural architecture for discriminating between lexico-semantic relations (synonymy, antonymy, hypernymy, and meronymy). Our Specialization Tensor Model (STM) simultaneously produces multiple different specializations of input distributional word vectors, tailored for predicting lexico-semantic relations for word pairs. STM outperforms more complex state-of-the-art architectures on two benchmark datasets and exhibits stable performance across languages. We also show that, if coupled with a bilingual distributional space, the proposed model can transfer the prediction of lexico-semantic relations to a resource-lean target language without any <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2030 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2030/>Evaluating bilingual word embeddings on the long tail</a></strong><br><a href=/people/f/fabienne-braune/>Fabienne Braune</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/t/tobias-eder/>Tobias Eder</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2030><div class="card-body p-3 small">Bilingual word embeddings are useful for bilingual lexicon induction, the task of mining translations of given words. Many studies have shown that bilingual word embeddings perform well for bilingual lexicon induction but they focused on frequent words in general domains. For many applications, bilingual lexicon induction of rare and domain-specific words is of critical importance. Therefore, we design a new task to evaluate bilingual word embeddings on rare words in different domains. We show that state-of-the-art approaches fail on this task and present simple new techniques to improve bilingual word embeddings for mining rare words. We release new gold standard datasets and code to stimulate research on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2031 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2031/>Frustratingly Easy Meta-Embedding Computing Meta-Embeddings by Averaging Source Word Embeddings</a></strong><br><a href=/people/j/joshua-coates/>Joshua Coates</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2031><div class="card-body p-3 small">Creating accurate meta-embeddings from pre-trained source embeddings has received attention lately. Methods based on global and locally-linear transformation and <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation</a> have shown to produce accurate meta-embeddings. In this paper, we show that the <a href=https://en.wikipedia.org/wiki/Arithmetic_mean>arithmetic mean</a> of two distinct word embedding sets yields a performant meta-embedding that is comparable or better than more complex meta-embedding learning methods. The result seems counter-intuitive given that <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a> in different source embeddings are not comparable and can not be simply averaged. We give insight into why <a href=https://en.wikipedia.org/wiki/Average>averaging</a> can still produce accurate meta-embedding despite the incomparability of the source vector spaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2032 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2032/>Introducing Two Vietnamese Datasets for Evaluating Semantic Models of (Dis-)Similarity and Relatedness<span class=acl-fixed-case>V</span>ietnamese Datasets for Evaluating Semantic Models of (Dis-)Similarity and Relatedness</a></strong><br><a href=/people/k/kim-anh-nguyen/>Kim Anh Nguyen</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2032><div class="card-body p-3 small">We present two novel datasets for the low-resource language Vietnamese to assess models of semantic similarity : ViCon comprises pairs of <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> and <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2037 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2037/>Similarity Measures for the Detection of Clinical Conditions with Verbal Fluency Tasks</a></strong><br><a href=/people/f/felipe-paula/>Felipe Paula</a>
|
<a href=/people/r/rodrigo-wilkens/>Rodrigo Wilkens</a>
|
<a href=/people/m/marco-idiart/>Marco Idiart</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2037><div class="card-body p-3 small">Semantic Verbal Fluency tests have been used in the detection of certain <a href=https://en.wikipedia.org/wiki/Disease>clinical conditions</a>, like <a href=https://en.wikipedia.org/wiki/Dementia>Dementia</a>. In particular, given a sequence of semantically related words, a large number of switches from one semantic class to another has been linked to clinical conditions. In this work, we investigate three similarity measures for automatically identifying switches in semantic chains : <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> from a manually constructed resource, and word association strength and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a>, both calculated from corpora. This information is used for building classifiers to distinguish healthy controls from clinical cases with early stages of Alzheimer&#8217;s Disease and Mild Cognitive Deficits. The overall results indicate that for clinical conditions the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> that use these <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measures</a> outperform those that use a gold standard taxonomy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2039 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2039/>The Word Analogy Testing Caveat</a></strong><br><a href=/people/n/natalie-schluter/>Natalie Schluter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2039><div class="card-body p-3 small">There are some important problems in the evaluation of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> using standard word analogy tests. In particular, in virtue of the assumptions made by systems generating the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>, these remain tests over randomness. We show that even supposing there were such word analogy regularities that should be detected in the word embeddings obtained via unsupervised means, standard word analogy test implementation practices provide distorted or contrived results. We raise concerns regarding the use of <a href=https://en.wikipedia.org/wiki/Principal_component_analysis>Principal Component Analysis</a> to 2 or 3 dimensions as a provision of visual evidence for the existence of word analogy relations in <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. Finally, we propose some solutions to these problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2040 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2040/>Transition-Based Chinese AMR Parsing<span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>AMR</span> Parsing</a></strong><br><a href=/people/c/chuan-wang/>Chuan Wang</a>
|
<a href=/people/b/bin-li/>Bin Li</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2040><div class="card-body p-3 small">This paper presents the first AMR parser built on the Chinese AMR bank. By applying a transition-based AMR parsing framework to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, we first investigate how well the transitions first designed for <a href=https://en.wikipedia.org/wiki/English_language>English AMR parsing</a> generalize to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and provide a comparative analysis between the transitions for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We then perform a detailed <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> to identify the major challenges in Chinese AMR parsing that we hope will inform future research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2041 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2041/>Knowledge-Enriched Two-Layered Attention Network for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/a/abhishek-kumar/>Abhishek Kumar</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2041><div class="card-body p-3 small">We propose a novel two-layered attention network based on Bidirectional Long Short-Term Memory for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. The novel two-layered attention network takes advantage of the <a href=https://en.wikipedia.org/wiki/Knowledge_base>external knowledge bases</a> to improve the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment prediction</a>. It uses the Knowledge Graph Embedding generated using the <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>. We build our model by combining the two-layered attention network with the supervised model based on Support Vector Regression using a Multilayer Perceptron network for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the benchmark dataset of SemEval 2017 Task 5. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> surpasses the top system of SemEval 2017 Task 5. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs significantly better by improving the state-of-the-art system at SemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2043/>Modeling Inter-Aspect Dependencies for Aspect-Based Sentiment Analysis</a></strong><br><a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/p/prateek-vij/>Prateek Vij</a>
|
<a href=/people/g/gangeshwar-krishnamurthy/>Gangeshwar Krishnamurthy</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2043><div class="card-body p-3 small">Aspect-based Sentiment Analysis is a fine-grained task of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment classification</a> for multiple aspects in a sentence. Present neural-based models exploit aspect and its contextual information in the sentence but largely ignore the inter-aspect dependencies. In this paper, we incorporate this pattern by simultaneous classification of all aspects in a sentence along with temporal dependency processing of their corresponding sentence representations using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent networks</a>. Results on the benchmark SemEval 2014 dataset suggest the effectiveness of our proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2045 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2045" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2045/>Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-Based Sentiment Analysis</a></strong><br><a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2045><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> have been shown to achieve impressive results for sentence-level sentiment analysis, targeted aspect-based sentiment analysis (TABSA) extraction of fine-grained opinion polarity w.r.t. a pre-defined set of aspects remains a difficult task. Motivated by recent advances in memory-augmented models for machine reading, we propose a novel architecture, utilising external memory chains with a delayed memory update mechanism to track entities. On a TABSA task, the proposed model demonstrates substantial improvements over state-of-the-art approaches, including those using external knowledge bases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2049 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2049.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276898113 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2049" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2049/>Modeling Semantic Plausibility by Injecting World Knowledge</a></strong><br><a href=/people/s/su-wang/>Su Wang</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/k/katrin-erk/>Katrin Erk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2049><div class="card-body p-3 small">Distributional data tells us that a man can swallow candy, but not that a man can swallow a paintball, since this is never attested. However both are physically plausible events. This paper introduces the task of semantic plausibility : recognizing plausible but possibly novel events. We present a new crowdsourced dataset of semantic plausibility judgments of single events such as man swallow paintball. Simple models based on <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional representations</a> perform poorly on this task, despite doing well on selection preference, but injecting manually elicited knowledge about entity properties provides a substantial performance boost. Our error analysis shows that our new dataset is a great testbed for semantic plausibility models : more sophisticated <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> and propagation could address many of the remaining errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2050 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2050/>A Bi-Model Based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling<span class=acl-fixed-case>RNN</span> Semantic Frame Parsing Model for Intent Detection and Slot Filling</a></strong><br><a href=/people/y/yu-wang/>Yu Wang</a>
|
<a href=/people/y/yilin-shen/>Yilin Shen</a>
|
<a href=/people/h/hongxia-jin/>Hongxia Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2050><div class="card-body p-3 small">Intent detection and slot filling are two main tasks for building a spoken language understanding(SLU) system. Multiple <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning based models</a> have demonstrated good results on these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. The most effective algorithms are based on the structures of sequence to sequence models (or encoder-decoder models), and generate the intents and semantic tags either using separate models. Most of the previous studies, however, either treat the <a href=https://en.wikipedia.org/wiki/Intention>intent detection</a> and slot filling as two separate parallel tasks, or use a sequence to sequence model to generate both semantic tags and <a href=https://en.wikipedia.org/wiki/Intention>intent</a>. None of the approaches consider the cross-impact between the intent detection task and the slot filling task. In this paper, new Bi-model based RNN semantic frame parsing network structures are designed to perform the intent detection and slot filling tasks jointly, by considering their cross-impact to each other using two correlated bidirectional LSTMs (BLSTM). Our Bi-model structure with a <a href=https://en.wikipedia.org/wiki/Code>decoder</a> achieves state-of-art result on the benchmark <a href=https://en.wikipedia.org/wiki/Automated_airport_weather_station>ATIS data</a>, with about 0.5 % intent accuracy improvement and 0.9 % slot filling improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2052/>A Laypeople Study on Terminology Identification across Domains and Task Definitions</a></strong><br><a href=/people/a/anna-hatty/>Anna Hätty</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2052><div class="card-body p-3 small">This paper introduces a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of term annotation. Given that even experts vary significantly in their understanding of termhood, and that term identification is mostly performed as a binary task, we offer a novel perspective to explore the common, natural understanding of what constitutes a term : Laypeople annotate single-word and multi-word terms, across four domains and across four task definitions. Analyses based on <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a> offer insights into differences in term specificity, <a href=https://en.wikipedia.org/wiki/Granularity>term granularity</a> and subtermhood.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2053 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2053/>A Novel Embedding Model for Knowledge Base Completion Based on <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Network</a></a></strong><br><a href=/people/d/dai-quoc-nguyen/>Dai Quoc Nguyen</a>
|
<a href=/people/t/tu-dinh-nguyen/>Tu Dinh Nguyen</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/d/dinh-phung/>Dinh Phung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2053><div class="card-body p-3 small">In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Embedding>embedding model</a>, named ConvKB, for knowledge base completion. Our model ConvKB advances state-of-the-art models by employing a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a>, so that it can capture global relationships and transitional characteristics between entities and relations in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. In ConvKB, each triple (head entity, relation, tail entity) is represented as a 3-column matrix where each column vector represents a triple element. This 3-column matrix is then fed to a convolution layer where multiple <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filters</a> are operated on the <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a> to generate different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature maps</a>. These <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature maps</a> are then concatenated into a single <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature vector</a> representing the input triple. The <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature vector</a> is multiplied with a <a href=https://en.wikipedia.org/wiki/Weight_vector>weight vector</a> via a <a href=https://en.wikipedia.org/wiki/Dot_product>dot product</a> to return a score. This <a href=https://en.wikipedia.org/wiki/Score_(game)>score</a> is then used to predict whether the triple is valid or not. Experiments show that ConvKB achieves better link prediction performance than previous state-of-the-art embedding models on two benchmark datasets WN18RR and FB15k-237.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2054 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2054/>Cross-language Article Linking Using Cross-Encyclopedia Entity Embedding</a></strong><br><a href=/people/c/chun-kai-wu/>Chun-Kai Wu</a>
|
<a href=/people/r/richard-tzong-han-tsai/>Richard Tzong-Han Tsai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2054><div class="card-body p-3 small">Cross-language article linking (CLAL) is the task of finding corresponding article pairs of different languages across <a href=https://en.wikipedia.org/wiki/Encyclopedia>encyclopedias</a>. This task is a difficult disambiguation problem in which one article must be selected among several candidate articles with similar titles and contents. Existing works focus on engineering text-based or link-based features for this task, which is a time-consuming job, and some of these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> are only applicable within the same <a href=https://en.wikipedia.org/wiki/Encyclopedia>encyclopedia</a>. In this paper, we address these problems by proposing cross-encyclopedia entity embedding. Unlike other works, our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> does not rely on known cross-language pairs. We apply our method to <a href=https://en.wikipedia.org/wiki/CLAL>CLAL</a> between <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a> and Chinese Baidu Baike. Our <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> improve performance relative to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by 29.62 %. Tested 30 times, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved an average improvement of 2.76 % over the current best <a href=https://en.wikipedia.org/wiki/System>system</a> (26.86 % over baseline), a statistically significant result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2055/>Identifying the Most Dominant Event in a News Article by Mining Event Coreference Relations</a></strong><br><a href=/people/p/prafulla-kumar-choubey/>Prafulla Kumar Choubey</a>
|
<a href=/people/k/kaushik-raju/>Kaushik Raju</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2055><div class="card-body p-3 small">Identifying the most dominant and central event of a document, which governs and connects other foreground and background events in the document, is useful for many applications, such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, storyline generation and <a href=https://en.wikipedia.org/wiki/Text_segmentation>text segmentation</a>. We observed that the central event of a document usually has many coreferential event mentions that are scattered throughout the document for enabling a smooth transition of subtopics. Our empirical experiments, using gold event coreference relations, have shown that the central event of a document can be well identified by mining properties of event coreference chains. But the performance drops when switching to system predicted event coreference relations. In addition, we found that the central event can be more accurately identified by further considering the number of sub-events as well as the realis status of an event.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2057/>Keep Your Bearings : Lightly-Supervised Information Extraction with <a href=https://en.wikipedia.org/wiki/Ladder_network>Ladder Networks</a> That Avoids <a href=https://en.wikipedia.org/wiki/Semantic_drift>Semantic Drift</a></a></strong><br><a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2057><div class="card-body p-3 small">We propose a novel approach to <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a> for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> that uses <a href=https://en.wikipedia.org/wiki/Ladder_network>ladder networks</a> (Rasmus et al., 2015). In particular, we focus on the task of named entity classification, defined as identifying the correct label (e.g., person or organization name) of an entity mention in a given context. Our approach is simple, efficient and has the benefit of being robust to <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a>, a dominant problem in most <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning systems</a>. We empirically demonstrate the superior performance of our <a href=https://en.wikipedia.org/wiki/System>system</a> compared to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on two standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for named entity classification. We obtain between 62 % and 200 % improvement over the state-of-art baseline on these two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2058 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2058/>Semi-Supervised Event Extraction with Paraphrase Clusters</a></strong><br><a href=/people/j/james-ferguson/>James Ferguson</a>
|
<a href=/people/c/colin-lockard/>Colin Lockard</a>
|
<a href=/people/d/daniel-s-weld/>Daniel Weld</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2058><div class="card-body p-3 small">Supervised event extraction systems are limited in their <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> due to the lack of available training data. We present a method for self-training event extraction systems by bootstrapping additional training data. This is done by taking advantage of the occurrence of multiple mentions of the same event instances across newswire articles from multiple sources. If our system can make a high-confidence extraction of some mentions in such a cluster, it can then acquire diverse training examples by adding the other mentions as well. Our experiments show significant performance improvements on multiple event extractors over ACE 2005 and TAC-KBP 2015 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2059.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2059/>Structure Regularized Neural Network for Entity Relation Classification for Chinese Literature Text<span class=acl-fixed-case>C</span>hinese Literature Text</a></strong><br><a href=/people/j/ji-wen/>Ji Wen</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/q/qi-su/>Qi Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2059><div class="card-body p-3 small">Relation classification is an important semantic processing task in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In this paper, we propose the task of <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation classification</a> for Chinese literature text. A new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Chinese_literature>Chinese literature text</a> is constructed to facilitate the study in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We present a novel model, named Structure Regularized Bidirectional Recurrent Convolutional Neural Network (SR-BRCNN), to identify the relation between entities. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns relation representations along the shortest dependency path (SDP) extracted from the structure regularized dependency tree, which has the benefits of reducing the complexity of the whole <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> significantly improves the <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> by 10.3, and outperforms the state-of-the-art approaches on <a href=https://en.wikipedia.org/wiki/Chinese_literature>Chinese literature text</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2061 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2061/>Syntactically Aware Neural Architectures for Definition Extraction</a></strong><br><a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2061><div class="card-body p-3 small">Automatically identifying definitional knowledge in text corpora (Definition Extraction or DE) is an important task with direct applications in, among others, Automatic Glossary Generation, Taxonomy Learning, Question Answering and Semantic Search. It is generally cast as a binary classification problem between definitional and non-definitional sentences. In this paper we present a set of neural architectures combining Convolutional and Recurrent Neural Networks, which are further enriched by incorporating linguistic information via syntactic dependencies. Our experimental results in the task of sentence classification, on two benchmarking DE datasets (one generic, one domain-specific), show that these models obtain consistent state of the art results. Furthermore, we demonstrate that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on clean Wikipedia-like definitions can successfully be applied to more noisy domain-specific corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2064 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2064/>Automatically Selecting the Best Dependency Annotation Design with Dynamic Oracles</a></strong><br><a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/o/ophelie-lacroix/>Ophélie Lacroix</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2064><div class="card-body p-3 small">This work introduces a new strategy to compare the numerous conventions that have been proposed over the years for expressing dependency structures and discover the one for which a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> will achieve the highest <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance. Instead of associating each sentence in the training set with a single gold reference we propose to consider a set of references encoding alternative syntactic representations. Training a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> with a dynamic oracle will then automatically select among all alternatives the reference that will be predicted with the highest <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Experiments on the UD corpora show the validity of this approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2065 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2065/>Consistent CCG Parsing over Multiple Sentences for Improved Logical Reasoning<span class=acl-fixed-case>CCG</span> Parsing over Multiple Sentences for Improved Logical Reasoning</a></strong><br><a href=/people/m/masashi-yoshikawa/>Masashi Yoshikawa</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/d/daisuke-bekki/>Daisuke Bekki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2065><div class="card-body p-3 small">In formal logic-based approaches to Recognizing Textual Entailment (RTE), a Combinatory Categorial Grammar (CCG) parser is used to parse input premises and hypotheses to obtain their logical formulas. Here, it is important that the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> processes the sentences consistently ; failing to recognize the similar syntactic structure results in inconsistent predicate argument structures among them, in which case the succeeding theorem proving is doomed to failure. In this work, we present a simple method to extend an existing CCG parser to parse a set of sentences consistently, which is achieved with an inter-sentence modeling with Markov Random Fields (MRF). When combined with existing <a href=https://en.wikipedia.org/wiki/Logic_programming>logic-based systems</a>, our method always shows improvement in the <a href=https://en.wikipedia.org/wiki/Real-time_computing>RTE</a> experiments on English and Japanese languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2066 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2066/>Exploiting Dynamic Oracles to Train Projective Dependency Parsers on Non-Projective Trees</a></strong><br><a href=/people/l/lauriane-aufrant/>Lauriane Aufrant</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2066><div class="card-body p-3 small">Because the most common transition systems are projective, training a transition-based dependency parser often implies to either ignore or rewrite the non-projective training examples, which has an adverse impact on accuracy. In this work, we propose a simple modification of dynamic oracles, which enables the use of non-projective data when training projective parsers. Evaluation on 73 treebanks shows that our method achieves significant gains (+2 to +7 UAS for the most non-projective languages) and consistently outperforms traditional projectivization and pseudo-projectivization approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2067 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2067" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2067/>Improving Coverage and Runtime Complexity for Exact Inference in Non-Projective Transition-Based Dependency Parsers</a></strong><br><a href=/people/t/tianze-shi/>Tianze Shi</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a>
|
<a href=/people/l/lillian-lee/>Lillian Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2067><div class="card-body p-3 small">We generalize Cohen, Gmez-Rodrguez, and Satta&#8217;s (2011) parser to a family of non-projective transition-based dependency parsers allowing polynomial-time exact inference. This includes novel <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> with better coverage than Cohen et al. (2011), and even a variant that reduces <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a> to O(n^6), improving over the known bounds in exact inference for non-projective transition-based parsing. We hope that this piece of theoretical work inspires design of novel <a href=https://en.wikipedia.org/wiki/Transition_system>transition systems</a> with better coverage and better <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time guarantees</a>.<tex-math>O(n^6)</tex-math>, improving over the known bounds in exact inference for non-projective transition-based parsing. We hope that this piece of theoretical work inspires design of novel transition systems with better coverage and better run-time guarantees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2068 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2068/>Towards a Variability Measure for Multiword Expressions</a></strong><br><a href=/people/c/caroline-pasquer/>Caroline Pasquer</a>
|
<a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/j/jean-yves-antoine/>Jean-Yves Antoine</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2068><div class="card-body p-3 small">One of the most outstanding properties of multiword expressions (MWEs), especially verbal ones (VMWEs), important both in theoretical models and applications, is their idiosyncratic variability. Some MWEs are always continuous, while some others admit certain types of insertions. Components of some MWEs are rarely or never modified, while some others admit either specific or unrestricted modification. This unpredictable variability profile of MWEs hinders modeling and processing them as <a href=https://en.wikipedia.org/wiki/Word_space>words-with-spaces</a> on the one hand, and as regular syntactic structures on the other hand. Since variability of MWEs is a matter of scale rather than a binary property, we propose a 2-dimensional language-independent measure of variability dedicated to verbal MWEs based on syntactic and discontinuity-related clues. We assess its relevance with respect to a linguistic benchmark and its utility for the tasks of VMWE classification and variant identification on a <a href=https://en.wikipedia.org/wiki/French_language>French corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2072 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2072" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2072/>Contextual Augmentation : Data Augmentation by Words with Paradigmatic Relations</a></strong><br><a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2072><div class="card-body p-3 small">We propose a novel <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for labeled sentences called contextual augmentation. We assume an invariance that sentences are natural even if the words in the sentences are replaced with other words with paradigmatic relations. We stochastically replace words with other words that are predicted by a bi-directional language model at the word positions. Words predicted according to a context are numerous but appropriate for the augmentation of the original words. Furthermore, we retrofit a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> with a label-conditional architecture, which allows the model to augment sentences without breaking the label-compatibility. Through the experiments for six various different text classification tasks, we demonstrate that the proposed method improves <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> based on the convolutional or recurrent neural networks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2074 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2074" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2074/>Self-Attention with Relative Position Representations</a></strong><br><a href=/people/p/peter-shaw/>Peter Shaw</a>
|
<a href=/people/j/jakob-uszkoreit/>Jakob Uszkoreit</a>
|
<a href=/people/a/ashish-vaswani/>Ashish Vaswani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2074><div class="card-body p-3 small">Relying entirely on an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>, the <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a> introduced by Vaswani et al. (2017) achieves state-of-the-art results for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>approach</a> yields improvements of 1.3 <a href=https://en.wikipedia.org/wiki/Bitwise_operation>BLEU</a> and 0.3 <a href=https://en.wikipedia.org/wiki/Bitwise_operation>BLEU</a> over <a href=https://en.wikipedia.org/wiki/Bitwise_operation>absolute position representations</a>, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2077 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2077/>Automated Paraphrase Lattice Creation for HyTER Machine Translation Evaluation<span class=acl-fixed-case>H</span>y<span class=acl-fixed-case>TER</span> Machine Translation Evaluation</a></strong><br><a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2077><div class="card-body p-3 small">We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER (Dreyer and Marcu, 2012), which exploits reference translations enriched with meaning equivalent expressions. The original HyTER metric relied on hand-crafted paraphrase networks which restricted its applicability to new data. We test, for the first time, HyTER with automatically built paraphrase lattices. We show that although the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> obtains good results on small and carefully curated data with both manually and automatically selected substitutes, it achieves medium performance on much larger and noisier datasets, demonstrating the limits of the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> for tuning and evaluation of current MT systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2078 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2078.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2078/>Exploiting <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Graph Convolutional Networks</a></strong><br><a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2078><div class="card-body p-3 small">Semantic representations have long been argued as potentially useful for enforcing meaning preservation and improving generalization performance of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation methods</a>. In this work, we are the first to incorporate information about predicate-argument structure of source sentences (namely, semantic-role representations) into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. We use Graph Convolutional Networks (GCNs) to inject a semantic bias into sentence encoders and achieve improvements in BLEU scores over the linguistic-agnostic and syntax-aware versions on the EnglishGerman language pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2079 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2079.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2079/>Incremental Decoding and Training Methods for <a href=https://en.wikipedia.org/wiki/Simultaneous_translation>Simultaneous Translation</a> in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/s/stephan-vogel/>Stephan Vogel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2079><div class="card-body p-3 small">We address the problem of <a href=https://en.wikipedia.org/wiki/Simultaneous_translation>simultaneous translation</a> by modifying the Neural MT decoder to operate with dynamically built encoder and <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. We propose a tunable agent which decides the best segmentation strategy for a user-defined BLEU loss and Average Proportion (AP) constraint. Our agent outperforms previously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova, 2016) on BLEU with a lower latency. Secondly we proposed data-driven changes to Neural MT training to better match the incremental decoding framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2080 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2080/>Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models</a></strong><br><a href=/people/d/david-vilar/>David Vilar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2080><div class="card-body p-3 small">In this paper we explore the use of Learning Hidden Unit Contribution for the task of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> was initially proposed in the context of <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> for adapting a general <a href=https://en.wikipedia.org/wiki/System>system</a> to the specific acoustic characteristics of each speaker. Similar in spirit, in a machine translation framework we want to adapt a general system to a specific domain. We show that the proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves improvements of up to 2.6 BLEU points over a general system, and up to 6 BLEU points if the initial <a href=https://en.wikipedia.org/wiki/System>system</a> has only been trained on out-of-domain data, a situation which may easily happen in practice. The good performance together with its short training time and small <a href=https://en.wikipedia.org/wiki/Memory_footprint>memory footprint</a> make it a very attractive solution for domain adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2081 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2081/>Neural Machine Translation Decoding with Terminology Constraints</a></strong><br><a href=/people/e/eva-hasler/>Eva Hasler</a>
|
<a href=/people/a/adria-de-gispert/>Adrià de Gispert</a>
|
<a href=/people/g/gonzalo-iglesias/>Gonzalo Iglesias</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2081><div class="card-body p-3 small">Despite the impressive quality improvements yielded by neural machine translation (NMT) systems, controlling their translation output to adhere to user-provided terminology constraints remains an open problem. We describe our approach to constrained neural decoding based on finite-state machines and multi-stack decoding which supports target-side constraints as well as <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> with corresponding aligned input text spans. We demonstrate the performance of our framework on multiple translation tasks and motivate the need for constrained decoding with attentions as a means of reducing misplacement and duplication when translating user constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2082 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2082" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2082/>On the Evaluation of Semantic Phenomena in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> Using Natural Language Inference</a></strong><br><a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/j/james-glass/>James Glass</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2082><div class="card-body p-3 small">We propose a process for investigating the extent to which <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a> arising from neural machine translation (NMT) systems encode distinct semantic phenomena. We use these representations as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to train a natural language inference (NLI) classifier based on datasets recast from existing semantic annotations. In applying this process to a representative NMT system, we find its <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> appears most suited to supporting inferences at the syntax-semantics interface, as compared to <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora resolution</a> requiring <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>. We conclude with a discussion on the merits and potential deficiencies of the existing process, and how it may be improved and extended as a broader framework for evaluating semantic coverage</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2083 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2083/>Using Word Vectors to Improve Word Alignments for Low Resource Machine Translation</a></strong><br><a href=/people/n/nima-pourdamghani/>Nima Pourdamghani</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2083><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for improving <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignments</a> using word similarities. This method is based on encouraging common alignment links between semantically similar words. We use <a href=https://en.wikipedia.org/wiki/Word_vector>word vectors</a> trained on <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> to estimate <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a>. Our experiments on translating fifteen languages into English show consistent BLEU score improvements across the languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2084 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2084.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2084" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2084/>When and Why Are Pre-Trained Word Embeddings Useful for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>?</a></strong><br><a href=/people/y/ye-qi/>Ye Qi</a>
|
<a href=/people/d/devendra-sachan/>Devendra Sachan</a>
|
<a href=/people/m/matthieu-felix/>Matthieu Felix</a>
|
<a href=/people/s/sarguna-padmanabhan/>Sarguna Padmanabhan</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2084><div class="card-body p-3 small">The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora can not be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for <a href=https://en.wikipedia.org/wiki/Nuclear_magnetic_resonance_spectroscopy>NMT</a> has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases providing gains of up to 20 BLEU points in the most favorable setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2086 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2086/>The <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>Computational Complexity</a> of Distinctive Feature Minimization in Phonology</a></strong><br><a href=/people/h/hubie-chen/>Hubie Chen</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2086><div class="card-body p-3 small">We analyze the complexity of the problem of determining whether a set of <a href=https://en.wikipedia.org/wiki/Phoneme>phonemes</a> forms a natural class and, if so, that of finding the minimal feature specification for the <a href=https://en.wikipedia.org/wiki/Class_(set_theory)>class</a>. A standard assumption in <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a> is that finding a minimal feature specification is an automatic part of <a href=https://en.wikipedia.org/wiki/Language_acquisition>acquisition</a> and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>. We find that the natural class decision problem is tractable (i.e. is in P), while the minimization problem is not ; the decision version of the problem which determines whether a <a href=https://en.wikipedia.org/wiki/Natural_class>natural class</a> can be defined with k features or less is NP-complete. We also show that, empirically, a <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy algorithm</a> for finding minimal feature specifications will sometimes fail, and thus can not be assumed to be the basis for human performance in solving the problem.<tex-math>k</tex-math> features or less is NP-complete. We also show that, empirically, a greedy algorithm for finding minimal feature specifications will sometimes fail, and thus cannot be assumed to be the basis for human performance in solving the problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2089 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2089" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2089/>Crowdsourcing Question-Answer Meaning Representations</a></strong><br><a href=/people/j/julian-michael/>Julian Michael</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/l/luheng-he/>Luheng He</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2089><div class="card-body p-3 small">We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs. We develop a crowdsourcing scheme to show that QAMRs can be labeled with very little training, and gather a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with over 5,000 sentences and 100,000 questions. A qualitative analysis demonstrates that the crowd-generated question-answer pairs cover the vast majority of predicate-argument relationships in existing datasets (including <a href=https://en.wikipedia.org/wiki/PropBank>PropBank</a>, NomBank, and QA-SRL) along with many previously under-resourced ones, including implicit arguments and relations. We also report baseline models for question generation and answering, and summarize a recent approach for using QAMR labels to improve an Open IE system. These results suggest the freely available QAMR data and annotation scheme should support significant future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2091 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2091/>Robust Machine Comprehension Models via Adversarial Training</a></strong><br><a href=/people/y/yicheng-wang/>Yicheng Wang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2091><div class="card-body p-3 small">It is shown that many published models for the Stanford Question Answering Dataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50 % decrease in F1 score during adversarial evaluation based on the AddSent (Jia and Liang, 2017) algorithm. It has also been shown that retraining models on data generated by AddSent has limited effect on their <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>. We propose a novel alternative adversary-generation algorithm, AddSentDiverse, that significantly increases the variance within the adversarial training data by providing effective examples that punish the model for making certain superficial assumptions. Further, in order to improve robustness to AddSent&#8217;s semantic perturbations (e.g., antonyms), we jointly improve the model&#8217;s semantic-relationship learning capabilities in addition to our AddSentDiverse-based adversarial training data augmentation. With these additions, we show that we can make a state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly more robust, achieving a 36.5 % increase in F1 score under many different types of <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial evaluation</a> while maintaining performance on the regular SQuAD task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2092 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2092/>Simple and Effective Semi-Supervised Question Answering</a></strong><br><a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/d/danish-danish/>Danish Danish</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2092><div class="card-body p-3 small">Recent success of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for the task of extractive Question Answering (QA) is hinged on the availability of large annotated corpora. However, large domain specific annotated corpora are limited and expensive to construct. In this work, we envision a <a href=https://en.wikipedia.org/wiki/System>system</a> where the end user specifies a set of base documents and only a few labelled examples. Our system exploits the document structure to create cloze-style questions from these base documents ; pre-trains a powerful <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> on the cloze style questions ; and further fine-tunes the model on the labeled examples. We evaluate our proposed system across three diverse datasets from different domains, and find it to be highly effective with very little labeled data. We attain more than 50 % F1 score on SQuAD and TriviaQA with less than a thousand labelled examples. We are also releasing a set of 3.2 M cloze-style questions for practitioners to use while building <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2094" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2094/>Community Member Retrieval on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> Using Textual Information</a></strong><br><a href=/people/a/aaron-jaech/>Aaron Jaech</a>
|
<a href=/people/s/shobhit-hathi/>Shobhit Hathi</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2094><div class="card-body p-3 small">This paper addresses the problem of community membership detection using only text features in a scenario where a small number of positive labeled examples defines the community. The solution introduces an unsupervised proxy task for learning user embeddings : user re-identification. Experiments with 16 different communities show that the resulting <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are more effective for community membership identification than common unsupervised representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2096 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2096/>Predicting Foreign Language Usage from English-Only Social Media Posts<span class=acl-fixed-case>E</span>nglish-Only Social Media Posts</a></strong><br><a href=/people/s/svitlana-volkova/>Svitlana Volkova</a>
|
<a href=/people/s/stephen-ranshous/>Stephen Ranshous</a>
|
<a href=/people/l/lawrence-phillips/>Lawrence Phillips</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2096><div class="card-body p-3 small">Social media is known for its multi-cultural and multilingual interactions, a natural product of which is <a href=https://en.wikipedia.org/wiki/Code_mixing>code-mixing</a>. Multilingual speakers mix languages they tweet to address a different audience, express certain feelings, or attract attention. This paper presents a large-scale analysis of 6 million tweets produced by 27 thousand multilingual users speaking 12 other languages besides <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We rely on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to build <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a> to infer non-English languages that users speak exclusively from their <a href=https://en.wikipedia.org/wiki/Twitter>English tweets</a>. Unlike native language identification task, we rely on large amounts of informal social media communications rather than <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>ESL essays</a>. We contrast the predictive power of the state-of-the-art <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> trained on lexical, syntactic, and stylistic signals with <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a> learned from word, character and byte representations extracted from <a href=https://en.wikipedia.org/wiki/Twitter>English only tweets</a>. We report that <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a>, style and <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> are the most predictive of non-English languages that users speak on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Neural network models learned from byte representations of user content combined with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> yield the best performance. Finally, by analyzing cross-lingual transfer the influence of non-English languages on various levels of linguistic performance in English, we present novel findings on stylistic and syntactic variations across speakers of 12 languages in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2098 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2098/>A Mixed Hierarchical Attention Based Encoder-Decoder Approach for Standard Table Summarization</a></strong><br><a href=/people/p/parag-jain/>Parag Jain</a>
|
<a href=/people/a/anirban-laha/>Anirban Laha</a>
|
<a href=/people/k/karthik-sankaranarayanan/>Karthik Sankaranarayanan</a>
|
<a href=/people/p/preksha-nema/>Preksha Nema</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a>
|
<a href=/people/s/shreyas-shetty/>Shreyas Shetty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2098><div class="card-body p-3 small">Structured data summarization involves <a href=https://en.wikipedia.org/wiki/Natural-language_generation>generation of natural language summaries</a> from <a href=https://en.wikipedia.org/wiki/Structured_data>structured input data</a>. In this work, we consider summarizing structured data occurring in the form of tables as they are prevalent across a wide variety of domains. We formulate the standard table summarization problem, which deals with tables conforming to a single predefined schema. To this end, we propose a mixed hierarchical attention based encoder-decoder model which is able to leverage the structure in addition to the content of the tables. Our experiments on the publicly available weathergov dataset show around 18 BLEU (around 30 %) improvement over the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2099 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2099.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2099/>Effective <a href=https://en.wikipedia.org/wiki/Crowdsourcing>Crowdsourcing</a> for a New Type of Summarization Task</a></strong><br><a href=/people/y/youxuan-jiang/>Youxuan Jiang</a>
|
<a href=/people/c/catherine-finegan-dollak/>Catherine Finegan-Dollak</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/w/walter-lasecki/>Walter Lasecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2099><div class="card-body p-3 small">Most <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> research focuses on summarizing the entire given text, but in practice readers are often interested in only one aspect of the document or conversation. We propose targeted summarization as an umbrella category for summarization tasks that intentionally consider only parts of the input data. This covers query-based summarization, update summarization, and a new task we propose where the goal is to summarize a particular aspect of a document. However, collecting data for this new task is hard because directly asking annotators (e.g., crowd workers) to write summaries leads to data with low accuracy when there are a large number of facts to include. We introduce a novel crowdsourcing workflow, Pin-Refine, that allows us to collect high-quality summaries for our task, a necessary step for the development of automatic systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2100 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2100.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2100/>Key2Vec : Automatic Ranked Keyphrase Extraction from Scientific Articles using Phrase Embeddings<span class=acl-fixed-case>K</span>ey2<span class=acl-fixed-case>V</span>ec: Automatic Ranked Keyphrase Extraction from Scientific Articles using Phrase Embeddings</a></strong><br><a href=/people/d/debanjan-mahata/>Debanjan Mahata</a>
|
<a href=/people/j/john-kuriakose/>John Kuriakose</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2100><div class="card-body p-3 small">Keyphrase extraction is a fundamental task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> that facilitates mapping of documents to a set of representative phrases. In this paper, we present an unsupervised technique (Key2Vec) that leverages phrase embeddings for ranking keyphrases extracted from scientific articles. Specifically, we propose an effective way of processing text documents for training multi-word phrase embeddings that are used for thematic representation of scientific articles and ranking of keyphrases extracted from them using theme-weighted PageRank. Evaluations are performed on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> producing state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2101 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2101/>Learning to Generate Wikipedia Summaries for Underserved Languages from Wikidata<span class=acl-fixed-case>W</span>ikipedia Summaries for Underserved Languages from <span class=acl-fixed-case>W</span>ikidata</a></strong><br><a href=/people/l/lucie-aimee-kaffee/>Lucie-Aimée Kaffee</a>
|
<a href=/people/h/hady-elsahar/>Hady Elsahar</a>
|
<a href=/people/p/pavlos-vougiouklis/>Pavlos Vougiouklis</a>
|
<a href=/people/c/christophe-gravier/>Christophe Gravier</a>
|
<a href=/people/f/frederique-laforest/>Frédérique Laforest</a>
|
<a href=/people/j/jonathon-hare/>Jonathon Hare</a>
|
<a href=/people/e/elena-simperl/>Elena Simperl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2101><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> exists in 287 languages, its content is unevenly distributed among them. In this work, we investigate the generation of open domain Wikipedia summaries in underserved languages using <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a>. To this end, we propose a neural network architecture equipped with copy actions that learns to generate single-sentence and comprehensible textual summaries from Wikidata triples. We demonstrate the effectiveness of the proposed approach by evaluating it against a set of baselines on two languages of different natures : <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, a morphological rich language with a larger vocabulary than <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and <a href=https://en.wikipedia.org/wiki/Esperanto>Esperanto</a>, a <a href=https://en.wikipedia.org/wiki/Constructed_language>constructed language</a> known for its easy acquisition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2102 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2102/>Multi-Reward Reinforced Summarization with Saliency and Entailment</a></strong><br><a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2102><div class="card-body p-3 small">Abstractive text summarization is the task of compressing and rewriting a long document into a short summary while maintaining saliency, <a href=https://en.wikipedia.org/wiki/Logical_consequence>directed logical entailment</a>, and <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>non-redundancy</a>. In this work, we address these three important aspects of a good summary via a reinforcement learning approach with two novel reward functions : ROUGESal and Entail, on top of a coverage-based baseline. The ROUGESal reward modifies the ROUGE metric by up-weighting the salient phrases / words detected via a keyphrase classifier. The Entail reward gives high (length-normalized) scores to logically-entailed summaries using an entailment classifier. Further, we show superior performance improvement when these <a href=https://en.wikipedia.org/wiki/Reward_system>rewards</a> are combined with traditional metric (ROUGE) based rewards, via our novel and effective multi-reward approach of optimizing multiple rewards simultaneously in alternate mini-batches. Our method achieves the new state-of-the-art results on CNN / Daily Mail dataset as well as strong improvements in a test-only transfer setup on DUC-2002.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2103 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2103/>Objective Function Learning to Match Human Judgements for Optimization-Based Summarization</a></strong><br><a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2103><div class="card-body p-3 small">Supervised summarization systems usually rely on supervision at the sentence or n-gram level provided by automatic metrics like ROUGE, which act as noisy proxies for human judgments. In this work, we learn a summary-level scoring function including human judgments as supervision and automatically generated data as <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a>. We extract summaries with a <a href=https://en.wikipedia.org/wiki/Genetic_algorithm>genetic algorithm</a> using as a <a href=https://en.wikipedia.org/wiki/Fitness_function>fitness function</a>. We observe strong and promising performances across datasets in both automatic and manual evaluation.<tex-math>\\theta</tex-math> including human judgments as supervision and automatically generated data as regularization. We extract summaries with a genetic algorithm using <tex-math>\\theta</tex-math> as a fitness function. We observe strong and promising performances across datasets in both automatic and manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2105 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2105.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2105" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2105/>Unsupervised Keyphrase Extraction with Multipartite Graphs</a></strong><br><a href=/people/f/florian-boudin/>Florian Boudin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2105><div class="card-body p-3 small">We propose an unsupervised keyphrase extraction model that encodes topical information within a multipartite graph structure. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> represents keyphrase candidates and topics in a single <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph</a> and exploits their mutually reinforcing relationship to improve candidate ranking. We further introduce a novel mechanism to incorporate keyphrase selection preferences into the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Experiments conducted on three widely used datasets show significant improvements over state-of-the-art <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph-based models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2106 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2106/>Where Have I Heard This Story Before? Identifying Narrative Similarity in Movie Remakes<span class=acl-fixed-case>I</span> Heard This Story Before? Identifying Narrative Similarity in Movie Remakes</a></strong><br><a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a>
|
<a href=/people/s/shashank-srivastava/>Shashank Srivastava</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2106><div class="card-body p-3 small">People can identify correspondences between narratives in everyday life. For example, an analogy with the <a href=https://en.wikipedia.org/wiki/Cinderella>Cinderella story</a> may be made in describing the unexpected success of an underdog in seemingly different stories. We present a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for story understanding : identifying instances of similar narratives from a collection of narrative texts. We present an initial approach for this problem, which finds correspondences between narratives in terms of <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>plot events</a>, and resemblances between characters and their <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>social relationships</a>. Our approach yields an 8 % absolute improvement in performance over a competitive information-retrieval baseline on a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>plot summaries</a> of 577 movie remakes from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2107 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277671532 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2107/>Multimodal Emoji Prediction</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/f/francesco-ronzano/>Francesco Ronzano</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2107><div class="card-body p-3 small">Emojis are small images that are commonly included in <a href=https://en.wikipedia.org/wiki/SMS>social media text messages</a>. The combination of visual and textual content in the same message builds up a modern way of communication, that <a href=https://en.wikipedia.org/wiki/Automation>automatic systems</a> are not used to deal with. In this paper we extend recent advances in emoji prediction by putting forward a <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multimodal approach</a> that is able to predict <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in <a href=https://en.wikipedia.org/wiki/Instagram>Instagram posts</a>. Instagram posts are composed of <a href=https://en.wikipedia.org/wiki/Photograph>pictures</a> together with texts which sometimes include <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a>. We show that these <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> can be predicted by using the text, but also using the picture. Our main finding is that incorporating the two synergistic modalities, in a combined model, improves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in an emoji prediction task. This result demonstrates that these two modalities (text and images) encode different information on the use of <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and therefore can complement each other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2108 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277672817 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2108/>Higher-Order Coreference Resolution with Coarse-to-Fine Inference</a></strong><br><a href=/people/k/kenton-lee/>Kenton Lee</a>
|
<a href=/people/l/luheng-he/>Luheng He</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2108><div class="card-body p-3 small">We introduce a <a href=https://en.wikipedia.org/wiki/Differentiable_function>fully-differentiable approximation</a> to higher-order inference for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. Our approach uses the antecedent distribution from a span-ranking architecture as an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to iteratively refine span representations. This enables the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to softly consider multiple hops in the predicted clusters. To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilinear factor, enabling more aggressive pruning without hurting <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Compared to the existing state-of-the-art span-ranking approach, our model significantly improves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the English OntoNotes benchmark, while being far more computationally efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2109 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277673868 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2109/>Non-Projective Dependency Parsing with Non-Local Transitions</a></strong><br><a href=/people/d/daniel-fernandez-gonzalez/>Daniel Fernández-González</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2109><div class="card-body p-3 small">We present a novel transition system, based on the Covington non-projective parser, introducing non-local transitions that can directly create arcs involving nodes to the left of the current focus positions. This avoids the need for long sequences of No-Arcs transitions to create long-distance arcs, thus alleviating <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. The resulting <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> outperforms the original version and achieves the best accuracy on the Stanford Dependencies conversion of the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> among greedy transition-based parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2110 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2110/>Detecting Linguistic Characteristics of Alzheimer’s Dementia by Interpreting Neural Models<span class=acl-fixed-case>A</span>lzheimer’s Dementia by Interpreting Neural Models</a></strong><br><a href=/people/s/sweta-karlekar/>Sweta Karlekar</a>
|
<a href=/people/t/tong-niu/>Tong Niu</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2110><div class="card-body p-3 small">Alzheimer&#8217;s disease (AD) is an irreversible and progressive brain disease that can be stopped or slowed down with <a href=https://en.wikipedia.org/wiki/Therapy>medical treatment</a>. Language changes serve as a sign that a patient&#8217;s cognitive functions have been impacted, potentially leading to early diagnosis. In this work, we use <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP techniques</a> to classify and analyze the <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic characteristics</a> of AD patients using the DementiaBank dataset. We apply three neural models based on CNNs, LSTM-RNNs, and their combination, to distinguish between language samples from AD and control patients. We achieve a new <a href=https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables>independent benchmark accuracy</a> for the AD classification task. More importantly, we next interpret what these neural models have learned about the linguistic characteristics of AD patients, via analysis based on activation clustering and first-derivative saliency techniques. We then perform novel automatic pattern discovery inside activation clusters, and consolidate AD patients&#8217; distinctive grammar patterns. Additionally, we show that first derivative saliency can not only rediscover previous language patterns of AD patients, but also shed light on the limitations of neural models. Lastly, we also include analysis of gender-separated AD data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2112 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2112/>Feudal Reinforcement Learning for <a href=https://en.wikipedia.org/wiki/Dialogue_management>Dialogue Management</a> in Large Domains</a></strong><br><a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2112><div class="card-body p-3 small">Reinforcement learning (RL) is a promising approach to solve dialogue policy optimisation. Traditional RL algorithms, however, fail to scale to large domains due to the curse of dimensionality. We propose a novel Dialogue Management architecture, based on Feudal RL, which decomposes the decision into two steps ; a first step where a master policy selects a subset of primitive actions, and a second step where a primitive action is chosen from the selected subset. The structural information included in the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>domain ontology</a> is used to abstract the dialogue state space, taking the decisions at each step using different parts of the abstracted state. This, combined with an information sharing mechanism between slots, increases the scalability to <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>large domains</a>. We show that an implementation of this approach, based on Deep-Q Networks, significantly outperforms previous state of the art in several dialogue domains and environments, without the need of any additional reward signal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2113 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2113/>Evaluating Historical Text Normalization Systems : How Well Do They Generalize?</a></strong><br><a href=/people/a/alexander-robertson/>Alexander Robertson</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2113><div class="card-body p-3 small">We highlight several issues in the evaluation of historical text normalization systems that make it hard to tell how well these <a href=https://en.wikipedia.org/wiki/System>systems</a> would actually work in practicei.e., for new datasets or languages ; in comparison to more nave systems ; or as a preprocessing step for downstream NLP tools. We illustrate these issues and exemplify our proposed evaluation practices by comparing two neural models against a nave baseline system. We show that the neural models generalize well to unseen words in tests on five languages ; nevertheless, they provide no clear benefit over the nave baseline for downstream POS tagging of an English historical collection. We conclude that future work should include more rigorous <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>, including both intrinsic and extrinsic measures where possible.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2115 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2115.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2115/>Natural Language to Structured Query Generation via Meta-Learning</a></strong><br><a href=/people/p/po-sen-huang/>Po-Sen Huang</a>
|
<a href=/people/c/chenglong-wang/>Chenglong Wang</a>
|
<a href=/people/r/rishabh-singh/>Rishabh Singh</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2115><div class="card-body p-3 small">In conventional supervised training, a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is trained to fit all the training examples. However, having a monolithic model may not always be the best strategy, as examples could vary widely. In this work, we explore a different learning protocol that treats each example as a unique pseudo-task, by reducing the original learning problem to a few-shot meta-learning scenario with the help of a domain-dependent relevance function. When evaluated on the WikiSQL dataset, our approach leads to faster convergence and achieves 1.1%5.4 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>absolute accuracy</a> gains over the non-meta-learning counterparts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2118 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2118/>Slot-Gated Modeling for Joint Slot Filling and Intent Prediction</a></strong><br><a href=/people/c/chih-wen-goo/>Chih-Wen Goo</a>
|
<a href=/people/g/guang-gao/>Guang Gao</a>
|
<a href=/people/y/yun-kai-hsu/>Yun-Kai Hsu</a>
|
<a href=/people/c/chih-li-huo/>Chih-Li Huo</a>
|
<a href=/people/t/tsung-chieh-chen/>Tsung-Chieh Chen</a>
|
<a href=/people/k/keng-wei-hsu/>Keng-Wei Hsu</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2118><div class="card-body p-3 small">Attention-based recurrent neural network models for joint intent detection and slot filling have achieved the state-of-the-art performance, while they have independent attention weights. Considering that slot and <a href=https://en.wikipedia.org/wiki/Intention>intent</a> have the strong relationship, this paper proposes a slot gate that focuses on learning the relationship between <a href=https://en.wikipedia.org/wiki/Intention>intent</a> and slot attention vectors in order to obtain better semantic frame results by the <a href=https://en.wikipedia.org/wiki/Global_optimization>global optimization</a>. The experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> significantly improves sentence-level semantic frame accuracy with 4.2 % and 1.9 % relative improvement compared to the attentional model on benchmark ATIS and Snips datasets respectively</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2119 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2119/>An Evaluation of Image-Based Verb Prediction Models against Human Eye-Tracking Data</a></strong><br><a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/f/frank-keller/>Frank Keller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2119><div class="card-body p-3 small">Recent research in language and vision has developed <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for predicting and disambiguating verbs from <a href=https://en.wikipedia.org/wiki/Image>images</a>. Here, we ask whether the predictions made by such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> correspond to human intuitions about visual verbs. We show that the image regions a verb prediction model identifies as salient for a given verb correlate with the regions fixated by human observers performing a verb classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2120 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2120" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2120/>Learning to Color from Language</a></strong><br><a href=/people/v/varun-manjunatha/>Varun Manjunatha</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/l/larry-davis/>Larry Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2120><div class="card-body p-3 small">Automatic colorization is the process of adding <a href=https://en.wikipedia.org/wiki/Color>color</a> to <a href=https://en.wikipedia.org/wiki/Grayscale>greyscale images</a>. We condition this <a href=https://en.wikipedia.org/wiki/Process_(computing)>process</a> on <a href=https://en.wikipedia.org/wiki/Language>language</a>, allowing end users to manipulate a colorized image by feeding in different captions. We present two different architectures for language-conditioned colorization, both of which produce more accurate and plausible colorizations than a language-agnostic version. Furthermore, we demonstrate through crowdsourced experiments that we can dramatically alter <a href=https://en.wikipedia.org/wiki/Colorization>colorizations</a> simply by manipulating descriptive color words in captions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2125 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2125/>Watch, Listen, and Describe : Globally and Locally Aligned Cross-Modal Attentions for Video Captioning</a></strong><br><a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/y/yuan-fang-wang/>Yuan-Fang Wang</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2125><div class="card-body p-3 small">A major challenge for video captioning is to combine audio and visual cues. Existing multi-modal fusion methods have shown encouraging results in video understanding. However, the temporal structures of multiple modalities at different granularities are rarely explored, and how to selectively fuse the multi-modal representations at different levels of details remains uncharted. In this paper, we propose a novel hierarchically aligned cross-modal attention (HACA) framework to learn and selectively fuse both global and local temporal dynamics of different modalities. Furthermore, for the first time, we validate the superior performance of the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep audio features</a> on the video captioning task. Finally, our HACA model significantly outperforms the previous best systems and achieves new state-of-the-art results on the widely used MSR-VTT dataset.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>