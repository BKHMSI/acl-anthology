<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Second Workshop on Insights from Negative Results in NLP - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the Second Workshop on Insights from Negative Results in NLP</h2><p class=lead><a href=/people/j/joao-sedoc/>João Sedoc</a>,
<a href=/people/a/anna-rogers/>Anna Rogers</a>,
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>,
<a href=/people/s/shabnam-tafreshi/>Shabnam Tafreshi</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.insights-1</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online and Punta Cana, Dominican Republic</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/insights/>insights</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.insights-1>https://aclanthology.org/2021.insights-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Second+Workshop+on+Insights+from+Negative+Results+in+NLP" title="Search for 'Proceedings of the Second Workshop on Insights from Negative Results in NLP' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.0/>Proceedings of the Second Workshop on Insights from Negative Results in NLP</a></strong><br><a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/s/shabnam-tafreshi/>Shabnam Tafreshi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.insights-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.1/>Corrected CBOW Performs as well as <a href=https://en.wikipedia.org/wiki/Skip-gram>Skip-gram</a><span class=acl-fixed-case>CBOW</span> Performs as well as Skip-gram</a></strong><br><a href=/people/o/ozan-irsoy/>Ozan İrsoy</a>
|
<a href=/people/a/adrian-benton/>Adrian Benton</a>
|
<a href=/people/k/karl-stratos/>Karl Stratos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--1><div class="card-body p-3 small">Mikolov et al. (2013a) observed that continuous bag-of-words (CBOW) word embeddings tend to underperform Skip-gram (SG) embeddings, and this finding has been reported in subsequent works. We find that these observations are driven not by fundamental differences in their training objectives, but more likely on faulty negative sampling CBOW implementations in popular <a href=https://en.wikipedia.org/wiki/Library_(computing)>libraries</a> such as the official implementation, word2vec.c, and <a href=https://en.wikipedia.org/wiki/Gensim>Gensim</a>. We show that after correcting a bug in the CBOW gradient update, one can learn CBOW word embeddings that are fully competitive with SG on various intrinsic and extrinsic tasks, while being many times faster to train.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.3/>BERT Can not Align Characters<span class=acl-fixed-case>BERT</span> Cannot Align Characters</a></strong><br><a href=/people/a/antonis-maronikolakis/>Antonis Maronikolakis</a>
|
<a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--3><div class="card-body p-3 small">In previous work, it has been shown that BERT can adequately align cross-lingual sentences on the <a href=https://en.wikipedia.org/wiki/Syntax>word level</a>. Here we investigate whether BERT can also operate as a char-level aligner. The languages examined are <a href=https://en.wikipedia.org/wiki/English_language>English</a>, Fake <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>. We show that the closer two languages are, the better BERT can align them on the character level. BERT indeed works well in <a href=https://en.wikipedia.org/wiki/English_language>English</a> to Fake English alignment, but this does not generalize to natural languages to the same extent. Nevertheless, the proximity of two languages does seem to be a factor. English is more related to <a href=https://en.wikipedia.org/wiki/German_language>German</a> than to <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a> and this is reflected in how well BERT aligns them ; <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a> is better than <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>. We examine multiple setups and show that the <a href=https://en.wikipedia.org/wiki/Similarity_matrix>similarity matrices</a> for <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a> show weaker relations the further apart two languages are.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.4/>Two Heads are Better than One? Verification of Ensemble Effect in Neural Machine Translation</a></strong><br><a href=/people/c/chanjun-park/>Chanjun Park</a>
|
<a href=/people/s/sungjin-park/>Sungjin Park</a>
|
<a href=/people/s/seolhwa-lee/>Seolhwa Lee</a>
|
<a href=/people/t/taesun-whang/>Taesun Whang</a>
|
<a href=/people/h/heui-seok-lim/>Heuiseok Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--4><div class="card-body p-3 small">In the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, <a href=https://en.wikipedia.org/wiki/Musical_ensemble>ensembles</a> are broadly known to be effective in improving performance. This paper analyzes how ensemble of neural machine translation (NMT) models affect performance improvement by designing various experimental setups (i.e., intra-, inter-ensemble, and non-convergence ensemble). To an in-depth examination, we analyze each ensemble method with respect to several aspects such as different attention models and vocab strategies. Experimental results show that ensembling is not always resulting in performance increases and give noteworthy negative findings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.8/>Comparing Euclidean and Hyperbolic Embeddings on the <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> Nouns Hypernymy Graph<span class=acl-fixed-case>E</span>uclidean and Hyperbolic Embeddings on the <span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Nouns Hypernymy Graph</a></strong><br><a href=/people/s/sameer-bansal/>Sameer Bansal</a>
|
<a href=/people/a/adrian-benton/>Adrian Benton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--8><div class="card-body p-3 small">Nickel and Kiela (2017) present a new method for embedding tree nodes in the Poincare ball, and suggest that these hyperbolic embeddings are far more effective than Euclidean embeddings at embedding nodes in large, hierarchically structured graphs like the WordNet nouns hypernymy tree. This is especially true in low dimensions (Nickel and Kiela, 2017, Table 1). In this work, we seek to reproduce their experiments on embedding and reconstructing the WordNet nouns hypernymy graph. Counter to what they report, we find that Euclidean embeddings are able to represent this <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> at least as well as Poincare embeddings, when allowed at least 50 dimensions. We note that this does not diminish the significance of their work given the impressive performance of hyperbolic embeddings in very low-dimensional settings. However, given the wide influence of their work, our aim here is to present an updated and more accurate comparison between the Euclidean and hyperbolic embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.insights-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.12/>The Highs and Lows of Simple Lexical Domain Adaptation Approaches for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/p/pinzhen-chen/>Pinzhen Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--12><div class="card-body p-3 small">Machine translation systems are vulnerable to domain mismatch, especially in a low-resource scenario. Out-of-domain translations are often of poor quality and prone to <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a>, due to <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a> and the <a href=https://en.wikipedia.org/wiki/Translator_(computing)>decoder</a> acting as a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. We adopt two approaches to alleviate this problem : lexical shortlisting restricted by IBM statistical alignments, and hypothesis reranking based on <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a>. The <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> are computationally cheap and show success on low-resource out-of-domain test sets. However, the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> lose advantage when there is sufficient data or too great domain mismatch. This is due to both the IBM model losing its advantage over the implicitly learned neural alignment, and issues with subword segmentation of unseen words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.13/>Backtranslation in Neural Morphological Inflection</a></strong><br><a href=/people/l/ling-liu/>Ling Liu</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--13><div class="card-body p-3 small">Backtranslation is a common technique for leveraging unlabeled data in low-resource scenarios in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The method is directly applicable to morphological inflection generation if unlabeled word forms are available. This paper evaluates the potential of backtranslation for morphological inflection using <a href=https://en.wikipedia.org/wiki/Data>data</a> from six languages with labeled data drawn from the SIGMORPHON shared task resource and unlabeled data from different sources. Our core finding is that backtranslation can offer modest improvements in low-resource scenarios, but only if the unlabeled data is very clean and has been filtered by the same annotation standards as the labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.insights-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.19/>Challenging the Semi-Supervised VAE Framework for Text Classification<span class=acl-fixed-case>VAE</span> Framework for Text Classification</a></strong><br><a href=/people/g/ghazi-felhi/>Ghazi Felhi</a>
|
<a href=/people/j/joseph-le-roux/>Joseph Le Roux</a>
|
<a href=/people/d/djame-seddah/>Djamé Seddah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--19><div class="card-body p-3 small">Semi-Supervised Variational Autoencoders (SSVAEs) are widely used models for data efficient learning. In this paper, we question the adequacy of the standard design of sequence SSVAEs for the task of text classification as we exhibit two sources of overcomplexity for which we provide simplifications. These simplifications to SSVAEs preserve their theoretical soundness while providing a number of practical advantages in the semi-supervised setup where the result of training is a text classifier. These simplifications are the removal of (i) the Kullback-Liebler divergence from its objective and (ii) the fully unobserved latent variable from its probabilistic model. These changes relieve users from choosing a prior for their <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>, make the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> smaller and faster, and allow for a better flow of information into the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>. We compare the simplified versions to standard SSVAEs on 4 text classification tasks. On top of the above-mentioned simplification, experiments show a speed-up of 26 %, while keeping equivalent classification scores. The code to reproduce our experiments is public.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.insights-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--insights-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.insights-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.insights-1.20.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.insights-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.insights-1.20/>Active Learning for Argument Strength Estimation</a></strong><br><a href=/people/n/nataliia-kees/>Nataliia Kees</a>
|
<a href=/people/m/michael-fromm/>Michael Fromm</a>
|
<a href=/people/e/evgeniy-faerman/>Evgeniy Faerman</a>
|
<a href=/people/t/thomas-seidl/>Thomas Seidl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--insights-1--20><div class="card-body p-3 small">High-quality arguments are an essential part of <a href=https://en.wikipedia.org/wiki/Decision-making>decision-making</a>. Automatically predicting the quality of an argument is a complex task that recently got much attention in <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>. However, the annotation effort for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is exceptionally high. Therefore, we test uncertainty-based active learning (AL) methods on two popular argument-strength data sets to estimate whether sample-efficient learning can be enabled. Our extensive empirical evaluation shows that uncertainty-based acquisition functions can not surpass the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> reached with the <a href=https://en.wikipedia.org/wiki/Random_variable>random acquisition</a> on these <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a>.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>