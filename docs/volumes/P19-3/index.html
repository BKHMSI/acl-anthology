<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/P19-3.pdf>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></h2><p class=lead><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>,
<a href=/people/e/enrique-alfonseca/>Enrique Alfonseca</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>P19-3</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Florence, Italy</dd><dt>Venue:</dt><dd><a href=/venues/acl/>ACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/P19-3>https://aclanthology.org/P19-3</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/P19-3.pdf>https://aclanthology.org/P19-3.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/P19-3.pdf title="Open PDF of 'Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+57th+Annual+Meeting+of+the+Association+for+Computational+Linguistics%3A+System+Demonstrations" title="Search for 'Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3000/>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/e/enrique-alfonseca/>Enrique Alfonseca</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-3002.Note.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3002/>SLATE : A Super-Lightweight Annotation Tool for Experts<span class=acl-fixed-case>SLATE</span>: A Super-Lightweight Annotation Tool for Experts</a></strong><br><a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3002><div class="card-body p-3 small">Many annotation tools have been developed, covering a wide variety of tasks and providing features like user management, pre-processing, and automatic labeling. However, all of these tools use <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>Graphical User Interfaces</a>, and often require substantial effort to install and configure. This paper presents a new annotation tool that is designed to fill the niche of a lightweight interface for users with a terminal-based workflow. SLATE supports annotation at different scales (spans of characters, tokens, and lines, or a document) and of different types (free text, labels, and links), with easily customisable keybindings, and unicode support. In a user study comparing with other tools it was consistently the easiest to install and use. SLATE fills a need not met by existing systems, and has already been used to annotate two corpora, one of which involved over 250 hours of annotation effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3003/>lingvis.io-A Linguistic Visual Analytics Framework</a></strong><br><a href=/people/m/mennatallah-el-assady/>Mennatallah El-Assady</a>
|
<a href=/people/w/wolfgang-jentner/>Wolfgang Jentner</a>
|
<a href=/people/f/fabian-sperrle/>Fabian Sperrle</a>
|
<a href=/people/r/rita-sevastjanova/>Rita Sevastjanova</a>
|
<a href=/people/a/annette-hautli/>Annette Hautli-Janisz</a>
|
<a href=/people/m/miriam-butt/>Miriam Butt</a>
|
<a href=/people/d/daniel-keim/>Daniel Keim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3003><div class="card-body p-3 small">We present a modular framework for the rapid-prototyping of linguistic, web-based, visual analytics applications. Our framework gives developers access to a rich set of machine learning and natural language processing steps, through encapsulating them into <a href=https://en.wikipedia.org/wiki/Microservices>micro-services</a> and combining them into a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>computational pipeline</a>. This processing pipeline is auto-configured based on the requirements of the visualization front-end, making the linguistic processing and visualization design, detached independent development tasks. This paper describes the constellation and modality of our framework, which continues to support the efficient development of various human-in-the-loop, linguistic visual analytics research techniques and applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3004/>SARAL : A Low-Resource Cross-Lingual Domain-Focused Information Retrieval System for Effective Rapid Document Triage<span class=acl-fixed-case>SARAL</span>: A Low-Resource Cross-Lingual Domain-Focused Information Retrieval System for Effective Rapid Document Triage</a></strong><br><a href=/people/e/elizabeth-boschee/>Elizabeth Boschee</a>
|
<a href=/people/j/joel-barry/>Joel Barry</a>
|
<a href=/people/j/jayadev-billa/>Jayadev Billa</a>
|
<a href=/people/m/marjorie-freedman/>Marjorie Freedman</a>
|
<a href=/people/t/thamme-gowda/>Thamme Gowda</a>
|
<a href=/people/c/constantine-lignos/>Constantine Lignos</a>
|
<a href=/people/c/chester-palen-michel/>Chester Palen-Michel</a>
|
<a href=/people/m/michael-pust/>Michael Pust</a>
|
<a href=/people/b/banriskhem-kayang-khonglah/>Banriskhem Kayang Khonglah</a>
|
<a href=/people/s/srikanth-madikeri/>Srikanth Madikeri</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/s/scott-miller/>Scott Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3004><div class="card-body p-3 small">With the increasing democratization of <a href=https://en.wikipedia.org/wiki/Electronic_media>electronic media</a>, vast information resources are available in less-frequently-taught languages such as <a href=https://en.wikipedia.org/wiki/Swahili_language>Swahili</a> or <a href=https://en.wikipedia.org/wiki/Somali_language>Somali</a>. That information, which may be crucially important and not available elsewhere, can be difficult for monolingual English speakers to effectively access. In this paper we present an end-to-end cross-lingual information retrieval (CLIR) and summarization system for low-resource languages that 1) enables English speakers to search foreign language repositories of text and audio using English queries, 2) summarizes the retrieved documents in English with respect to a particular information need, and 3) provides complete transcriptions and translations as needed. The SARAL system achieved the top end-to-end performance in the most recent IARPA MATERIAL CLIR+summarization evaluations. Our demonstration system provides end-to-end open query retrieval and summarization capability, and presents the original source text or audio, <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>speech transcription</a>, and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, for two low resource languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3005/>Jiuge : A Human-Machine Collaborative Chinese Classical Poetry Generation System<span class=acl-fixed-case>J</span>iuge: A Human-Machine Collaborative <span class=acl-fixed-case>C</span>hinese Classical Poetry Generation System</a></strong><br><a href=/people/g/guo-zhipeng/>Guo Zhipeng</a>
|
<a href=/people/x/xiaoyuan-yi/>Xiaoyuan Yi</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/w/wenhao-li/>Wenhao Li</a>
|
<a href=/people/c/cheng-yang/>Cheng Yang</a>
|
<a href=/people/j/jiannan-liang/>Jiannan Liang</a>
|
<a href=/people/h/huimin-chen/>Huimin Chen</a>
|
<a href=/people/y/yuhui-zhang/>Yuhui Zhang</a>
|
<a href=/people/r/ruoyu-li/>Ruoyu Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3005><div class="card-body p-3 small">Research on the automatic generation of poetry, the treasure of human culture, has lasted for decades. Most existing systems, however, are merely model-oriented, which input some user-specified keywords and directly complete the generation process in one pass, with little user participation. We believe that the machine, being a collaborator or an assistant, should not replace human beings in poetic creation. Therefore, we proposed <a href=https://en.wikipedia.org/wiki/Jiuge>Jiuge</a>, a human-machine collaborative Chinese classical poetry generation system. Unlike previous systems, <a href=https://en.wikipedia.org/wiki/Jiuge>Jiuge</a> allows users to revise the unsatisfied parts of a generated poem draft repeatedly. According to the revision, the <a href=https://en.wikipedia.org/wiki/Poetry>poem</a> will be dynamically updated and regenerated. After the revision and modification procedure, the user can write a satisfying poem together with Jiuge system collaboratively. Besides, <a href=https://en.wikipedia.org/wiki/Jiuge>Jiuge</a> can accept multi-modal inputs, such as <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a>, <a href=https://en.wikipedia.org/wiki/Plain_text>plain text</a> or <a href=https://en.wikipedia.org/wiki/Image>images</a>. By exposing the options of poetry genres, styles and revision modes, <a href=https://en.wikipedia.org/wiki/Jiuge>Jiuge</a>, acting as a professional assistant, allows constant and active participation of users in poetic creation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-3007.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3007/>A Multiscale Visualization of Attention in the Transformer Model</a></strong><br><a href=/people/j/jesse-vig/>Jesse Vig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3007><div class="card-body p-3 small">The Transformer is a sequence model that forgoes traditional <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent architectures</a> in favor of a fully attention-based approach. Besides improving performance, an advantage of using <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is that it can also help to interpret a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by showing how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source tool</a> that visualizes <a href=https://en.wikipedia.org/wiki/Attention>attention</a> at multiple scales, each of which provides a unique perspective on the <a href=https://en.wikipedia.org/wiki/Attention>attention mechanism</a>. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases : detecting model bias, locating relevant attention heads, and linking neurons to model behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3008/>PostAc : A Visual Interactive Search, Exploration, and Analysis Platform for PhD Intensive Job Postings<span class=acl-fixed-case>P</span>ost<span class=acl-fixed-case>A</span>c : A Visual Interactive Search, Exploration, and Analysis Platform for <span class=acl-fixed-case>P</span>h<span class=acl-fixed-case>D</span> Intensive Job Postings</a></strong><br><a href=/people/c/chenchen-xu/>Chenchen Xu</a>
|
<a href=/people/i/inger-mewburn/>Inger Mewburn</a>
|
<a href=/people/w/will-j-grant/>Will J Grant</a>
|
<a href=/people/h/hanna-suominen/>Hanna Suominen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3008><div class="card-body p-3 small">Over 60 % of Australian PhD graduates land their first job after graduation outside academia, but this <a href=https://en.wikipedia.org/wiki/Job_market>job market</a> remains largely hidden to these <a href=https://en.wikipedia.org/wiki/Job_hunting>job seekers</a>. Employers&#8217; low awareness and interest in attracting PhD graduates means that the term PhD is rarely used as a keyword in job advertisements ; 80 % of companies looking to employ similar researchers do not specifically ask for a PhD qualification. As a result, typing in <a href=https://en.wikipedia.org/wiki/Doctor_of_Philosophy>PhD</a> to a <a href=https://en.wikipedia.org/wiki/Employment_website>job search engine</a> tends to return mostly academic jobs. We set out to make the market for advanced research skills more visible to <a href=https://en.wikipedia.org/wiki/Job_hunting>job seekers</a>. In this paper, we present PostAc, an online platform of authentic job postings that helps PhD graduates sharpen their career thinking. The platform is underpinned by research on the key factors that identify what an employer is looking for when they want to hire a highly skilled researcher. Its ranking model leverages the free-form text embedded in the <a href=https://en.wikipedia.org/wiki/Job_description>job description</a> to quantify the most sought-after PhD skills and educate information seekers about the Australian job-market appetite for <a href=https://en.wikipedia.org/wiki/Doctor_of_Philosophy>PhD skills</a>. The platform makes visible the geographic location, industry sector, <a href=https://en.wikipedia.org/wiki/International_Standard_Classification_of_Occupations>job title</a>, <a href=https://en.wikipedia.org/wiki/Working_time>working hours</a>, continuity, and wage of the research intensive jobs. This is the first data-driven exploration in this field. Both empirical results and <a href=https://en.wikipedia.org/wiki/Online_and_offline>online platform</a> will be presented in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3009 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3009/>An adaptable task-oriented dialog system for stand-alone embedded devices</a></strong><br><a href=/people/l/long-duong/>Long Duong</a>
|
<a href=/people/v/vu-cong-duy-hoang/>Vu Cong Duy Hoang</a>
|
<a href=/people/t/tuyen-quang-pham/>Tuyen Quang Pham</a>
|
<a href=/people/y/yu-heng-hong/>Yu-Heng Hong</a>
|
<a href=/people/v/vladislavs-dovgalecs/>Vladislavs Dovgalecs</a>
|
<a href=/people/g/guy-bashkansky/>Guy Bashkansky</a>
|
<a href=/people/j/jason-black/>Jason Black</a>
|
<a href=/people/a/andrew-bleeker/>Andrew Bleeker</a>
|
<a href=/people/s/serge-le-huitouze/>Serge Le Huitouze</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3009><div class="card-body p-3 small">This paper describes a spoken-language end-to-end task-oriented dialogue system for small embedded devices such as <a href=https://en.wikipedia.org/wiki/Home_appliance>home appliances</a>. While the current <a href=https://en.wikipedia.org/wiki/System>system</a> implements a smart alarm clock with advanced calendar scheduling functionality, the <a href=https://en.wikipedia.org/wiki/System>system</a> is designed to make it easy to port to other application domains (e.g., the dialogue component factors out domain-specific execution from domain-general actions such as requesting and updating slot values). The system does not require internet connectivity because all components, including <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, dialogue management, execution and <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech</a>, run locally on the <a href=https://en.wikipedia.org/wiki/Embedded_system>embedded device</a> (our demo uses a Raspberry Pi). This simplifies deployment, minimizes server costs and most importantly, eliminates user privacy risks. The demo video in alarm domain is here youtu.be/N3IBMGocvHU</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3013/>FASTDial : Abstracting Dialogue Policies for Fast Development of Task Oriented Agents<span class=acl-fixed-case>FASTD</span>ial: Abstracting Dialogue Policies for Fast Development of Task Oriented Agents</a></strong><br><a href=/people/s/serra-sinem-tekiroglu/>Serra Sinem Tekiroglu</a>
|
<a href=/people/b/bernardo-magnini/>Bernardo Magnini</a>
|
<a href=/people/m/marco-guerini/>Marco Guerini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3013><div class="card-body p-3 small">We present a novel abstraction framework called FASTDial for designing task oriented dialogue agents, built on top of the OpenDial toolkit. This framework is meant to facilitate prototyping and development of dialogue systems from scratch also by non tech savvy especially when limited training data is available. To this end, we use a generic and simple frame-slots data-structure with pre-defined dialogue policies that allows for fast design and implementation at the price of some flexibility reduction. Moreover, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> allows for minimizing programming effort and domain expert training time, by hiding away many implementation details. We provide a system demonstration screencast video in the following link : https://vimeo.com/329840716</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3014" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3014/>A Neural, Interactive-predictive System for Multimodal Sequence to Sequence Tasks</a></strong><br><a href=/people/a/alvaro-peris/>Álvaro Peris</a>
|
<a href=/people/f/francisco-casacuberta/>Francisco Casacuberta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3014><div class="card-body p-3 small">We present a demonstration of a neural interactive-predictive system for tackling multimodal sequence to sequence tasks. The system generates text predictions to different sequence to sequence tasks : <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, image and video captioning. These predictions are revised by a human agent, who introduces corrections in the form of characters. The <a href=https://en.wikipedia.org/wiki/System>system</a> reacts to each correction, providing alternative hypotheses, compelling with the feedback provided by the user. The final objective is to reduce the human effort required during this correction process. This <a href=https://en.wikipedia.org/wiki/System>system</a> is implemented following a <a href=https://en.wikipedia.org/wiki/Client&#8211;server_model>client-server architecture</a>. For accessing the <a href=https://en.wikipedia.org/wiki/System>system</a>, we developed a website, which communicates with the neural model, hosted in a local server. From this <a href=https://en.wikipedia.org/wiki/Website>website</a>, the different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> can be tackled following the interactivepredictive framework. We open-source all the code developed for building this <a href=https://en.wikipedia.org/wiki/System>system</a>. The demonstration in hosted in http://casmacat.prhlt.upv.es/interactive-seq2seq.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3017 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3017/>KCAT : A Knowledge-Constraint Typing Annotation Tool<span class=acl-fixed-case>KCAT</span>: A Knowledge-Constraint Typing Annotation Tool</a></strong><br><a href=/people/s/sheng-lin/>Sheng Lin</a>
|
<a href=/people/l/luye-zheng/>Luye Zheng</a>
|
<a href=/people/b/bo-chen/>Bo Chen</a>
|
<a href=/people/s/siliang-tang/>Siliang Tang</a>
|
<a href=/people/z/zhigang-chen/>Zhigang Chen</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3017><div class="card-body p-3 small">In this paper, we propose an efficient Knowledge Constraint Fine-grained Entity Typing Annotation Tool, which further improves the entity typing process through entity linking together with some practical functions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3020 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Demo Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-3020/>OpenKiwi : An Open Source Framework for <a href=https://en.wikipedia.org/wiki/Quality_assurance>Quality Estimation</a><span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>K</span>iwi: An Open Source Framework for Quality Estimation</a></strong><br><a href=/people/f/fabio-kepler/>Fabio Kepler</a>
|
<a href=/people/j/jonay-trenous/>Jonay Trénous</a>
|
<a href=/people/m/marcos-treviso/>Marcos Treviso</a>
|
<a href=/people/m/miguel-vera/>Miguel Vera</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3020><div class="card-body p-3 small">We introduce OpenKiwi, a Pytorch-based open source framework for translation quality estimation. OpenKiwi supports training and testing of word-level and sentence-level quality estimation systems, implementing the winning systems of the WMT 201518 quality estimation campaigns. We benchmark OpenKiwi on two datasets from WMT 2018 (English-German SMT and NMT), yielding state-of-the-art performance on the word-level tasks and near state-of-the-art in the sentence-level tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3022 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3022/>PerspectroScope : A Window to the World of Diverse Perspectives<span class=acl-fixed-case>P</span>erspectro<span class=acl-fixed-case>S</span>cope: A Window to the World of Diverse Perspectives</a></strong><br><a href=/people/s/sihao-chen/>Sihao Chen</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3022><div class="card-body p-3 small">This work presents PerspectroScope, a web-based system which lets users query a discussion-worthy natural language claim, and extract and visualize various perspectives in support or against the claim, along with evidence supporting each perspective. The <a href=https://en.wikipedia.org/wiki/System>system</a> thus lets users explore various perspectives that could touch upon aspects of the issue at hand. The <a href=https://en.wikipedia.org/wiki/System>system</a> is built as a combination of retrieval engines and learned textual-entailment-like classifiers built using a few recent developments in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. To make the <a href=https://en.wikipedia.org/wiki/System>system</a> more adaptive, expand its coverage, and improve its decisions over time, our <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> employs various mechanisms to get corrections from the users. PerspectroScope is available at github.com/CogComp/perspectroscope Web demo link : http://orwell.seas.upenn.edu:4002/ Link to demo video : https://www.youtube.com/watch?v=MXBTR1Sp3Bs</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3023/>HEIDL : Learning <a href=https://en.wikipedia.org/wiki/Linguistic_description>Linguistic Expressions</a> with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Human-in-the-Loop<span class=acl-fixed-case>HEIDL</span>: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop</a></strong><br><a href=/people/p/prithviraj-sen/>Prithviraj Sen</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/e/eser-kandogan/>Eser Kandogan</a>
|
<a href=/people/y/yiwei-yang/>Yiwei Yang</a>
|
<a href=/people/w/walter-lasecki/>Walter Lasecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3023><div class="card-body p-3 small">While the role of humans is increasingly recognized in machine learning community, representation of and interaction with <a href=https://en.wikipedia.org/wiki/Conceptual_model_(computer_science)>models</a> in current human-in-the-loop machine learning (HITL-ML) approaches are too low-level and far-removed from human&#8217;s conceptual models. We demonstrate HEIDL, a prototype HITL-ML system that exposes the machine-learned model through high-level, explainable linguistic expressions formed of predicates representing semantic structure of text. In HEIDL, human&#8217;s role is elevated from simply evaluating model predictions to interpreting and even updating the model logic directly by enabling interaction with rule predicates themselves. Raising the currency of interaction to such semantic levels calls for new interaction paradigms between humans and machines that result in improved productivity for text analytics model development process. Moreover, by involving humans in the process, the human-machine co-created models generalize better to unseen data as domain experts are able to instill their expertise by extrapolating from what has been learned by automated algorithms from few labelled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3026 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3026/>ClaimPortal : Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter<span class=acl-fixed-case>C</span>laim<span class=acl-fixed-case>P</span>ortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/s/sarthak-majithia/>Sarthak Majithia</a>
|
<a href=/people/f/fatma-arslan/>Fatma Arslan</a>
|
<a href=/people/s/sumeet-lubal/>Sumeet Lubal</a>
|
<a href=/people/d/damian-jimenez/>Damian Jimenez</a>
|
<a href=/people/p/priyank-arora/>Priyank Arora</a>
|
<a href=/people/j/josue-caraballo/>Josue Caraballo</a>
|
<a href=/people/c/chengkai-li/>Chengkai Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3026><div class="card-body p-3 small">We present ClaimPortal, a <a href=https://en.wikipedia.org/wiki/Web_application>web-based platform</a> for monitoring, searching, checking, and analyzing English factual claims on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> from the <a href=https://en.wikipedia.org/wiki/Politics_of_the_United_States>American political domain</a>. We explain the <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> of ClaimPortal, its components and functions, and the <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a>. While the last several years have witnessed a substantial growth in interests and efforts in the area of computational fact-checking, ClaimPortal is a novel infrastructure in that <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checkers</a> have largely skipped factual claims in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. It can be a highly powerful tool to both general web users and <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checkers</a>. It will also be an educational resource in helping cultivate a society that is less susceptible to <a href=https://en.wikipedia.org/wiki/Falsity>falsehoods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3028 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3028" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3028/>Parallax : Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae<span class=acl-fixed-case>P</span>arallax: Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae</a></strong><br><a href=/people/p/piero-molino/>Piero Molino</a>
|
<a href=/people/y/yang-wang/>Yang Wang</a>
|
<a href=/people/j/jiawei-zhang/>Jiawei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3028><div class="card-body p-3 small">Embeddings are a fundamental component of many modern machine learning and natural language processing models. Understanding <a href=https://en.wikipedia.org/wiki/Mathematical_model>them</a> and visualizing <a href=https://en.wikipedia.org/wiki/Mathematical_model>them</a> is essential for gathering insights about the information they capture and the behavior of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we introduce <a href=https://en.wikipedia.org/wiki/Parallax>Parallax</a>, a <a href=https://en.wikipedia.org/wiki/Tool>tool</a> explicitly designed for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Parallax allows the user to use both state-of-the-art embedding analysis methods (PCA and t-SNE) and a simple yet effective task-oriented approach where users can explicitly define the axes of the projection through algebraic formulae. % consists in projecting them in two-dimensional planes without any interpretable semantics associated to the axes of the <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection</a>, which makes detailed analyses and comparison among multiple sets of embeddings challenging. In this approach, <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are projected into a semantically meaningful subspace, which enhances interpretability and allows for more fine-grained analysis. We demonstrate the power of the <a href=https://en.wikipedia.org/wiki/Tool>tool</a> and the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> through a series of <a href=https://en.wikipedia.org/wiki/Case_study>case studies</a> and a <a href=https://en.wikipedia.org/wiki/User_study>user study</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3031 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3031" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3031/>TARGER : Neural Argument Mining at Your Fingertips<span class=acl-fixed-case>TARGER</span>: Neural Argument Mining at Your Fingertips</a></strong><br><a href=/people/a/artem-chernodub/>Artem Chernodub</a>
|
<a href=/people/o/oleksiy-oliynyk/>Oleksiy Oliynyk</a>
|
<a href=/people/p/philipp-heidenreich/>Philipp Heidenreich</a>
|
<a href=/people/a/alexander-bondarenko/>Alexander Bondarenko</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3031><div class="card-body p-3 small">We present TARGER, an open source neural argument mining framework for tagging arguments in free input texts and for keyword-based retrieval of arguments from an argument-tagged web-scale corpus. The currently available models are pre-trained on three recent argument mining datasets and enable the use of neural argument mining without any reproducibility effort on the user&#8217;s side. The open source code ensures portability to other domains and use cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3032 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3032" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3032/>MoNoise : A Multi-lingual and Easy-to-use Lexical Normalization Tool<span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>N</span>oise: A Multi-lingual and Easy-to-use Lexical Normalization Tool</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3032><div class="card-body p-3 small">In this paper, we introduce and demonstrate the online demo as well as the command line interface of a lexical normalization system (MoNoise) for a variety of languages. We further improve this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> by using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from the original word for every <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization candidate</a>. For comparison with future work, we propose the bundling of seven datasets in six languages to form a new <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>, together with a novel evaluation metric which is particularly suitable for cross-dataset comparisons. MoNoise reaches a new state-of-art performance for six out of seven of these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Furthermore, we allow the user to tune the &#8216;aggressiveness&#8217; of the normalization, and show how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be made more efficient with only a small loss in performance. The online demo can be found on : http://www.robvandergoot.com/monoise and the corresponding code on : https://bitbucket.org/robvanderg/monoise/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3033/>Level-Up : Learning to Improve Proficiency Level of Essays</a></strong><br><a href=/people/w/wen-bin-han/>Wen-Bin Han</a>
|
<a href=/people/j/jhih-jie-chen/>Jhih-Jie Chen</a>
|
<a href=/people/c/chingyu-yang/>Chingyu Yang</a>
|
<a href=/people/j/jason-s-chang/>Jason Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3033><div class="card-body p-3 small">We introduce a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for generating suggestions on a given sentence for improving the proficiency level. In our approach, the sentence is transformed into a sequence of grammatical elements aimed at providing suggestions of more advanced grammar elements based on originals. The method involves parsing the sentence, identifying <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical elements</a>, and ranking related elements to recommend a higher level of <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical element</a>. We present a prototype tutoring system, Level-Up, that applies the method to English learners&#8217; essays in order to assist them in writing and reading. Evaluation on a set of <a href=https://en.wikipedia.org/wiki/Essay>essays</a> shows that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> does assist user in writing.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>