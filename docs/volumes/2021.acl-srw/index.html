<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2021.acl-srw.pdf>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop</a></h2><p class=lead><a href=/people/j/jad-kabbara/>Jad Kabbara</a>,
<a href=/people/h/haitao-lin/>Haitao Lin</a>,
<a href=/people/a/amandalynne-paullada/>Amandalynne Paullada</a>,
<a href=/people/j/jannis-vamvas/>Jannis Vamvas</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.acl-srw</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/acl/>ACL</a>
| <a href=/venues/ijcnlp/>IJCNLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.acl-srw>https://aclanthology.org/2021.acl-srw</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.acl-srw.pdf>https://aclanthology.org/2021.acl-srw.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.acl-srw.pdf title="Open PDF of 'Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+59th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+and+the+11th+International+Joint+Conference+on+Natural+Language+Processing%3A+Student+Research+Workshop" title="Search for 'Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.0/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop</a></strong><br><a href=/people/j/jad-kabbara/>Jad Kabbara</a>
|
<a href=/people/h/haitao-lin/>Haitao Lin</a>
|
<a href=/people/a/amandalynne-paullada/>Amandalynne Paullada</a>
|
<a href=/people/j/jannis-vamvas/>Jannis Vamvas</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.3/>Transformer-Based Direct Hidden Markov Model for Machine Translation<span class=acl-fixed-case>M</span>arkov Model for Machine Translation</a></strong><br><a href=/people/w/weiyue-wang/>Weiyue Wang</a>
|
<a href=/people/z/zijian-yang/>Zijian Yang</a>
|
<a href=/people/y/yingbo-gao/>Yingbo Gao</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--3><div class="card-body p-3 small">The neural hidden Markov model has been proposed as an alternative to attention mechanism in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. However, since the introduction of the transformer models, its performance has been surpassed. This work proposes to introduce the concept of the <a href=https://en.wikipedia.org/wiki/Hidden_Markov_model>hidden Markov model</a> to the transformer architecture, which outperforms the transformer baseline. Interestingly, we find that the zero-order model already provides promising performance, giving it an edge compared to a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with first-order dependency, which performs similarly but is significantly slower in <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-srw.5.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-srw.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.5/>How Low is Too Low? A Computational Perspective on Extremely Low-Resource Languages</a></strong><br><a href=/people/r/rachit-bansal/>Rachit Bansal</a>
|
<a href=/people/h/himanshu-choudhary/>Himanshu Choudhary</a>
|
<a href=/people/r/ravneet-punia/>Ravneet Punia</a>
|
<a href=/people/n/niko-schenk/>Niko Schenk</a>
|
<a href=/people/e/emilie-page-perron/>Émilie Pagé-Perron</a>
|
<a href=/people/j/jacob-dahl/>Jacob Dahl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--5><div class="card-body p-3 small">Despite the recent advancements of attention-based deep learning architectures across a majority of Natural Language Processing tasks, their application remains limited in a low-resource setting because of a lack of pre-trained models for such languages. In this study, we make the first attempt to investigate the challenges of adapting these techniques to an extremely low-resource language Sumerian cuneiform one of the world&#8217;s oldest written languages attested from at least the beginning of the 3rd millennium BC. Specifically, we introduce the first cross-lingual information extraction pipeline for <a href=https://en.wikipedia.org/wiki/Sumerian_language>Sumerian</a>, which includes <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We introduce InterpretLR, an interpretability toolkit for low-resource NLP and use it alongside human evaluations to gauge the trained models. Notably, all our techniques and most components of our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> can be generalised to any low-resource language. We publicly release all our implementations including a novel <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> with domain-specific pre-processing to promote further research in this domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-srw.7.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-srw.7/>Long Document Summarization in a Low Resource Setting using Pretrained Language Models</a></strong><br><a href=/people/a/ahsaas-bajaj/>Ahsaas Bajaj</a>
|
<a href=/people/p/pavitra-dangati/>Pavitra Dangati</a>
|
<a href=/people/k/kalpesh-krishna/>Kalpesh Krishna</a>
|
<a href=/people/p/pradhiksha-ashok-kumar/>Pradhiksha Ashok Kumar</a>
|
<a href=/people/r/rheeya-uppaal/>Rheeya Uppaal</a>
|
<a href=/people/b/bradford-windsor/>Bradford Windsor</a>
|
<a href=/people/e/eliot-brenner/>Eliot Brenner</a>
|
<a href=/people/d/dominic-dotterrer/>Dominic Dotterrer</a>
|
<a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--7><div class="card-body p-3 small">Abstractive summarization is the task of compressing a long document into a coherent short document while retaining <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>salient information</a>. Modern abstractive summarization methods are based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pre-trained abstractive summarizer BART, which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> based on GPT-2 language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to <a href=https://en.wikipedia.org/wiki/Bay_Area_Rapid_Transit>BART</a>, we observe a 6.0 ROUGE-L improvement. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with independent human labeling by domain experts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.14/>Situation-Based Multiparticipant Chat Summarization : a Concept, an Exploration-Annotation Tool and an Example Collection</a></strong><br><a href=/people/a/anna-smirnova/>Anna Smirnova</a>
|
<a href=/people/e/evgeniy-slobodkin/>Evgeniy Slobodkin</a>
|
<a href=/people/g/george-chernishev/>George Chernishev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--14><div class="card-body p-3 small">Currently, <a href=https://en.wikipedia.org/wiki/Text_messaging>text chatting</a> is one of the primary means of communication. However, modern <a href=https://en.wikipedia.org/wiki/Online_chat>text chat</a> still in general does not offer any <a href=https://en.wikipedia.org/wiki/Navigation>navigation</a> or even full-featured search, although the high volumes of messages demand it. In order to mitigate these inconveniences, we formulate the problem of situation-based summarization and propose a special data annotation tool intended for developing <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and gold-standard data</a>. A <a href=https://en.wikipedia.org/wiki/Situation>situation</a> is a subset of messages revolving around a single event in both temporal and contextual senses : e.g, a group of friends arranging a meeting in chat, agreeing on date, time, and place. Situations can be extracted via <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning techniques</a>. Since the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is novel, neither training nor gold-standard datasets for it have been created yet. In this paper, we present the formulation of the situation-based summarization problem. Next, we describe Chat Corpora Annotator (CCA): the first annotation system designed specifically for exploring and annotating chat log data. We also introduce a custom <a href=https://en.wikipedia.org/wiki/Query_language>query language</a> for semi-automatic situation extraction. Finally, we present the first <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold-standard dataset</a> for situation-based summarization. The software source code and the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.15/>Modeling Text using the Continuous Space Topic Model with Pre-Trained Word Embeddings</a></strong><br><a href=/people/s/seiichi-inoue/>Seiichi Inoue</a>
|
<a href=/people/t/taichi-aida/>Taichi Aida</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/m/manabu-asai/>Manabu Asai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--15><div class="card-body p-3 small">In this study, we propose a model that extends the continuous space topic model (CSTM), which flexibly controls word probability in a document, using pre-trained word embeddings. To develop the proposed model, we pre-train word embeddings, which capture the semantics of words and plug them into the CSTM. Intrinsic experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exhibits a superior performance over the CSTM in terms of <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> and <a href=https://en.wikipedia.org/wiki/Convergence_of_random_variables>convergence speed</a>. Furthermore, extrinsic experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is useful for a document classification task when compared with the baseline model. We qualitatively show that the latent coordinates obtained by training the proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> are better than those of the baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.19/>Hold on honey, men at work : A semi-supervised approach to detecting sexism in sitcoms</a></strong><br><a href=/people/s/smriti-singh/>Smriti Singh</a>
|
<a href=/people/t/tanvi-anand/>Tanvi Anand</a>
|
<a href=/people/a/arijit-ghosh-chowdhury/>Arijit Ghosh Chowdhury</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--19><div class="card-body p-3 small">Television shows play an important role inpropagating societal norms. Owing to the popularity of the situational comedy (sitcom) genre, it contributes significantly to the over-all development of society. In an effort to analyze the content of <a href=https://en.wikipedia.org/wiki/Television_show>television shows</a> belong-ing to this <a href=https://en.wikipedia.org/wiki/Genre>genre</a>, we present a dataset of dialogue turns from popular sitcoms annotated for the presence of <a href=https://en.wikipedia.org/wiki/Sexism>sexist remarks</a>. We train a text classification model to detect <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> using domain adaptive learning. We apply the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to analyze the evolution of sexist content over the years. We propose a domain-specific semi-supervised architecture for the aforementioned detection of sexism. Through extensive experiments, we show that our model often yields better <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance over generic deep learn-ing based sentence classification that does not employ domain-specific training. We find that while <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> decreases over time on average, the proportion of sexist dialogue for the most sexist sitcom actually increases. A quantitative analysis along with a detailed error analysis presents the case for our proposed methodology</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-srw.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.21/>Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation<span class=acl-fixed-case>QA</span> Models to Challenge Sets with Variational Question-Answer Pair Generation</a></strong><br><a href=/people/k/kazutoshi-shinoda/>Kazutoshi Shinoda</a>
|
<a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--21><div class="card-body p-3 small">Question answering (QA) models for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> have achieved human-level accuracy on in-distribution test sets. However, they have been demonstrated to lack <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> to challenge sets, whose <a href=https://en.wikipedia.org/wiki/Probability_distribution>distribution</a> is different from that of training sets. Existing data augmentation methods mitigate this problem by simply augmenting training sets with synthetic examples sampled from the same distribution as the challenge sets. However, these methods assume that the <a href=https://en.wikipedia.org/wiki/Probability_distribution>distribution</a> of a challenge set is known a priori, making them less applicable to unseen challenge sets. In this study, we focus on question-answer pair generation (QAG) to mitigate this problem. While most existing QAG methods aim to improve the quality of synthetic examples, we conjecture that diversity-promoting QAG can mitigate the sparsity of training sets and lead to better <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>. We present a variational QAG model that generates multiple diverse QA pairs from a paragraph. Our experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 12 challenge sets, as well as the in-distribution accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.24/>Edit Distance Based Curriculum Learning for Paraphrase Generation</a></strong><br><a href=/people/s/sora-kadotani/>Sora Kadotani</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a>
|
<a href=/people/m/makoto-onizuka/>Makoto Onizuka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--24><div class="card-body p-3 small">Curriculum learning has improved the quality of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, where only source-side features are considered in the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to determine the difficulty of <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In this study, we apply <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum learning</a> to <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> for the first time. Different from <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> allows a certain level of discrepancy in <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> between source and target, which results in diverse transformations from <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> to reordering of clauses. Hence, the difficulty of transformations requires considering both source and target contexts. Experiments on formality transfer using GYAFC showed that our curriculum learning with <a href=https://en.wikipedia.org/wiki/Edit_distance>edit distance</a> improves the quality of <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>. Additionally, the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> improves the quality of difficult samples, which was not possible for previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.28/>CMTA : COVID-19 Misinformation Multilingual Analysis on Twitter<span class=acl-fixed-case>CMTA</span>: <span class=acl-fixed-case>COVID</span>-19 Misinformation Multilingual Analysis on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/r/raj-pranesh/>Raj Pranesh</a>
|
<a href=/people/m/mehrdad-farokhenajd/>Mehrdad Farokhenajd</a>
|
<a href=/people/a/ambesh-shekhar/>Ambesh Shekhar</a>
|
<a href=/people/g/genoveva-vargas-solar/>Genoveva Vargas-Solar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--28><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Internet>internet</a> has actually come to be an essential resource of health knowledge for individuals around the world in the present situation of the coronavirus condition pandemic(COVID-19). During pandemic situations, myths, <a href=https://en.wikipedia.org/wiki/Sensationalism>sensationalism</a>, <a href=https://en.wikipedia.org/wiki/Rumor>rumours</a> and <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a>, generated intentionally or unintentionally, spread rapidly through <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. Twitter is one of these popular social networks people use to share COVID-19 related news, information, and thoughts that reflect their perception and opinion about the pandemic. Evaluation of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> for recognizing <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> can create beneficial understanding to review the top quality and also the readability of online information concerning the COVID-19. This paper presents a multilingual COVID-19 related tweet analysis method, CMTA, that uses BERT, a deep learning model for multilingual tweet misinformation detection and classification. CMTA extracts features from multilingual textual data, which is then categorized into specific information classes. Classification is done by a Dense-CNN model trained on tweets manually annotated into information classes (i.e., &#8216;false&#8217;, &#8216;partly false&#8217;, &#8216;misleading&#8217;). The paper presents an analysis of multilingual tweets from February to June, showing the distribution type of information spread across different languages. To access the performance of the CMTA multilingual model, we performed a comparative analysis of 8 monolingual model and CMTA for the misinformation detection task. The results show that our proposed CMTA model has surpassed various monolingual models which consolidated the fact that through <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> a multilingual framework could be developed.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>