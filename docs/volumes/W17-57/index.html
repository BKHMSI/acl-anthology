<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 4th Workshop on Asian Translation (WAT2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W17-57.pdf>Proceedings of the 4th Workshop on <span class=acl-fixed-case>A</span>sian Translation (<span class=acl-fixed-case>WAT</span>2017)</a></h2><p class=lead><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>,
<a href=/people/i/isao-goto/>Isao Goto</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W17-57</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Taipei, Taiwan</dd><dt>Venues:</dt><dd><a href=/venues/wat/>WAT</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Asian Federation of Natural Language Processing</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W17-57>https://aclanthology.org/W17-57</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W17-57.pdf>https://aclanthology.org/W17-57.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W17-57.pdf title="Open PDF of 'Proceedings of the 4th Workshop on Asian Translation (WAT2017)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+4th+Workshop+on+Asian+Translation+%28WAT2017%29" title="Search for 'Proceedings of the 4th Workshop on Asian Translation (WAT2017)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5700/>Proceedings of the 4th Workshop on <span class=acl-fixed-case>A</span>sian Translation (<span class=acl-fixed-case>WAT</span>2017)</a></strong><br><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/i/isao-goto/>Isao Goto</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5702 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5702/>Controlling Target Features in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> via Prefix Constraints</a></strong><br><a href=/people/s/shunsuke-takeno/>Shunsuke Takeno</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a>
|
<a href=/people/k/kazuhide-yamamoto/>Kazuhide Yamamoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5702><div class="card-body p-3 small">We propose prefix constraints, a novel method to enforce constraints on target sentences in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. It places a sequence of special tokens at the beginning of target sentence (target prefix), while side constraints places a special token at the end of source sentence (source suffix). Prefix constraints can be predicted from source sentence jointly with target sentence, while side constraints (Sennrich et al., 2016) must be provided by the user or predicted by some other methods. In both <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a>, special tokens are designed to encode arbitrary features on target-side or metatextual information. We show that prefix constraints are more flexible than side constraints and can be used to control the behavior of neural machine translation, in terms of output length, bidirectional decoding, domain adaptation, and unaligned target word generation.<i>prefix constraints</i>, a novel method to enforce constraints on\n target sentences in neural machine translation. It places a sequence of\n special tokens at the beginning of target sentence (target prefix), while\n side constraints places a special token at the end of source sentence\n (source suffix). Prefix constraints can be predicted from source sentence\n jointly with target sentence, while side constraints (Sennrich et al., 2016) must be provided by\n the user or predicted by some other methods. In both methods, special\n tokens are designed to encode arbitrary features on target-side or\n metatextual information. We show that prefix constraints are more flexible\n than side constraints and can be used to control the behavior of neural\n machine translation, in terms of output length, bidirectional decoding,\n domain adaptation, and unaligned target word generation.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5703/>Improving Japanese-to-English Neural Machine Translation by Paraphrasing the Target Language<span class=acl-fixed-case>J</span>apanese-to-<span class=acl-fixed-case>E</span>nglish Neural Machine Translation by Paraphrasing the Target Language</a></strong><br><a href=/people/y/yuuki-sekizawa/>Yuuki Sekizawa</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5703><div class="card-body p-3 small">Neural machine translation (NMT) produces sentences that are more fluent than those produced by <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation (SMT)</a>. However, NMT has a very high <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> because of the high dimensionality of the output layer. Generally, NMT restricts the size of vocabulary, which results in infrequent words being treated as out-of-vocabulary (OOV) and degrades the performance of the <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In evaluation, we achieved a statistically significant BLEU score improvement of 0.55-0.77 over the baselines including the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5707 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5707/>XMU Neural Machine Translation Systems for WAT 2017<span class=acl-fixed-case>XMU</span> Neural Machine Translation Systems for <span class=acl-fixed-case>WAT</span> 2017</a></strong><br><a href=/people/b/boli-wang/>Boli Wang</a>
|
<a href=/people/z/zhixing-tan/>Zhixing Tan</a>
|
<a href=/people/j/jinming-hu/>Jinming Hu</a>
|
<a href=/people/y/yidong-chen/>Yidong Chen</a>
|
<a href=/people/x/xiaodong-shi/>Xiaodong Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5707><div class="card-body p-3 small">This paper describes the Neural Machine Translation systems of Xiamen University for the shared translation tasks of WAT 2017. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> are based on the Encoder-Decoder framework with <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. We participated in three subtasks. We experimented subword segmentation, <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>synthetic training data</a> and model ensembling. Experiments show that all these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can give substantial improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5708" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5708/>A Bag of Useful Tricks for Practical <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> : Embedding Layer Initialization and Large Batch Size</a></strong><br><a href=/people/m/masato-neishi/>Masato Neishi</a>
|
<a href=/people/j/jin-sakuma/>Jin Sakuma</a>
|
<a href=/people/s/satoshi-tohda/>Satoshi Tohda</a>
|
<a href=/people/s/shonosuke-ishiwatari/>Shonosuke Ishiwatari</a>
|
<a href=/people/n/naoki-yoshinaga/>Naoki Yoshinaga</a>
|
<a href=/people/m/masashi-toyoda/>Masashi Toyoda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5708><div class="card-body p-3 small">In this paper, we describe the team UT-IIS&#8217;s system and results for the WAT 2017 translation tasks. We further investigated several tricks including a novel technique for initializing embedding layers using only the <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a>, which increased the BLEU score by 1.28, found a practical large batch size of 256, and gained insights regarding <a href=https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)>hyperparameter settings</a>. Ultimately, our <a href=https://en.wikipedia.org/wiki/System>system</a> obtained a better result than the state-of-the-art <a href=https://en.wikipedia.org/wiki/System>system</a> of WAT 2016. Our code is available on.<url>https://github.com/nem6ishi/wat17</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5709/>Patent NMT integrated with Large Vocabulary Phrase Translation by SMT at WAT 2017<span class=acl-fixed-case>NMT</span> integrated with Large Vocabulary Phrase Translation by <span class=acl-fixed-case>SMT</span> at <span class=acl-fixed-case>WAT</span> 2017</a></strong><br><a href=/people/z/zi-long/>Zi Long</a>
|
<a href=/people/r/ryuichiro-kimura/>Ryuichiro Kimura</a>
|
<a href=/people/t/takehito-utsuro/>Takehito Utsuro</a>
|
<a href=/people/t/tomoharu-mitsuhashi/>Tomoharu Mitsuhashi</a>
|
<a href=/people/m/mikio-yamamoto/>Mikio Yamamoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5709><div class="card-body p-3 small">Neural machine translation (NMT) can not handle a larger vocabulary because the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training complexity</a> and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many <a href=https://en.wikipedia.org/wiki/Jargon>technical terms</a> that are observed infrequently. Long et al. (2017) proposed to select phrases that contain out-of-vocabulary words using the statistical approach of branching entropy. The selected phrases are then replaced with tokens during training and post-translated by the phrase translation table of SMT. In this paper, we apply the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> proposed by Long et al. (2017) to the WAT 2017 Japanese-Chinese and Japanese-English patent datasets. Evaluation on Japanese-to-Chinese, Chinese-to-Japanese, Japanese-to-English and English-to-Japanese patent sentence translation proved the effectiveness of phrases selected with branching entropy, where the NMT model of Long et al. (2017) achieves a substantial improvement over a baseline NMT model without the technique proposed by Long et al.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5712.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5712 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5712 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5712/>A Simple and Strong Baseline : NAIST-NICT Neural Machine Translation System for WAT2017 English-Japanese Translation Task<span class=acl-fixed-case>NAIST</span>-<span class=acl-fixed-case>NICT</span> Neural Machine Translation System for <span class=acl-fixed-case>WAT</span>2017 <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>J</span>apanese Translation Task</a></strong><br><a href=/people/y/yusuke-oda/>Yusuke Oda</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5712><div class="card-body p-3 small">This paper describes the details about the NAIST-NICT machine translation system for WAT2017 English-Japanese Scientific Paper Translation Task. The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves higher translation accuracy than any systems submitted previous campaigns despite simple <a href=https://en.wikipedia.org/wiki/Conceptual_model>model architecture</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5713 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5713/>Comparison of <a href=https://en.wikipedia.org/wiki/Simultaneous_multithreading>SMT</a> and <a href=https://en.wikipedia.org/wiki/Simultaneous_multithreading>NMT</a> trained with large Patent Corpora : Japio at WAT2017<span class=acl-fixed-case>SMT</span> and <span class=acl-fixed-case>NMT</span> trained with large Patent Corpora: <span class=acl-fixed-case>J</span>apio at <span class=acl-fixed-case>WAT</span>2017</a></strong><br><a href=/people/s/satoshi-kinoshita/>Satoshi Kinoshita</a>
|
<a href=/people/t/tadaaki-oshio/>Tadaaki Oshio</a>
|
<a href=/people/t/tomoharu-mitsuhashi/>Tomoharu Mitsuhashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5713><div class="card-body p-3 small">Japio participates in patent subtasks (JPC-EJ / JE / CJ / KJ) with phrase-based statistical machine translation (SMT) and neural machine translation (NMT) systems which are trained with its own patent corpora in addition to the subtask corpora provided by organizers of WAT2017. In EJ and CJ subtasks, SMT and NMT systems whose sizes of training corpora are about 50 million and 10 million sentence pairs respectively achieved comparable scores for automatic evaluations, but NMT systems were superior to SMT systems for both official and in-house human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5714 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5714" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5714/>Kyoto University Participation to WAT 2017<span class=acl-fixed-case>K</span>yoto <span class=acl-fixed-case>U</span>niversity Participation to <span class=acl-fixed-case>WAT</span> 2017</a></strong><br><a href=/people/f/fabien-cromieres/>Fabien Cromieres</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5714><div class="card-body p-3 small">We describe here our approaches and results on the WAT 2017 shared translation tasks. Following our good results with <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> in the previous shared task, we continue this approach this year, with incremental improvements in <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and training methods. We focused on the ASPEC dataset and could improve the state-of-the-art results for Chinese-to-Japanese and Japanese-to-Chinese translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5715.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5715 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5715 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5715/>CUNI NMT System for WAT 2017 Translation Tasks<span class=acl-fixed-case>CUNI</span> <span class=acl-fixed-case>NMT</span> System for <span class=acl-fixed-case>WAT</span> 2017 Translation Tasks</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/d/dusan-varis/>Dušan Variš</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5715><div class="card-body p-3 small">The paper presents this year&#8217;s CUNI submissions to the WAT 2017 Translation Task focusing on the Japanese-English translation, namely Scientific papers subtask, Patents subtask and Newswire subtask. We compare two neural network architectures, the standard sequence-to-sequence with attention (Seq2Seq) and an architecture using convolutional sentence encoder (FBConv2Seq), both implemented in the NMT framework Neural Monkey that we currently participate in developing. We also compare various types of preprocessing of the source <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>Japanese sentences</a> and their impact on the overall results. Furthermore, we include the results of our experiments with out-of-domain data obtained by combining the corpora provided for each subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5716 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5716/>Tokyo Metropolitan University Neural Machine Translation System for WAT 2017<span class=acl-fixed-case>T</span>okyo Metropolitan University Neural Machine Translation System for <span class=acl-fixed-case>WAT</span> 2017</a></strong><br><a href=/people/y/yukio-matsumura/>Yukio Matsumura</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5716><div class="card-body p-3 small">In this paper, we describe our neural machine translation (NMT) system, which is based on the attention-based NMT and uses long short-term memories (LSTM) as RNN. We implemented <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> and ensemble decoding in the NMT system. The <a href=https://en.wikipedia.org/wiki/System>system</a> was tested on the 4th Workshop on Asian Translation (WAT 2017) shared tasks. In our experiments, we participated in the scientific paper subtasks and attempted Japanese-English, English-Japanese, and Japanese-Chinese translation tasks. The experimental results showed that implementation of <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> and ensemble decoding can effectively improve the translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5717.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5717 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5717 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5717/>Comparing Recurrent and Convolutional Architectures for English-Hindi Neural Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>H</span>indi Neural Machine Translation</a></strong><br><a href=/people/s/sandhya-singh/>Sandhya Singh</a>
|
<a href=/people/r/ritesh-panjwani/>Ritesh Panjwani</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5717><div class="card-body p-3 small">In this paper, we empirically compare the two encoder-decoder neural machine translation architectures : convolutional sequence to sequence model (ConvS2S) and recurrent sequence to sequence model (RNNS2S) for English-Hindi language pair as part of IIT Bombay&#8217;s submission to WAT2017 shared task. We report the results for both English-Hindi and Hindi-English direction of language pair.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>