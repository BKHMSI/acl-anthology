<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W18-34.pdf>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource <span class=acl-fixed-case>NLP</span></a></h2><p class=lead><a href=/people/g/gholamreza-haffari/>Reza Haffari</a>,
<a href=/people/c/colin-cherry/>Colin Cherry</a>,
<a href=/people/g/george-foster/>George Foster</a>,
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>,
<a href=/people/b/bahar-salehi/>Bahar Salehi</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W18-34</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Melbourne</dd><dt>Venues:</dt><dd><a href=/venues/acl/>ACL</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W18-34>https://aclanthology.org/W18-34</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W18-34.pdf>https://aclanthology.org/W18-34.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W18-34.pdf title="Open PDF of 'Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Workshop+on+Deep+Learning+Approaches+for+Low-Resource+NLP" title="Search for 'Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3400/>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/g/gholamreza-haffari/>Reza Haffari</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/b/bahar-salehi/>Bahar Salehi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3403.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3403/>Multi-task learning for historical text normalization : Size matters</a></strong><br><a href=/people/m/marcel-bollmann/>Marcel Bollmann</a>
|
<a href=/people/a/anders-sogaard/>Anders SÃ¸gaard</a>
|
<a href=/people/j/joachim-bingel/>Joachim Bingel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3403><div class="card-body p-3 small">Historical text normalization suffers from small datasets that exhibit high variance, and previous work has shown that <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> can be used to leverage data from related problems in order to obtain more robust models. Previous work has been limited to datasets from a specific language and a specific historical period, and it is not clear whether results generalize. It therefore remains an open problem, when historical text normalization benefits from <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. We explore the benefits of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> across 10 different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, representing different languages and periods. Our main findingcontrary to what has been observed for other NLP tasksis that <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> mainly works when target task data is very scarce.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3404/>Compositional Language Modeling for Icon-Based Augmentative and Alternative Communication</a></strong><br><a href=/people/s/shiran-dudy/>Shiran Dudy</a>
|
<a href=/people/s/steven-bedrick/>Steven Bedrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3404><div class="card-body p-3 small">Icon-based communication systems are widely used in the field of <a href=https://en.wikipedia.org/wiki/Augmentative_and_alternative_communication>Augmentative and Alternative Communication</a>. Typically, icon-based systems have lagged behind word- and character-based systems in terms of predictive typing functionality, due to the challenges inherent to training icon-based language models. We propose a method for synthesizing training data for use in icon-based language models, and explore two different modeling strategies. We propose a method to generate <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> for corpus-less symbol-set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3405/>Multimodal Neural Machine Translation for Low-resource Language Pairs using Synthetic Data</a></strong><br><a href=/people/k/koel-dutta-chowdhury/>Koel Dutta Chowdhury</a>
|
<a href=/people/m/mohammed-hasanuzzaman/>Mohammed Hasanuzzaman</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3405><div class="card-body p-3 small">In this paper, we investigate the effectiveness of training a multimodal neural machine translation (MNMT) system with image features for a low-resource language pair, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, using synthetic data. A three-way parallel corpus which contains <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual texts</a> and corresponding images is required to train a MNMT system with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>image features</a>. However, such a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is not available for low resource language pairs. To address this, we developed both a synthetic training dataset and a manually curated development / test dataset for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> based on an existing English-image parallel corpus. We used these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to build our image description translation system by adopting state-of-the-art MNMT models. Our results show that it is possible to train a MNMT system for low-resource language pairs through the use of synthetic data and that such a <a href=https://en.wikipedia.org/wiki/System>system</a> can benefit from image features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3406/>Multi-Task Active Learning for Neural Semantic Role Labeling on Low Resource Conversational Corpus</a></strong><br><a href=/people/f/fariz-ikhwantri/>Fariz Ikhwantri</a>
|
<a href=/people/s/samuel-louvan/>Samuel Louvan</a>
|
<a href=/people/k/kemal-kurniawan/>Kemal Kurniawan</a>
|
<a href=/people/b/bagas-abisena/>Bagas Abisena</a>
|
<a href=/people/v/valdi-rachman/>Valdi Rachman</a>
|
<a href=/people/a/alfan-farizki-wicaksono/>Alfan Farizki Wicaksono</a>
|
<a href=/people/r/rahmad-mahendra/>Rahmad Mahendra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3406><div class="card-body p-3 small">Most Semantic Role Labeling (SRL) approaches are supervised methods which require a significant amount of annotated corpus, and the annotation requires linguistic expertise. In this paper, we propose a Multi-Task Active Learning framework for Semantic Role Labeling with Entity Recognition (ER) as the auxiliary task to alleviate the need for extensive data and use additional information from ER to help SRL. We evaluate our approach on Indonesian conversational dataset. Our experiments show that multi-task active learning can outperform single-task active learning method and standard <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. According to our results, <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> is more efficient by using 12 % less of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> compared to <a href=https://en.wikipedia.org/wiki/Passive_learning>passive learning</a> in both single-task and multi-task setting. We also introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for SRL in Indonesian conversational domain to encourage further research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3407.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3407/>Domain Adapted Word Embeddings for Improved Sentiment Classification</a></strong><br><a href=/people/p/prathusha-kameswara-sarma/>Prathusha Kameswara Sarma</a>
|
<a href=/people/y/yingyu-liang/>Yingyu Liang</a>
|
<a href=/people/b/bill-sethares/>Bill Sethares</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3407><div class="card-body p-3 small">Generic word embeddings are trained on large-scale generic corpora ; Domain Specific (DS) word embeddings are trained only on data from a domain of interest. This paper proposes a method to combine the breadth of generic embeddings with the specificity of domain specific embeddings. The resulting embeddings, called Domain Adapted (DA) word embeddings, are formed by first aligning corresponding word vectors using Canonical Correlation Analysis (CCA) or the related nonlinear Kernel CCA (KCCA) and then combining them via <a href=https://en.wikipedia.org/wiki/Convex_optimization>convex optimization</a>. Results from evaluation on sentiment classification tasks show that the DA embeddings substantially outperform both generic, DS embeddings when used as input features to standard or state-of-the-art sentence encoding algorithms for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3408/>Investigating Effective Parameters for Fine-tuning of Word Embeddings Using Only a Small Corpus</a></strong><br><a href=/people/k/kanako-komiya/>Kanako Komiya</a>
|
<a href=/people/h/hiroyuki-shinnou/>Hiroyuki Shinnou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3408><div class="card-body p-3 small">Fine-tuning is a popular <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> to achieve better performance when only a small target corpus is available. However, <a href=https://en.wikipedia.org/wiki/Italian_language>it</a> requires tuning of a number of metaparameters and thus it might carry risk of adverse effect when inappropriate metaparameters are used. Therefore, we investigate effective parameters for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> when only a small target corpus is available. In the current study, we target at improving Japanese word embeddings created from a <a href=https://en.wikipedia.org/wiki/Text_corpus>huge corpus</a>. First, we demonstrate that even the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> created from the huge corpus are affected by domain shift. After that, we investigate effective <a href=https://en.wikipedia.org/wiki/Parameter>parameters</a> for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> of the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> using a small target corpus. We used perplexity of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> obtained from a Long Short-Term Memory network to assess the word embeddings input into the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a>. The experiments revealed that <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> sometimes give adverse effect when only a small target corpus is used and batch size is the most important parameter for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. In addition, we confirmed that effect of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is higher when size of a target corpus was larger.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-3409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-3409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-3409.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-3409/>Semi-Supervised Learning with Auxiliary Evaluation Component for Large Scale e-Commerce Text Classification</a></strong><br><a href=/people/m/mingkuan-liu/>Mingkuan Liu</a>
|
<a href=/people/m/musen-wen/>Musen Wen</a>
|
<a href=/people/s/selcuk-kopru/>Selcuk Kopru</a>
|
<a href=/people/x/xianjing-liu/>Xianjing Liu</a>
|
<a href=/people/a/alan-lu/>Alan Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-3409><div class="card-body p-3 small">The lack of high-quality labeled training data has been one of the critical challenges facing many industrial machine learning tasks. To tackle this challenge, in this paper, we propose a semi-supervised learning method to utilize unlabeled data and user feedback signals to improve the performance of ML models. The method employs a primary model Main and an auxiliary evaluation model Eval, where Main and Eval models are trained iteratively by automatically generating labeled data from unlabeled data and/or users&#8217; feedback signals. The proposed approach is applied to different text classification tasks. We report results on both the publicly available Yahoo ! Answers dataset and our e-commerce product classification dataset. The experimental results show that the proposed method reduces the classification error rate by 4 % and up to 15 % across various experimental setups and datasets. A detailed comparison with other semi-supervised learning approaches is also presented later in the paper. The results from various text classification tasks demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms those developed in previous related studies.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>