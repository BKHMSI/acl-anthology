<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2021.nodalida-main.pdf>Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)</a></h2><p class=lead><a href=/people/s/simon-dobnik/>Simon Dobnik</a>,
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.nodalida-main</dd><dt>Month:</dt><dd>May 31--2 June</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Reykjavik, Iceland (Online)</dd><dt>Venue:</dt><dd><a href=/venues/nodalida/>NoDaLiDa</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Linköping University Electronic Press, Sweden</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.nodalida-main>https://aclanthology.org/2021.nodalida-main</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.nodalida-main.pdf>https://aclanthology.org/2021.nodalida-main.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.nodalida-main.pdf title="Open PDF of 'Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+23rd+Nordic+Conference+on+Computational+Linguistics+%28NoDaLiDa%29" title="Search for 'Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.0/>Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)</a></strong><br><a href=/people/s/simon-dobnik/>Simon Dobnik</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.3/>Operationalizing a National Digital Library : The Case for a Norwegian Transformer Model<span class=acl-fixed-case>N</span>orwegian Transformer Model</a></strong><br><a href=/people/p/per-e-kummervold/>Per E Kummervold</a>
|
<a href=/people/j/javier-de-la-rosa/>Javier De la Rosa</a>
|
<a href=/people/f/freddy-wetjen/>Freddy Wetjen</a>
|
<a href=/people/s/svein-arne-brygfjeld/>Svein Arne Brygfjeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--3><div class="card-body p-3 small">In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library. The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian</a> outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian Bokml</a> and <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian Nynorsk</a>. Our model also improves the mBERT performance for other languages present in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> such as <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>, and <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>. For <a href=https://en.wikipedia.org/wiki/Language>languages</a> not included in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, the weights degrade moderately while keeping strong multilingual properties. Therefore, we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nodalida-main.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.4/>Large-Scale Contextualised Language Modelling for <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian</a><span class=acl-fixed-case>N</span>orwegian</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--4><div class="card-body p-3 small">We present the ongoing NorLM initiative to support the creation and use of very large contextualised language models for <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian</a> (and in principle other Nordic languages), including a ready-to-use software environment, as well as an experience report for data preparation and training. This paper introduces the first large-scale monolingual language models for <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian</a>, based on both the ELMo and BERT frameworks. In addition to detailing the training process, we present contrastive benchmark results on a suite of NLP tasks for <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian</a>. For additional background and access to the data, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, and software, please see : http://norlm.nlpl.eu</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.11/>A Baseline Document Planning Method for <a href=https://en.wikipedia.org/wiki/Automated_journalism>Automated Journalism</a></a></strong><br><a href=/people/l/leo-leppanen/>Leo Leppänen</a>
|
<a href=/people/h/hannu-toivonen/>Hannu Toivonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--11><div class="card-body p-3 small">In this work, we present a method for content selection and document planning for automated news and report generation from structured statistical data such as that offered by the European Union&#8217;s statistical agency, <a href=https://en.wikipedia.org/wiki/EuroStat>EuroStat</a>. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is driven by the data and is highly topic-independent within the <a href=https://en.wikipedia.org/wiki/Data_set>statistical dataset domain</a>. As our approach is not based on <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, it is suitable for introducing news automation to the wide variety of domains where no <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is available. As such, it is suitable as a low-cost (in terms of implementation effort) baseline for document structuring prior to introduction of domain-specific knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.16/>Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification</a></strong><br><a href=/people/s/samuel-ronnqvist/>Samuel Rönnqvist</a>
|
<a href=/people/v/valtteri-skantsi/>Valtteri Skantsi</a>
|
<a href=/people/m/miika-oinonen/>Miika Oinonen</a>
|
<a href=/people/v/veronika-laippala/>Veronika Laippala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--16><div class="card-body p-3 small">This article studies register classification of documents from the unrestricted web, such as news articles or opinion blogs, in a multilingual setting, exploring both the benefit of training on multiple languages and the capabilities for zero-shot cross-lingual transfer. While the wide range of linguistic variation found on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved. In this study, we show that training on multiple languages 1) benefits languages with limited amounts of register-annotated data, 2) on average achieves performance on par with monolingual models, and 3) greatly improves upon previous zero-shot results in <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. The best results are achieved with the multilingual XLM-R model. As data, we use the CORE corpus series featuring register annotated data from the unrestricted web.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nodalida-main.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.21/>De-identification of Privacy-related Entities in Job Postings</a></strong><br><a href=/people/k/kristian-norgaard-jensen/>Kristian Nørgaard Jensen</a>
|
<a href=/people/m/mike-zhang/>Mike Zhang</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--21><div class="card-body p-3 small">De-identification is the task of detecting privacy-related entities in text, such as person names, <a href=https://en.wikipedia.org/wiki/Email>emails</a> and contact data. It has been well-studied within the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>. The need for de-identification technology is increasing, as privacy-preserving data handling is in high demand in many domains. In this paper, we focus on <a href=https://en.wikipedia.org/wiki/Employment_website>job postings</a>. We present JobStack, a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for de-identification of personal data in job vacancies on <a href=https://en.wikipedia.org/wiki/Stackoverflow>Stackoverflow</a>. We introduce <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, comparing Long-Short Term Memory (LSTM) and Transformer models. To improve these baselines, we experiment with BERT representations, and distantly related auxiliary data via <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Our results show that auxiliary data helps to improve <a href=https://en.wikipedia.org/wiki/De-identification>de-identification</a> performance. While BERT representations improve performance, surprisingly vanilla BERT turned out to be more effective than BERT trained on <a href=https://en.wikipedia.org/wiki/Stackoverflow>Stackoverflow-related data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nodalida-main.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.28/>NLI Data Sanity Check : Assessing the Effect of <a href=https://en.wikipedia.org/wiki/Data_corruption>Data Corruption</a> on Model Performance<span class=acl-fixed-case>NLI</span> Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--28><div class="card-body p-3 small">Pre-trained neural language models give high performance on natural language inference (NLI) tasks. But whether they actually understand the meaning of the processed sequences is still unclear. We propose a new diagnostics test suite which allows to assess whether a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> constitutes a good testbed for evaluating the models&#8217; meaning understanding capabilities. We specifically apply controlled corruption transformations to widely used <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs. If model accuracy on the corrupted data remains high, then the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is likely to contain <a href=https://en.wikipedia.org/wiki/Bias_(statistics)>statistical biases</a> and artefacts that guide <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. Inversely, a large decrease in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>model accuracy</a> indicates that the original <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> provides a proper challenge to the models&#8217; reasoning capabilities. Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.33/>Towards cross-lingual application of language-specific PoS tagging schemes<span class=acl-fixed-case>P</span>o<span class=acl-fixed-case>S</span> tagging schemes</a></strong><br><a href=/people/h/hinrik-hafsteinsson/>Hinrik Hafsteinsson</a>
|
<a href=/people/a/anton-karl-ingason/>Anton Karl Ingason</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--33><div class="card-body p-3 small">We describe the process of conversion between the PoS tagging schemes of two languages, the Icelandic MIM-GOLD tagging scheme and the Faroese Sosialurin tagging scheme. These tagging schemes are functionally similar but use separate ways to encode fine-grained morphological information on tokenised text. As Faroese and Icelandic are lexically and grammatically similar, having a systematic method to convert between these two tagging schemes would be beneficial in the field of <a href=https://en.wikipedia.org/wiki/Language_technology>language technology</a>, specifically in research on <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> between the two languages. As a product of our work, we present a provisional version of Icelandic corpora, prepared in the Faroese PoS tagging scheme, ready for use in cross-lingual NLP applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nodalida-main.34" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.34/>Exploring the Importance of Source Text in Automatic Post-Editing for Context-Aware Machine Translation</a></strong><br><a href=/people/c/chaojun-wang/>Chaojun Wang</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--34><div class="card-body p-3 small">Accurate <a href=https://en.wikipedia.org/wiki/Translation>translation</a> requires document-level information, which is ignored by sentence-level machine translation. Recent work has demonstrated that document-level consistency can be improved with automatic post-editing (APE) using only target-language (TL) information. We study an extended APE model that additionally integrates <a href=https://en.wikipedia.org/wiki/Context_(language_use)>source context</a>. A human evaluation of fluency and adequacy in EnglishRussian translation reveals that the model with access to source context significantly outperforms monolingual APE in terms of adequacy, an effect largely ignored by automatic evaluation metrics. Our results show that TL-only modelling increases <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> without improving adequacy, demonstrating the need for conditioning on source text for automatic post-editing. They also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nodalida-main.36" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.36/>Grapheme-Based Cross-Language Forced Alignment : Results with <a href=https://en.wikipedia.org/wiki/Uralic_languages>Uralic Languages</a></a></strong><br><a href=/people/j/juho-leinonen/>Juho Leinonen</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a>
|
<a href=/people/m/mikko-kurimo/>Mikko Kurimo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--36><div class="card-body p-3 small">Forced alignment is an effective <a href=https://en.wikipedia.org/wiki/Process_(engineering)>process</a> to speed up <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic research</a>. However, most forced aligners are language-dependent, and under-resourced languages rarely have enough resources to train an <a href=https://en.wikipedia.org/wiki/Acoustic_model>acoustic model</a> for an aligner. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple <a href=https://en.wikipedia.org/wiki/Uralic_languages>Uralic languages</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> as an unrelated language. We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.40/>Decentralized Word2Vec Using Gossip Learning<span class=acl-fixed-case>W</span>ord2<span class=acl-fixed-case>V</span>ec Using Gossip Learning</a></strong><br><a href=/people/a/abdul-aziz-alkathiri/>Abdul Aziz Alkathiri</a>
|
<a href=/people/l/lodovico-giaretta/>Lodovico Giaretta</a>
|
<a href=/people/s/sarunas-girdzijauskas/>Sarunas Girdzijauskas</a>
|
<a href=/people/m/magnus-sahlgren/>Magnus Sahlgren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--40><div class="card-body p-3 small">Advanced NLP models require huge amounts of data from various domains to produce high-quality representations. It is useful then for a few large public and private organizations to join their corpora during training. However, factors such as legislation and user emphasis on <a href=https://en.wikipedia.org/wiki/Information_privacy>data privacy</a> may prevent centralized orchestration and <a href=https://en.wikipedia.org/wiki/Data_sharing>data sharing</a> among these organizations. Therefore, for this specific scenario, we investigate how gossip learning, a massively-parallel, data-private, decentralized protocol, compares to a shared-dataset solution. We find that the application of Word2Vec in a gossip learning framework is viable. Without any tuning, the results are comparable to a traditional centralized setting, with a loss of quality as low as 4.3 %. Furthermore, the results are up to 54.8 % better than independent local training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.41/>Multilingual ELMo and the Effects of Corpus Sampling<span class=acl-fixed-case>ELM</span>o and the Effects of Corpus Sampling</a></strong><br><a href=/people/v/vinit-ravishankar/>Vinit Ravishankar</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--41><div class="card-body p-3 small">Multilingual pretrained language models are rapidly gaining popularity in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a> for non-English languages. Most of these models feature an important corpus sampling step in the process of accumulating training data in different languages, to ensure that the signal from better resourced languages does not drown out poorly resourced ones. In this study, we train multiple multilingual recurrent language models, based on the ELMo architecture, and analyse both the effect of varying corpus size ratios on downstream performance, as well as the performance difference between monolingual models for each language, and broader multilingual language models. As part of this effort, we also make these trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> available for public use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.46/>The Danish Gigaword Corpus<span class=acl-fixed-case>D</span>anish <span class=acl-fixed-case>G</span>igaword Corpus</a></strong><br><a href=/people/l/leon-derczynski/>Leon Strømberg-Derczynski</a>
|
<a href=/people/m/manuel-r-ciosici/>Manuel Ciosici</a>
|
<a href=/people/r/rebekah-baglini/>Rebekah Baglini</a>
|
<a href=/people/m/morten-h-christiansen/>Morten H. Christiansen</a>
|
<a href=/people/j/jacob-aarup-dalsgaard/>Jacob Aarup Dalsgaard</a>
|
<a href=/people/r/riccardo-fusaroli/>Riccardo Fusaroli</a>
|
<a href=/people/p/peter-juel-henrichsen/>Peter Juel Henrichsen</a>
|
<a href=/people/r/rasmus-hvingelby/>Rasmus Hvingelby</a>
|
<a href=/people/a/andreas-kirkedal/>Andreas Kirkedal</a>
|
<a href=/people/a/alex-speed-kjeldsen/>Alex Speed Kjeldsen</a>
|
<a href=/people/c/claus-ladefoged/>Claus Ladefoged</a>
|
<a href=/people/f/finn-arup-nielsen/>Finn Årup Nielsen</a>
|
<a href=/people/j/jens-madsen/>Jens Madsen</a>
|
<a href=/people/m/malte-lau-petersen/>Malte Lau Petersen</a>
|
<a href=/people/j/jonathan-hvithamar-rystrom/>Jonathan Hvithamar Rystrøm</a>
|
<a href=/people/d/daniel-varab/>Daniel Varab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--46><div class="card-body p-3 small">Danish language technology has been hindered by a lack of broad-coverage corpora at the scale modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> prefers. This paper describes the Danish Gigaword Corpus, the result of a focused effort to provide a diverse and freely-available one billion word corpus of <a href=https://en.wikipedia.org/wiki/Danish_language>Danish text</a>. The Danish Gigaword corpus covers a wide array of time periods, domains, speakers&#8217; socio-economic status, and <a href=https://en.wikipedia.org/wiki/Danish_dialects>Danish dialects</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.47.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--47 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.47 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nodalida-main.47" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.47/>DanFEVER : claim verification dataset for Danish<span class=acl-fixed-case>D</span>an<span class=acl-fixed-case>FEVER</span>: claim verification dataset for <span class=acl-fixed-case>D</span>anish</a></strong><br><a href=/people/j/jeppe-norregaard/>Jeppe Nørregaard</a>
|
<a href=/people/l/leon-derczynski/>Leon Derczynski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--47><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, DanFEVER, intended for multilingual misinformation research. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is in <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> and has the same format as the well-known English FEVER dataset. It can be used for testing methods in multilingual settings, as well as for creating models in production for the <a href=https://en.wikipedia.org/wiki/Danish_language>Danish language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nodalida-main.51" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.51/>NorDial : A Preliminary Corpus of Written Norwegian Dialect Use<span class=acl-fixed-case>N</span>or<span class=acl-fixed-case>D</span>ial: A Preliminary Corpus of Written <span class=acl-fixed-case>N</span>orwegian Dialect Use</a></strong><br><a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/p/petter-maehlum/>Petter Mæhlum</a>
|
<a href=/people/s/samia-touileb/>Samia Touileb</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--51><div class="card-body p-3 small">Norway has a large amount of dialectal variation, as well as a general tolerance to its use in the public sphere. There are, however, few available resources to study this variation and its change over time and in more informal areas, on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In this paper, we propose a first step to creating a <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus of dialectal variation</a> of <a href=https://en.wikipedia.org/wiki/Norwegian_language>written Norwegian</a>. We collect a small corpus of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> and manually annotate them as Bokml, <a href=https://en.wikipedia.org/wiki/Nynorsk>Nynorsk</a>, any dialect, or a mix. We further perform preliminary experiments with state-of-the-art models, as well as an analysis of the data to expand this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> in the future. Finally, we make the annotations available for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.52/>The Swedish Winogender Dataset<span class=acl-fixed-case>S</span>wedish <span class=acl-fixed-case>W</span>inogender Dataset</a></strong><br><a href=/people/s/saga-hansson/>Saga Hansson</a>
|
<a href=/people/k/konstantinos-mavromatakis/>Konstantinos Mavromatakis</a>
|
<a href=/people/y/yvonne-adesam/>Yvonne Adesam</a>
|
<a href=/people/g/gerlof-bouma/>Gerlof Bouma</a>
|
<a href=/people/d/dana-dannells/>Dana Dannélls</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--52><div class="card-body p-3 small">We introduce the SweWinogender test set, a diagnostic dataset to measure gender bias in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. It is modelled after the English Winogender benchmark, and is released with reference statistics on the distribution of men and women between occupations and the association between gender and occupation in modern corpus material. The paper discusses the design and creation of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, and presents a small investigation of the supplementary statistics.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>