<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/N19-1.pdf>Proceedings of the 2019 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></h2><p class=lead><a href=/people/j/jill-burstein/>Jill Burstein</a>,
<a href=/people/c/christy-doran/>Christy Doran</a>,
<a href=/people/t/thamar-solorio/>Thamar Solorio</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>N19-1</dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Minneapolis, Minnesota</dd><dt>Venue:</dt><dd><a href=/venues/naacl/>NAACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/N19-1>https://aclanthology.org/N19-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/N19-1.pdf>https://aclanthology.org/N19-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/N19-1.pdf title="Open PDF of 'Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2019+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics%3A+Human+Language+Technologies%2C+Volume+1+%28Long+and+Short+Papers%29" title="Search for 'Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1000/>Proceedings of the 2019 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></strong><br><a href=/people/j/jill-burstein/>Jill Burstein</a>
|
<a href=/people/c/christy-doran/>Christy Doran</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347364761 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1001/>Entity Recognition at First Sight : Improving <a href=https://en.wikipedia.org/wiki/Near-sightedness>NER</a> with Eye Movement Information<span class=acl-fixed-case>I</span>mproving <span class=acl-fixed-case>NER</span> with Eye Movement Information</a></strong><br><a href=/people/n/nora-hollenstein/>Nora Hollenstein</a>
|
<a href=/people/c/ce-zhang/>Ce Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1001><div class="card-body p-3 small">Previous research shows that eye-tracking data contains information about the lexical and syntactic properties of text, which can be used to improve <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language processing models</a>. In this work, we leverage eye movement features from three corpora with recorded gaze information to augment a state-of-the-art neural model for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition (NER)</a> with gaze embeddings. These <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> were manually annotated with named entity labels. Moreover, we show how gaze features, generalized on word type level, eliminate the need for recorded eye-tracking data at test time. The gaze-augmented models for NER using token-level and type-level features outperform the baselines. We present the benefits of eye-tracking features by evaluating the NER models on both individual datasets as well as in cross-domain settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1002.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347368203 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1002/>The emergence of number and syntax units in LSTM language models<span class=acl-fixed-case>LSTM</span> language models</a></strong><br><a href=/people/y/yair-lakretz/>Yair Lakretz</a>
|
<a href=/people/g/german-kruszewski/>German Kruszewski</a>
|
<a href=/people/t/theo-desbordes/>Theo Desbordes</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/s/stanislas-dehaene/>Stanislas Dehaene</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1002><div class="card-body p-3 small">Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> that do not truly take <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structure</a> into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two number units. Importantly, the behaviour of these <a href=https://en.wikipedia.org/wiki/Unit_of_measurement>units</a> is partially controlled by other units independently shown to track <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347377574 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1004/>Neural language models as psycholinguistic subjects : Representations of syntactic state</a></strong><br><a href=/people/r/richard-futrell/>Richard Futrell</a>
|
<a href=/people/e/ethan-wilcox/>Ethan Wilcox</a>
|
<a href=/people/t/takashi-morita/>Takashi Morita</a>
|
<a href=/people/p/peng-qian/>Peng Qian</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1004><div class="card-body p-3 small">We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the <a href=https://en.wikipedia.org/wiki/Social_network>networks</a> have a representation of syntactic state, they also reveal the specific lexical cues that <a href=https://en.wikipedia.org/wiki/Social_network>networks</a> use to update these states. We test four models : two publicly available LSTM sequence models of English (Jozefowicz et al., 2016 ; Gulordava et al., 2018) trained on large datasets ; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset ; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, but only the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347381430 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1005/>Understanding language-elicited EEG data by predicting it from a fine-tuned language model<span class=acl-fixed-case>EEG</span> data by predicting it from a fine-tuned language model</a></strong><br><a href=/people/d/dan-schwartz/>Dan Schwartz</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1005><div class="card-body p-3 small">Electroencephalography (EEG) recordings of brain activity taken while participants read or listen to language are widely used within the <a href=https://en.wikipedia.org/wiki/Cognitive_neuroscience>cognitive neuroscience</a> and psycholinguistics communities as a tool to study <a href=https://en.wikipedia.org/wiki/Sentence_processing>language comprehension</a>. Several time-locked stereotyped EEG responses to word-presentations known collectively as event-related potentials (ERPs) are thought to be markers for semantic or syntactic processes that take place during comprehension. However, the characterization of each individual <a href=https://en.wikipedia.org/wiki/Enterprise_resource_planning>ERP</a> in terms of what features of a stream of language trigger the response remains controversial. Improving this characterization would make ERPs a more useful tool for studying <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>language comprehension</a>. We take a step towards better understanding the ERPs by finetuning a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to predict them. This new approach to analysis shows for the first time that all of the ERPs are predictable from embeddings of a stream of language. Prior work has only found two of the <a href=https://en.wikipedia.org/wiki/Excitatory_postsynaptic_potential>ERPs</a> to be predictable. In addition to this <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a>, we examine which ERPs benefit from sharing parameters during joint training. We find that two pairs of <a href=https://en.wikipedia.org/wiki/Excitatory_postsynaptic_potential>ERPs</a> previously identified in the literature as being related to each other benefit from joint training, while several other pairs of <a href=https://en.wikipedia.org/wiki/Excitatory_postsynaptic_potential>ERPs</a> that benefit from joint training are suggestive of potential relationships. Extensions of this analysis that further examine what kinds of information in the model embeddings relate to each ERP have the potential to elucidate the processes involved in human language comprehension.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353440477 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1007/>Measuring the perceptual availability of <a href=https://en.wikipedia.org/wiki/Phonology>phonological features</a> during <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a> using unsupervised binary stochastic autoencoders</a></strong><br><a href=/people/c/cory-shain/>Cory Shain</a>
|
<a href=/people/m/micha-elsner/>Micha Elsner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1007><div class="card-body p-3 small">In this paper, we deploy binary stochastic neural autoencoder networks as models of infant language learning in two typologically unrelated languages (Xitsonga and English). We show that the drive to model auditory percepts leads to latent clusters that partially align with theory-driven phonemic categories. We further evaluate the degree to which theory-driven phonological features are encoded in the latent bit patterns, finding that some (e.g. [ + -approximant ]), are well represented by the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> in both languages, while others (e.g. [ + -spread glottis ]) are less so. Together, these findings suggest that many reliable cues to phonemic structure are immediately available to infants from <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>bottom-up perceptual characteristics</a> alone, but that these cues must eventually be supplemented by <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>top-down lexical and phonotactic information</a> to achieve adult-like phone discrimination. Our results also suggest differences in degree of perceptual availability between <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a>, yielding testable predictions as to which <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> might depend more or less heavily on <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>top-down cues</a> during child language acquisition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353444632 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1008/>Giving Attention to the Unexpected : Using Prosody Innovations in Disfluency Detection</a></strong><br><a href=/people/v/vicky-zayats/>Vicky Zayats</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1008><div class="card-body p-3 small">Disfluencies in spontaneous speech are known to be associated with prosodic disruptions. However, most <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for disfluency detection use only <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>word transcripts</a>. Integrating prosodic cues has proved difficult because of the many sources of variability affecting the acoustic correlates. This paper introduces a new approach to extracting acoustic-prosodic cues using text-based distributional prediction of acoustic cues to derive vector z-score features (innovations). We explore both early and late fusion techniques for integrating text and prosody, showing gains over a high-accuracy text-only model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353450338 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1009/>Massively Multilingual Adversarial Speech Recognition</a></strong><br><a href=/people/o/oliver-adams/>Oliver Adams</a>
|
<a href=/people/m/matthew-wiesner/>Matthew Wiesner</a>
|
<a href=/people/s/shinji-watanabe/>Shinji Watanabe</a>
|
<a href=/people/d/david-yarowsky/>David Yarowsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1009><div class="card-body p-3 small">We report on adaptation of multilingual end-to-end speech recognition models trained on as many as 100 languages. Our findings shed light on the relative importance of similarity between the target and pretraining languages along the dimensions of <a href=https://en.wikipedia.org/wiki/Phonetics>phonetics</a>, <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a>, <a href=https://en.wikipedia.org/wiki/Language_family>language family</a>, geographical location, and <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a>. In this context, experiments demonstrate the effectiveness of two additional pretraining objectives in encouraging language-independent encoder representations : a context-independent phoneme objective paired with a language-adversarial classification objective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1013.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353418933 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1013/>Answer-based Adversarial Training for Generating Clarification Questions<span class=acl-fixed-case>A</span>nswer-based <span class=acl-fixed-case>A</span>dversarial <span class=acl-fixed-case>T</span>raining for <span class=acl-fixed-case>G</span>enerating <span class=acl-fixed-case>C</span>larification <span class=acl-fixed-case>Q</span>uestions</a></strong><br><a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1013><div class="card-body p-3 small">We present an approach for generating clarification questions with the goal of eliciting new information that would make the given textual context more complete. We propose that modeling hypothetical answers (to clarification questions) as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> can guide our approach into generating more useful clarification questions. We develop a Generative Adversarial Network (GAN) where the generator is a sequence-to-sequence model and the discriminator is a utility function that models the value of updating the context with the answer to the clarification question. We evaluate on two datasets, using both automatic metrics and human judgments of usefulness, specificity and relevance, showing that our approach outperforms both a retrieval-based model and ablations that exclude the utility model and the adversarial training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353425373 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1014" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1014/>Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data</a></strong><br><a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/l/liang-wang/>Liang Wang</a>
|
<a href=/people/k/kewei-shen/>Kewei Shen</a>
|
<a href=/people/r/ruoyu-jia/>Ruoyu Jia</a>
|
<a href=/people/j/jingming-liu/>Jingming Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1014><div class="card-body p-3 small">Neural machine translation systems have become state-of-the-art approaches for Grammatical Error Correction (GEC) task. In this paper, we propose a copy-augmented architecture for the GEC task by copying the unchanged words from the source sentence to the target sentence. Since the GEC suffers from not having enough labeled training data to achieve high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We pre-train the copy-augmented architecture with a denoising auto-encoder using the unlabeled One Billion Benchmark and make comparisons between the fully pre-trained model and a partially pre-trained model. It is the first time copying words from the source context and fully pre-training a sequence to sequence model are experimented on the GEC task. Moreover, We add token-level and sentence-level multi-task learning for the GEC task. The evaluation results on the CoNLL-2014 test set show that our approach outperforms all recently published state-of-the-art results by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353433493 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1015/>Topic-Guided Variational Auto-Encoder for Text Generation</a></strong><br><a href=/people/w/wenlin-wang/>Wenlin Wang</a>
|
<a href=/people/z/zhe-gan/>Zhe Gan</a>
|
<a href=/people/h/hongteng-xu/>Hongteng Xu</a>
|
<a href=/people/r/ruiyi-zhang/>Ruiyi Zhang</a>
|
<a href=/people/g/guoyin-wang/>Guoyin Wang</a>
|
<a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/c/changyou-chen/>Changyou Chen</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1015><div class="card-body p-3 small">We propose a topic-guided variational auto-encoder (TGVAE) model for text generation. Distinct from existing variational auto-encoder (VAE) based approaches, which assume a simple Gaussian prior for latent code, our model specifies the prior as a Gaussian mixture model (GMM) parametrized by a neural topic module. Each mixture component corresponds to a latent topic, which provides a guidance to generate sentences under the topic. The neural topic module and the VAE-based neural sequence module in our model are learned jointly. In particular, a sequence of invertible Householder transformations is applied to endow the approximate posterior of the latent code with high flexibility during the <a href=https://en.wikipedia.org/wiki/Statistical_inference>model inference</a>. Experimental results show that our TGVAE outperforms its competitors on both unconditional and conditional text generation, which can also generate semantically-meaningful sentences with various topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360494509 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1018/>Discontinuous Constituency Parsing with a Stack-Free Transition System and a Dynamic Oracle</a></strong><br><a href=/people/m/maximin-coavoux/>Maximin Coavoux</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1018><div class="card-body p-3 small">We introduce a novel <a href=https://en.wikipedia.org/wiki/Transition_system>transition system</a> for discontinuous constituency parsing. Instead of storing subtrees in a <a href=https://en.wikipedia.org/wiki/Stack_(abstract_data_type)>stack</a> i.e. a <a href=https://en.wikipedia.org/wiki/Data_structure>data structure</a> with linear-time sequential access the proposed system uses a set of parsing items, with constant-time random access. This change makes it possible to construct any discontinuous constituency tree in exactly 4n2 transitions for a sentence of length n. At each parsing step, the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> considers every item in the set to be combined with a focus item and to construct a new constituent in a bottom-up fashion. The parsing strategy is based on the assumption that most syntactic structures can be parsed incrementally and that the set the memory of the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> remains reasonably small on average. Moreover, we introduce a provably correct dynamic oracle for the new transition system, and present the first experiments in discontinuous constituency parsing using a dynamic oracle. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> obtains state-of-the-art results on three English and German discontinuous treebanks.<tex-math>4n&#8211;2</tex-math> transitions for a sentence of length n. At each parsing step, the parser considers every item in the set to be combined with a focus item and to construct a new constituent in a bottom-up fashion. The parsing strategy is based on the assumption that most syntactic structures can be parsed incrementally and that the set &#8211;the memory of the parser&#8211; remains reasonably small on average. Moreover, we introduce a provably correct dynamic oracle for the new transition system, and present the first experiments in discontinuous constituency parsing using a dynamic oracle. Our parser obtains state-of-the-art results on three English and German discontinuous treebanks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360516550 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1020/>CCG Parsing Algorithm with Incremental Tree Rotation<span class=acl-fixed-case>CCG</span> Parsing Algorithm with Incremental Tree Rotation</a></strong><br><a href=/people/m/milos-stanojevic/>Miloš Stanojević</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1020><div class="card-body p-3 small">The main obstacle to incremental sentence processing arises from right-branching constituent structures, which are present in the majority of English sentences, as well as optional constituents that adjoin on the right, such as <a href=https://en.wikipedia.org/wiki/Adjunct_(grammar)>right adjuncts</a> and right conjuncts. In CCG, many right-branching derivations can be replaced by semantically equivalent left-branching incremental derivations. The problem of right-adjunction is more resistant to solution, and has been tackled in the past using revealing-based approaches that often rely either on the <a href=https://en.wikipedia.org/wiki/Unification_(computer_science)>higher-order unification</a> over lambda terms (Pareschi and Steedman,1987) or heuristics over dependency representations that do not cover the whole CCGbank (Ambati et al., 2015). We propose a new incremental parsing algorithm for CCG following the same revealing tradition of work but having a purely syntactic approach that does not depend on access to a distinct level of semantic representation. This <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can cover the whole CCGbank, with greater incrementality and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than previous proposals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1021.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1021/>Cyclical Annealing Schedule : A Simple Approach to Mitigating KL Vanishing<span class=acl-fixed-case>KL</span> Vanishing</a></strong><br><a href=/people/h/hao-fu/>Hao Fu</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1021><div class="card-body p-3 small">Variational autoencoders (VAE) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. VAE objective consists of two terms, the KL regularization term and the reconstruction term, balanced by a weighting hyper-parameter. One notorious training difficulty is that the KL term tends to vanish. In this paper we study different scheduling schemes for, and show that KL vanishing is caused by the lack of good latent codes in training decoder at the beginning of <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a>. To remedy the issue, we propose a cyclical annealing schedule, which simply repeats the process of increasing multiple times. This new procedure allows us to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles as warm re-restart. The effectiveness of cyclical annealing schedule is validated on a broad range of NLP tasks, including <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, dialog response generation and <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised text classification</a>.<tex-math>\\beta</tex-math>. One notorious training difficulty is that the KL term tends to vanish. In this paper we study different scheduling schemes for <tex-math>\\beta</tex-math>, and show that KL vanishing is caused by the lack of good latent codes in training decoder at the beginning of optimization. To remedy the issue, we propose a cyclical annealing schedule, which simply repeats the process of increasing <tex-math>\\beta</tex-math> multiple times. This new procedure allows us to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles as warm re-restart. The effectiveness of cyclical annealing schedule is validated on a broad range of NLP tasks, including language modeling, dialog response generation and semi-supervised text classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1022/>Recurrent models and lower bounds for projective syntactic decoding</a></strong><br><a href=/people/n/natalie-schluter/>Natalie Schluter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1022><div class="card-body p-3 small">The current state-of-the-art in neural graph-based parsing uses only approximate decoding at the training phase. In this paper aim to understand this result better. We show how recurrent models can carry out projective maximum spanning tree decoding. This result holds for both current state-of-the-art models for shift-reduce and graph-based parsers, projective or not. We also provide the first proof on the <a href=https://en.wikipedia.org/wiki/Upper_and_lower_bounds>lower bounds</a> of projective maximum spanning tree decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1023" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1023/>Evaluating Composition Models for Verb Phrase Elliptical Sentence Embeddings</a></strong><br><a href=/people/g/gijs-wijnholds/>Gijs Wijnholds</a>
|
<a href=/people/m/mehrnoosh-sadrzadeh/>Mehrnoosh Sadrzadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1023><div class="card-body p-3 small">Ellipsis is a natural language phenomenon where part of a sentence is missing and its information must be recovered from its surrounding context, as in Cats chase dogs and so do foxes.. Formal semantics has different methods for resolving <a href=https://en.wikipedia.org/wiki/Ellipsis_(linguistics)>ellipsis</a> and recovering the missing information, but the problem has not been considered for <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>, where words have vector embeddings and combinations thereof provide <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for sentences. In elliptical sentences these combinations go beyond linear as copying of elided information is necessary. In this paper, we develop different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for embedding VP-elliptical sentences. We extend existing verb disambiguation and sentence similarity datasets to ones containing elliptical phrases and evaluate our models on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for a variety of non-linear combinations and their linear counterparts. We compare results of these compositional models to state of the art holistic sentence encoders. Our results show that non-linear addition and a non-linear tensor-based composition outperform the naive non-compositional baselines and the linear models, and that sentence encoders perform well on sentence similarity, but not on verb disambiguation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1025.Software.tar data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1025.Presentation.pptx data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1025" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1025/>Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for Text Modeling<span class=acl-fixed-case>R</span>iemannian Normalizing Flow on Variational <span class=acl-fixed-case>W</span>asserstein Autoencoder for Text Modeling</a></strong><br><a href=/people/p/prince-zizhuang-wang/>Prince Zizhuang Wang</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1025><div class="card-body p-3 small">Recurrent Variational Autoencoder has been widely used for language modeling and text generation tasks. These models often face a difficult optimization problem, also known as KL vanishing, where the <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior</a> easily collapses to the <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a> and model will ignore latent codes in generative tasks. To address this problem, we introduce an improved Variational Wasserstein Autoencoder (WAE) with Riemannian Normalizing Flow (RNF) for text modeling. The RNF transforms a <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> into a space that respects the geometric characteristics of input space, which makes posterior impossible to collapse to the <a href=https://en.wikipedia.org/wiki/Non-informative_prior>non-informative prior</a>. The Wasserstein objective minimizes the distance between <a href=https://en.wikipedia.org/wiki/Marginal_distribution>marginal distribution</a> and the <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a> directly and therefore does not force the posterior to match the <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a>. Empirical experiments show that our model avoids KL vanishing over a range of datasets and has better performance in tasks such as <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, <a href=https://en.wikipedia.org/wiki/Likelihood_function>likelihood approximation</a>, and text generation. Through a series of experiments and analysis over latent space, we show that our model learns latent distributions that respect latent space geometry and is able to generate sentences that are more diverse.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1027/>ComQA : A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters<span class=acl-fixed-case>C</span>om<span class=acl-fixed-case>QA</span>: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters</a></strong><br><a href=/people/a/abdalghani-abujabal/>Abdalghani Abujabal</a>
|
<a href=/people/r/rishiraj-saha-roy/>Rishiraj Saha Roy</a>
|
<a href=/people/m/mohamed-yahya/>Mohamed Yahya</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1027><div class="card-body p-3 small">To bridge the gap between the capabilities of the state-of-the-art in factoid question answering (QA) and what users ask, we need large datasets of real user questions that capture the various question phenomena users are interested in, and the diverse ways in which these <a href=https://en.wikipedia.org/wiki/Questionnaire>questions</a> are formulated. We introduce ComQA, a large dataset of real user questions that exhibit different challenging aspects such as <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a>, temporal reasoning, and comparisons. ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology. Through a large crowdsourcing effort, we clean the question dataset, group questions into paraphrase clusters, and annotate <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a> with their answers. ComQA contains 11,214 questions grouped into 4,834 paraphrase clusters. We detail the process of constructing ComQA, including the measures taken to ensure its high quality while making effective use of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We also present an extensive analysis of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and the results achieved by state-of-the-art systems on ComQA, demonstrating that our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can be a driver of future research on <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1030/>Learning to Attend On Essential Terms : An Enhanced Retriever-Reader Model for Open-domain Question Answering</a></strong><br><a href=/people/j/jianmo-ni/>Jianmo Ni</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1030><div class="card-body p-3 small">Open-domain question answering remains a challenging task as it requires <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that are capable of understanding questions and answers, collecting useful information, and reasoning over evidence. Previous work typically formulates this task as a reading comprehension or entailment problem given evidence retrieved from <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a>. However, existing techniques struggle to retrieve indirectly related evidence when no directly related evidence is provided, especially for complex questions where it is hard to parse precisely what the question asks. In this paper we propose a retriever-reader model that learns to attend on essential terms during the <a href=https://en.wikipedia.org/wiki/Question_answering>question answering process</a>. We build (1) an essential term selector which first identifies the most important words in a question, then reformulates the query and searches for related evidence ; and (2) an enhanced reader that distinguishes between essential terms and distracting words to predict the answer. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on multiple open-domain QA datasets, notably achieving the level of the state-of-the-art on the AI2 Reasoning Challenge (ARC) dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1034/>Multi-task Learning for Multi-modal Emotion Recognition and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/m/md-shad-akhtar/>Md Shad Akhtar</a>
|
<a href=/people/d/dushyant-chauhan/>Dushyant Chauhan</a>
|
<a href=/people/d/deepanway-ghosal/>Deepanway Ghosal</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1034><div class="card-body p-3 small">Related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both. The <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multi-modal inputs</a> (i.e. text, acoustic and visual frames) of a video convey diverse and distinctive information, and usually do not have equal contribution in the <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>. We propose a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an utterance. We evaluate our proposed approach on CMU-MOSEI dataset for multi-modal sentiment and emotion analysis. Evaluation results suggest that multi-task learning framework offers improvement over the single-task framework. The proposed approach reports new state-of-the-art performance for both <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and emotion analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1038.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1038/>Learning Interpretable Negation Rules via Weak Supervision at Document Level : A Reinforcement Learning Approach</a></strong><br><a href=/people/n/nicolas-prollochs/>Nicolas Pröllochs</a>
|
<a href=/people/s/stefan-feuerriegel/>Stefan Feuerriegel</a>
|
<a href=/people/d/dirk-neumann/>Dirk Neumann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1038><div class="card-body p-3 small">Negation scope detection is widely performed as a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning task</a> which relies upon <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation labels</a> at word level. This suffers from two key drawbacks : (1) such granular annotations are costly and (2) highly subjective, since, due to the absence of explicit linguistic resolution rules, human annotators often disagree in the perceived negation scopes. To the best of our knowledge, our work presents the first approach that eliminates the need for world-level negation labels, replacing it instead with document-level sentiment annotations. For this, we present a novel strategy for learning fully interpretable negation rules via weak supervision : we apply <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to find a policy that reconstructs negation rules from sentiment predictions at document level. Our experiments demonstrate that our approach for weak supervision can effectively learn negation rules. Furthermore, an <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>out-of-sample evaluation</a> via <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> reveals consistent improvements (of up to 4.66 %) over both a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> with (i) no <a href=https://en.wikipedia.org/wiki/Negation>negation handling</a> and (ii) the use of word-level annotations from humans. Moreover, the inferred negation rules are fully interpretable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1041.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1041/>ReWE : Regressing Word Embeddings for Regularization of Neural Machine Translation Systems<span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>WE</span>: Regressing Word Embeddings for Regularization of Neural Machine Translation Systems</a></strong><br><a href=/people/i/inigo-jauregi-unanue/>Inigo Jauregi Unanue</a>
|
<a href=/people/e/ehsan-zare-borzeshi/>Ehsan Zare Borzeshi</a>
|
<a href=/people/n/nazanin-esmaili/>Nazanin Esmaili</a>
|
<a href=/people/m/massimo-piccardi/>Massimo Piccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1041><div class="card-body p-3 small">Regularization of neural machine translation is still a significant problem, especially in <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>low-resource settings</a>. To mollify this problem, we propose regressing word embeddings (ReWE) as a new regularization technique in a system that is jointly trained to predict the next word in the translation (categorical value) and its word embedding (continuous value). Such a joint training allows the proposed system to learn the distributional properties represented by the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, empirically improving the <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> to unseen sentences. Experiments over three translation datasets have showed a consistent improvement over a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, ranging between 0.91 and 2.4 BLEU points, and also a marked improvement over a <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1042" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1042/>Lost in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> : A Method to Reduce Meaning Loss</a></strong><br><a href=/people/r/reuben-cohn-gordon/>Reuben Cohn-Gordon</a>
|
<a href=/people/n/noah-goodman/>Noah Goodman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1042><div class="card-body p-3 small">A desideratum of high-quality translation systems is that they preserve meaning, in the sense that two sentences with different meanings should not translate to one and the same sentence in another language. However, state-of-the-art systems often fail in this regard, particularly in cases where the source and target languages partition the meaning space in different ways. For instance, I cut my finger. and I cut my finger off. describe different states of the world but are translated to <a href=https://en.wikipedia.org/wiki/French_language>French</a> (by both Fairseq and Google Translate) as Je me suis coup le doigt., which is ambiguous as to whether the finger is detached. More generally, <a href=https://en.wikipedia.org/wiki/Translation>translation systems</a> are typically many-to-one (non-injective) functions from source to target language, which in many cases results in important distinctions in meaning being lost in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. Building on Bayesian models of informative utterance production, we present a method to define a less ambiguous translation system in terms of an underlying pre-trained neural sequence-to-sequence model. This method increases <a href=https://en.wikipedia.org/wiki/Injectivity>injectivity</a>, resulting in greater preservation of meaning as measured by improvement in cycle-consistency, without impeding translation quality (measured by BLEU score).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1044" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1044/>Code-Switching for Enhancing <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> with Pre-Specified Translation<span class=acl-fixed-case>NMT</span> with Pre-Specified Translation</a></strong><br><a href=/people/k/kai-song/>Kai Song</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/h/heng-yu/>Heng Yu</a>
|
<a href=/people/w/weihua-luo/>Weihua Luo</a>
|
<a href=/people/k/kun-wang/>Kun Wang</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1044><div class="card-body p-3 small">Leveraging user-provided translation to constrain <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> has practical significance. Existing methods can be classified into two main categories, namely the use of <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>placeholder tags</a> for lexicon words and the use of <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>hard constraints</a> during decoding. Both methods can hurt translation fidelity for various reasons. We investigate a data augmentation method, making code-switched training data by replacing source phrases with their target translations. Our method does not change the MNT model or decoding algorithm, allowing the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to learn lexicon translations by copying source-side target words. Extensive experiments show that our method achieves consistent improvements over existing approaches, improving translation of constrained words without hurting unconstrained words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1047 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1047.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1047.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1047.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1047" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1047/>Content Differences in Syntactic and Semantic Representation</a></strong><br><a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/a/ari-rappoport/>Ari Rappoport</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1047><div class="card-body p-3 small">Syntactic analysis plays an important role in <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, but the nature of this <a href=https://en.wikipedia.org/wiki/Role>role</a> remains a topic of ongoing debate. The debate has been constrained by the scarcity of empirical comparative studies between syntactic and semantic schemes, which hinders the development of parsing methods informed by the details of target schemes and constructions. We target this gap, and take Universal Dependencies (UD) and <a href=https://en.wikipedia.org/wiki/UCCA>UCCA</a> as a test case. After abstracting away from differences of <a href=https://en.wikipedia.org/wiki/Convention_(norm)>convention</a> or formalism, we find that most content divergences can be ascribed to : (1) UCCA&#8217;s distinction between a Scene and a non-Scene ; (2) UCCA&#8217;s distinction between primary relations, secondary ones and participants ; (3) different treatment of multi-word expressions, and (4) different treatment of inter-clause linkage. We further discuss the long tail of cases where the two <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>schemes</a> take markedly different approaches. Finally, we show that the proposed comparison methodology can be used for fine-grained evaluation of UCCA parsing, highlighting both challenges and potential sources for improvement. The substantial differences between the schemes suggest that semantic parsers are likely to benefit downstream text understanding applications beyond their syntactic counterparts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1048.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1048/>Attentive Mimicking : Better Word Embeddings by Attending to Informative Contexts</a></strong><br><a href=/people/t/timo-schick/>Timo Schick</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1048><div class="card-body p-3 small">Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution : given <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> learned by a standard algorithm, a model is first trained to reproduce <a href=https://en.wikipedia.org/wiki/Embedding>embeddings of frequent words</a> from their surface form and then used to compute <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for rare words. In this paper, we introduce attentive mimicking : the mimicking model is given access not only to a word&#8217;s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a>. In an evaluation on four tasks, we show that attentive mimicking outperforms previous <a href=https://en.wikipedia.org/wiki/Work_(physics)>work</a> for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for a much larger part of the vocabulary, including the <a href=https://en.wikipedia.org/wiki/Medium_frequency>medium-frequency range</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1049" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1049/>Evaluating Style Transfer for Text</a></strong><br><a href=/people/r/remi-mir/>Remi Mir</a>
|
<a href=/people/b/bjarke-felbo/>Bjarke Felbo</a>
|
<a href=/people/n/nick-obradovich/>Nick Obradovich</a>
|
<a href=/people/i/iyad-rahwan/>Iyad Rahwan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1049><div class="card-body p-3 small">Research in the area of style transfer for <a href=https://en.wikipedia.org/wiki/Writing>text</a> is currently bottlenecked by a lack of standard evaluation practices. This paper aims to alleviate this issue by experimentally identifying best practices with a Yelp sentiment dataset. We specify three aspects of interest (style transfer intensity, content preservation, and naturalness) and show how to obtain more reliable measures of them from human evaluation than in previous work. We propose a set of metrics for automated evaluation and demonstrate that they are more strongly correlated and in agreement with human judgment : direction-corrected Earth Mover&#8217;s Distance, Word Mover&#8217;s Distance on style-masked texts, and adversarial classification for the respective aspects. We also show that the three examined <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> exhibit tradeoffs between aspects of interest, demonstrating the importance of evaluating style transfer models at specific points of their tradeoff plots. We release software with our evaluation metrics to facilitate research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1051/>Outlier Detection for Improved Data Quality and Diversity in Dialog Systems</a></strong><br><a href=/people/s/stefan-larson/>Stefan Larson</a>
|
<a href=/people/a/anish-mahendran/>Anish Mahendran</a>
|
<a href=/people/a/andrew-lee/>Andrew Lee</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/p/parker-hill/>Parker Hill</a>
|
<a href=/people/m/michael-a-laurenzano/>Michael A. Laurenzano</a>
|
<a href=/people/j/johann-hauswald/>Johann Hauswald</a>
|
<a href=/people/l/lingjia-tang/>Lingjia Tang</a>
|
<a href=/people/j/jason-mars/>Jason Mars</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1051><div class="card-body p-3 small">In a <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus of data</a>, <a href=https://en.wikipedia.org/wiki/Outlier>outliers</a> are either errors : mistakes in the data that are counterproductive, or are unique : informative samples that improve <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a>. Identifying outliers can lead to better datasets by (1) removing noise in datasets and (2) guiding collection of additional data to fill gaps. However, the problem of detecting both outlier types has received relatively little attention in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, particularly for dialog systems. We introduce a simple and effective technique for detecting both erroneous and unique samples in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of short texts</a> using neural sentence embeddings combined with distance-based outlier detection. We also present a novel data collection pipeline built atop our detection technique to automatically and iteratively mine unique data samples while discarding erroneous samples. Experiments show that our outlier detection technique is effective at finding errors while our data collection pipeline yields highly diverse corpora that in turn produce more robust intent classification and slot-filling models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1053/>Seeing Things from a Different Angle : Discovering Diverse Perspectives about Claims</a></strong><br><a href=/people/s/sihao-chen/>Sihao Chen</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1053><div class="card-body p-3 small">One key consequence of the <a href=https://en.wikipedia.org/wiki/Information_revolution>information revolution</a> is a significant increase and a contamination of our information supply. The practice of <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a> wo n&#8217;t suffice to eliminate the biases in text data we observe, as the degree of <a href=https://en.wikipedia.org/wiki/Fact>factuality</a> alone does not determine whether biases exist in the spectrum of opinions visible to us. To better understand controversial issues, one needs to view them from a diverse yet comprehensive set of perspectives. For example, there are many ways to respond to a claim such as animals should have lawful rights, and these responses form a spectrum of perspectives, each with a stance relative to this claim and, ideally, with evidence supporting it. Inherently, this is a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding task</a>, and we propose to address it as such. Specifically, we propose the task of substantiated perspective discovery where, given a claim, a system is expected to discover a diverse set of well-corroborated perspectives that take a stance with respect to the claim. Each perspective should be substantiated by evidence paragraphs which summarize pertinent results and facts. We construct PERSPECTRUM, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of claims, perspectives and evidence, making use of online debate websites to create the initial <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a>, and augmenting it using <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> in order to expand and diversify our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We use <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> to filter out noise and ensure high-quality data. Our dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353455637 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1057/>Improving Dialogue State Tracking by Discerning the Relevant Context</a></strong><br><a href=/people/s/sanuj-sharma/>Sanuj Sharma</a>
|
<a href=/people/p/prafulla-kumar-choubey/>Prafulla Kumar Choubey</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1057><div class="card-body p-3 small">A typical conversation comprises of multiple turns between participants where they go back and forth between different topics. At each user turn, dialogue state tracking (DST) aims to estimate user&#8217;s goal by processing the current utterance. However, in many turns, users implicitly refer to the previous goal, necessitating the use of relevant dialogue history. Nonetheless, distinguishing relevant history is challenging and a popular method of using dialogue recency for that is inefficient. We, therefore, propose a novel framework for DST that identifies relevant historical context by referring to the past utterances where a particular slot-value changes and uses that together with weighted system utterance to identify the relevant context. Specifically, we use the current user utterance and the most recent system utterance to determine the relevance of a system utterance. Empirical analyses show that our method improves <a href=https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)>joint goal accuracy</a> by 2.75 % and 2.36 % on WoZ 2.0 and Multi-WoZ restaurant domain datasets respectively over the previous state-of-the-art GLAD model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347386459 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1060/>Detection of Abusive Language : the Problem of Biased Datasets<span class=acl-fixed-case>D</span>etection of <span class=acl-fixed-case>A</span>busive <span class=acl-fixed-case>L</span>anguage: the <span class=acl-fixed-case>P</span>roblem of <span class=acl-fixed-case>B</span>iased <span class=acl-fixed-case>D</span>atasets</a></strong><br><a href=/people/m/michael-wiegand/>Michael Wiegand</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a>
|
<a href=/people/t/thomas-kleinbauer/>Thomas Kleinbauer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1060><div class="card-body p-3 small">We discuss the impact of data bias on abusive language detection. We show that classification scores on popular <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> reported in previous work are much lower under realistic settings in which this bias is reduced. Such biases are most notably observed on <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that are created by focused sampling instead of <a href=https://en.wikipedia.org/wiki/Simple_random_sample>random sampling</a>. Datasets with a higher proportion of implicit abuse are more affected than <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with a lower proportion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347389631 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1061" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1061/>Lipstick on a Pig : Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them<span class=acl-fixed-case>D</span>ebiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them</a></strong><br><a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1061><div class="card-body p-3 small">Word embeddings are widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for a vast range of tasks. It was shown that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> derived from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, demonstrating convincing results. However, we argue that this removal is superficial. While the <a href=https://en.wikipedia.org/wiki/Bias>bias</a> is indeed substantially reduced according to the provided <a href=https://en.wikipedia.org/wiki/Bias>bias definition</a>, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between gender-neutralized words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1063.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1063.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347394290 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1063" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1063/>On Measuring Social Biases in Sentence Encoders</a></strong><br><a href=/people/c/chandler-may/>Chandler May</a>
|
<a href=/people/a/alex-wang/>Alex Wang</a>
|
<a href=/people/s/shikha-bordia/>Shikha Bordia</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1063><div class="card-body p-3 small">The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test&#8217;s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347396468 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1064/>Gender Bias in Contextualized Word Embeddings</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1064><div class="card-body p-3 small">In this paper, we quantify, analyze and mitigate <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> exhibited in ELMo&#8217;s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1065 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356020948 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1065" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1065/>Combining Sentiment Lexica with a Multi-View Variational Autoencoder<span class=acl-fixed-case>C</span>ombining <span class=acl-fixed-case>S</span>entiment <span class=acl-fixed-case>L</span>exica with a <span class=acl-fixed-case>M</span>ulti-<span class=acl-fixed-case>V</span>iew <span class=acl-fixed-case>V</span>ariational <span class=acl-fixed-case>A</span>utoencoder</a></strong><br><a href=/people/a/alexander-miserlis-hoyle/>Alexander Miserlis Hoyle</a>
|
<a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1065><div class="card-body p-3 small">When assigning quantitative labels to a dataset, different <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a> may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets ; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1067.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1067.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355760337 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1067/>Frowning Frodo, Wincing Leia, and a Seriously Great Friendship : Learning to Classify Emotional Relationships of Fictional Characters<span class=acl-fixed-case>F</span>rodo, Wincing <span class=acl-fixed-case>L</span>eia, and a Seriously Great Friendship: Learning to Classify Emotional Relationships of Fictional Characters</a></strong><br><a href=/people/e/evgeny-kim/>Evgeny Kim</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1067><div class="card-body p-3 small">The development of a <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>fictional plot</a> is centered around characters who closely interact with each other forming dynamic social networks. In literature analysis, such networks have mostly been analyzed without particular relation types or focusing on roles which the characters take with respect to each other. We argue that an important aspect for the analysis of stories and their development is the emotion between characters. In this paper, we combine these aspects into a unified framework to classify emotional relationships of fictional characters. We formalize it as a new task and describe the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> of a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, based on <a href=https://en.wikipedia.org/wiki/Fan_fiction>fan-fiction short stories</a>. The extraction pipeline which we propose consists of character identification (which we treat as given by an oracle here) and the relation classification. For the latter, we provide results using several approaches previously proposed for relation identification with <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural methods</a>. The best result of 0.45 F1 is achieved with a <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>GRU</a> with character position indicators on the task of predicting undirected emotion relations in the associated <a href=https://en.wikipedia.org/wiki/Social_network>social network graph</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1071.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353462534 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1071/>SEQ3 : Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression<span class=acl-fixed-case>SEQ</span>ˆ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression</a></strong><br><a href=/people/c/christos-baziotis/>Christos Baziotis</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/a/alexandros-potamianos/>Alexandros Potamianos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1071><div class="card-body p-3 small">Neural sequence-to-sequence models are currently the dominant approach in several natural language processing tasks, but require large parallel corpora. We present a sequence-to-sequence-to-sequence autoencoder (SEQ3), consisting of two chained encoder-decoder pairs, with words used as a sequence of discrete latent variables. We apply the proposed model to unsupervised abstractive sentence compression, where the first and last sequences are the input and reconstructed sentences, respectively, while the middle sequence is the compressed sentence. Constraining the length of the latent word sequences forces the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to distill important information from the input. A pretrained language model, acting as a prior over the latent sequences, encourages the compressed sentences to be human-readable. Continuous relaxations enable us to sample from <a href=https://en.wikipedia.org/wiki/Categorical_distribution>categorical distributions</a>, allowing gradient-based optimization, unlike alternatives that rely on <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> does not require parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353467177 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1072" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1072/>Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation</a></strong><br><a href=/people/o/ori-shapira/>Ori Shapira</a>
|
<a href=/people/d/david-gabay/>David Gabay</a>
|
<a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/h/hadar-ronen/>Hadar Ronen</a>
|
<a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/y/yael-amsterdamer/>Yael Amsterdamer</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1072><div class="card-body p-3 small">Conducting a manual evaluation is considered an essential part of summary evaluation methodology. Traditionally, the Pyramid protocol, which exhaustively compares system summaries to references, has been perceived as very reliable, providing objective scores. Yet, due to the high cost of the Pyramid method and the required expertise, researchers resorted to cheaper and less thorough manual evaluation methods, such as <a href=https://en.wikipedia.org/wiki/Responsiveness>Responsiveness</a> and <a href=https://en.wikipedia.org/wiki/Pairwise_comparison>pairwise comparison</a>, attainable via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We revisit the Pyramid approach, proposing a lightweight sampling-based version that is crowdsourcable. We analyze the performance of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in comparison to original expert-based Pyramid evaluations, showing higher <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation</a> relative to the common Responsiveness method. We release our crowdsourced Summary-Content-Units, along with all crowdsourcing scripts, for future evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1076" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1076/>Left-to-Right Dependency Parsing with Pointer Networks</a></strong><br><a href=/people/d/daniel-fernandez-gonzalez/>Daniel Fernández-González</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1076><div class="card-body p-3 small">We propose a novel transition-based algorithm that straightforwardly parses sentences from left to right by building n attachments, with n being the length of the input sentence. Similarly to the recent stack-pointer parser by Ma et al. (2018), we use the pointer network framework that, given a word, can directly point to a position from the sentence. However, our left-to-right approach is simpler than the original top-down stack-pointer parser (not requiring a stack) and reduces transition sequence length in half, from 2n-1 actions to n. This results in a quadratic non-projective parser that runs twice as fast as the original while achieving the best accuracy to date on the English PTB dataset (96.04 % UAS, 94.43 % LAS) among fully-supervised single-model dependency parsers, and improves over the former top-down transition system in the majority of languages tested.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1079.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1079.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360565437 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1079/>Better Modeling of Incomplete Annotations for Named Entity Recognition</a></strong><br><a href=/people/z/zhanming-jie/>Zhanming Jie</a>
|
<a href=/people/p/pengjun-xie/>Pengjun Xie</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/r/ruixue-ding/>Ruixue Ding</a>
|
<a href=/people/l/linlin-li/>Linlin Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1079><div class="card-body p-3 small">Supervised approaches to named entity recognition (NER) are largely developed based on the assumption that the training data is fully annotated with named entity information. However, in practice, annotated data can often be imperfect with one typical issue being the training data may contain incomplete annotations. We highlight several pitfalls associated with learning under such a setup in the context of <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>NER</a> and identify limitations associated with existing approaches, proposing a novel yet easy-to-implement approach for recognizing named entities with incomplete data annotations. We demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> through extensive experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1088" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1088/>Adversarial Decomposition of Text Representation</a></strong><br><a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/d/david-donahue/>David Donahue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1088><div class="card-body p-3 small">In this paper, we present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for adversarial decomposition of text representation. This method can be used to decompose a representation of an input sentence into several independent vectors, each of them responsible for a specific aspect of the input sentence. We evaluate the proposed method on two <a href=https://en.wikipedia.org/wiki/Case_study>case studies</a> : the conversion between different <a href=https://en.wikipedia.org/wiki/Register_(sociolinguistics)>social registers</a> and <a href=https://en.wikipedia.org/wiki/Language_change>diachronic language change</a>. We show that the proposed method is capable of fine-grained controlled change of these aspects of the input sentence. It is also learning a continuous (rather than categorical) representation of the style of the sentence, which is more linguistically realistic. The model uses adversarial-motivational training and includes a special motivational loss, which acts opposite to the discriminator and encourages a better decomposition. Furthermore, we evaluate the obtained meaning embeddings on a downstream task of <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a> and show that they significantly outperform the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> of a regular autoencoder.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1095 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1095" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1095/>Recovering dropped pronouns in Chinese conversations via modeling their referents<span class=acl-fixed-case>C</span>hinese conversations via modeling their referents</a></strong><br><a href=/people/j/jingxuan-yang/>Jingxuan Yang</a>
|
<a href=/people/j/jianzhuo-tong/>Jianzhuo Tong</a>
|
<a href=/people/s/si-li/>Si Li</a>
|
<a href=/people/s/sheng-gao/>Sheng Gao</a>
|
<a href=/people/j/jun-guo/>Jun Guo</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1095><div class="card-body p-3 small">Pronouns are often dropped in Chinese sentences, and this happens more frequently in conversational genres as their referents can be easily understood from context. Recovering dropped pronouns is essential to applications such as <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a> where the referents of these dropped pronouns need to be resolved, or <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> when Chinese is the source language. In this work, we present a novel end-to-end neural network model to recover <a href=https://en.wikipedia.org/wiki/Pronoun>dropped pronouns</a> in <a href=https://en.wikipedia.org/wiki/Conversation>conversational data</a>. Our model is based on a structured attention mechanism that models the referents of dropped pronouns utilizing both sentence-level and word-level information. Results on three different conversational genres show that our approach achieves a significant improvement over the current state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1097" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1097/>A Systematic Study of Leveraging Subword Information for Learning Word Representations</a></strong><br><a href=/people/y/yi-zhu/>Yi Zhu</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1097><div class="card-body p-3 small">The use of subword-level information (e.g., <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters</a>, <a href=https://en.wikipedia.org/wiki/Character_(symbol)>character n-grams</a>, morphemes) has become ubiquitous in modern word representation learning. Its importance is attested especially for <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphologically rich languages</a> which generate a large number of rare words. Despite a steadily increasing interest in such subword-informed word representations, their systematic comparative analysis across typologically diverse languages and different tasks is still missing. In this work, we deliver such a study focusing on the variation of two crucial components required for subword-level integration into word representation models : 1) segmentation of words into subword units, and 2) subword composition functions to obtain final word representations. We propose a general framework for learning subword-informed word representations that allows for easy experimentation with different segmentation and composition components, also including more advanced techniques based on position embeddings and self-attention. Using the unified framework, we run experiments over a large number of subword-informed word representation configurations (60 in total) on 3 tasks (general and rare word similarity, dependency parsing, fine-grained entity typing) for 5 languages representing 3 language types. Our main results clearly indicate that there is no one-size-fits-all configuration, as performance is both language- and task-dependent. We also show that configurations based on unsupervised segmentation (e.g., BPE, Morfessor) are sometimes comparable to or even outperform the ones based on supervised word segmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1099/>Integration of Knowledge Graph Embedding Into <a href=https://en.wikipedia.org/wiki/Topic_model>Topic Modeling</a> with Hierarchical Dirichlet Process<span class=acl-fixed-case>D</span>irichlet Process</a></strong><br><a href=/people/d/dingcheng-li/>Dingcheng Li</a>
|
<a href=/people/s/siamak-zamani/>Siamak Zamani</a>
|
<a href=/people/j/jingyuan-zhang/>Jingyuan Zhang</a>
|
<a href=/people/p/ping-li/>Ping Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1099><div class="card-body p-3 small">Leveraging <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> is an effective strategy for enhancing the quality of inferred low-dimensional representations of documents by <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a>. In this paper, we develop topic modeling with knowledge graph embedding (TMKGE), a Bayesian nonparametric model to employ knowledge graph (KG) embedding in the context of <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a>, for extracting more coherent topics. Specifically, we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow information from KG to improve the interpretability of topics. An efficient online variational inference method based on a stick-breaking construction of HDP is developed for TMKGE, making TMKGE suitable for large document corpora and KGs. Experiments on three public datasets illustrate the superior performance of TMKGE in terms of topic coherence and document classification accuracy, compared to state-of-the-art topic modeling methods.<i>topic modeling with knowledge graph embedding</i> (TMKGE), a Bayesian nonparametric model to employ knowledge graph (KG) embedding in the context of topic modeling, for extracting more coherent topics. Specifically, we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow information from KG to improve the interpretability of topics. An efficient online variational inference method based on a stick-breaking construction of HDP is developed for TMKGE, making TMKGE suitable for large document corpora and KGs. Experiments on three public datasets illustrate the superior performance of TMKGE in terms of topic coherence and document classification accuracy, compared to state-of-the-art topic modeling methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1101/>Generating Token-Level Explanations for Natural Language Inference</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1101><div class="card-body p-3 small">The task of Natural Language Inference (NLI) is widely modeled as supervised sentence pair classification. While there has been a lot of work recently on generating explanations of the predictions of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on a single piece of text, there have been no attempts to generate explanations of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> operating on pairs of sentences. In this paper, we show that it is possible to generate token-level explanations for NLI without the need for training data explicitly annotated for this purpose. We use a simple LSTM architecture and evaluate both LIME and Anchor explanations for this task. We compare these to a Multiple Instance Learning (MIL) method that uses thresholded attention make token-level predictions. The approach we present in this paper is a novel extension of zero-shot single-sentence tagging to sentence pairs for NLI. We conduct our experiments on the well-studied SNLI dataset that was recently augmented with manually annotation of the tokens that explain the <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment relation</a>. We find that our white-box MIL-based method, while orders of magnitude faster, does not reach the same <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> as the black-box methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353480570 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1103/>Adaptive Convolution for Multi-Relational Learning</a></strong><br><a href=/people/x/xiaotian-jiang/>Xiaotian Jiang</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/b/bin-wang/>Bin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1103><div class="card-body p-3 small">We consider the problem of learning <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> for entities and relations of multi-relational data so as to predict missing links therein. Convolutional neural networks have recently shown their superiority for this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>, bringing increased model expressiveness while remaining parameter efficient. Despite the success, previous convolution designs fail to model full interactions between input entities and relations, which potentially limits the performance of link prediction. In this work we introduce ConvR, an adaptive convolutional network designed to maximize entity-relation interactions in a convolutional fashion. ConvR adaptively constructs convolution filters from <a href=https://en.wikipedia.org/wiki/Binary_relation>relation representations</a>, and applies these filters across <a href=https://en.wikipedia.org/wiki/Binary_relation>entity representations</a> to generate <a href=https://en.wikipedia.org/wiki/Convolution>convolutional features</a>. As such, ConvR enables rich interactions between entity and relation representations at diverse regions, and all the <a href=https://en.wikipedia.org/wiki/Convolution>convolutional features</a> generated will be able to capture such <a href=https://en.wikipedia.org/wiki/Interaction>interactions</a>. We evaluate ConvR on multiple benchmark datasets. Experimental results show that : (1) ConvR performs substantially better than competitive baselines in almost all the metrics and on all the datasets ; (2) Compared with state-of-the-art convolutional models, ConvR is not only more effective but also more efficient. It offers a 7 % increase in MRR and a 6 % increase in Hits@10, while saving 12 % in parameter storage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360608466 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1107" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1107/>Relation Extraction with Temporal Reasoning Based on Memory Augmented Distant Supervision</a></strong><br><a href=/people/j/jianhao-yan/>Jianhao Yan</a>
|
<a href=/people/l/lin-he/>Lin He</a>
|
<a href=/people/r/ruqin-huang/>Ruqin Huang</a>
|
<a href=/people/j/jian-li/>Jian Li</a>
|
<a href=/people/y/ying-liu/>Ying Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1107><div class="card-body p-3 small">Distant supervision (DS) is an important paradigm for automatically extracting relations. It utilizes existing <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> to collect examples for the relation we intend to extract, and then uses these examples to automatically generate the training data. However, the examples collected can be very noisy, and pose significant challenge for obtaining high quality labels. Previous work has made remarkable progress in predicting the relation from distant supervision, but typically ignores the temporal relations among those supervising instances. This paper formulates the problem of relation extraction with temporal reasoning and proposes a solution to predict whether two given entities participate in a relation at a given time spot. For this purpose, we construct a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> called WIKI-TIME which additionally includes the valid period of a certain relation of two entities in the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. We propose a novel neural model to incorporate both the temporal information encoding and sequential reasoning. The experimental results show that, compared with the best of existing models, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves better performance in both WIKI-TIME dataset and the well-studied NYT-10 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1108.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355765532 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1108/>Integrating Semantic Knowledge to Tackle Zero-shot Text Classification</a></strong><br><a href=/people/j/jingqing-zhang/>Jingqing Zhang</a>
|
<a href=/people/p/piyawat-lertvittayakumjorn/>Piyawat Lertvittayakumjorn</a>
|
<a href=/people/y/yike-guo/>Yike Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1108><div class="card-body p-3 small">Insufficient or even unavailable training data of emerging classes is a big challenge of many classification tasks, including <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. Recognising text documents of classes that have never been seen in the learning stage, so-called zero-shot text classification, is therefore difficult and only limited previous works tackled this problem. In this paper, we propose a two-phase framework together with <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> and feature augmentation to solve this problem. Four kinds of semantic knowledge (word embeddings, class descriptions, class hierarchy, and a general knowledge graph) are incorporated into the proposed framework to deal with instances of unseen classes effectively. Experimental results show that each and the combination of the two phases achieve the best overall accuracy compared with baselines and recent approaches in classifying real-world texts under the zero-shot scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355773895 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1109/>Word-Node2Vec : Improving Word Embedding with Document-Level Non-Local Word Co-occurrences<span class=acl-fixed-case>N</span>ode2<span class=acl-fixed-case>V</span>ec: Improving Word Embedding with Document-Level Non-Local Word Co-occurrences</a></strong><br><a href=/people/p/procheta-sen/>Procheta Sen</a>
|
<a href=/people/d/debasis-ganguly/>Debasis Ganguly</a>
|
<a href=/people/g/gareth-jones/>Gareth Jones</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1109><div class="card-body p-3 small">A standard word embedding algorithm, such as <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and glove, makes a strong assumption that words are likely to be semantically related only if they co-occur locally within a window of fixed size. However, this strong assumption may not capture the semantic association between words that co-occur frequently but non-locally within documents. In this paper, we propose a graph-based word embedding method, named &#8216;word-node2vec&#8217;. By relaxing the strong constraint of <a href=https://en.wikipedia.org/wiki/Principle_of_locality>locality</a>, our method is able to capture both the local and non-local co-occurrences. Word-node2vec constructs a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> where every <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>node</a> represents a word and an edge between two nodes represents a combination of both local (e.g. word2vec) and document-level co-occurrences. Our experiments show that word-node2vec outperforms <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and glove on a range of different tasks, such as predicting word-pair similarity, word analogy and concept categorization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1111.Software.txt data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356031269 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1111/>What just happened? Evaluating retrofitted distributional word vectors<span class=acl-fixed-case>E</span>valuating retrofitted distributional word vectors</a></strong><br><a href=/people/d/dmetri-hayes/>Dmetri Hayes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1111><div class="card-body p-3 small">Recent work has attempted to enhance vector space representations using information from structured semantic resources. This process, dubbed <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting</a> (Faruqui et al., 2015), has yielded improvements in word similarity performance. Research has largely focused on the retrofitting algorithm, or on the kind of structured semantic resources used, but little research has explored why some <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resources</a> perform better than others. We conducted a fine-grained analysis of the original <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting process</a>, and found that the utility of different lexical resources for <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting</a> depends on two factors : the coverage of the resource and the evaluation metric. Our assessment suggests that the common practice of using correlation measures to evaluate increases in performance against full word similarity benchmarks 1) obscures the benefits offered by smaller resources, and 2) overlooks incremental gains in word similarity performance. We propose root-mean-square error (RMSE) as an alternative evaluation metric, and demonstrate that <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation measures</a> and RMSE sometimes yield opposite conclusions concerning the efficacy of <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting</a>. This point is illustrated by word vectors retrofitted with novel treatments of the FrameNet data (Fillmore and Baker, 2010).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1115.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364675378 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1115/>Cooperative Learning of Disjoint Syntax and Semantics</a></strong><br><a href=/people/s/serhii-havrylov/>Serhii Havrylov</a>
|
<a href=/people/g/german-kruszewski/>Germán Kruszewski</a>
|
<a href=/people/a/armand-joulin/>Armand Joulin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1115><div class="card-body p-3 small">There has been considerable attention devoted to <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that learn to jointly infer an expression&#8217;s <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a> and its <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. Yet, Nangia and Bowman (2018) has recently shown that the current best systems fail to learn the correct parsing strategy on mathematical expressions generated from a simple <a href=https://en.wikipedia.org/wiki/Context-free_grammar>context-free grammar</a>. In this work, we present a <a href=https://en.wikipedia.org/wiki/Recursion_(computer_science)>recursive model</a> inspired by Choi et al. (2018) that reaches near perfect <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is composed of two separated <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> for syntax and semantics. They are cooperatively trained with standard continuous and discrete optimisation schemes. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> does not require any linguistic structure for <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>, and its recursive nature allows for out-of-domain generalisation. Additionally, our approach performs competitively on several <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language tasks</a>, such as <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1116/>Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders</a></strong><br><a href=/people/a/andrew-drozdov/>Andrew Drozdov</a>
|
<a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/m/mohit-yadav/>Mohit Yadav</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1116><div class="card-body p-3 small">We introduce the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence. During training we use <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a> to consider all possible binary trees over the sentence, and for <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> we use the <a href=https://en.wikipedia.org/wiki/CKY_algorithm>CKY algorithm</a> to extract the highest scoring parse. DIORA outperforms previously reported results for unsupervised binary constituency parsing on the benchmark WSJ dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347403902 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1118/>Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations</a></strong><br><a href=/people/m/meishan-zhang/>Meishan Zhang</a>
|
<a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/g/guohong-fu/>Guohong Fu</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1118><div class="card-body p-3 small">Syntax has been demonstrated highly effective in neural machine translation (NMT). Previous NMT models integrate <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>syntax</a> by representing 1-best tree outputs from a well-trained parsing system, e.g., the representative Tree-RNN and Tree-Linearization methods, which may suffer from <a href=https://en.wikipedia.org/wiki/Propagation_of_uncertainty>error propagation</a>. In this work, we propose a novel method to integrate source-side syntax implicitly for <a href=https://en.wikipedia.org/wiki/Network_topology>NMT</a>. The basic idea is to use the intermediate hidden representations of a well-trained end-to-end dependency parser, which are referred to as syntax-aware word representations (SAWRs). Then, we simply concatenate such SAWRs with ordinary <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to enhance basic NMT models. The <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> can be straightforwardly integrated into the widely-used sequence-to-sequence (Seq2Seq) NMT models. We start with a representative RNN-based Seq2Seq baseline system, and test the effectiveness of our proposed method on two benchmark datasets of the Chinese-English and English-Vietnamese translation tasks, respectively. Experimental results show that the proposed approach is able to bring significant BLEU score improvements on the two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> compared with the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a>, 1.74 points for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese-English translation</a> and 0.80 point for <a href=https://en.wikipedia.org/wiki/Vietnamese_language>English-Vietnamese translation</a>, respectively. In addition, the <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>approach</a> also outperforms the explicit Tree-RNN and Tree-Linearization methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347406566 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1119/>Competence-based Curriculum Learning for Neural Machine Translation</a></strong><br><a href=/people/e/emmanouil-antonios-platanios/>Emmanouil Antonios Platanios</a>
|
<a href=/people/o/otilia-stretcu/>Otilia Stretcu</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/b/barnabas-poczos/>Barnabas Poczos</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1119><div class="card-body p-3 small">Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> at different times during training, based on the estimated difficulty of a sample and the current competence of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the <a href=https://en.wikipedia.org/wiki/Time_complexity>training time</a> and the performance of both <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network models</a> and Transformers, achieving up to a 70 % decrease in <a href=https://en.wikipedia.org/wiki/Time_complexity>training time</a>, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347411013 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1121" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1121/>Consistency by Agreement in Zero-Shot Neural Machine Translation</a></strong><br><a href=/people/m/maruan-al-shedivat/>Maruan Al-Shedivat</a>
|
<a href=/people/a/ankur-parikh/>Ankur Parikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1121><div class="card-body p-3 small">Generalization and reliability of multilingual translation often highly depend on the amount of available parallel data for each language pair of interest. In this paper, we focus on zero-shot generalizationa challenging setup that tests <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on translation directions they have not been optimized for at training time. To solve the problem, we (i) reformulate multilingual translation as probabilistic inference, (ii) define the notion of zero-shot consistency and show why standard training often results in models unsuitable for zero-shot tasks, and (iii) introduce a consistent agreement-based training method that encourages the model to produce equivalent translations of parallel sentences in auxiliary languages. We test our multilingual NMT models on multiple public zero-shot translation benchmarks (IWSLT17, UN corpus, Europarl) and show that agreement-based learning often results in 2-3 BLEU zero-shot improvement over strong baselines without any loss in performance on supervised translation directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1123 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360620730 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1123" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1123/>Rethinking Action Spaces for <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> in End-to-end Dialog Agents with Latent Variable Models</a></strong><br><a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/k/kaige-xie/>Kaige Xie</a>
|
<a href=/people/m/maxine-eskenazi/>Maxine Eskenazi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1123><div class="card-body p-3 small">Defining action spaces for conversational agents and optimizing their <a href=https://en.wikipedia.org/wiki/Decision-making>decision-making process</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> is an enduring challenge. Common practice has been to use handcrafted dialog acts, or the output vocabulary, e.g. in neural encoder decoders, as the action spaces. Both have their own limitations. This paper proposes a novel latent action framework that treats the action spaces of an end-to-end dialog agent as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> and develops <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> in order to induce its own action space from the data. Comprehensive experiments are conducted examining both continuous and discrete action types and two different <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization methods</a> based on stochastic variational inference. Results show that the proposed latent actions achieve superior empirical performance improvement over previous word-level policy gradient methods on both DealOrNoDeal and MultiWoz dialogs. Our detailed analysis also provides insights about various latent variable approaches for <a href=https://en.wikipedia.org/wiki/Policy_learning>policy learning</a> and can serve as a foundation for developing better latent actions in future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1128 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1128/>WiC : the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations<span class=acl-fixed-case>W</span>i<span class=acl-fixed-case>C</span>: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations</a></strong><br><a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1128><div class="card-body p-3 small">By design, word embeddings are unable to model the <a href=https://en.wikipedia.org/wiki/Semantics>dynamic nature of words&#8217; semantics</a>, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few <a href=https://en.wikipedia.org/wiki/Benchmarking>evaluation benchmarks</a> exist that specifically focus on the dynamic semantics of words. In this paper we show that existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in https://pilehvar.github.io/wic/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1130 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1130/>Casting Light on Invisible Cities : Computationally Engaging with Literary Criticism<span class=acl-fixed-case>C</span>asting <span class=acl-fixed-case>L</span>ight on <span class=acl-fixed-case>I</span>nvisible <span class=acl-fixed-case>C</span>ities: <span class=acl-fixed-case>C</span>omputationally <span class=acl-fixed-case>E</span>ngaging with <span class=acl-fixed-case>L</span>iterary <span class=acl-fixed-case>C</span>riticism</a></strong><br><a href=/people/s/shufan-wang/>Shufan Wang</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1130><div class="card-body p-3 small">Literary critics often attempt to uncover meaning in a single work of literature through careful reading and analysis. Applying <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing methods</a> to aid in such <a href=https://en.wikipedia.org/wiki/Literary_criticism>literary analyses</a> remains a challenge in <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities</a>. While most previous work focuses on distant reading by algorithmically discovering high-level patterns from large collections of literary works, here we sharpen the focus of our methods to a single <a href=https://en.wikipedia.org/wiki/Literary_theory>literary theory</a> about Italo Calvino&#8217;s postmodern novel Invisible Cities, which consists of 55 short descriptions of imaginary cities. Calvino has provided a classification of these <a href=https://en.wikipedia.org/wiki/City>cities</a> into eleven thematic groups, but literary scholars disagree as to how trustworthy his categorization is. Due to the unique structure of this novel, we can computationally weigh in on this debate : we leverage pretrained contextualized representations to embed each city&#8217;s description and use <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> to cluster these embeddings. Additionally, we compare results of our <a href=https://en.wikipedia.org/wiki/Computational_model>computational approach</a> to similarity judgments generated by <a href=https://en.wikipedia.org/wiki/User_(computing)>human readers</a>. Our work is a first step towards incorporating <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> into <a href=https://en.wikipedia.org/wiki/Literary_criticism>literary criticism</a>.<i>Invisible Cities</i>, which consists of 55 short descriptions of imaginary cities. Calvino has provided a classification of these cities into eleven thematic groups, but literary scholars disagree as to how trustworthy his categorization is. Due to the unique structure of this novel, we can computationally weigh in on this debate: we leverage pretrained contextualized representations to embed each city&#8217;s description and use unsupervised methods to cluster these embeddings. Additionally, we compare results of our computational approach to similarity judgments generated by human readers. Our work is a first step towards incorporating natural language processing into literary criticism.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1131 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1131" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1131/>PAWS : Paraphrase Adversaries from Word Scrambling<span class=acl-fixed-case>PAWS</span>: Paraphrase Adversaries from Word Scrambling</a></strong><br><a href=/people/y/yuan-zhang/>Yuan Zhang</a>
|
<a href=/people/j/jason-baldridge/>Jason Baldridge</a>
|
<a href=/people/l/luheng-he/>Luheng He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1131><div class="card-body p-3 small">Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS (40 % accuracy) ; however, including PAWS training data for these models improves their accuracy to 85 % while maintaining performance on existing tasks. In contrast, <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that better exploit <a href=https://en.wikipedia.org/wiki/Structure>structure</a>, <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>, and <a href=https://en.wikipedia.org/wiki/Pairwise_comparison>pairwise comparisons</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1134 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1134/>Adaptation of Hierarchical Structured Models for <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Act Recognition</a> in Asynchronous Conversation</a></strong><br><a href=/people/m/muhammad-tasnim-mohiuddin/>Tasnim Mohiuddin</a>
|
<a href=/people/t/thanh-tung-nguyen/>Thanh-Tung Nguyen</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1134><div class="card-body p-3 small">We address the problem of speech act recognition (SAR) in asynchronous conversations (forums, emails). Unlike <a href=https://en.wikipedia.org/wiki/Synchronization>synchronous conversations</a> (e.g., meetings, phone), asynchronous domains lack large labeled datasets to train an effective SAR model. In this paper, we propose methods to effectively leverage abundant unlabeled conversational data and the available labeled data from synchronous domains. We carry out our research in three main steps. First, we introduce a neural architecture based on hierarchical LSTMs and conditional random fields (CRF) for SAR, and show that our method outperforms existing methods when trained on in-domain data only. Second, we improve our initial SAR models by <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a> in the form of pretrained word embeddings learned from a large unlabeled conversational corpus. Finally, we employ adversarial training to improve the results further by leveraging the labeled data from synchronous domains and by explicitly modeling the distributional shift in two domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1137/>Multi-Channel Convolutional Neural Network for Twitter Emotion and Sentiment Recognition<span class=acl-fixed-case>T</span>witter Emotion and Sentiment Recognition</a></strong><br><a href=/people/j/jumayel-islam/>Jumayel Islam</a>
|
<a href=/people/r/robert-e-mercer/>Robert E. Mercer</a>
|
<a href=/people/l/lu-xiao/>Lu Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1137><div class="card-body p-3 small">The advent of micro-blogging sites has paved the way for researchers to collect and analyze huge volumes of data in recent years. Twitter, being one of the leading social networking sites worldwide, provides a great opportunity to its users for expressing their states of mind via short messages which are called <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. The urgency of identifying emotions and sentiments conveyed through <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> has led to several research works. It provides a great way to understand <a href=https://en.wikipedia.org/wiki/Human_psychology>human psychology</a> and impose a challenge to researchers to analyze their content easily. In this paper, we propose a novel use of a multi-channel convolutional neural architecture which can effectively use different emotion and sentiment indicators such as <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a>, <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a> and <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> that are present in the <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> and improve the performance of emotion and sentiment identification. We also investigate the incorporation of different lexical features in the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> and its effect on the emotion and sentiment identification task. We analyze our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on some standard datasets and compare its effectiveness with existing techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1138 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1138.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1138/>Detecting Cybersecurity Events from Noisy Short Text</a></strong><br><a href=/people/s/semih-yagcioglu/>Semih Yagcioglu</a>
|
<a href=/people/m/mehmet-saygin-seyfioglu/>Mehmet Saygin Seyfioglu</a>
|
<a href=/people/b/begum-citamak/>Begum Citamak</a>
|
<a href=/people/b/batuhan-bardak/>Batuhan Bardak</a>
|
<a href=/people/s/seren-guldamlasioglu/>Seren Guldamlasioglu</a>
|
<a href=/people/a/azmi-yuksel/>Azmi Yuksel</a>
|
<a href=/people/e/emin-islam-tatli/>Emin Islam Tatli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1138><div class="card-body p-3 small">It is very critical to analyze messages shared over <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> for <a href=https://en.wikipedia.org/wiki/Cyber_threat_intelligence>cyber threat intelligence</a> and cyber-crime prevention. In this study, we propose a method that leverages both domain-specific word embeddings and task-specific features to detect cyber security events from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. Our model employs a convolutional neural network (CNN) and a long short-term memory (LSTM) recurrent neural network which takes word level meta-embeddings as inputs and incorporates contextual embeddings to classify noisy short text. We collected a new dataset of cyber security related tweets from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and manually annotated a subset of 2 K of them. We experimented with this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and concluded that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms both traditional and neural baselines. The results suggest that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> works well for detecting cyber security events from noisy short text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1139" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1139/>White-to-Black : Efficient Distillation of Black-Box Adversarial Attacks</a></strong><br><a href=/people/y/yotam-gil/>Yotam Gil</a>
|
<a href=/people/y/yoav-chai/>Yoav Chai</a>
|
<a href=/people/o/or-gorodissky/>Or Gorodissky</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1139><div class="card-body p-3 small">Adversarial examples are important for understanding the behavior of neural models, and can improve their <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> through adversarial training. Recent work in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> generated adversarial examples by assuming white-box access to the attacked model, and optimizing the input directly against it (Ebrahimi et al., 2018). In this work, we show that the knowledge implicit in the <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization procedure</a> can be distilled into another more efficient <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. We train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to emulate the behavior of a white-box attack and show that it generalizes well across examples. Moreover, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> reduces adversarial example generation time by 19x-39x. We also show that our approach transfers to a black-box setting, by attacking The Google Perspective API and exposing its vulnerability. Our attack flips the API-predicted label in 42 % of the generated examples, while humans maintain high-accuracy in predicting the gold label.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1141 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1141/>Fake News Detection using Deep Markov Random Fields<span class=acl-fixed-case>M</span>arkov Random Fields</a></strong><br><a href=/people/d/duc-minh-nguyen/>Duc Minh Nguyen</a>
|
<a href=/people/t/tien-huu-do/>Tien Huu Do</a>
|
<a href=/people/r/robert-calderbank/>Robert Calderbank</a>
|
<a href=/people/n/nikos-deligiannis/>Nikos Deligiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1141><div class="card-body p-3 small">Deep-learning-based models have been successfully applied to the problem of <a href=https://en.wikipedia.org/wiki/Fake_news>detecting fake news</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. While the correlations among news articles have been shown to be effective cues for online news analysis, existing deep-learning-based methods often ignore this information and only consider each news article individually. To overcome this limitation, we develop a graph-theoretic method that inherits the power of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> while at the same time utilizing the correlations among the articles. We formulate fake news detection as an inference problem in a Markov random field (MRF) which can be solved by the iterative mean-field algorithm. We then unfold the mean-field algorithm into hidden layers that are composed of common neural network operations. By integrating these hidden layers on top of a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep network</a>, which produces the MRF potentials, we obtain our deep MRF model for fake news detection. Experimental results on well-known datasets show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves upon various <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1143 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1143/>Vector of Locally Aggregated Embeddings for Text Representation</a></strong><br><a href=/people/h/hadi-amiri/>Hadi Amiri</a>
|
<a href=/people/m/mitra-mohtarami/>Mitra Mohtarami</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1143><div class="card-body p-3 small">We present Vector of Locally Aggregated Embeddings (VLAE) for effective and, ultimately, lossless representation of textual content. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> encodes each input text by effectively identifying and integrating the representations of its semantically-relevant parts. The proposed model generates high quality representation of textual content and improves the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance of current state-of-the-art deep averaging networks across several text classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1145 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1145/>Biomedical Event Extraction based on Knowledge-driven Tree-LSTM<span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/d/diya-li/>Diya Li</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1145><div class="card-body p-3 small">Event extraction for the biomedical domain is more challenging than that in the general news domain since it requires broader acquisition of domain-specific knowledge and deeper understanding of complex contexts. To better encode contextual information and external background knowledge, we propose a novel knowledge base (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework, incorporating two new types of features : (1) dependency structures to capture wide contexts ; (2) entity properties (types and category descriptions) from external ontologies via entity linking. We evaluate our approach on the BioNLP shared task with Genia dataset and achieve a new state-of-the-art result. In addition, both quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM and the external knowledge representation for biomedical event extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1150 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1150/>Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction</a></strong><br><a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/o/oshin-agarwal/>Oshin Agarwal</a>
|
<a href=/people/c/chris-tar/>Chris Tar</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a>
|
<a href=/people/a/ani-nenkova/>Ani Nenkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1150><div class="card-body p-3 small">Modern NLP systems require high-quality annotated data. For specialized domains, expert annotations may be prohibitively expensive ; the alternative is to rely on <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> to reduce costs at the risk of introducing noise. In this paper we demonstrate that directly modeling instance difficulty can be used to improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance and to route instances to appropriate annotators. Our difficulty prediction model combines two learned representations : a &#8216;universal&#8217; encoder trained on out of domain data, and a task-specific encoder. Experiments on a complex biomedical information extraction task using expert and lay annotators show that : (i) simply excluding from the training data instances predicted to be difficult yields a small boost in performance ; (ii) using difficulty scores to weight instances during training provides further, consistent gains ; (iii) assigning instances predicted to be difficult to domain experts is an effective strategy for task routing. Further, our experiments confirm the expectation that for such domain-specific tasks expert annotations are of much higher quality and preferable to obtain if practical and that augmenting small amounts of expert data with a larger set of lay annotations leads to further improvements in model performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1151 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1151/>Detecting Depression in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> using Fine-Grained Emotions</a></strong><br><a href=/people/m/mario-ezra-aragon/>Mario Ezra Aragón</a>
|
<a href=/people/a/adrian-pastor-lopez-monroy/>Adrian Pastor López-Monroy</a>
|
<a href=/people/l/luis-carlos-gonzalez-gurrola/>Luis Carlos González-Gurrola</a>
|
<a href=/people/m/manuel-montes/>Manuel Montes-y-Gómez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1151><div class="card-body p-3 small">Nowadays <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> are the most popular way for people to share information, from work issues to personal matters. For example, people with health disorders tend to share their concerns for advice, support or simply to relieve suffering. This provides a great opportunity to proactively detect these users and refer them as soon as possible to professional help. We propose a new representation called Bag of Sub-Emotions (BoSE), which represents social media documents by a set of fine-grained emotions automatically generated using a lexical resource of emotions and subword embeddings. The proposed <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a> is evaluated in the task of <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression detection</a>. The results are encouraging ; the usage of fine-grained emotions improved the results from a representation based on the <a href=https://en.wikipedia.org/wiki/Emotion>core emotions</a> and obtained competitive results in comparison to state of the art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1154 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360694967 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1154/>One Size Does Not Fit All : Comparing NMT Representations of Different Granularities<span class=acl-fixed-case>NMT</span> Representations of Different Granularities</a></strong><br><a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1154><div class="card-body p-3 small">Recent work has shown that contextualized word representations derived from <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters</a> have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>, <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, and <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. We found that while representations derived from subwords are slightly better for modeling <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, character-based representations are superior for modeling <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> and are also more robust to noisy input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1155 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360705702 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1155/>A Simple Joint Model for Improved Contextual Neural Lemmatization</a></strong><br><a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1155><div class="card-body p-3 small">English verbs have multiple forms. For instance, talk may also appear as talks, talked or talking, depending on the context. The NLP task of <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> seeks to map these diverse forms back to a canonical one, known as the lemma. We present a simple joint neural model for <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> and morphological tagging that achieves state-of-the-art results on 20 languages from the Universal Dependencies corpora. Our paper describes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in addition to training and decoding procedures. Error analysis indicates that joint morphological tagging and <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> is especially helpful in low-resource lemmatization and languages that display a larger degree of morphological complexity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1159 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1159.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364704101 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1159" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1159/>Recursive Subtree Composition in LSTM-Based Dependency Parsing<span class=acl-fixed-case>LSTM</span>-Based Dependency Parsing</a></strong><br><a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1159><div class="card-body p-3 small">The need for tree structure modelling on top of sequence modelling is an open issue in neural dependency parsing. We investigate the impact of adding a <a href=https://en.wikipedia.org/wiki/Tree_layer>tree layer</a> on top of a <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential model</a> by recursively composing subtree representations (composition) in a transition-based parser that uses features extracted by a BiLSTM. Composition seems superfluous with such a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, suggesting that BiLSTMs capture information about <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>subtrees</a>. We perform model ablations to tease out the conditions under which <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> helps. When ablating the backward LSTM, performance drops and <a href=https://en.wikipedia.org/wiki/Musical_composition>composition</a> does not recover much of the gap. When ablating the forward LSTM, performance drops less dramatically and <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> recovers a substantial part of the gap, indicating that a forward LSTM and <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> capture similar information. We take the backward LSTM to be related to lookahead features and the forward LSTM to the rich history-based features both crucial for transition-based parsers. To capture history-based information, composition is better than a forward LSTM on its own, but it is even better to have a forward LSTM as part of a BiLSTM. We correlate results with language properties, showing that the improved <a href=https://en.wikipedia.org/wiki/Ahead-of-time_compilation>lookahead</a> of a backward LSTM is especially important for head-final languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1161 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364706803 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1161" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1161/>Density Matching for Bilingual Word Embedding</a></strong><br><a href=/people/c/chunting-zhou/>Chunting Zhou</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/d/di-wang/>Di Wang</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1161><div class="card-body p-3 small">Recent approaches to cross-lingual word embedding have generally been based on <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformations</a> between the sets of embedding vectors in the two languages. In this paper, we propose an approach that instead expresses the two monolingual embedding spaces as <a href=https://en.wikipedia.org/wiki/Probability_density_function>probability densities</a> defined by a Gaussian mixture model, and matches the two densities using a method called normalizing flow. The method requires no explicit supervision, and can be learned with only a seed dictionary of words that have identical strings. We argue that this formulation has several intuitively attractive properties, particularly with the respect to improving <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> to mappings between difficult language pairs or word pairs. On a benchmark data set of bilingual lexicon induction and cross-lingual word similarity, our approach can achieve competitive or superior performance compared to state-of-the-art published results, with particularly strong results being found on etymologically distant and/or morphologically rich languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1162 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364708233 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1162/>Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing</a></strong><br><a href=/people/t/tal-schuster/>Tal Schuster</a>
|
<a href=/people/o/ori-ram/>Ori Ram</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/a/amir-globerson/>Amir Globerson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1162><div class="card-body p-3 small">We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts, aligning them poses a challenge due to their dynamic nature. To this end, we construct context-independent variants of the original monolingual spaces and utilize their <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> to derive an <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> for the context-dependent spaces. This <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> readily supports processing of a target language, improving <a href=https://en.wikipedia.org/wiki/Language_transfer>transfer</a> by context-aware embeddings. Our experimental results demonstrate the effectiveness of this approach for zero-shot and few-shot learning of dependency parsing. Specifically, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> consistently outperforms the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on 6 tested languages, yielding an improvement of 6.8 LAS points on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1164 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364687803 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1164" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1164/>Microblog Hashtag Generation via Encoding Conversation Contexts</a></strong><br><a href=/people/y/yue-wang/>Yue Wang</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/i/irwin-king/>Irwin King</a>
|
<a href=/people/m/michael-r-lyu/>Michael R. Lyu</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1164><div class="card-body p-3 small">Automatic hashtag annotation plays an important role in content understanding for <a href=https://en.wikipedia.org/wiki/Microblogging>microblog posts</a>. To date, progress made in this field has been restricted to phrase selection from limited candidates, or word-level hashtag discovery using <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a>. Different from previous work considering <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> to be inseparable, our work is the first effort to annotate <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> with a novel sequence generation framework via viewing the <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> as a short sequence of words. Moreover, to address the data sparsity issue in processing short microblog posts, we propose to jointly model the target posts and the conversation contexts initiated by them with bidirectional attention. Extensive experimental results on two large-scale datasets, newly collected from <a href=https://en.wikipedia.org/wiki/Twitter>English Twitter</a> and <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Chinese Weibo</a>, show that our model significantly outperforms state-of-the-art models based on <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Further studies demonstrate our ability to effectively generate rare and even unseen hashtags, which is however not possible for most existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1166 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1166.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364697819 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1166/>Something’s Brewing ! Early Prediction of Controversy-causing Posts from Discussion Features</a></strong><br><a href=/people/j/jack-hessel/>Jack Hessel</a>
|
<a href=/people/l/lillian-lee/>Lillian Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1166><div class="card-body p-3 small">Controversial posts are those that split the preferences of a community, receiving both significant positive and significant negative feedback. Our inclusion of the word community here is deliberate : what is controversial to some audiences may not be so to others. Using data from several different communities on <a href=https://en.wikipedia.org/wiki/Reddit>reddit.com</a>, we predict the ultimate controversiality of posts, leveraging features drawn from both the textual content and the <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a> of the early comments that initiate the discussion. We find that even when only a handful of comments are available, e.g., the first 5 comments made within 15 minutes of the original post, discussion features often add predictive capacity to strong content-and- rate only baselines. Additional experiments on <a href=https://en.wikipedia.org/wiki/Domain_transfer>domain transfer</a> suggest that conversation- structure features often generalize to other communities better than conversation-content features do.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1167 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364700832 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1167" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1167/>No Permanent Friends or Enemies : Tracking Relationships between Nations from News<span class=acl-fixed-case>F</span>riends or Enemies: Tracking Relationships between Nations from News</a></strong><br><a href=/people/x/xiaochuang-han/>Xiaochuang Han</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/c/chenhao-tan/>Chenhao Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1167><div class="card-body p-3 small">Understanding the dynamics of international politics is important yet challenging for civilians. In this work, we explore <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised neural models</a> to infer <a href=https://en.wikipedia.org/wiki/International_relations>relations between nations</a> from <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. We extend existing models by incorporating shallow linguistics information and propose a new automatic evaluation metric that aligns relationship dynamics with manually annotated key events. As understanding <a href=https://en.wikipedia.org/wiki/International_relations>international relations</a> requires carefully analyzing <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>complex relationships</a>, we conduct in-person human evaluations with three groups of participants. Overall, humans prefer the outputs of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and give insightful feedback that suggests future directions for human-centered models. Furthermore, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> reveals interesting regional differences in <a href=https://en.wikipedia.org/wiki/News_media>news coverage</a>. For instance, with respect to <a href=https://en.wikipedia.org/wiki/China&#8211;United_States_relations>US-China relations</a>, <a href=https://en.wikipedia.org/wiki/Media_of_Singapore>Singaporean media</a> focus more on strengthening and purchasing, while <a href=https://en.wikipedia.org/wiki/Media_of_the_United_States>US media</a> focus more on criticizing and denouncing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1168 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361580764 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1168/>Improving Human Text Comprehension through Semi-Markov CRF-based Neural Section Title Generation<span class=acl-fixed-case>M</span>arkov <span class=acl-fixed-case>CRF</span>-based Neural Section Title Generation</a></strong><br><a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/s/steven-layne/>Steven Layne</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1168><div class="card-body p-3 small">Titles of short sections within long documents support readers by guiding their focus towards relevant passages and by providing anchor-points that help to understand the progression of the document. The positive effects of section titles are even more pronounced when measured on readers with less developed reading abilities, for example in communities with limited labeled text resources. We, therefore, aim to develop <a href=https://en.wikipedia.org/wiki/Scientific_technique>techniques</a> to generate section titles in <a href=https://en.wikipedia.org/wiki/Developing_country>low-resource environments</a>. In particular, we present an extractive pipeline for section title generation by first selecting the most salient sentence and then applying deletion-based compression. Our compression approach is based on a Semi-Markov Conditional Random Field that leverages unsupervised word-representations such as ELMo or BERT, eliminating the need for a complex encoder-decoder architecture. The results show that this approach leads to competitive performance with sequence-to-sequence models with <a href=https://en.wikipedia.org/wiki/High-throughput_screening>high resources</a>, while strongly outperforming it with low resources. In a human-subject study across subjects with varying reading abilities, we find that our section titles improve the speed of completing comprehension tasks while retaining similar <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1172 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359670150 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1172" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1172/>Pun Generation with Surprise</a></strong><br><a href=/people/h/he-he/>He He</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1172><div class="card-body p-3 small">We tackle the problem of generating a pun sentence given a pair of homophones (e.g., died and dyed). Puns are by their very nature statistically anomalous and not amenable to most text generation methods that are supervised by a <a href=https://en.wikipedia.org/wiki/Text_corpus>large corpus</a>. In this paper, we propose an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> to <a href=https://en.wikipedia.org/wiki/Pun>pun generation</a> based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., dyed) and the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>distant context</a>, but a strong association between the alternative word (e.g., died) and the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>immediate context</a>. We instantiate the surprisal principle in two ways : (i) as a measure based on the ratio of probabilities given by a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Based on human evaluation, our retrieve-and-edit approach generates puns successfully 30 % of the time, doubling the success rate of a neural generation baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1173" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1173/>Single Document Summarization as Tree Induction</a></strong><br><a href=/people/y/yang-liu-edinburgh/>Yang Liu</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1173><div class="card-body p-3 small">In this paper, we conceptualize single-document extractive summarization as a tree induction problem. In contrast to previous approaches which have relied on linguistically motivated document representations to generate summaries, our model induces a multi-root dependency tree while predicting the output summary. Each root node in the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree</a> is a summary sentence, and the subtrees attached to it are sentences whose content relates to or explains the summary sentence. We design a new iterative refinement algorithm : it induces the <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a> through repeatedly refining the structures predicted by previous iterations. We demonstrate experimentally on two benchmark datasets that our <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarizer</a> performs competitively against state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1174" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1174/>Fixed That for You : Generating Contrastive Claims with Semantic Edits</a></strong><br><a href=/people/c/christopher-hidey/>Christopher Hidey</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1174><div class="card-body p-3 small">Understanding contrastive opinions is a key component of argument generation. Central to an argument is the claim, a statement that is in dispute. Generating a counter-argument then requires generating a response in contrast to the main claim of the original argument. To generate contrastive claims, we create a corpus of Reddit comment pairs self-labeled by posters using the acronym FTFY (fixed that for you). We then train neural models on these pairs to edit the original claim and produce a new claim with a different view. We demonstrate significant improvement over a sequence-to-sequence baseline in BLEU score and a human evaluation for <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a>, and <a href=https://en.wikipedia.org/wiki/Contrast_(vision)>contrast</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1178" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1178/>Unsupervised Dialog Structure Learning</a></strong><br><a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1178><div class="card-body p-3 small">Learning a shared dialog structure from a set of task-oriented dialogs is an important challenge in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. The learned dialog structure can shed light on how to analyze human dialogs, and more importantly contribute to the design and evaluation of dialog systems. We propose to extract dialog structures using a modified VRNN model with discrete latent vectors. Different from existing HMM-based models, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is based on variational-autoencoder (VAE). Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to capture more dynamics in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogs</a> beyond the surface forms of the language. We find that qualitatively, our method extracts meaningful dialog structure, and quantitatively, outperforms previous models on the ability to predict unseen data. We further evaluate the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s effectiveness in a downstream task, the dialog system building task. Experiments show that, by integrating the learned dialog structure into the reward function design, the model converges faster and to a better outcome in a reinforcement learning setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1181 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1181/>Text Similarity Estimation Based on Word Embeddings and Matrix Norms for Targeted Marketing</a></strong><br><a href=/people/t/tim-vor-der-bruck/>Tim vor der Brück</a>
|
<a href=/people/m/marc-pouly/>Marc Pouly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1181><div class="card-body p-3 small">The prevalent way to estimate the <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> of two documents based on word embeddings is to apply the cosine similarity measure to the two <a href=https://en.wikipedia.org/wiki/Centroid>centroids</a> obtained from the embedding vectors associated with the words in each document. Motivated by an industrial application from the domain of <a href=https://en.wikipedia.org/wiki/Youth_marketing>youth marketing</a>, where this approach produced only mediocre results, we propose an alternative way of combining the <a href=https://en.wikipedia.org/wiki/Word_vectors>word vectors</a> using <a href=https://en.wikipedia.org/wiki/Matrix_norms>matrix norms</a>. The evaluation shows superior results for most of the investigated matrix norms in comparison to both the classical cosine measure and several other document similarity estimates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1182/>Glocal : Incorporating Global Information in Local Convolution for Keyphrase Extraction<span class=acl-fixed-case>G</span>local: Incorporating Global Information in Local Convolution for Keyphrase Extraction</a></strong><br><a href=/people/a/animesh-prasad/>Animesh Prasad</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1182><div class="card-body p-3 small">Graph Convolutional Networks (GCNs) are a class of spectral clustering techniques that leverage localized convolution filters to perform <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classification</a> directly on <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphical structures</a>. While such methods model nodes&#8217; local pairwise importance, they lack the capability to model global importance relative to other nodes of the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. This causes such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to miss critical information in tasks where global ranking is a key component for the task, such as in <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>keyphrase extraction</a>. We address this shortcoming by allowing the proper incorporation of global information into the GCN family of models through the use of scaled node weights. In the context of <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>keyphrase extraction</a>, incorporating global random walk scores obtained from TextRank boosts performance significantly. With our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, we achieve state-of-the-art results, bettering a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by an absolute 2 % increase in F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1183 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1183/>A Study of Latent Structured Prediction Approaches to Passage Reranking</a></strong><br><a href=/people/i/iryna-haponchyk/>Iryna Haponchyk</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1183><div class="card-body p-3 small">The structured output framework provides a helpful tool for learning to rank problems. In this paper, we propose a structured output approach which regards <a href=https://en.wikipedia.org/wiki/Ranking>rankings</a> as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>. Our approach addresses the complex optimization of Mean Average Precision (MAP) ranking metric. We provide an <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference procedure</a> to find the max-violating ranking based on the decomposition of the corresponding loss. The results of our experiments on WikiQA and TREC13 datasets show that our reranking based on <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> is a promising research direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1185 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1185/>Tweet Stance Detection Using an Attention based Neural Ensemble Model<span class=acl-fixed-case>T</span>weet Stance Detection Using an Attention based Neural Ensemble Model</a></strong><br><a href=/people/u/umme-aymun-siddiqua/>Umme Aymun Siddiqua</a>
|
<a href=/people/a/abu-nowshed-chy/>Abu Nowshed Chy</a>
|
<a href=/people/m/masaki-aono/>Masaki Aono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1185><div class="card-body p-3 small">Stance detection in twitter aims at mining user stances expressed in a tweet towards a single or multiple target entities. To tackle this problem, most of the prior studies have been explored the traditional <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, e.g., <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTM</a> and GRU. However, in compared to these traditional approaches, recently proposed densely connected Bi-LSTM and nested LSTMs architectures effectively address the vanishing-gradient and overfitting problems as well as dealing with long-term dependencies. In this paper, we propose a neural ensemble model that adopts the strengths of these two LSTM variants to learn better long-term dependencies, where each module coupled with an attention mechanism that amplifies the contribution of important elements in the final representation. We also employ a multi-kernel convolution on top of them to extract the higher-level tweet representations. Results of extensive experiments on single and multi-target stance detection datasets show that our proposed method achieves substantial improvement over the current state-of-the-art deep learning based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1188 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1188/>Learning Unsupervised Multilingual Word Embeddings with Incremental Multilingual Hubs</a></strong><br><a href=/people/g/geert-heyman/>Geert Heyman</a>
|
<a href=/people/b/bregt-verreet/>Bregt Verreet</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1188><div class="card-body p-3 small">Recent research has discovered that a shared bilingual word embedding space can be induced by projecting monolingual word embedding spaces from two languages using a self-learning paradigm without any bilingual supervision. However, it has also been shown that for distant language pairs such fully unsupervised self-learning methods are unstable and often get stuck in poor local optima due to reduced isomorphism between starting monolingual spaces. In this work, we propose a new <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robust framework</a> for learning unsupervised multilingual word embeddings that mitigates the instability issues. We learn a shared multilingual embedding space for a variable number of languages by incrementally adding new languages one by one to the current multilingual space. Through the gradual language addition the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> can leverage the <a href=https://en.wikipedia.org/wiki/Interconnection>interdependencies</a> between the new language and all other languages in the current multilingual space. We find that it is beneficial to project more distant languages later in the iterative process. Our fully unsupervised multilingual embedding spaces yield results that are on par with the state-of-the-art methods in the bilingual lexicon induction (BLI) task, and simultaneously obtain state-of-the-art scores on two downstream tasks : multilingual document classification and multilingual dependency parsing, outperforming even supervised baselines. This finding also accentuates the need to establish evaluation protocols for cross-lingual word embeddings beyond the omnipresent intrinsic BLI task in future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1189 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1189/>Curriculum Learning for Domain Adaptation in Neural Machine Translation</a></strong><br><a href=/people/x/xuan-zhang/>Xuan Zhang</a>
|
<a href=/people/p/pamela-shapiro/>Pamela Shapiro</a>
|
<a href=/people/g/gaurav-kumar/>Gaurav Kumar</a>
|
<a href=/people/p/paul-mcnamee/>Paul McNamee</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1189><div class="card-body p-3 small">We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training algorithm</a> with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1192/>Online Distilling from Checkpoints for Neural Machine Translation</a></strong><br><a href=/people/h/hao-ran-wei/>Hao-Ran Wei</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/r/ran-wang/>Ran Wang</a>
|
<a href=/people/x/xinyu-dai/>Xin-yu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1192><div class="card-body p-3 small">Current predominant neural machine translation (NMT) models often have a deep structure with large amounts of parameters, making these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> hard to train and easily suffering from <a href=https://en.wikipedia.org/wiki/Overfitting>over-fitting</a>. A common practice is to utilize a validation set to evaluate the training process and select the best checkpoint. Average and ensemble techniques on checkpoints can lead to further performance improvement. However, as these methods do not affect the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training process</a>, the <a href=https://en.wikipedia.org/wiki/System>system</a> performance is restricted to the checkpoints generated in original <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training procedure</a>. In contrast, we propose an online knowledge distillation method. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on-the-fly generates a teacher model from <a href=https://en.wikipedia.org/wiki/Checkpoint>checkpoints</a>, guiding the <a href=https://en.wikipedia.org/wiki/Training>training process</a> to obtain better performance. Experiments on several datasets and language pairs show steady improvement over a strong self-attention-based baseline system. We also provide analysis on data-limited setting against <a href=https://en.wikipedia.org/wiki/Overfitting>over-fitting</a>. Furthermore, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> leads to an improvement in a machine reading experiment as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1193" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1193/>Value-based Search in Execution Space for Mapping Instructions to Programs</a></strong><br><a href=/people/d/dor-muhlgay/>Dor Muhlgay</a>
|
<a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1193><div class="card-body p-3 small">Training models to map natural language instructions to <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a>, given target world supervision only, requires searching for good programs at training time. Search is commonly done using <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> in the space of partial programs or program trees, but as the length of the instructions grows finding a good program becomes difficult. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithm</a> that uses the target world state, known at training time, to train a critic network that predicts the expected reward of every search state. We then score search states on the beam by interpolating their expected reward with the likelihood of programs represented by the search state. Moreover, we search not in the space of programs but in a more compressed state of program executions, augmented with recent entities and actions. On the SCONE dataset, we show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> dramatically improves performance on all three domains compared to standard <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> and other baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1200 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354228781 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1200" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1200/>Cross-lingual Visual Verb Sense Disambiguation</a></strong><br><a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a>
|
<a href=/people/f/frank-keller/>Frank Keller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1200><div class="card-body p-3 small">Recent work has shown that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>visual context</a> improves cross-lingual sense disambiguation for <a href=https://en.wikipedia.org/wiki/Noun>nouns</a>. We extend this line of work to the more challenging task of cross-lingual verb sense disambiguation, introducing the MultiSense dataset of 9,504 images annotated with English, German, and Spanish verbs. Each image in MultiSense is annotated with an English verb and its translation in <a href=https://en.wikipedia.org/wiki/German_language>German</a> or Spanish. We show that cross-lingual verb sense disambiguation models benefit from <a href=https://en.wikipedia.org/wiki/Context_(language_use)>visual context</a>, compared to unimodal baselines. We also show that the verb sense predicted by our best disambiguation model can improve the results of a text-only machine translation system when used for a multimodal translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354264673 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1201/>Subword-Level Language Identification for Intra-Word Code-Switching</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/o/ozlem-cetinoglu/>Özlem Çetinoğlu</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1201><div class="card-body p-3 small">Language identification for code-switching (CS), the phenomenon of alternating between two or more languages in conversations, has traditionally been approached under the assumption of a single language per token. However, if at least one language is morphologically rich, a large number of words can be composed of morphemes from more than one language (intra-word CS). In this paper, we extend the language identification task to the subword-level, such that it includes splitting mixed words while tagging each part with a language ID. We further propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for this task, which is based on a segmental recurrent neural network. In experiments on a new SpanishWixarika dataset and on an adapted GermanTurkish dataset, our proposed model performs slightly better than or roughly on par with our best baseline, respectively. Considering only mixed words, however, it strongly outperforms all baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354264026 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1203/>Contextualization of Morphological Inflection</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1203><div class="card-body p-3 small">Critical to <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> is the production of correctly inflected text. In this paper, we isolate the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional <a href=https://en.wikipedia.org/wiki/Inflection>morphological inflection</a> or surface realization, our task input does not provide gold tags that specify what morphological features to realize on each lemmatized word ; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a> before predicting the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a>, and compare this to a system that directly predicts the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a> without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1206.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355794917 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1206/>Measuring Immediate Adaptation Performance for Neural Machine Translation</a></strong><br><a href=/people/p/patrick-simianer/>Patrick Simianer</a>
|
<a href=/people/j/joern-wuebker/>Joern Wuebker</a>
|
<a href=/people/j/john-denero/>John DeNero</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1206><div class="card-body p-3 small">Incremental domain adaptation, in which a system learns from the correct output for each input immediately after making its prediction for that input, can dramatically improve <a href=https://en.wikipedia.org/wiki/System>system</a> performance for <a href=https://en.wikipedia.org/wiki/Interactive_machine_translation>interactive machine translation</a>. Users of interactive systems are sensitive to the speed of adaptation and how often a system repeats mistakes, despite being corrected. Adaptation is most commonly assessed using corpus-level BLEU- or TER-derived metrics that do not explicitly take adaptation speed into account. We find that these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> often do not capture immediate adaptation effects, such as zero-shot and one-shot learning of domain-specific lexical items. To this end, we propose new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that directly evaluate immediate adaptation performance for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We use these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to choose the most suitable adaptation method from a range of different adaptation techniques for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1208 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355798547 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1208/>Reinforcement Learning based Curriculum Optimization for Neural Machine Translation</a></strong><br><a href=/people/g/gaurav-kumar/>Gaurav Kumar</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/m/maxim-krikun/>Maxim Krikun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1208><div class="card-body p-3 small">We consider the problem of making efficient use of heterogeneous training data in neural machine translation (NMT). Specifically, given a training dataset with a sentence-level feature such as <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>, we seek an optimal <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum</a>, or order for presenting examples to the system during training. Our curriculum framework allows examples to appear an arbitrary number of times, and thus generalizes <a href=https://en.wikipedia.org/wiki/Weighting>data weighting</a>, <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filtering</a>, and fine-tuning schemes. Rather than relying on prior knowledge to design a <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum</a>, we use <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to learn one automatically, jointly with the NMT system, in the course of a single training run. We show that this approach can beat <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>uniform baselines</a> on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1209 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356056256 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1209/>Overcoming <a href=https://en.wikipedia.org/wiki/Catastrophic_Forgetting>Catastrophic Forgetting</a> During <a href=https://en.wikipedia.org/wiki/Domain_adaptation>Domain Adaptation</a> of Neural Machine Translation</a></strong><br><a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1209><div class="card-body p-3 small">Continued training is an effective method for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC)a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354246126 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1210/>Short-Term Meaning Shift : A Distributional Exploration</a></strong><br><a href=/people/m/marco-del-tredici/>Marco Del Tredici</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a>
|
<a href=/people/g/gemma-boleda/>Gemma Boleda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1210><div class="card-body p-3 small">We present the first exploration of meaning shift over short periods of time in <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a> using distributional representations. We create a small annotated dataset and use it to assess the performance of a standard <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for meaning shift detection on short-term meaning shift. We find that the model has problems distinguishing meaning shift from referential phenomena, and propose a measure of contextual variability to remedy this.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1213 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354239263 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1213" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1213/>An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models</a></strong><br><a href=/people/a/alexandra-chronopoulou/>Alexandra Chronopoulou</a>
|
<a href=/people/c/christos-baziotis/>Christos Baziotis</a>
|
<a href=/people/a/alexandros-potamianos/>Alexandros Potamianos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1213><div class="card-body p-3 small">A growing number of state-of-the-art transfer learning methods employ <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning methods</a> with greater level of <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1217 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355805085 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1217" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1217/>Joint Detection and Location of English Puns<span class=acl-fixed-case>E</span>nglish Puns</a></strong><br><a href=/people/y/yanyan-zou/>Yanyan Zou</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1217><div class="card-body p-3 small">A pun is a form of <a href=https://en.wikipedia.org/wiki/Word_play>wordplay</a> for an intended humorous or rhetorical effect, where a word suggests two or more meanings by exploiting <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> (homographic pun) or phonological similarity to another word (heterographic pun). This paper presents an approach that addresses pun detection and pun location jointly from a sequence labeling perspective. We employ a new tagging scheme such that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is capable of performing such a joint task, where useful structural information can be properly captured. We show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is effective in handling both homographic and heterographic puns. Empirical results on the benchmark datasets demonstrate that our approach can achieve new state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1219 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1219.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355808962 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1219/>Argument Mining for Understanding Peer Reviews</a></strong><br><a href=/people/x/xinyu-hua/>Xinyu Hua</a>
|
<a href=/people/m/mitko-nikolov/>Mitko Nikolov</a>
|
<a href=/people/n/nikhil-badugu/>Nikhil Badugu</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1219><div class="card-body p-3 small">Peer-review plays a critical role in the scientific writing and publication ecosystem. To assess the efficiency and efficacy of the reviewing process, one essential element is to understand and evaluate the reviews themselves. In this work, we study the content and structure of <a href=https://en.wikipedia.org/wiki/Peer_review>peer reviews</a> under the argument mining framework, through automatically detecting (1) the <a href=https://en.wikipedia.org/wiki/Argument>argumentative propositions</a> put forward by reviewers, and (2) their types (e.g., evaluating the work or making suggestions for improvement). We first collect 14.2 K reviews from major machine learning and natural language processing venues. 400 reviews are annotated with 10,386 propositions and corresponding types of Evaluation, Request, Fact, Reference, or Quote. We then train state-of-the-art proposition segmentation and classification models on the data to evaluate their utilities and identify new challenges for this new domain, motivating future directions for <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>. Further experiments show that proposition usage varies across venues in amount, type, and topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1221 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355811189 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1221/>Abusive Language Detection with Graph Convolutional Networks<span class=acl-fixed-case>A</span>busive <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>D</span>etection with <span class=acl-fixed-case>G</span>raph <span class=acl-fixed-case>C</span>onvolutional <span class=acl-fixed-case>N</span>etworks</a></strong><br><a href=/people/p/pushkar-mishra/>Pushkar Mishra</a>
|
<a href=/people/m/marco-del-tredici/>Marco Del Tredici</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1221><div class="card-body p-3 small">Abuse on the <a href=https://en.wikipedia.org/wiki/Internet>Internet</a> represents a significant societal problem of our time. Previous research on automated abusive language detection in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling followerfollowing relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1223 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364735719 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1223/>Factorising AMR generation through syntax<span class=acl-fixed-case>AMR</span> generation through syntax</a></strong><br><a href=/people/k/kris-cao/>Kris Cao</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1223><div class="card-body p-3 small">Generating from Abstract Meaning Representation (AMR) is an underspecified problem, as many syntactic decisions are not specified by the semantic graph. To explicitly account for this variation, we break down generating from AMR into two steps : first generate a <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>, and then generate the surface form. We show that decomposing the generation process this way leads to state-of-the-art single model performance generating from AMR without additional unlabelled data. We also demonstrate that we can generate meaning-preserving syntactic paraphrases of the same AMR graph, as judged by humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1224 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1224.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1224.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364709844 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1224" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1224/>A Crowdsourced Frame Disambiguation Corpus with Ambiguity</a></strong><br><a href=/people/a/anca-dumitrache/>Anca Dumitrache</a>
|
<a href=/people/l/lora-aroyo/>Lora Aroyo</a>
|
<a href=/people/c/chris-welty/>Chris Welty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1224><div class="card-body p-3 small">We present a resource for the task of FrameNet semantic frame disambiguation of over 5,000 word-sentence pairs from the Wikipedia corpus. The <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> were collected using a novel crowdsourcing approach with multiple workers per sentence to capture inter-annotator disagreement. In contrast to the typical approach of attributing the best single frame to each word, we provide a list of frames with disagreement-based scores that express the confidence with which each frame applies to the word. This is based on the idea that inter-annotator disagreement is at least partly caused by <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> that is inherent to the text and frames. We have found many examples where the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of individual frames overlap sufficiently to make them acceptable alternatives for interpreting a sentence. We have argued that ignoring this <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> creates an overly arbitrary target for training and evaluating natural language processing systems-if humans can not agree, why would we expect the correct answer from a machine to be any different? To process this data we also utilized an expanded lemma-set provided by the Framester system, which merges FN with <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> to enhance coverage. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> includes annotations of 1,000 sentence-word pairs whose lemmas are not part of FN. Finally we present <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for evaluating frame disambiguation systems that account for <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1227 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1227.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1227/>Partial Or Complete, That’s The Question</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/h/hangfeng-he/>Hangfeng He</a>
|
<a href=/people/c/chuchu-fan/>Chuchu Fan</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1227><div class="card-body p-3 small">For many structured learning tasks, the data annotation process is complex and costly. Existing annotation schemes usually aim at acquiring completely annotated structures, under the common perception that partial structures are of low quality and could hurt the learning process. This paper questions this common perception, motivated by the fact that <a href=https://en.wikipedia.org/wiki/Structure_(mathematical_logic)>structures</a> consist of interdependent sets of variables. Thus, given a fixed budget, partly annotating each structure may provide the same level of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>, while allowing for more <a href=https://en.wikipedia.org/wiki/Structure_(mathematical_logic)>structures</a> to be annotated. We provide an information theoretic formulation for this perspective and use it, in the context of three diverse structured learning tasks, to show that learning from partial structures can sometimes outperform learning from complete ones. Our findings may provide important insights into structured data annotation schemes and could support progress in learning protocols for structured tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1228 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1228" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1228/>Sequential Attention with Keyword Mask Model for Community-based Question Answering<span class=acl-fixed-case>S</span>equential <span class=acl-fixed-case>A</span>ttention with <span class=acl-fixed-case>K</span>eyword <span class=acl-fixed-case>M</span>ask <span class=acl-fixed-case>M</span>odel for <span class=acl-fixed-case>C</span>ommunity-based <span class=acl-fixed-case>Q</span>uestion <span class=acl-fixed-case>A</span>nswering</a></strong><br><a href=/people/j/jianxin-yang/>Jianxin Yang</a>
|
<a href=/people/w/wenge-rong/>Wenge Rong</a>
|
<a href=/people/l/libin-shi/>Libin Shi</a>
|
<a href=/people/z/zhang-xiong/>Zhang Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1228><div class="card-body p-3 small">In Community-based Question Answering system(CQA), Answer Selection(AS) is a critical task, which focuses on finding a suitable answer within a list of candidate answers. For neural network models, the key issue is how to model the representations of QA text pairs and calculate the interactions between them. We propose a Sequential Attention with Keyword Mask model(SAKM) for CQA to imitate human reading behavior. Question and answer text regard each other as context within keyword-mask attention when encoding the representations, and repeat multiple times(hops) in a sequential style. So the QA pairs capture features and information from both question text and answer text, interacting and improving vector representations iteratively through hops. The flexibility of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> allows to extract meaningful <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> from the sentences and enhance diverse mutual information. We perform on <a href=https://en.wikipedia.org/wiki/Question_answering>answer selection tasks</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>multi-level answer ranking tasks</a>. Experiment results demonstrate the superiority of our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on community-based QA datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1229 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1229/>Simple Attention-Based Representation Learning for Ranking Short Social Media Posts</a></strong><br><a href=/people/p/peng-shi/>Peng Shi</a>
|
<a href=/people/j/jinfeng-rao/>Jinfeng Rao</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1229><div class="card-body p-3 small">This paper explores the problem of ranking short social media posts with respect to user queries using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Instead of starting with a complex <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>, we proceed from the bottom up and examine the effectiveness of a simple, word-level Siamese architecture augmented with attention-based mechanisms for capturing semantic soft matches between query and post tokens. Extensive experiments on datasets from the TREC Microblog Tracks show that our simple models not only achieve better effectiveness than existing approaches that are far more complex or exploit a more diverse set of relevance signals, but are also much faster.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1230/>AttentiveChecker : A Bi-Directional Attention Flow Mechanism for Fact Verification<span class=acl-fixed-case>A</span>ttentive<span class=acl-fixed-case>C</span>hecker: A Bi-Directional Attention Flow Mechanism for Fact Verification</a></strong><br><a href=/people/s/santosh-tokala/>Santosh Tokala</a>
|
<a href=/people/v/vishal-g/>Vishal G</a>
|
<a href=/people/a/avirup-saha/>Avirup Saha</a>
|
<a href=/people/n/niloy-ganguly/>Niloy Ganguly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1230><div class="card-body p-3 small">The recently released FEVER dataset provided benchmark results on a fact-checking task in which given a factual claim, the system must extract textual evidence (sets of sentences from Wikipedia pages) that support or refute the claim. In this paper, we present a completely task-agnostic pipelined system, AttentiveChecker, consisting of three homogeneous Bi-Directional Attention Flow (BIDAF) networks, which are multi-layer hierarchical networks that represent the context at different levels of granularity. We are the first to apply to this task a bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. AttentiveChecker can be used to perform <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, sentence selection, and claim verification. Experiments on the FEVER dataset indicate that AttentiveChecker is able to achieve the state-of-the-art results on the FEVER test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1231 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1231" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1231/>Practical, Efficient, and Customizable <a href=https://en.wikipedia.org/wiki/Active_learning>Active Learning</a> for Named Entity Recognition in the <a href=https://en.wikipedia.org/wiki/Digital_humanities>Digital Humanities</a></a></strong><br><a href=/people/a/alexander-erdmann/>Alexander Erdmann</a>
|
<a href=/people/d/david-joseph-wrisley/>David Joseph Wrisley</a>
|
<a href=/people/b/benjamin-allen/>Benjamin Allen</a>
|
<a href=/people/c/christopher-brown/>Christopher Brown</a>
|
<a href=/people/s/sophie-cohen-bodenes/>Sophie Cohen-Bodénès</a>
|
<a href=/people/m/micha-elsner/>Micha Elsner</a>
|
<a href=/people/y/yukun-feng/>Yukun Feng</a>
|
<a href=/people/b/brian-joseph/>Brian Joseph</a>
|
<a href=/people/b/beatrice-joyeux-prunel/>Béatrice Joyeux-Prunel</a>
|
<a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1231><div class="card-body p-3 small">Scholars in inter-disciplinary fields like the <a href=https://en.wikipedia.org/wiki/Digital_humanities>Digital Humanities</a> are increasingly interested in semantic annotation of specialized corpora. Yet, under-resourced languages, imperfect or noisily structured data, and user-specific classification tasks make it difficult to meet their needs using off-the-shelf models. Manual annotation of large corpora from scratch, meanwhile, can be prohibitively expensive. Thus, we propose an active learning solution for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, attempting to maximize a custom model&#8217;s improvement per additional unit of manual annotation. Our system robustly handles any domain or user-defined label set and requires no external resources, enabling quality <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> for Humanities corpora where such resources are not available. Evaluating on typologically disparate languages and datasets, we reduce required annotation by 20-60 % and greatly outperform a competitive active learning baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1232 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1232" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1232/>Doc2hash : Learning Discrete Latent variables for Documents Retrieval<span class=acl-fixed-case>D</span>oc2hash: Learning Discrete Latent variables for Documents Retrieval</a></strong><br><a href=/people/y/yifei-zhang/>Yifei Zhang</a>
|
<a href=/people/h/hao-zhu/>Hao Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1232><div class="card-body p-3 small">Learning to hash via <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> has become a powerful paradigm for fast similarity search in documents retrieval. To get <a href=https://en.wikipedia.org/wiki/Binary_number>binary representation</a> (i.e., hash codes), the discrete distribution prior (i.e., <a href=https://en.wikipedia.org/wiki/Bernoulli_distribution>Bernoulli Distribution</a>) is applied to train the variational autoencoder (VAE). However, the discrete stochastic layer is usually incompatible with the <a href=https://en.wikipedia.org/wiki/Backpropagation>backpropagation</a> in the training stage, and thus causes a gradient flow problem because of non-differentiable operators. The reparameterization trick of sampling from a <a href=https://en.wikipedia.org/wiki/Probability_distribution>discrete distribution</a> usually inc <a href=https://en.wikipedia.org/wiki/Differentiable_function>non-differentiable operators</a>. In this paper, we propose a method, Doc2hash, that solves the gradient flow problem of the discrete stochastic layer by using continuous relaxation on priors, and trains the generative model in an end-to-end manner to generate hash codes. In qualitative and quantitative experiments, we show the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms other state-of-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1235 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1235" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1235/>Neural Text Generation from Rich Semantic Representations</a></strong><br><a href=/people/v/valerie-hajdik/>Valerie Hajdik</a>
|
<a href=/people/j/jan-buys/>Jan Buys</a>
|
<a href=/people/m/michael-wayne-goodman/>Michael Wayne Goodman</a>
|
<a href=/people/e/emily-m-bender/>Emily M. Bender</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1235><div class="card-body p-3 small">We propose neural models to generate high-quality text from structured representations based on Minimal Recursion Semantics (MRS). MRS is a rich semantic representation that encodes more precise semantic detail than other representations such as Abstract Meaning Representation (AMR). We show that a sequence-to-sequence model that maps a linearization of Dependency MRS, a graph-based representation of MRS, to <a href=https://en.wikipedia.org/wiki/Plain_text>text</a> can achieve a BLEU score of 66.11 when trained on gold data. The performance of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> can be improved further using a high-precision, broad coverage grammar-based parser to generate a large silver training corpus, achieving a final BLEU score of 77.17 on the full test set, and 83.37 on the subset of test data most closely matching the silver data domain. Our results suggest that MRS-based representations are a good choice for applications that need both structured semantics and the ability to produce natural language text as output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1239 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1239/>Open Information Extraction from Question-Answer Pairs</a></strong><br><a href=/people/n/nikita-bhutani/>Nikita Bhutani</a>
|
<a href=/people/y/yoshihiko-suhara/>Yoshihiko Suhara</a>
|
<a href=/people/w/wang-chiew-tan/>Wang-Chiew Tan</a>
|
<a href=/people/a/alon-halevy/>Alon Halevy</a>
|
<a href=/people/h/h-v-jagadish/>H. V. Jagadish</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1239><div class="card-body p-3 small">Open Information Extraction (OpenIE) extracts meaningful structured tuples from free-form text. Most previous work on OpenIE considers extracting data from one sentence at a time. We describe NeurON, a <a href=https://en.wikipedia.org/wiki/System>system</a> for extracting tuples from question-answer pairs. One of the main motivations for <a href=https://en.wikipedia.org/wiki/Neuron>NeurON</a> is to be able to extend <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> in a way that considers precisely the information that users care about. NeurON addresses several challenges. First, an answer text is often hard to understand without knowing the question, and second, relevant information can span multiple sentences. To address these, NeurON formulates extraction as a multi-source sequence-to-sequence learning task, wherein it combines distributed representations of a question and an answer to generate knowledge facts. We describe experiments on two real-world datasets that demonstrate that NeurON can find a significant number of new and interesting facts to extend a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> compared to state-of-the-art OpenIE methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1240 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1240" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1240/>Question Answering by Reasoning Across Documents with Graph Convolutional Networks</a></strong><br><a href=/people/n/nicola-de-cao/>Nicola De Cao</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1240><div class="card-body p-3 small">Most research in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> has focused on answering questions based on individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. Mentions of entities are <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> of this <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> while <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> encode relations between different <a href=https://en.wikipedia.org/wiki/Note_(typography)>mentions</a> (e.g., within- and cross-document co-reference). Graph convolutional networks (GCNs) are applied to these <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, WikiHop (Welbl et al., 2018).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1241 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1241" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1241/>A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>QA</span>, <span class=acl-fixed-case>SQ</span>u<span class=acl-fixed-case>AD</span> 2.0 and <span class=acl-fixed-case>Q</span>u<span class=acl-fixed-case>AC</span></a></strong><br><a href=/people/m/mark-yatskar/>Mark Yatskar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1241><div class="card-body p-3 small">We compare three new datasets for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> : <a href=https://en.wikipedia.org/wiki/Question_answering>SQuAD 2.0</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>QuAC</a>, and <a href=https://en.wikipedia.org/wiki/Question_answering>CoQA</a>, along several of their new features : (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers. We show that the datasets provide complementary coverage of the first two aspects, but weak coverage of the third. Because of the datasets&#8217; structural similarity, a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both <a href=https://en.wikipedia.org/wiki/Question_answering>SQuAD 2.0</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>CoQA</a>. Despite the similarity, <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on one <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> are ineffective on another <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, but we find moderate performance improvement through pretraining. To encourage cross-evaluation, we release code for conversion between datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1242 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1242/>BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis<span class=acl-fixed-case>BERT</span> Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1242><div class="card-body p-3 small">Question-answering plays an important role in <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce</a> as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making. Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions. We call this problem Review Reading Comprehension (RRC). To the best of our knowledge, no existing work has been done on RRC. In this work, we first build an RRC dataset called ReviewRC based on a popular <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for aspect-based sentiment analysis. Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC. To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> and aspect sentiment classification in aspect-based sentiment analysis. Experimental results demonstrate that the proposed post-training is highly effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1243 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1243/>Old is Gold : Linguistic Driven Approach for Entity and Relation Linking of Short Text</a></strong><br><a href=/people/a/ahmad-sakor/>Ahmad Sakor</a>
|
<a href=/people/i/isaiah-onando-mulang/>Isaiah Onando Mulang’</a>
|
<a href=/people/k/kuldeep-singh/>Kuldeep Singh</a>
|
<a href=/people/s/saeedeh-shekarpour/>Saeedeh Shekarpour</a>
|
<a href=/people/m/maria-esther-vidal/>Maria Esther Vidal</a>
|
<a href=/people/j/jens-lehmann/>Jens Lehmann</a>
|
<a href=/people/s/soren-auer/>Sören Auer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1243><div class="card-body p-3 small">Short texts challenge NLP tasks such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>disambiguation</a>, linking and relation inference because they do not provide sufficient context or are partially malformed (e.g. wrt. capitalization, <a href=https://en.wikipedia.org/wiki/Long_tail>long tail entities</a>, implicit relations). In this work, we present the Falcon approach which effectively maps entities and relations within a short text to its mentions of a background knowledge graph. Falcon overcomes the challenges of short text using a light-weight linguistic approach relying on a background knowledge graph. Falcon performs joint entity and relation linking of a short text by leveraging several fundamental principles of <a href=https://en.wikipedia.org/wiki/English_language>English morphology</a> (e.g. compounding, headword identification) and utilizes an extended <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> created by merging entities and relations from various knowledge sources. It uses the context of entities for finding relations and does not require <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>. Our empirical study using several standard benchmarks and datasets show that Falcon significantly outperforms state-of-the-art entity and relation linking for short text query inventories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1244.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1244 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1244 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1244" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1244/>Be Consistent ! Improving Procedural Text Comprehension using Label Consistency</a></strong><br><a href=/people/x/xinya-du/>Xinya Du</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1244><div class="card-body p-3 small">Our goal is procedural text comprehension, namely tracking how the properties of entities (e.g., their location) change with time given a procedural text (e.g., a paragraph about photosynthesis, a recipe). This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is challenging as the world is changing throughout the text, and despite recent advances, current <a href=https://en.wikipedia.org/wiki/System>systems</a> still struggle with this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our approach is to leverage the fact that, for many procedural texts, multiple independent descriptions are readily available, and that predictions from them should be consistent (label consistency). We present a new learning framework that leverages label consistency during training, allowing consistency bias to be built into the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. Evaluation on a standard benchmark dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach significantly improves prediction performance (F1) over prior state-of-the-art systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1246 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1246.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1246" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1246/>DROP : A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs<span class=acl-fixed-case>DROP</span>: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs</a></strong><br><a href=/people/d/dheeru-dua/>Dheeru Dua</a>
|
<a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1246><div class="card-body p-3 small">Reading comprehension has recently seen rapid progress, with <a href=https://en.wikipedia.org/wiki/Computer>systems</a> matching humans on the most popular datasets for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, a large body of work has highlighted the brittleness of these <a href=https://en.wikipedia.org/wiki/System>systems</a>, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and show that the best <a href=https://en.wikipedia.org/wiki/System>systems</a> only achieve 38.4 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> on our generalized accuracy metric, while expert human performance is 96 %. We additionally present a new model that combines reading comprehension methods with simple <a href=https://en.wikipedia.org/wiki/Numerical_analysis>numerical reasoning</a> to achieve 51 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1251 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1251/>A Simple and Robust Approach to Detecting Subject-Verb Agreement Errors</a></strong><br><a href=/people/s/simon-flachs/>Simon Flachs</a>
|
<a href=/people/o/ophelie-lacroix/>Ophélie Lacroix</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1251><div class="card-body p-3 small">While rule-based detection of subject-verb agreement (SVA) errors is sensitive to syntactic parsing errors and irregularities and exceptions to the main rules, neural sequential labelers have a tendency to overfit their training data. We observe that rule-based error generation is less sensitive to syntactic parsing errors and irregularities than <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection</a> and explore a simple, yet efficient approach to getting the best of both worlds : We train neural sequential labelers on the combination of large volumes of silver standard data, obtained through rule-based error generation, and gold standard data. We show that our simple protocol leads to more robust detection of SVA errors on both in-domain and out-of-domain data, as well as in the context of other errors and long-distance dependencies ; and across four standard benchmarks, the induced model on average achieves a new state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1252 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1252" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1252/>A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource Languages</a></strong><br><a href=/people/r/ronald-cardenas/>Ronald Cardenas</a>
|
<a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1252><div class="card-body p-3 small">Unsupervised part of speech (POS) tagging is often framed as a clustering problem, but practical taggers need to ground their clusters as well. Grounding generally requires reference labeled data, a luxury a low-resource language might not have. In this work, we describe an approach for low-resource unsupervised POS tagging that yields fully grounded output and requires no labeled training data. We find the classic <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> of Brown et al. (1992) clusters well in our use case and employ a decipherment-based approach to grounding. This approach presumes a sequence of cluster IDs is a &#8216;ciphertext&#8217; and seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We show intrinsically that, despite the difficulty of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we obtain reasonable performance across a variety of languages. We also show extrinsically that incorporating our <a href=https://en.wikipedia.org/wiki/POS_tagger>POS tagger</a> into a name tagger leads to state-of-the-art tagging performance in <a href=https://en.wikipedia.org/wiki/Sinhala_language>Sinhalese</a> and <a href=https://en.wikipedia.org/wiki/Kinyarwanda>Kinyarwanda</a>, two languages with nearly no labeled POS data available. We further demonstrate our tagger&#8217;s utility by incorporating it into a true &#8216;zero-resource&#8217; variant of the MALOPA (Ammar et al., 2016) dependency parser model that removes the current reliance on multilingual resources and gold POS tags for new languages. Experiments show that including our <a href=https://en.wikipedia.org/wiki/Tagger>tagger</a> makes up much of the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> lost when gold POS tags are unavailable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1253.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1253 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1253 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1253.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1253" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1253/>On Difficulties of Cross-Lingual Transfer with Order Differences : A Case Study on Dependency Parsing</a></strong><br><a href=/people/w/wasi-ahmad/>Wasi Ahmad</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1253><div class="card-body p-3 small">Different languages might have different <a href=https://en.wikipedia.org/wiki/Part_of_speech>word orders</a>. In this paper, we investigate crosslingual transfer and posit that an orderagnostic model will perform better when transferring to distant foreign languages. To test our hypothesis, we train dependency parsers on an <a href=https://en.wikipedia.org/wiki/English_language>English corpus</a> and evaluate their transfer performance on 30 other languages. Specifically, we compare <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> and <a href=https://en.wikipedia.org/wiki/Code>decoders</a> based on <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks (RNNs)</a> and modified self-attentive architectures. The former relies on sequential information while the latter is more flexible at modeling <a href=https://en.wikipedia.org/wiki/Word_order>word order</a>. Rigorous experiments and detailed analysis shows that RNN-based architectures transfer well to languages that are close to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, while self-attentive models have better overall cross-lingual transferability and perform especially well on distant languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1255 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355814096 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1255/>Self-Discriminative Learning for Unsupervised Document Embedding</a></strong><br><a href=/people/h/hong-you-chen/>Hong-You Chen</a>
|
<a href=/people/c/chin-hua-hu/>Chin-Hua Hu</a>
|
<a href=/people/l/leila-wehbe/>Leila Wehbe</a>
|
<a href=/people/s/shou-de-lin/>Shou-De Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1255><div class="card-body p-3 small">Unsupervised document representation learning is an important <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> providing pre-trained features for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>. Unlike most previous work which learn the <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> based on self-prediction of the surface of text, we explicitly exploit the inter-document information and directly model the relations of documents in <a href=https://en.wikipedia.org/wiki/Embedding>embedding space</a> with a discriminative network and a novel objective. Extensive experiments on both small and large public datasets show the competitiveness of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. In evaluations on standard <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has errors that are 5 to 13 % lower than state-of-the-art unsupervised embedding models. The reduction in error is even more pronounced in scarce label setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1256 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1256.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1256/>Adaptive Convolution for Text Classification</a></strong><br><a href=/people/b/byung-ju-choi/>Byung-Ju Choi</a>
|
<a href=/people/j/jun-hyung-park/>Jun-Hyung Park</a>
|
<a href=/people/s/sangkeun-lee/>SangKeun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1256><div class="card-body p-3 small">In this paper, we present an adaptive convolution for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> to give flexibility to convolutional neural networks (CNNs). Unlike traditional convolutions which utilize the same set of <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filters</a> regardless of different inputs, the adaptive convolution employs adaptively generated convolutional filters conditioned on inputs. We achieve this by attaching filter-generating networks, which are carefully designed to generate input-specific filters, to convolution blocks in existing CNNs. We show the efficacy of our approach in existing CNNs based on the <a href=https://en.wikipedia.org/wiki/Performance_evaluation>performance evaluation</a>. Our evaluation indicates that all of our baselines achieve performance improvements with adaptive convolutions as much as up to 2.6 percentage point in seven benchmark text classification datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1257.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1257 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1257 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359684150 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1257/>Zero-Shot Cross-Lingual Opinion Target Extraction<span class=acl-fixed-case>Z</span>ero-Shot Cross-Lingual Opinion Target Extraction</a></strong><br><a href=/people/s/soufian-jebbara/>Soufian Jebbara</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1257><div class="card-body p-3 small">Aspect-based sentiment analysis involves the recognition of so called opinion target expressions (OTEs). To automatically extract OTEs, supervised learning algorithms are usually employed which are trained on manually annotated corpora. The creation of these <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> is labor-intensive and sufficiently large datasets are therefore usually only available for a very narrow selection of languages and domains. In this work, we address the lack of available annotated data for specific languages by proposing a zero-shot cross-lingual approach for the extraction of opinion target expressions. We leverage multilingual word embeddings that share a common vector space across various languages and incorporate these into a convolutional neural network architecture for OTE extraction. Our experiments with 5 languages give promising results : We can successfully train a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on annotated data of a source language and perform accurate <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> on a target language without ever using any annotated samples in that target language. Depending on the source and target language pairs, we reach performances in a zero-shot regime of up to 77 % of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on target language data. Furthermore, we can increase this performance up to 87 % of a baseline model trained on target language data by performing cross-lingual learning from multiple source languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1260.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1260 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1260 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1260/>Abstractive Summarization of Reddit Posts with Multi-level Memory Networks<span class=acl-fixed-case>R</span>eddit Posts with Multi-level Memory Networks</a></strong><br><a href=/people/b/byeongchang-kim/>Byeongchang Kim</a>
|
<a href=/people/h/hyunwoo-kim/>Hyunwoo Kim</a>
|
<a href=/people/g/gunhee-kim/>Gunhee Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1260><div class="card-body p-3 small">We address the problem of abstractive summarization in two directions : proposing a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and a new <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. First, we collect Reddit TIFU dataset, consisting of 120 K posts from the online discussion forum Reddit. We use such informal crowd-generated posts as text source, in contrast with existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that mostly use formal documents as source such as <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. Thus, our dataset could less suffer from some biases that key sentences usually located at the beginning of the text and favorable summary candidates are already inside the text in similar forms. Second, we propose a novel abstractive summarization model named multi-level memory networks (MMN), equipped with multi-level memory to store the information of text from different levels of abstraction. With quantitative evaluation and user studies via <a href=https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk>Amazon Mechanical Turk</a>, we show the Reddit TIFU dataset is highly abstractive and the MMN outperforms the state-of-the-art summarization models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1263 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1263.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1263/>Text Generation with Exemplar-based Adaptive Decoding</a></strong><br><a href=/people/h/hao-peng/>Hao Peng</a>
|
<a href=/people/a/ankur-parikh/>Ankur Parikh</a>
|
<a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/d/dipanjan-das/>Dipanjan Das</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1263><div class="card-body p-3 small">We propose a novel conditioned text generation model. It draws inspiration from traditional template-based text generation techniques, where the source provides the content (i.e., what to say), and the template influences how to say it. Building on the successful encoder-decoder paradigm, it first encodes the content representation from the given input text ; to produce the output, it retrieves exemplar text from the training data as soft templates, which are then used to construct an exemplar-specific decoder. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on abstractive text summarization and data-to-text generation. Empirical results show that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves strong performance and outperforms comparable baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1267 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1267.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364226255 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1267" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1267/>Strong and Simple Baselines for Multimodal Utterance Embeddings</a></strong><br><a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/y/yao-chong-lim/>Yao Chong Lim</a>
|
<a href=/people/y/yao-hung-hubert-tsai/>Yao-Hung Hubert Tsai</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1267><div class="card-body p-3 small">Human language is a rich multimodal signal consisting of <a href=https://en.wikipedia.org/wiki/Speech>spoken words</a>, <a href=https://en.wikipedia.org/wiki/Facial_expression>facial expressions</a>, <a href=https://en.wikipedia.org/wiki/Gesture>body gestures</a>, and <a href=https://en.wikipedia.org/wiki/Intonation_(linguistics)>vocal intonations</a>. Learning representations for these spoken utterances is a complex research problem due to the presence of multiple heterogeneous sources of information. Recent advances in <a href=https://en.wikipedia.org/wiki/Multimodal_learning>multimodal learning</a> have followed the general trend of building more complex models that utilize various attention, memory and recurrent components. In this paper, we propose two simple but strong baselines to learn embeddings of multimodal utterances. The first baseline assumes a conditional factorization of the utterance into unimodal factors. Each <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal factor</a> is modeled using the simple form of a <a href=https://en.wikipedia.org/wiki/Likelihood_function>likelihood function</a> obtained via a linear transformation of the embedding. We show that the optimal embedding can be derived in closed form by taking a weighted average of the unimodal features. In order to capture richer representations, our second baseline extends the first by factorizing into unimodal, bimodal, and trimodal factors, while retaining simplicity and efficiency during <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. From a set of experiments across two tasks, we show strong performance on both supervised and semi-supervised multimodal prediction, as well as significant (10 times) speedups over neural models during <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. Overall, we believe that our strong baseline models offer new benchmarking options for future research in <a href=https://en.wikipedia.org/wiki/Multimodal_learning>multimodal learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1269.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1269 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1269 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364740187 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1269/>Towards Content Transfer through Grounded Text Generation</a></strong><br><a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a>
|
<a href=/people/m/michel-galley/>Michel Galley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1269><div class="card-body p-3 small">Recent work in neural generation has attracted significant interest in controlling the form of text, such as <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>style</a>, <a href=https://en.wikipedia.org/wiki/Persona>persona</a>, and <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a>. However, there has been less work on controlling neural text generation for <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a>. This paper introduces the notion of Content Transfer for long-form text generation, where the task is to generate a next sentence in a document that both fits its context and is grounded in a content-rich external textual source such as a news story. Our experiments on Wikipedia data show significant improvements against competitive <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1270 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364746823 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1270" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1270/>Improving Machine Reading Comprehension with General Reading Strategies</a></strong><br><a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1270><div class="card-body p-3 small">Reading strategies have been shown to improve <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension levels</a>, especially for readers lacking adequate prior knowledge. Just as the process of knowledge accumulation is time-consuming for human readers, it is resource-demanding to impart rich general domain knowledge into a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep language model</a> via pre-training. Inspired by reading strategies identified in <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>, and given limited computational resources-just a pre-trained model and a fixed number of training instances-we propose three general strategies aimed to improve non-extractive machine reading comprehension (MRC): (i) BACK AND FORTH READING that considers both the original and reverse order of an input sequence, (ii) HIGHLIGHTING, which adds a trainable embedding to the text embedding of tokens that are relevant to the question and candidate answers, and (iii) SELF-ASSESSMENT that generates practice questions and candidate answers directly from the text in an unsupervised manner. By fine-tuning a pre-trained language model (Radford et al., 2018) with our proposed strategies on the largest general domain multiple-choice MRC dataset RACE, we obtain a 5.8 % absolute increase in accuracy over the previous best result achieved by the same pre-trained model fine-tuned on RACE without the use of strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1271 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364750438 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1271" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1271/>Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension</a></strong><br><a href=/people/y/yichong-xu/>Yichong Xu</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/j/jingjing-liu/>Jingjing Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1271><div class="card-body p-3 small">We propose a multi-task learning framework to learn a joint Machine Reading Comprehension (MRC) model that can be applied to a wide range of MRC tasks in different domains. Inspired by recent ideas of data selection in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, we develop a novel sample re-weighting scheme to assign sample-specific weights to the loss. Empirical study shows that our approach can be applied to many existing MRC models. Combined with contextual representations from pre-trained language models (such as ELMo), we achieve new state-of-the-art results on a set of MRC benchmark datasets. We release our code at.<url>https://github.com/xycforgithub/MultiTask-MRC</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1273 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1273.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361691015 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1273/>Iterative Search for Weakly Supervised Semantic Parsing</a></strong><br><a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/s/shikhar-murty/>Shikhar Murty</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1273><div class="card-body p-3 small">Training semantic parsers from question-answer pairs typically involves searching over an exponentially large space of <a href=https://en.wikipedia.org/wiki/Logical_form>logical forms</a>, and an unguided search can easily be misled by spurious logical forms that coincidentally evaluate to the correct answer. We propose a novel iterative training algorithm that alternates between searching for consistent logical forms and maximizing the marginal likelihood of the retrieved ones. This training scheme lets us iteratively train <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that provide guidance to subsequent ones to search for logical forms of increasing <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a>, thus dealing with the problem of spuriousness. We evaluate these techniques on two hard datasets : WikiTableQuestions (WTQ) and Cornell Natural Language Visual Reasoning (NLVR), and show that our training algorithm outperforms the previous best systems, on WTQ in a comparable setting, and on NLVR with significantly less supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1275 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1275" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1275/>Bridging the Gap : Attending to Discontinuity in Identification of Multiword Expressions<span class=acl-fixed-case>B</span>ridging the Gap: <span class=acl-fixed-case>A</span>ttending to Discontinuity in Identification of Multiword Expressions</a></strong><br><a href=/people/o/omid-rohanian/>Omid Rohanian</a>
|
<a href=/people/s/shiva-taslimipoor/>Shiva Taslimipoor</a>
|
<a href=/people/s/samaneh-kouchaki/>Samaneh Kouchaki</a>
|
<a href=/people/l/le-an-ha/>Le An Ha</a>
|
<a href=/people/r/ruslan-mitkov/>Ruslan Mitkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1275><div class="card-body p-3 small">We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture. We specifically target <a href=https://en.wikipedia.org/wiki/Classification_of_discontinuities>discontinuity</a>, an under-explored aspect that poses a significant challenge to computational treatment of MWEs. Two neural architectures are explored : Graph Convolutional Network (GCN) and multi-head self-attention. GCN leverages dependency parse information, and self-attention attends to long-range relations. We finally propose a combined <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that integrates complementary information from both, through a gating mechanism. The experiments on a standard multilingual dataset for verbal MWEs show that our model outperforms the baselines not only in the case of discontinuous MWEs but also in overall F-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1277.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1277 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1277 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1277" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1277/>VCWE : Visual Character-Enhanced Word Embeddings<span class=acl-fixed-case>VCWE</span>: Visual Character-Enhanced Word Embeddings</a></strong><br><a href=/people/c/chi-sun/>Chi Sun</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1277><div class="card-body p-3 small">Chinese is a <a href=https://en.wikipedia.org/wiki/Logogram>logographic writing system</a>, and the shape of Chinese characters contain rich syntactic and semantic information. In this paper, we propose a model to learn Chinese word embeddings via three-level composition : (1) a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> to extract the intra-character compositionality from the visual shape of a character ; (2) a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> with self-attention to compose character representation into word embeddings ; (3) the Skip-Gram framework to capture non-compositionality directly from the contextual information. Evaluations demonstrate the superior performance of our model on four tasks : word similarity, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1278.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1278 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1278 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1278.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1278" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1278/>Subword Encoding in Lattice LSTM for Chinese Word Segmentation<span class=acl-fixed-case>LSTM</span> for <span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/j/jie-yang/>Jie Yang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/s/shuailong-liang/>Shuailong Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1278><div class="card-body p-3 small">We investigate subword information for Chinese word segmentation, by integrating sub word embeddings trained using byte-pair encoding into a Lattice LSTM (LaLSTM) network over a character sequence. Experiments on standard benchmark show that subword information brings significant gains over strong character-based segmentation models. To our knowledge, this is the first research on the effectiveness of <a href=https://en.wikipedia.org/wiki/Subword>subwords</a> on neural word segmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1281 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1281/>Shrinking Japanese Morphological Analyzers With <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> and <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>Semi-supervised Learning</a><span class=acl-fixed-case>J</span>apanese Morphological Analyzers With Neural Networks and Semi-supervised Learning</a></strong><br><a href=/people/a/arseny-tolmachev/>Arseny Tolmachev</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1281><div class="card-body p-3 small">For languages without natural word boundaries, like <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> is a prerequisite for downstream analysis. For <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, segmentation is often done jointly with <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part of speech tagging</a>, and this process is usually referred to as <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a>. Morphological analyzers are trained on data hand-annotated with segmentation boundaries and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>part of speech tags</a>. A segmentation dictionary or character n-gram information is also provided as additional inputs to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Incorporating this extra information makes <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> large. Modern neural morphological analyzers can consume gigabytes of <a href=https://en.wikipedia.org/wiki/Computer_memory>memory</a>. We propose a compact alternative to these cumbersome approaches which do not rely on any externally provided n-gram or word representations. The model uses only unigram character embeddings, encodes them using either stacked bi-LSTM or a self-attention network, and independently infers both segmentation and part of speech information. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained in an end-to-end and semi-supervised fashion, on labels produced by a state-of-the-art analyzer. We demonstrate that the proposed technique rivals performance of a previous dictionary-based state-of-the-art approach and can even surpass it when training with the combination of human-annotated and automatically-annotated data. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> itself is significantly smaller than the dictionary-based one : it uses less than 15 megabytes of space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1282.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1282 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1282 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1282/>Neural Constituency Parsing of Speech Transcripts</a></strong><br><a href=/people/p/paria-jamshid-lou/>Paria Jamshid Lou</a>
|
<a href=/people/y/yufei-wang/>Yufei Wang</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1282><div class="card-body p-3 small">This paper studies the performance of a neural self-attentive parser on <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcribed speech</a>. Speech presents parsing challenges that do not appear in written text, such as the lack of <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a> and the presence of <a href=https://en.wikipedia.org/wiki/Speech_disfluency>speech disfluencies</a> (including filled pauses, <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>repetitions</a>, corrections, etc.). Disfluencies are especially problematic for conventional syntactic parsers, which typically fail to find any EDITED disfluency nodes at all. This motivated the development of special disfluency detection systems, and special mechanisms added to <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> specifically to handle <a href=https://en.wikipedia.org/wiki/Disfluency>disfluencies</a>. However, we show here that <a href=https://en.wikipedia.org/wiki/Parsing>neural parsers</a> can find EDITED disfluency nodes, and the best <a href=https://en.wikipedia.org/wiki/Parsing>neural parsers</a> find them with an accuracy surpassing that of specialized disfluency detection systems, thus making these specialized mechanisms unnecessary. This paper also investigates a modified <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> that puts more weight on EDITED nodes. It also describes tree-transformations that simplify the disfluency detection task by providing alternative encodings of disfluencies and syntactic information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1283 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1283/>Acoustic-to-Word Models with Conversational Context Information</a></strong><br><a href=/people/s/suyoun-kim/>Suyoun Kim</a>
|
<a href=/people/f/florian-metze/>Florian Metze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1283><div class="card-body p-3 small">Conversational context information, higher-level knowledge that spans across sentences, can help to recognize a long conversation. However, existing speech recognition models are typically built at a <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level</a>, and thus it may not capture important <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversational context information</a>. The recent progress in end-to-end speech recognition enables integrating context with other available information (e.g., acoustic, linguistic resources) and directly recognizing words from <a href=https://en.wikipedia.org/wiki/Speech>speech</a>. In this work, we present a direct acoustic-to-word, end-to-end speech recognition model capable of utilizing the conversational context to better process long conversations. We evaluate our proposed approach on the Switchboard conversational speech corpus and show that our system outperforms a standard end-to-end speech recognition system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1286.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1286 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1286 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1286/>Relation Classification Using Segment-Level Attention-based CNN and Dependency-based RNN<span class=acl-fixed-case>CNN</span> and Dependency-based <span class=acl-fixed-case>RNN</span></a></strong><br><a href=/people/v/van-hien-tran/>Van-Hien Tran</a>
|
<a href=/people/v/van-thuy-phi/>Van-Thuy Phi</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1286><div class="card-body p-3 small">Recently, relation classification has gained much success by exploiting <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. In this paper, we propose a new model effectively combining Segment-level Attention-based Convolutional Neural Networks (SACNNs) and Dependency-based Recurrent Neural Networks (DepRNNs). While SACNNs allow the model to selectively focus on the important information segment from the raw sequence, DepRNNs help to handle the long-distance relations from the shortest dependency path of relation entities. Experiments on the SemEval-2010 Task 8 dataset show that our model is comparable to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> without using any external lexical features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1288 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1288" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1288/>Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions</a></strong><br><a href=/people/z/zhi-xiu-ye/>Zhi-Xiu Ye</a>
|
<a href=/people/z/zhen-hua-ling/>Zhen-Hua Ling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1288><div class="card-body p-3 small">This paper presents a neural relation extraction method to deal with the noisy training data generated by distant supervision. Previous studies mainly focus on sentence-level de-noising by designing <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> with intra-bag attentions. In this paper, both intra-bag and inter-bag attentions are considered in order to deal with the <a href=https://en.wikipedia.org/wiki/Noise>noise</a> at sentence-level and bag-level respectively. First, relation-aware bag representations are calculated by weighting <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> using intra-bag attentions. Here, each possible relation is utilized as the query for attention calculation instead of only using the target relation in conventional methods. Furthermore, the representation of a group of bags in the training set which share the same relation label is calculated by weighting bag representations using a similarity-based inter-bag attention module. Finally, a bag group is utilized as a training sample when building our relation extractor. Experimental results on the New York Times dataset demonstrate the effectiveness of our proposed intra-bag and inter-bag attention modules. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> also achieves better relation extraction accuracy than state-of-the-art methods on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1289 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1289/>Ranking-Based Autoencoder for Extreme Multi-label Classification</a></strong><br><a href=/people/b/bingyu-wang/>Bingyu Wang</a>
|
<a href=/people/l/li-chen/>Li Chen</a>
|
<a href=/people/w/wei-sun/>Wei Sun</a>
|
<a href=/people/k/kechen-qin/>Kechen Qin</a>
|
<a href=/people/k/kefeng-li/>Kefeng Li</a>
|
<a href=/people/h/hui-zhou/>Hui Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1289><div class="card-body p-3 small">Extreme Multi-label classification (XML) is an important yet challenging machine learning task, that assigns to each instance its most relevant candidate labels from an extremely large label collection, where the numbers of labels, <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and instances could be thousands or millions. XML is more and more on demand in the Internet industries, accompanied with the increasing business scale / scope and data accumulation. The extremely large label collections yield challenges such as <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a>, inter-label dependency and noisy labeling. Many methods have been proposed to tackle these challenges, based on different mathematical formulations. In this paper, we propose a deep learning XML method, with a word-vector-based self-attention, followed by a ranking-based AutoEncoder architecture. The proposed method has three major advantages : 1) the autoencoder simultaneously considers the inter-label dependencies and the feature-label dependencies, by projecting labels and <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> onto a common embedding space ; 2) the ranking loss not only improves the training efficiency and accuracy but also can be extended to handle noisy labeled data ; 3) the efficient attention mechanism improves feature representation by highlighting feature importance. Experimental results on benchmark datasets show the proposed method is competitive to state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1290 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1290/>Posterior-regularized REINFORCE for Instance Selection in Distant Supervision<span class=acl-fixed-case>REINFORCE</span> for Instance Selection in Distant Supervision</a></strong><br><a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/s/siliang-tang/>Siliang Tang</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/s/shiliang-pu/>Shiliang Pu</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1290><div class="card-body p-3 small">This paper provides a new way to improve the efficiency of the REINFORCE training process. We apply <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to the task of <a href=https://en.wikipedia.org/wiki/Instance_selection>instance selection</a> in distant supervision. Modeling the instance selection in one bag as a sequential decision process, a reinforcement learning agent is trained to determine whether an instance is valuable or not and construct a new bag with less noisy instances. However <a href=https://en.wikipedia.org/wiki/Bias_(statistics)>unbiased methods</a>, such as REINFORCE, could usually take much time to train. This paper adopts posterior regularization (PR) to integrate some domain-specific rules in instance selection using REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classifier trained on cleaned distant supervision dataset as well as the efficiency of the REINFORCE training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1291.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1291 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1291 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1291" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1291/>Scalable Collapsed Inference for High-Dimensional Topic Models</a></strong><br><a href=/people/r/rashidul-islam/>Rashidul Islam</a>
|
<a href=/people/j/james-foulds/>James Foulds</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1291><div class="card-body p-3 small">The bigger the corpus, the more topics it can potentially support. To truly make full use of massive text corpora, a topic model inference algorithm must therefore scale efficiently in 1) documents and 2) topics, while 3) achieving accurate <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. Previous methods have achieved two out of three of these criteria simultaneously, but never all three at once. In this paper, we develop an online inference algorithm for <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> which leverages <a href=https://en.wikipedia.org/wiki/Stochastic>stochasticity</a> to scale well in the number of documents, sparsity to scale well in the number of topics, and which operates in the collapsed representation of the topic model for improved accuracy and run-time performance. We use a <a href=https://en.wikipedia.org/wiki/Monte_Carlo_method>Monte Carlo inner loop</a> in the online setting to approximate the collapsed variational Bayes updates in a sparse and efficient way, which we accomplish via the MetropolisHastings Walker method. We showcase our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> on LDA and the recently proposed mixed membership skip-gram topic model. Our method requires only amortized O(k_d) computation per word token instead of O(K) operations, where the number of topics occurring for a particular document k_d the total number of topics in the corpus K, to converge to a high-quality solution.<tex-math>O(k_{d})</tex-math> computation per word token instead of <tex-math>O(K)</tex-math> operations, where the number of topics occurring for a particular document <tex-math>k_{d}\\ll</tex-math> the total number of topics in the corpus <tex-math>K</tex-math>, to converge to a high-quality solution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1293.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1293 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1293 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1293/>Predicting Malware Attributes from Cybersecurity Texts</a></strong><br><a href=/people/a/arpita-roy/>Arpita Roy</a>
|
<a href=/people/y/youngja-park/>Youngja Park</a>
|
<a href=/people/s/shimei-pan/>Shimei Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1293><div class="card-body p-3 small">Text analytics is a useful tool for studying <a href=https://en.wikipedia.org/wiki/Malware>malware behavior</a> and <a href=https://en.wikipedia.org/wiki/Threat_(computer)>tracking emerging threats</a>. The task of automated malware attribute identification based on cybersecurity texts is very challenging due to a large number of malware attribute labels and a small number of training instances. In this paper, we propose a novel feature learning method to leverage diverse knowledge sources such as small amount of human annotations, unlabeled text and specifications about malware attribute labels. Our evaluation has demonstrated the effectiveness of our method over the state-of-the-art malware attribute prediction systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1298.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1298 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1298 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1298" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1298/>A Richer-but-Smarter Shortest Dependency Path with Attentive Augmentation for <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/d/duy-cat-can/>Duy-Cat Can</a>
|
<a href=/people/h/hoang-quynh-le/>Hoang-Quynh Le</a>
|
<a href=/people/q/quang-thuy-ha/>Quang-Thuy Ha</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1298><div class="card-body p-3 small">To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence. Each <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> suffers from its own disadvantage of either <a href=https://en.wikipedia.org/wiki/Information_asymmetry>missing or redundant information</a>. In this work, we propose a novel <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a> that combines the advantages of these two <a href=https://en.wikipedia.org/wiki/Scientific_modelling>approaches</a>. This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but-Smarter SDP). To exploit the representation behind the RbSP structure effectively, we develop a combined <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural model</a> with a LSTM network on word sequences and a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. The data and source code are available at https://github.com/catcd/RbSP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1299 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356071812 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1299" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1299/>Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases</a></strong><br><a href=/people/y/yu-chen/>Yu Chen</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/m/mohammed-j-zaki/>Mohammed J. Zaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1299><div class="card-body p-3 small">When answering <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language questions</a> over knowledge bases (KBs), different question components and KB aspects play different roles. However, most existing embedding-based methods for knowledge base question answering (KBQA) ignore the subtle inter-relationships between the question and the KB (e.g., <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity types</a>, <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation paths</a> and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>). In this work, we propose to directly model the two-way flow of interactions between the questions and the KB via a novel Bidirectional Attentive Memory Network, called BAMnet. Requiring no external resources and only very few hand-crafted features, on the WebQuestions benchmark, our method significantly outperforms existing information-retrieval based methods, and remains competitive with (hand-crafted) semantic parsing based methods. Also, since we use <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>, our method offers better <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> compared to other baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1301 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356088995 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1301/>Enhancing Key-Value Memory Neural Networks for Knowledge Based Question Answering</a></strong><br><a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/y/yuxuan-lai/>Yuxuan Lai</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/z/zhiguo-wang/>Zhiguo Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1301><div class="card-body p-3 small">Traditional Key-value Memory Neural Networks (KV-MemNNs) are proved to be effective to support shallow reasoning over a collection of documents in domain specific Question Answering or Reading Comprehension tasks. However, extending KV-MemNNs to Knowledge Based Question Answering (KB-QA) is not trivia, which should properly decompose a complex question into a sequence of queries against the <a href=https://en.wikipedia.org/wiki/Random-access_memory>memory</a>, and update the query representations to support multi-hop reasoning over the <a href=https://en.wikipedia.org/wiki/Random-access_memory>memory</a>. In this paper, we propose a novel mechanism to enable conventional KV-MemNNs models to perform interpretable reasoning for complex questions. To achieve this, we design a new query updating strategy to mask previously-addressed memory information from the query representations, and introduce a novel STOP strategy to avoid invalid or repeated memory reading without strong annotation signals. This also enables KV-MemNNs to produce structured queries and work in a semantic parsing fashion. Experimental results on benchmark datasets show that our solution, trained with question-answer pairs only, can provide conventional KV-MemNNs models with better reasoning abilities on complex questions, and achieve state-of-art performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1304 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359689303 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1304" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1304/>Analyzing Polarization in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> : Method and Application to <a href=https://en.wikipedia.org/wiki/Twitter>Tweets</a> on 21 Mass Shootings</a></strong><br><a href=/people/d/dorottya-demszky/>Dorottya Demszky</a>
|
<a href=/people/n/nikhil-garg/>Nikhil Garg</a>
|
<a href=/people/r/rob-voigt/>Rob Voigt</a>
|
<a href=/people/j/james-zou/>James Zou</a>
|
<a href=/people/j/jesse-shapiro/>Jesse Shapiro</a>
|
<a href=/people/m/matthew-gentzkow/>Matthew Gentzkow</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1304><div class="card-body p-3 small">We provide an NLP framework to uncover four linguistic dimensions of <a href=https://en.wikipedia.org/wiki/Political_polarization>political polarization</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> : topic choice, <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a>, <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect</a> and <a href=https://en.wikipedia.org/wiki/Illocutionary_force>illocutionary force</a>. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events ; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to study 4.4 M tweets on 21 <a href=https://en.wikipedia.org/wiki/Mass_shooting>mass shootings</a>. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a> rather than topic choice. We identify <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing devices</a>, such as grounding and the contrasting use of the terms terrorist and crazy, that contribute to <a href=https://en.wikipedia.org/wiki/Political_polarization>polarization</a>. Results pertaining to topic choice, <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect</a> and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational methods</a> for studying them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1306 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355830579 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1306/>Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks</a></strong><br><a href=/people/n/ningyu-zhang/>Ningyu Zhang</a>
|
<a href=/people/s/shumin-deng/>Shumin Deng</a>
|
<a href=/people/z/zhanlin-sun/>Zhanlin Sun</a>
|
<a href=/people/g/guanying-wang/>Guanying Wang</a>
|
<a href=/people/x/xi-chen/>Xi Chen</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/h/huajun-chen/>Huajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1306><div class="card-body p-3 small">We propose a distance supervised relation extraction approach for long-tailed, imbalanced data which is prevalent in real-world settings. Here, the challenge is to learn accurate few-shot models for classes existing at the tail of the class distribution, for which little data is available. Inspired by the rich semantic correlations between classes at the long tail and those at the head, we take advantage of the knowledge from data-rich classes at the head of the distribution to boost the performance of the data-poor classes at the tail. First, we propose to leverage implicit relational knowledge among class labels from knowledge graph embeddings and learn explicit relational knowledge using graph convolution networks. Second, we integrate that <a href=https://en.wikipedia.org/wiki/Relational_model>relational knowledge</a> into relation extraction model by coarse-to-fine knowledge-aware attention mechanism. We demonstrate our results for a large-scale benchmark dataset which show that our approach significantly outperforms other <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a>, especially for long-tail relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1309 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355837778 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1309/>OpenCeres : When <a href=https://en.wikipedia.org/wiki/Open_information_extraction>Open Information Extraction</a> Meets the Semi-Structured Web<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>C</span>eres: <span class=acl-fixed-case>W</span>hen Open Information Extraction Meets the Semi-Structured Web</a></strong><br><a href=/people/c/colin-lockard/>Colin Lockard</a>
|
<a href=/people/p/prashant-shiralkar/>Prashant Shiralkar</a>
|
<a href=/people/x/xin-luna-dong/>Xin Luna Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1309><div class="card-body p-3 small">Open Information Extraction (OpenIE), the problem of harvesting triples from natural language text whose predicate relations are not aligned to any pre-defined ontology, has been a popular subject of research for the last decade. However, this research has largely ignored the vast quantity of facts available in semi-structured webpages. In this paper, we define the problem of OpenIE from <a href=https://en.wikipedia.org/wiki/Semi-structured_model>semi-structured websites</a> to extract such facts, and present an approach for solving it. We also introduce a labeled evaluation dataset to motivate research in this area. Given a semi-structured website and a set of seed facts for some relations existing on its pages, we employ a semi-supervised label propagation technique to automatically create <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> for the relations present on the site. We then use this <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> to learn a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> for relation extraction. Experimental results of this method on our new benchmark dataset obtained a <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> of over 70 %. A larger scale extraction experiment on 31 websites in the movie vertical resulted in the extraction of over 2 million triples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1313 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1313.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361725345 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1313" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1313/>Selective Attention for Context-aware Neural Machine Translation</a></strong><br><a href=/people/s/sameen-maruf/>Sameen Maruf</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1313><div class="card-body p-3 small">Despite the progress made in sentence-level NMT, current systems still fall short at achieving fluent, good quality translation for a full document. Recent works in context-aware NMT consider only a few previous sentences as context and may not scale to entire documents. To this end, we propose a novel and scalable top-down approach to hierarchical attention for context-aware NMT which uses sparse attention to selectively focus on relevant sentences in the document context and then attends to key words in those sentences. We also propose single-level attention approaches based on sentence or word-level information in the context. The document-level context representation, produced from these attention modules, is integrated into the encoder or decoder of the Transformer model depending on whether we use monolingual or bilingual context. Our experiments and evaluation on English-German datasets in different document MT settings show that our selective attention approach not only significantly outperforms context-agnostic baselines but also surpasses context-aware baselines in most cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1315 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1315.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356125366 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1315/>Accelerated Reinforcement Learning for Sentence Generation by Vocabulary Prediction</a></strong><br><a href=/people/k/kazuma-hashimoto/>Kazuma Hashimoto</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1315><div class="card-body p-3 small">A major obstacle in reinforcement learning-based sentence generation is the large action space whose size is equal to the vocabulary size of the target-side language. To improve the efficiency of <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, we present a novel approach for reducing the action space based on dynamic vocabulary prediction. Our method first predicts a fixed-size small vocabulary for each input to generate its target sentence. The input-specific vocabularies are then used at supervised and reinforcement learning steps, and also at test time. In our experiments on six <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and two image captioning datasets, our method achieves faster <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> (~2.7x faster) with less <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU memory</a> (~2.3x less) than the full-vocabulary counterpart. We also show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> more effectively receives rewards with fewer iterations of supervised pre-training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1316 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347415373 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1316" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1316/>Mitigating Uncertainty in Document Classification</a></strong><br><a href=/people/x/xuchao-zhang/>Xuchao Zhang</a>
|
<a href=/people/f/fanglan-chen/>Fanglan Chen</a>
|
<a href=/people/c/chang-tien-lu/>Chang-Tien Lu</a>
|
<a href=/people/n/naren-ramakrishnan/>Naren Ramakrishnan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1316><div class="card-body p-3 small">The uncertainty measurement of classifiers&#8217; predictions is especially important in applications such as <a href=https://en.wikipedia.org/wiki/Medical_diagnosis>medical diagnoses</a> that need to ensure limited human resources can focus on the most uncertain predictions returned by <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>. However, few existing uncertainty models attempt to improve overall prediction accuracy where <a href=https://en.wikipedia.org/wiki/Human_resources>human resources</a> are involved in the text classification task. In this paper, we propose a novel neural-network-based model that applies a new dropout-entropy method for uncertainty measurement. We also design a metric learning method on <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature representations</a>, which can boost the performance of dropout-based uncertainty methods with smaller prediction variance in accurate prediction trials. Extensive experiments on real-world data sets demonstrate that our method can achieve a considerable improvement in overall prediction accuracy compared to existing approaches. In particular, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improved the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> from 0.78 to 0.92 when 30 % of the most uncertain predictions were handed over to <a href=https://en.wikipedia.org/wiki/Expert_witness>human experts</a> in 20NewsGroup data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1322 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1322/>Customizing Grapheme-to-Phoneme System for Non-Trivial Transcription Problems in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla Language</a><span class=acl-fixed-case>B</span>angla Language</a></strong><br><a href=/people/s/sudipta-saha-shubha/>Sudipta Saha Shubha</a>
|
<a href=/people/n/nafis-sadeq/>Nafis Sadeq</a>
|
<a href=/people/s/shafayat-ahmed/>Shafayat Ahmed</a>
|
<a href=/people/m/md-nahidul-islam/>Md. Nahidul Islam</a>
|
<a href=/people/m/muhammad-abdullah-adnan/>Muhammad Abdullah Adnan</a>
|
<a href=/people/m/md-yasin-ali-khan/>Md. Yasin Ali Khan</a>
|
<a href=/people/m/mohammad-zuberul-islam/>Mohammad Zuberul Islam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1322><div class="card-body p-3 small">Grapheme to phoneme (G2P) conversion is an integral part in various text and speech processing systems, such as : <a href=https://en.wikipedia.org/wiki/Speech_synthesis>Text to Speech system</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Recognition system</a>, etc. The existing <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a> for G2P conversion in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla language</a> are mostly rule-based. However, data-driven approaches have proved their superiority over rule-based approaches for large-scale G2P conversion in other languages, such as : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, etc. As the performance of data-driven approaches for G2P conversion depend largely on pronunciation lexicon on which the system is trained, in this paper, we investigate on developing an improved training lexicon by identifying and categorizing the critical cases in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla language</a> and include those critical cases in training lexicon for developing a robust G2P conversion system in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla language</a>. Additionally, we have incorporated <a href=https://en.wikipedia.org/wiki/Nasal_vowel>nasal vowels</a> in our proposed phoneme list. Our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> outperforms other state-of-the-art approaches for G2P conversion in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1325 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1325/>Exploiting Noisy Data in Distant Supervision Relation Classification</a></strong><br><a href=/people/k/kaijia-yang/>Kaijia Yang</a>
|
<a href=/people/l/liang-he/>Liang He</a>
|
<a href=/people/x/xinyu-dai/>Xin-yu Dai</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1325><div class="card-body p-3 small">Distant supervision has obtained great progress on relation classification task. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> still suffers from noisy labeling problem. Different from previous works that underutilize <a href=https://en.wikipedia.org/wiki/Noisy_data>noisy data</a> which inherently characterize the property of classification, in this paper, we propose RCEND, a novel framework to enhance Relation Classification by Exploiting <a href=https://en.wikipedia.org/wiki/Noisy_data>Noisy Data</a>. First, an instance discriminator with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> is designed to split the noisy data into correctly labeled data and incorrectly labeled data. Second, we learn a robust relation classifier in semi-supervised learning way, whereby the correctly and incorrectly labeled data are treated as labeled and unlabeled data respectively. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms the state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1327 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1327/>Learning Relational Representations by Analogy using Hierarchical Siamese Networks<span class=acl-fixed-case>S</span>iamese Networks</a></strong><br><a href=/people/g/gaetano-rossiello/>Gaetano Rossiello</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a>
|
<a href=/people/r/robert-farrell/>Robert Farrell</a>
|
<a href=/people/n/nicolas-r-fauceglia/>Nicolas Fauceglia</a>
|
<a href=/people/m/michael-glass/>Michael Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1327><div class="card-body p-3 small">We address <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> as an analogy problem by proposing a novel approach to learn representations of relations expressed by their textual mentions. In our assumption, if two pairs of entities belong to the same relation, then those two pairs are analogous. Following this idea, we collect a large set of analogous pairs by matching triples in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> with web-scale corpora through distant supervision. We leverage this dataset to train a hierarchical siamese network in order to learn entity-entity embeddings which encode relational information through the different linguistic paraphrasing expressing the same relation. We evaluate our model in a one-shot learning task by showing a promising generalization capability in order to classify unseen relation types, which makes this approach suitable to perform automatic knowledge base population with minimal supervision. Moreover, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be used to generate pre-trained embeddings which provide a valuable signal when integrated into an existing neural-based model by outperforming the state-of-the-art methods on a downstream relation extraction task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1328 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1328/>An Effective Label Noise Model for DNN Text Classification<span class=acl-fixed-case>DNN</span> Text Classification</a></strong><br><a href=/people/i/ishan-jindal/>Ishan Jindal</a>
|
<a href=/people/d/daniel-pressel/>Daniel Pressel</a>
|
<a href=/people/b/brian-lester/>Brian Lester</a>
|
<a href=/people/m/matthew-nokleby/>Matthew Nokleby</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1328><div class="card-body p-3 small">Because large, human-annotated datasets suffer from labeling errors, it is crucial to be able to train <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> in the presence of label noise. While training <a href=https://en.wikipedia.org/wiki/Image_classification>image classification models</a> with label noise have received much attention, training <a href=https://en.wikipedia.org/wiki/Text_classification>text classification models</a> have not. In this paper, we propose an approach to training <a href=https://en.wikipedia.org/wiki/Deep_learning>deep networks</a> that is robust to label noise. This approach introduces a non-linear processing layer (noise model) that models the statistics of the label noise into a convolutional neural network (CNN) architecture. The noise model and the CNN weights are learned jointly from noisy training data, which prevents the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> from overfitting to erroneous labels. Through extensive experiments on several text classification datasets, we show that this approach enables the CNN to learn better sentence representations and is robust even to extreme label noise. We find that proper initialization and <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> of this noise model is critical. Further, by contrast to results focusing on large batch sizes for mitigating label noise for image classification, we find that altering the batch size does not have much effect on <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1330 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1330/>Using Large Corpus N-gram Statistics to Improve Recurrent Neural Language Models</a></strong><br><a href=/people/y/yiben-yang/>Yiben Yang</a>
|
<a href=/people/j/ji-ping-wang/>Ji-Ping Wang</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1330><div class="card-body p-3 small">Recurrent neural network language models (RNNLM) form a valuable foundation for many NLP systems, but training the models can be computationally expensive, and may take days to train on a large corpus. We explore a technique that uses large corpus n-gram statistics as a regularizer for training a neural network LM on a smaller corpus. In experiments with the Billion-Word and Wikitext corpora, we show that the technique is effective, and more time-efficient than simply training on a larger sequential corpus. We also introduce new <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> for selecting the most informative n-grams, and show that these boost efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1332 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1332" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1332/>Relation Discovery with Out-of-Relation Knowledge Base as Supervision</a></strong><br><a href=/people/y/yan-liang/>Yan Liang</a>
|
<a href=/people/x/xin-liu/>Xin Liu</a>
|
<a href=/people/j/jianwen-zhang/>Jianwen Zhang</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1332><div class="card-body p-3 small">Unsupervised relation discovery aims to discover new relations from a given <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> without <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a>. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> does not consider existing human annotated knowledge bases even when they are relevant to the relations to be discovered. In this paper, we study the problem of how to use out-of-relation knowledge bases to supervise the discovery of unseen relations, where out-of-relation means that relations to discover from the <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> and those in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> are not overlapped. We construct a set of <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> between entity pairs based on the knowledge base embedding and then incorporate <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> into the relation discovery by a variational auto-encoder based algorithm. Experiments show that our new approach can improve the state-of-the-art relation discovery performance by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1336 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1336/>Evaluating and Enhancing the Robustness of Dialogue Systems : A Case Study on a Negotiation Agent</a></strong><br><a href=/people/m/minhao-cheng/>Minhao Cheng</a>
|
<a href=/people/w/wei-wei/>Wei Wei</a>
|
<a href=/people/c/cho-jui-hsieh/>Cho-Jui Hsieh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1336><div class="card-body p-3 small">Recent research has demonstrated that goal-oriented dialogue agents trained on large datasets can achieve striking performance when interacting with human users. In real world applications, however, it is important to ensure that the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> performs smoothly interacting with not only regular users but also those malicious ones who would attack the <a href=https://en.wikipedia.org/wiki/System>system</a> through interactions in order to achieve goals for their own advantage. In this paper, we develop <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> to evaluate the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of a dialogue agent by carefully designed attacks using <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial agents</a>. Those <a href=https://en.wikipedia.org/wiki/Attack_(computing)>attacks</a> are performed in both black-box and white-box settings. Furthermore, we demonstrate that adversarial training using our attacks can significantly improve the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of a goal-oriented dialogue system. On a case-study of the negotiation agent developed by (Lewis et al., 2017), our attacks reduced the average advantage of rewards between the attacker and the trained RL-based agent from 2.68 to -5.76 on a scale from -10 to 10 for randomized goals. Moreover, we show that with the adversarial training, we are able to improve the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of negotiation agents by 1.5 points on average against all our attacks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1340 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1340" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1340/>Semantic Role Labeling with Associated Memory Network</a></strong><br><a href=/people/c/chaoyu-guan/>Chaoyu Guan</a>
|
<a href=/people/y/yuhao-cheng/>Yuhao Cheng</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1340><div class="card-body p-3 small">Semantic role labeling (SRL) is a task to recognize all the predicate-argument pairs of a sentence, which has been in a performance improvement bottleneck after a series of latest works were presented. This paper proposes a novel syntax-agnostic SRL model enhanced by the proposed associated memory network (AMN), which makes use of inter-sentence attention of label-known associated sentences as a kind of <a href=https://en.wikipedia.org/wiki/Memory>memory</a> to further enhance dependency-based SRL. In detail, we use sentences and their labels from train dataset as an <a href=https://en.wikipedia.org/wiki/Association_(psychology)>associated memory cue</a> to help label the target sentence. Furthermore, we compare several associated sentences selecting strategies and label merging methods in AMN to find and utilize the label of associated sentences while attending them. By leveraging the attentive memory from known training data, Our full model reaches state-of-the-art on CoNLL-2009 benchmark datasets for syntax-agnostic setting, showing a new effective research line of SRL enhancement other than exploiting external resources such as well pre-trained language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1341 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1341" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1341/>Better, Faster, Stronger Sequence Tagging Constituent Parsers</a></strong><br><a href=/people/d/david-vilares/>David Vilares</a>
|
<a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1341><div class="card-body p-3 small">Sequence tagging models for constituent parsing are faster, but less accurate than other types of <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. In this work, we address the following weaknesses of such constituent parsers : (a) high error rates around closing brackets of long constituents, (b) large label sets, leading to sparsity, and (c) error propagation arising from greedy decoding. To effectively close brackets, we train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that learns to switch between tagging schemes. To reduce sparsity, we decompose the label set and use <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> to jointly learn to predict sublabels. Finally, we mitigate issues from greedy decoding through auxiliary losses and sentence-level fine-tuning with policy gradient. Combining these techniques, we clearly surpass the performance of sequence tagging constituent parsers on the English and Chinese Penn Treebanks, and reduce their parsing time even further. On the SPMRL datasets, we observe even greater improvements across the board, including a new state of the art on <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>, <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1347 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1347" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1347/>Learning Hierarchical Discourse-level Structure for Fake News Detection</a></strong><br><a href=/people/h/hamid-karimi/>Hamid Karimi</a>
|
<a href=/people/j/jiliang-tang/>Jiliang Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1347><div class="card-body p-3 small">On the one hand, nowadays, <a href=https://en.wikipedia.org/wiki/Fake_news>fake news articles</a> are easily propagated through various online media platforms and have become a grand threat to the trustworthiness of information. On the other hand, our understanding of the language of fake news is still minimal. Incorporating hierarchical discourse-level structure of fake and real news articles is one crucial step toward a better understanding of how these <a href=https://en.wikipedia.org/wiki/Article_(publishing)>articles</a> are structured. Nevertheless, this has rarely been investigated in the fake news detection domain and faces tremendous challenges. First, existing methods for capturing discourse-level structure rely on annotated corpora which are not available for fake news datasets. Second, how to extract out useful information from such discovered <a href=https://en.wikipedia.org/wiki/Biomolecular_structure>structures</a> is another challenge. To address these challenges, we propose Hierarchical Discourse-level Structure for Fake news detection. HDSF learns and constructs a discourse-level structure for fake / real news articles in an automated and data-driven manner. Moreover, we identify insightful structure-related properties, which can explain the discovered structures and boost our understating of fake news. Conducted experiments show the effectiveness of the proposed approach. Further structural analysis suggests that real and fake news present substantial differences in the hierarchical discourse-level structures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1357 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359703968 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1357" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1357/>Attention is not Explanation<span class=acl-fixed-case>A</span>ttention is not <span class=acl-fixed-case>E</span>xplanation</a></strong><br><a href=/people/s/sarthak-jain/>Sarthak Jain</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1357><div class="card-body p-3 small">Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency : models equipped with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a> that aim to assess the degree to which attention weights provide meaningful explanations for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard <a href=https://en.wikipedia.org/wiki/Attentional_control>attention modules</a> do not provide meaningful explanations and should not be treated as though they do.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1358.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1358 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1358 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359702665 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1358" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1358/>Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning</a></strong><br><a href=/people/p/prithviraj-ammanabrolu/>Prithviraj Ammanabrolu</a>
|
<a href=/people/m/mark-riedl/>Mark Riedl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1358><div class="card-body p-3 small">Text-based adventure games provide a platform on which to explore <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> in the context of a combinatorial action space, such as <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. We present a deep reinforcement learning architecture that represents the game state as a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> which is learned during exploration. This <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph</a> is used to prune the action space, enabling more efficient <a href=https://en.wikipedia.org/wiki/Exploration>exploration</a>. The question of which action to take can be reduced to a question-answering task, a form of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> that pre-trains certain parts of our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>. In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives. We have also open-sourced our code at https://github.com/rajammanabrolu/KG-DQN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1360 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1360.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359699975 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1360/>Context Dependent Semantic Parsing over Temporally Structured Data</a></strong><br><a href=/people/c/charles-chen-jr/>Charles Chen</a>
|
<a href=/people/r/razvan-bunescu/>Razvan Bunescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1360><div class="card-body p-3 small">We describe a new semantic parsing setting that allows users to query the system using both natural language questions and actions within a <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>graphical user interface</a>. Multiple <a href=https://en.wikipedia.org/wiki/Time_series>time series</a> belonging to an entity of interest are stored in a database and the user interacts with the system to obtain a better understanding of the entity&#8217;s state and behavior, entailing sequences of actions and questions whose answers may depend on previous factual or navigational interactions. We design an LSTM-based encoder-decoder architecture that models context dependency through copying mechanisms and multiple levels of attention over inputs and previous outputs. When trained to predict tokens using <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>, the proposed <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> substantially outperforms standard sequence generation baselines. Training the <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> using policy gradient leads to further improvements in performance, reaching a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>sequence-level accuracy</a> of 88.7 % on artificial data and 74.8 % on real data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1362.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1362 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1362 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356133444 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1362" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1362/>pair2vec : Compositional Word-Pair Embeddings for Cross-Sentence Inference</a></strong><br><a href=/people/m/mandar-joshi/>Mandar Joshi</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/d/daniel-s-weld/>Daniel Weld</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1362><div class="card-body p-3 small">Reasoning about implied relationships (e.g. paraphrastic, <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a>, encyclopedic) between pairs of words is crucial for many cross-sentence inference problems. This paper proposes new methods for learning and using embeddings of word pairs that implicitly represent background knowledge about such relationships. Our pairwise embeddings are computed as a compositional function of each word&#8217;s representation, which is learned by maximizing the pointwise mutual information (PMI) with the contexts in which the the two words co-occur. We add these <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> to the cross-sentence attention layer of existing <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference models</a> (e.g. BiDAF for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>, ESIM for NLI), instead of extending or replacing existing <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Experiments show a gain of 2.7 % on the recently released SQuAD 2.0 and 1.3 % on MultiNLI. Our representations also aid in better generalization with gains of around 6-7 % on adversarial SQuAD datasets, and 8.8 % on the adversarial entailment test set by Glockner et al.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1364 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356153695 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1364/>Let’s Make Your Request More Persuasive : Modeling Persuasive Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms</a></strong><br><a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/j/jiaao-chen/>Jiaao Chen</a>
|
<a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1364><div class="card-body p-3 small">Modeling what makes a request persuasive-eliciting the desired response from a reader-is critical to the study of <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>, <a href=https://en.wikipedia.org/wiki/Behavioral_economics>behavioral economics</a>, and <a href=https://en.wikipedia.org/wiki/Advertising>advertising</a>. Yet current <a href=https://en.wikipedia.org/wiki/Scientific_modelling>models</a> ca n&#8217;t quantify the persuasiveness of requests or extract successful persuasive strategies. Building on theories of <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion</a>, we propose a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to quantify persuasiveness and identify the persuasive strategies in advocacy requests. Our semi-supervised hierarchical neural network model is supervised by the number of people persuaded to take actions and partially supervised at the sentence level with human-labeled rhetorical strategies. Our method outperforms several baselines, uncovers persuasive strategies-offering increased interpretability of persuasive speech-and has applications for other situations with document-level supervision but only partial sentence supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1365 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356167288 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1365/>Recursive Routing Networks : Learning to Compose Modules for Language Understanding</a></strong><br><a href=/people/i/ignacio-cases/>Ignacio Cases</a>
|
<a href=/people/c/clemens-rosenbaum/>Clemens Rosenbaum</a>
|
<a href=/people/m/matthew-riemer/>Matthew Riemer</a>
|
<a href=/people/a/atticus-geiger/>Atticus Geiger</a>
|
<a href=/people/t/tim-klinger/>Tim Klinger</a>
|
<a href=/people/a/alex-tamkin/>Alex Tamkin</a>
|
<a href=/people/o/olivia-li/>Olivia Li</a>
|
<a href=/people/s/sandhini-agarwal/>Sandhini Agarwal</a>
|
<a href=/people/j/joshua-d-greene/>Joshua D. Greene</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/l/lauri-karttunen/>Lauri Karttunen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1365><div class="card-body p-3 small">We introduce Recursive Routing Networks (RRNs), which are modular, adaptable models that learn effectively in diverse environments. RRNs consist of a set of <a href=https://en.wikipedia.org/wiki/Subroutine>functions</a>, typically organized into a <a href=https://en.wikipedia.org/wiki/Grid_(spatial_index)>grid</a>, and a meta-learner decision-making component called the <a href=https://en.wikipedia.org/wiki/Router_(computing)>router</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> jointly optimizes the parameters of the <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a> and the meta-learner&#8217;s policy for routing inputs through those <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a>. RRNs can be incorporated into existing architectures in a number of ways ; we explore adding them to word representation layers, recurrent network hidden layers, and classifier layers. Our evaluation task is natural language inference (NLI). Using the MultiNLI corpus, we show that an RRN&#8217;s routing decisions reflect the high-level genre structure of that <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. To show that RRNs can learn to specialize to more fine-grained semantic distinctions, we introduce a new corpus of NLI examples involving implicative predicates, and show that the model components become fine-tuned to the inferential signatures that are characteristic of these <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1366 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1366.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356184145 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1366" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1366/>Structural Neural Encoders for AMR-to-text Generation<span class=acl-fixed-case>AMR</span>-to-text Generation</a></strong><br><a href=/people/m/marco-damonte/>Marco Damonte</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1366><div class="card-body p-3 small">AMR-to-text generation is a problem recently introduced to the NLP community, in which the goal is to generate sentences from Abstract Meaning Representation (AMR) graphs. Sequence-to-sequence models can be used to this end by converting the AMR graphs to strings. Approaching the problem while working directly with <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> requires the use of graph-to-sequence models that encode the AMR graph into a <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representation</a>. Such <a href=https://en.wikipedia.org/wiki/Code>encoding</a> has been shown to be beneficial in the past, and unlike sequential encoding, it allows us to explicitly capture reentrant structures in the AMR graphs. We investigate the extent to which <a href=https://en.wikipedia.org/wiki/Reentrancy_(computing)>reentrancies</a> (nodes with multiple parents) have an impact on AMR-to-text generation by comparing graph encoders to tree encoders, where <a href=https://en.wikipedia.org/wiki/Reentrancy_(computing)>reentrancies</a> are not preserved. We show that improvements in the treatment of reentrancies and <a href=https://en.wikipedia.org/wiki/Long-range_dependence>long-range dependencies</a> contribute to higher overall scores for graph encoders. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 24.40 BLEU on LDC2015E86, outperforming the state of the art by 1.1 points and 24.54 BLEU on LDC2017T10, outperforming the <a href=https://en.wikipedia.org/wiki/State_(computer_science)>state</a> of the art by 1.24 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1378 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1378" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1378/>What do Entity-Centric Models Learn? Insights from Entity Linking in Multi-Party Dialogue</a></strong><br><a href=/people/l/laura-aina/>Laura Aina</a>
|
<a href=/people/c/carina-silberer/>Carina Silberer</a>
|
<a href=/people/i/ionut-sorodoc/>Ionut-Teodor Sorodoc</a>
|
<a href=/people/m/matthijs-westera/>Matthijs Westera</a>
|
<a href=/people/g/gemma-boleda/>Gemma Boleda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1378><div class="card-body p-3 small">Humans use <a href=https://en.wikipedia.org/wiki/Language>language</a> to refer to entities in the external world. Motivated by this, in recent years several <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that incorporate a bias towards learning entity representations have been proposed. Such entity-centric models have shown empirical success, but we still know little about why. In this paper we analyze the behavior of two recently proposed entity-centric models in a referential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4). We show that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> outperform the state of the art on this task, and that they do better on lower frequency entities than a counterpart model that is not entity-centric, with the same model size. We argue that making models entity-centric naturally fosters good architectural decisions. However, we also show that these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> do not really build entity representations and that they make poor use of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>linguistic context</a>. These negative results underscore the need for model analysis, to test whether the motivations for particular <a href=https://en.wikipedia.org/wiki/Software_architecture>architectures</a> are borne out in how <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> behave when deployed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1380.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1380 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1380 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1380/>Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog</a></strong><br><a href=/people/s/sebastian-schuster/>Sebastian Schuster</a>
|
<a href=/people/s/sonal-gupta/>Sonal Gupta</a>
|
<a href=/people/r/rushin-shah/>Rushin Shah</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1380><div class="card-body p-3 small">One of the first steps in the utterance interpretation pipeline of many task-oriented conversational AI systems is to identify user intents and the corresponding slots. Since data collection for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> for this task is time-consuming, it is desirable to make use of existing <a href=https://en.wikipedia.org/wiki/Data>data</a> in a high-resource language to train models in low-resource languages. However, development of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> has largely been hindered by the lack of multilingual training data. In this paper, we present a new <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> of 57k annotated utterances in <a href=https://en.wikipedia.org/wiki/English_language>English</a> (43k), <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> (8.6k) and <a href=https://en.wikipedia.org/wiki/Thai_language>Thai</a> (5k) across the domains weather, alarm, and reminder. We use this data set to evaluate three different cross-lingual transfer methods : (1) translating the training data, (2) using cross-lingual pre-trained embeddings, and (3) a novel method of using a multilingual machine translation encoder as contextual word representations. We find that given several hundred training examples in the the target language, the latter two methods outperform translating the training data. Further, in very low-resource settings, multilingual contextual word representations give better results than using cross-lingual static embeddings. We also compare the cross-lingual methods to using monolingual resources in the form of contextual ELMo representations and find that given just small amounts of target language data, this method outperforms all cross-lingual methods, which highlights the need for more sophisticated cross-lingual methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1381 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1381" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1381/>Evaluating Coherence in <a href=https://en.wikipedia.org/wiki/Dialogue_system>Dialogue Systems</a> using Entailment</a></strong><br><a href=/people/n/nouha-dziri/>Nouha Dziri</a>
|
<a href=/people/e/ehsan-kamalloo/>Ehsan Kamalloo</a>
|
<a href=/people/k/kory-mathewson/>Kory Mathewson</a>
|
<a href=/people/o/osmar-r-zaiane/>Osmar Zaiane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1381><div class="card-body p-3 small">Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a> based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1382.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1382 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1382 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1382/>On Knowledge distillation from <a href=https://en.wikipedia.org/wiki/Complex_network>complex networks</a> for response prediction</a></strong><br><a href=/people/s/siddhartha-arora/>Siddhartha Arora</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a>
|
<a href=/people/h/harish-g-ramaswamy/>Harish G. Ramaswamy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1382><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> have lead to the development of very complex models which compute rich representations for query and documents by capturing all pairwise interactions between query and document words. This makes these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> expensive in space and time, and in practice one has to restrict the length of the documents that can be fed to these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have also been recently employed for the task of predicting dialog responses from available background documents (e.g., Holl-E dataset). However, here the documents are longer, thereby rendering these complex models infeasible except in select restricted settings. In order to overcome this, we use standard simple models which do not capture all pairwise interactions, but learn to emulate certain characteristics of a complex teacher network. Specifically, we first investigate the conicity of representations learned by a complex model and observe that it is significantly lower than that of simpler models. Based on this insight, we modify the simple architecture to mimic this <a href=https://en.wikipedia.org/wiki/Property_(philosophy)>characteristic</a>. We go further by using knowledge distillation approaches, where the simple model acts as a student and learns to match the output from the complex teacher network. We experiment with the Holl-E dialog data set and show that by mimicking characteristics and matching outputs from a teacher, even a simple network can give improved performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1384 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1384/>Unsupervised Extraction of Partial Translations for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1384><div class="card-body p-3 small">In neural machine translation (NMT), monolingual data are usually exploited through a so-called <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> : sentences in the target language are translated into the source language to synthesize new parallel data. While this method provides more training data to better model the target language, on the source side, it only exploits translations that the NMT system is already able to generate using a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on existing parallel data. In this work, we assume that new translation knowledge can be extracted from <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a>, without relying at all on existing parallel data. We propose a new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for extracting from monolingual data what we call partial translations : pairs of source and target sentences that contain sequences of tokens that are translations of each other. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is fully unsupervised and takes only source and target monolingual data as input. Our empirical evaluation points out that our partial translations can be used in combination with <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> to further improve NMT models. Furthermore, while partial translations are particularly useful for low-resource language pairs, they can also be successfully exploited in resource-rich scenarios to improve translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1385 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1385/>Low-Resource Syntactic Transfer with Unsupervised Source Reordering</a></strong><br><a href=/people/m/mohammad-sadegh-rasooli/>Mohammad Sadegh Rasooli</a>
|
<a href=/people/m/michael-collins/>Michael Collins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1385><div class="card-body p-3 small">We describe a cross-lingual transfer method for dependency parsing that takes into account the problem of word order differences between source and target languages. Our model only relies on the <a href=https://en.wikipedia.org/wiki/Bible>Bible</a>, a considerably smaller parallel data than the commonly used parallel data in transfer methods. We use the concatenation of projected trees from the <a href=https://en.wikipedia.org/wiki/Text_corpus>Bible corpus</a>, and the gold-standard treebanks in multiple source languages along with cross-lingual word representations. We demonstrate that reordering the source treebanks before training on them for a target language improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of languages outside the <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European language family</a>. Our experiments on 68 treebanks (38 languages) in the Universal Dependencies corpus achieve a high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for all languages. Among them, our experiments on 16 <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> of 12 <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>non-European languages</a> achieve an average UAS absolute improvement of 3.3 % over a <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1388.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1388 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1388 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1388/>Massively Multilingual Neural Machine Translation</a></strong><br><a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/m/melvin-johnson/>Melvin Johnson</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1388><div class="card-body p-3 small">Multilingual Neural Machine Translation enables training a single model that supports <a href=https://en.wikipedia.org/wiki/Translation>translation</a> from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1390 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1390/>Combining Discourse Markers and Cross-lingual Embeddings for SynonymAntonym Classification</a></strong><br><a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/s/shyam-upadhyay/>Shyam Upadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1390><div class="card-body p-3 small">It is well-known that distributional semantic approaches have difficulty in distinguishing between <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> and <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> (Grefenstette, 1992 ; Pad and Lapata, 2003). Recent work has shown that supervision available in <a href=https://en.wikipedia.org/wiki/English_language>English</a> for this <a href=https://en.wikipedia.org/wiki/Task_force>task</a> (e.g., lexical resources) can be transferred to other languages via cross-lingual word embeddings. However, this kind of transfer misses monolingual distributional information available in a target language, such as <a href=https://en.wikipedia.org/wiki/Contrast_(linguistics)>contrast relations</a> that are indicative of <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonymy</a> (e.g. hot... while... cold). In this work, we improve the transfer by exploiting <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual information</a>, expressed in the form of co-occurrences with <a href=https://en.wikipedia.org/wiki/Discourse_marker>discourse markers</a> that convey contrast. Our approach makes use of less than a dozen <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>markers</a>, which can easily be obtained for many languages. Compared to a baseline using only cross-lingual embeddings, we show absolute improvements of 410 % F1-score in <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1391.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1391 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1391 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1391" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1391/>Context-Aware Cross-Lingual Mapping</a></strong><br><a href=/people/h/hanan-aldarmaki/>Hanan Aldarmaki</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1391><div class="card-body p-3 small">Cross-lingual word vectors are typically obtained by fitting an <a href=https://en.wikipedia.org/wiki/Orthogonal_matrix>orthogonal matrix</a> that maps the entries of a <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a> from a source to a target vector space. Word vectors, however, are most commonly used for sentence or document-level representations that are calculated as the weighted average of word embeddings. In this paper, we propose an alternative to word-level mapping that better reflects sentence-level cross-lingual similarity. We incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> in the <a href=https://en.wikipedia.org/wiki/Transformation_matrix>transformation matrix</a> by directly mapping the averaged embeddings of aligned sentences in a <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>. We also implement cross-lingual mapping of deep contextualized word embeddings using parallel sentences with word alignments. In our experiments, both approaches resulted in cross-lingual sentence embeddings that outperformed context-independent word mapping in sentence translation retrieval. Furthermore, the sentence-level transformation could be used for word-level mapping without loss in word translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1394.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1394 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1394 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1394" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1394/>Recommendations for Datasets for Source Code Summarization</a></strong><br><a href=/people/a/alexander-leclair/>Alexander LeClair</a>
|
<a href=/people/c/collin-mcmillan/>Collin McMillan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1394><div class="card-body p-3 small">Source Code Summarization is the task of writing short, natural language descriptions of source code. The main use for these descriptions is in <a href=https://en.wikipedia.org/wiki/Software_documentation>software documentation</a> e.g. the one-sentence Java method descriptions in JavaDocs. Code summarization is rapidly becoming a popular research problem, but progress is restrained due to a lack of suitable datasets. In addition, a lack of community standards for creating datasets leads to confusing and unreproducible research results we observe swings in performance of more than 33 % due only to changes in dataset design. In this paper, we make recommendations for these standards from experimental results. We release a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> based on prior work of over 2.1 m pairs of Java methods and one sentence method descriptions from over 28k Java projects. We describe the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and point out key differences from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language data</a>, to guide and support future researchers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1396.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1396 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1396 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1396/>Understanding the Behaviour of Neural Abstractive Summarizers using Contrastive Examples<span class=acl-fixed-case>U</span>nderstanding the <span class=acl-fixed-case>B</span>ehaviour of <span class=acl-fixed-case>N</span>eural <span class=acl-fixed-case>A</span>bstractive <span class=acl-fixed-case>S</span>ummarizers using <span class=acl-fixed-case>C</span>ontrastive <span class=acl-fixed-case>E</span>xamples</a></strong><br><a href=/people/k/krtin-kumar/>Krtin Kumar</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1396><div class="card-body p-3 small">Neural abstractive summarizers generate summary texts using a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> conditioned on the input source text, and have recently achieved high ROUGE scores on benchmark summarization datasets. We investigate how they achieve this performance with respect to human-written gold-standard abstracts, and whether the systems are able to understand deeper syntactic and semantic structures. We generate a set of contrastive summaries which are perturbed, deficient versions of human-written summaries, and test whether existing neural summarizers score them more highly than the human-written summaries. We analyze their performance on different datasets and find that these <a href=https://en.wikipedia.org/wiki/System>systems</a> fail to understand the source text, in a majority of the cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1401 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1401" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1401/>Positional Encoding to Control Output Sequence Length</a></strong><br><a href=/people/s/sho-takase/>Sho Takase</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1401><div class="card-body p-3 small">Neural encoder-decoder models have been successful in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation tasks</a>. However, real applications of abstractive summarization must consider an additional constraint that a generated summary should not exceed a desired length. In this paper, we propose a simple but effective extension of a sinusoidal positional encoding (Vaswani et al., 2017) so that a neural encoder-decoder model preserves the length constraint. Unlike previous studies that learn length embeddings, the proposed method can generate a text of any length even if the target length is unseen in training data. The experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is able not only to control generation length but also improve ROUGE scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1404 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361773751 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1404/>Saliency Learning : Teaching the Model Where to Pay Attention<span class=acl-fixed-case>S</span>aliency <span class=acl-fixed-case>L</span>earning: <span class=acl-fixed-case>T</span>eaching the <span class=acl-fixed-case>M</span>odel <span class=acl-fixed-case>W</span>here to <span class=acl-fixed-case>P</span>ay <span class=acl-fixed-case>A</span>ttention</a></strong><br><a href=/people/r/reza-ghaeini/>Reza Ghaeini</a>
|
<a href=/people/x/xiaoli-fern/>Xiaoli Fern</a>
|
<a href=/people/h/hamed-shahbazi/>Hamed Shahbazi</a>
|
<a href=/people/p/prasad-tadepalli/>Prasad Tadepalli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1404><div class="card-body p-3 small">Deep learning has emerged as a compelling solution to many NLP tasks with remarkable performances. However, due to their opacity, such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are hard to interpret and trust. Recent work on explaining deep models has introduced approaches to provide insights toward the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s behaviour and predictions, which are helpful for assessing the reliability of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s predictions. However, such methods do not improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model&#8217;s reliability</a>. In this paper, we aim to teach the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to make the right prediction for the right reason by providing explanation training and ensuring the alignment of the model&#8217;s explanation with the ground truth explanation. Our experimental results on multiple tasks and datasets demonstrate the effectiveness of the proposed method, which produces more reliable predictions while delivering better results compared to traditionally trained models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1407 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359716954 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1407/>Convolutional Self-Attention Networks</a></strong><br><a href=/people/b/baosong-yang/>Baosong Yang</a>
|
<a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/d/derek-f-wong/>Derek F. Wong</a>
|
<a href=/people/l/lidia-s-chao/>Lidia S. Chao</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1407><div class="card-body p-3 small">Self-attention networks (SANs) have drawn increasing interest due to their high <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallelization in computation</a> and flexibility in modeling <a href=https://en.wikipedia.org/wiki/Coupling_(computer_programming)>dependencies</a>. SANs can be further enhanced with multi-head attention by allowing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is parameter free in terms of introducing no more parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1415 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359721173 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1415/>On the Idiosyncrasies of the Mandarin Chinese Classifier System<span class=acl-fixed-case>M</span>andarin <span class=acl-fixed-case>C</span>hinese Classifier System</a></strong><br><a href=/people/s/shijia-liu/>Shijia Liu</a>
|
<a href=/people/h/hongyuan-mei/>Hongyuan Mei</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1415><div class="card-body p-3 small">While idiosyncrasies of the Chinese classifier system have been a richly studied topic among linguists (Adams and Conklin, 1973 ; Erbaugh, 1986 ; Lakoff, 1986), not much work has been done to quantify them with statistical methods. In this paper, we introduce an information-theoretic approach to measuring idiosyncrasy ; we examine how much the uncertainty in Mandarin Chinese classifiers can be reduced by knowing semantic information about the nouns that the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> modify. Using the empirical distribution of <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> from the parsed Chinese Gigaword corpus (Graff et al., 2005), we compute the <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> (in bits) between the distribution over <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> and distributions over other linguistic quantities. We investigate whether semantic classes of nouns and adjectives differ in how much they reduce uncertainty in classifier choice, and find that it is not fully idiosyncratic ; while there are no obvious trends for the majority of semantic classes, shape nouns reduce uncertainty in classifier choice the most.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1416 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361815756 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1416/>Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech Tagging</a></strong><br><a href=/people/s/sara-meftah/>Sara Meftah</a>
|
<a href=/people/y/youssef-tamaazousti/>Youssef Tamaazousti</a>
|
<a href=/people/n/nasredine-semmar/>Nasredine Semmar</a>
|
<a href=/people/h/hassane-essafi/>Hassane Essafi</a>
|
<a href=/people/f/fatiha-sadat/>Fatiha Sadat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1416><div class="card-body p-3 small">Fine-tuning neural networks is widely used to transfer valuable knowledge from high-resource to low-resource domains. In a standard fine-tuning scheme, source and target problems are trained using the same <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>. Although capable of adapting to new domains, pre-trained units struggle with learning uncommon target-specific patterns. In this paper, we propose to augment the target-network with normalised, weighted and randomly initialised units that beget a better <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> while maintaining the valuable source knowledge. Our experiments on POS tagging of social media texts (Tweets domain) demonstrate that our method achieves state-of-the-art performances on 3 commonly used datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1418 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1418.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361822826 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1418/>Data Augmentation for Context-Sensitive Neural Lemmatization Using Inflection Tables and Raw Text</a></strong><br><a href=/people/t/toms-bergmanis/>Toms Bergmanis</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1418><div class="card-body p-3 small">Lemmatization aims to reduce the sparse data problem by relating the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a> of a word to its <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary form</a>. Using <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> can help, both for unseen and ambiguous words. Yet most context-sensitive approaches require full lemma-annotated sentences for training, which may be scarce or unavailable in low-resource languages. In addition (as shown here), in a low-resource setting, a <a href=https://en.wikipedia.org/wiki/Lemmatizer>lemmatizer</a> can learn more from n labeled examples of distinct words (types) than from n (contiguous) labeled tokens, since the latter contain far fewer distinct types. To combine the efficiency of type-based learning with the benefits of context, we propose a way to train a context-sensitive lemmatizer with little or no labeled corpus data, using inflection tables from the UniMorph project and raw text examples from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> that provide sentence contexts for the unambiguous UniMorph examples. Despite these being unambiguous examples, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> successfully generalizes from them, leading to improved results (both overall, and especially on unseen words) in comparison to a baseline that does not use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1419 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361827125 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1419" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1419/>A Structural Probe for Finding Syntax in Word Representations<span class=acl-fixed-case>A</span> Structural Probe for Finding Syntax in Word Representations</a></strong><br><a href=/people/j/john-hewitt/>John Hewitt</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1419><div class="card-body p-3 small">Recent work has improved our ability to detect linguistic knowledge in <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>word representations</a>. However, current methods for detecting syntactic knowledge do not test whether <a href=https://en.wikipedia.org/wiki/Syntax_tree>syntax trees</a> are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a> of a neural network&#8217;s word representation space. The probe identifies a <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a> under which squared L2 distance encodes the distance between words in the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a>, and one in which squared L2 norm encodes depth in the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a>. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models&#8217; vector geometry.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1422 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Short Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1422.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation>
<i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/365146894 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1422/>Probing the Need for Visual Context in Multimodal Machine Translation</a></strong><br><a href=/people/o/ozan-caglayan/>Ozan Caglayan</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/l/loic-barrault/>Loïc Barrault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1422><div class="card-body p-3 small">Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30 K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are capable of leveraging the <a href=https://en.wikipedia.org/wiki/Visual_system>visual input</a> to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1424 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Thematic Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/365132300 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1424/>What’s in a Name? Reducing Bias in Bios without Access to Protected Attributes<span class=acl-fixed-case>R</span>educing Bias in Bios without Access to Protected Attributes</a></strong><br><a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/m/maria-de-arteaga/>Maria De-Arteaga</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/j/jennifer-chayes/>Jennifer Chayes</a>
|
<a href=/people/c/christian-borgs/>Christian Borgs</a>
|
<a href=/people/a/alexandra-chouldechova/>Alexandra Chouldechova</a>
|
<a href=/people/s/sahin-geyik/>Sahin Geyik</a>
|
<a href=/people/k/krishnaram-kenthapadi/>Krishnaram Kenthapadi</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/a/adam-kalai/>Adam Kalai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1424><div class="card-body p-3 small">There is a growing body of work that proposes methods for mitigating bias in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning systems</a>. These methods typically rely on access to protected attributes such as <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>race</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, or age. However, this raises two significant challenges : (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classification, we propose a method for discouraging correlation between the predicted probability of an individual&#8217;s true occupation and a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> of their name. This method leverages the societal biases that are encoded in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, eliminating the need for access to <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>protected attributes</a>. Crucially, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> only requires access to individuals&#8217; names at training time and not at deployment time. We evaluate two variations of our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> using a large-scale dataset of online biographies. We find that both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier&#8217;s overall true positive rate.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>