<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/E17-2.pdf>Proceedings of the 15th Conference of the <span class=acl-fixed-case>E</span>uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></h2><p class=lead><a href=/people/m/mirella-lapata/>Mirella Lapata</a>,
<a href=/people/p/phil-blunsom/>Phil Blunsom</a>,
<a href=/people/a/alexander-koller/>Alexander Koller</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>E17-2</dd><dt>Month:</dt><dd>April</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Valencia, Spain</dd><dt>Venue:</dt><dd><a href=/venues/eacl/>EACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/E17-2>https://aclanthology.org/E17-2</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/E17-2.pdf>https://aclanthology.org/E17-2.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/E17-2.pdf title="Open PDF of 'Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+15th+Conference+of+the+European+Chapter+of+the+Association+for+Computational+Linguistics%3A+Volume+2%2C+Short+Papers" title="Search for 'Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2000/>Proceedings of the 15th Conference of the <span class=acl-fixed-case>E</span>uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></strong><br><a href=/people/m/mirella-lapata/>Mirella Lapata</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2001/>Multilingual Back-and-Forth Conversion between Content and Function Head for Easy Dependency Parsing</a></strong><br><a href=/people/r/ryosuke-kohita/>Ryosuke Kohita</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2001><div class="card-body p-3 small">Universal Dependencies (UD) is becoming a standard annotation scheme cross-linguistically, but it is argued that this scheme centering on content words is harder to parse than the conventional one centering on function words. To improve the <a href=https://en.wikipedia.org/wiki/Parsing>parsability</a> of UD, we propose a back-and-forth conversion algorithm, in which we preprocess the training treebank to increase <a href=https://en.wikipedia.org/wiki/Parsing>parsability</a>, and reconvert the parser outputs to follow the UD scheme as a postprocess. We show that this technique consistently improves <a href=https://en.wikipedia.org/wiki/Lisp_(programming_language)>LAS</a> across languages even with a state-of-the-art <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>, in particular on core dependency arcs such as nominal modifier. We also provide an in-depth analysis to understand why our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> increases <a href=https://en.wikipedia.org/wiki/Parsing>parsability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2005/>Using Twitter Language to Predict the Real Estate Market<span class=acl-fixed-case>T</span>witter Language to Predict the Real Estate Market</a></strong><br><a href=/people/m/mohammadzaman-zamani/>Mohammadzaman Zamani</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2005><div class="card-body p-3 small">We explore whether <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> can provide a window into community real estate -foreclosure rates and price changes- beyond that of traditional economic and demographic variables. We find language use in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> not only predicts real estate outcomes as well as traditional variables across counties, but that including <a href=https://en.wikipedia.org/wiki/Twitter>Twitter language</a> in traditional models leads to a significant improvement (e.g. from Pearson r = : 50 to r = : 59 for price changes). We overcome the challenge of the relative sparsity and noise in Twitter language variables by showing that training on the <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>residual error</a> of the traditional <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> leads to more accurate overall assessments. Finally, we discover that it is <a href=https://en.wikipedia.org/wiki/List_of_Latin-script_digraphs>Twitter language</a> related to business (e.g. &#8216;company&#8217;, &#8216;marketing&#8217;) and <a href=https://en.wikipedia.org/wiki/Technology>technology</a> (e.g. &#8216;technology&#8217;, &#8216;internet&#8217;), among others, that yield predictive power over <a href=https://en.wikipedia.org/wiki/Economics>economics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2006/>Lexical Simplification with Neural Ranking</a></strong><br><a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2006><div class="card-body p-3 small">We present a new Lexical Simplification approach that exploits Neural Networks to learn substitutions from the Newsela corpus-a large set of professionally produced simplifications. We extract candidate substitutions by combining the Newsela corpus with a retrofitted context-aware word embeddings model and rank them using a new neural regression model that learns rankings from annotated data. This strategy leads to the highest <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>Accuracy</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>Precision</a> and F1 scores to date in standard datasets for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2008/>Crowd-Sourced Iterative Annotation for Narrative Summarization Corpora</a></strong><br><a href=/people/j/jessica-ouyang/>Jessica Ouyang</a>
|
<a href=/people/s/serina-chang/>Serina Chang</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2008><div class="card-body p-3 small">We present an iterative annotation process for producing aligned, parallel corpora of abstractive and extractive summaries for <a href=https://en.wikipedia.org/wiki/Narrative>narrative</a>. Our approach uses a combination of trained annotators and <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a>, allowing us to elicit human-generated summaries and alignments quickly and at low cost. We use <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> to annotate aligned phrases with the text-to-text generation techniques needed to transform each phrase into the other. We apply this process to a corpus of 476 personal narratives, which we make available on the Web.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2009/>Broad Context Language Modeling as Reading Comprehension</a></strong><br><a href=/people/z/zewei-chu/>Zewei Chu</a>
|
<a href=/people/h/hai-wang/>Hai Wang</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a>
|
<a href=/people/d/david-mcallester/>David McAllester</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2009><div class="card-body p-3 small">Progress in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>text understanding</a> has been driven by large datasets that test particular capabilities, like recent <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> (Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence. We view LAMBADA as a <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension problem</a> and apply comprehension models based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Though these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3 % to 49 %. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> or external knowledge is needed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2010/>Detecting negation scope is easy, except when it is n’t</a></strong><br><a href=/people/f/federico-fancellu/>Federico Fancellu</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/h/hangfeng-he/>Hangfeng He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2010><div class="card-body p-3 small">Several <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> have been annotated with negation scopethe set of words whose meaning is negated by a cue like the word notleading to the development of <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> that detect negation scope with high accuracy. We show that for nearly all of these corpora, this high accuracy can be attributed to a single fact : they frequently annotate negation scope as a single span of text delimited by <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>. For negation scopes not of this form, detection accuracy is low and under-sampling the easy training examples does not substantially improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We demonstrate that this is partly an artifact of annotation guidelines, and we argue that future negation scope annotation efforts should focus on these more difficult cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2011/>MT / IE : Cross-lingual Open Information Extraction with Neural Sequence-to-Sequence Models<span class=acl-fixed-case>MT</span>/<span class=acl-fixed-case>IE</span>: Cross-lingual Open Information Extraction with Neural Sequence-to-Sequence Models</a></strong><br><a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2011><div class="card-body p-3 small">Cross-lingual information extraction is the task of distilling facts from foreign language (e.g. Chinese text) into representations in another language that is preferred by the user (e.g. English tuples). Conventional pipeline solutions decompose the task as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> followed by <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> (or vice versa). We propose a joint solution with a neural sequence model, and show that it outperforms the pipeline in a cross-lingual open information extraction setting by 1-4 BLEU and 0.5-0.8 <a href=https://en.wikipedia.org/wiki/F-number>F1</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2012/>Learning to Negate Adjectives with Bilinear Models</a></strong><br><a href=/people/l/laura-rimell/>Laura Rimell</a>
|
<a href=/people/a/amandla-mabona/>Amandla Mabona</a>
|
<a href=/people/l/luana-bulat/>Luana Bulat</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2012><div class="card-body p-3 small">We learn a <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> that negates <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> by predicting an adjective&#8217;s antonym in an arbitrary word embedding model. We show that both <a href=https://en.wikipedia.org/wiki/Linear_model>linear models</a> and <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> improve on this task when they have access to a vector representing the <a href=https://en.wikipedia.org/wiki/Semantic_domain>semantic domain</a> of the input word, e.g. a centroid of temperature words when predicting the antonym of &#8216;cold&#8217;. We introduce a continuous class-conditional bilinear neural network which is able to negate <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2013 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2013/>Instances and concepts in distributional space</a></strong><br><a href=/people/g/gemma-boleda/>Gemma Boleda</a>
|
<a href=/people/a/abhijeet-gupta/>Abhijeet Gupta</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2013><div class="card-body p-3 small">Instances (Mozart) are ontologically distinct from concepts or classes (composer). Natural language encompasses both, but instances have received comparatively little attention in <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>. Our results show that instances and concepts differ in their <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional properties</a>. We also establish that <a href=https://en.wikipedia.org/wiki/Instance_(computer_science)>instantiation detection (Mozart composer)</a> is generally easier than hypernymy detection (chemist scientist), and that results on the influence of input representation do not transfer from <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponymy</a> to <a href=https://en.wikipedia.org/wiki/Instance_(computer_science)>instantiation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2014/>Is this a Child, a Girl or a Car? Exploring the Contribution of Distributional Similarity to Learning Referential Word Meanings</a></strong><br><a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2014><div class="card-body p-3 small">There has recently been a lot of work trying to use images of referents of words for improving vector space meaning representations derived from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>. We investigate the opposite direction, as it were, trying to improve visual word predictors that identify objects in images, by exploiting distributional similarity information during <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>. We show that for certain <a href=https://en.wikipedia.org/wiki/Word>words</a> (such as entry-level nouns or hypernyms), we can indeed learn better referential word meanings by taking into account their semantic similarity to other words. For other words, there is no or even a detrimental effect, compared to a learning setup that presents even semantically related objects as negative instances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2015/>The Semantic Proto-Role Linking Model</a></strong><br><a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/k/kyle-rawlins/>Kyle Rawlins</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2015><div class="card-body p-3 small">We propose the semantic proto-role linking model, which jointly induces both predicate-specific semantic roles and predicate-general semantic proto-roles based on semantic proto-role property likelihood judgments. We use this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to empirically evaluate Dowty&#8217;s thematic proto-role linking theory.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2016/>The Language of Place : Semantic Value from <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>Geospatial Context</a></a></strong><br><a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2016><div class="card-body p-3 small">There is a relationship between what we say and where we say it. Word embeddings are usually trained assuming that semantically-similar words occur within the same textual contexts. We investigate the extent to which <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantically-similar words</a> occur within the same <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>geospatial contexts</a>. We enrich a corpus of geolocated Twitter posts with physical data derived from <a href=https://en.wikipedia.org/wiki/Google_Places>Google Places</a> and <a href=https://en.wikipedia.org/wiki/OpenStreetMap>OpenStreetMap</a>, and train word embeddings using the resulting geospatial contexts. Intrinsic evaluation of the resulting vectors shows that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>geographic context</a> alone does provide useful information about <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2018/>A Rich Morphological Tagger for <a href=https://en.wikipedia.org/wiki/English_language>English</a> : Exploring the Cross-Linguistic Tradeoff Between <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a> and <a href=https://en.wikipedia.org/wiki/Syntax>Syntax</a><span class=acl-fixed-case>E</span>nglish: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax</a></strong><br><a href=/people/c/christo-kirov/>Christo Kirov</a>
|
<a href=/people/j/john-sylak-glassman/>John Sylak-Glassman</a>
|
<a href=/people/r/rebecca-knowles/>Rebecca Knowles</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/matt-post/>Matt Post</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2018><div class="card-body p-3 small">A traditional claim in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> is that all human languages are equally expressiveable to convey the same wide range of meanings. Morphologically rich languages, such as <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, rely on overt inflectional and derivational morphology to convey many semantic distinctions. Languages with comparatively limited morphology, such as <a href=https://en.wikipedia.org/wiki/English_language>English</a>, should be able to accomplish the same using a combination of syntactic and contextual cues. We capitalize on this idea by training a tagger for <a href=https://en.wikipedia.org/wiki/English_language>English</a> that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. The high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2019/>Context-Aware Prediction of Derivational Word-forms</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2019><div class="card-body p-3 small">Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose a new task of predicting the <a href=https://en.wikipedia.org/wiki/Derivation_(differential_algebra)>derivational form</a> of a given base-form lemma that is appropriate for a given context. We present an encoder-decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under lexicon agnostic setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2020/>Comparing Character-level Neural Language Models Using a Lexical Decision Task</a></strong><br><a href=/people/g/gael-le-godais/>Gaël Le Godais</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2020><div class="card-body p-3 small">What is the information captured by neural network models of language? We address this question in the case of character-level recurrent neural language models. These <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not have explicit word representations ; do they acquire implicit ones? We assess the lexical capacity of a network using the <a href=https://en.wikipedia.org/wiki/Lexical_decision_task>lexical decision task</a> common in <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> : the <a href=https://en.wikipedia.org/wiki/System>system</a> is required to decide whether or not a string of characters forms a word. We explore how accuracy on this task is affected by the architecture of the <a href=https://en.wikipedia.org/wiki/Telecommunications_network>network</a>, focusing on cell type (LSTM vs. SRN), depth and width. We also compare these architectural properties to a simple count of the parameters of the <a href=https://en.wikipedia.org/wiki/Network_analysis_(electrical_circuits)>network</a>. The overall number of parameters in the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a> turns out to be the most important predictor of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> ; in particular, there is little evidence that deeper networks are beneficial for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2021/>Optimal encoding !-Information Theory constrains article omission in newspaper headlines</a></strong><br><a href=/people/r/robin-lemke/>Robin Lemke</a>
|
<a href=/people/e/eva-horch/>Eva Horch</a>
|
<a href=/people/i/ingo-reich/>Ingo Reich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2021><div class="card-body p-3 small">In this paper we pursue the hypothesis that the distribution of article omission specifically is constrained by principles of <a href=https://en.wikipedia.org/wiki/Information_theory>Information Theory</a> (Shannon 1948). In particular, <a href=https://en.wikipedia.org/wiki/Information_theory>Information Theory</a> predicts a stronger preference for article omission before nouns which are relatively unpredictable in context of the preceding words. We investigated article omission in <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_Germany>German newspaper headlines</a> with a corpus and acceptability rating study. Both support our hypothesis : Articles are inserted more often before unpredictable nouns and subjects perceive article omission before predictable nouns as more well-formed than before unpredictable ones. This suggests that <a href=https://en.wikipedia.org/wiki/Information_theory>information theoretic principles</a> constrain the distribution of article omission in <a href=https://en.wikipedia.org/wiki/Headline>headlines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2022 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2022/>A Computational Analysis of the Language of Drug Addiction</a></strong><br><a href=/people/c/carlo-strapparava/>Carlo Strapparava</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2022><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational analysis</a> of the language of drug users when talking about their drug experiences. We introduce a new dataset of over 4,000 descriptions of experiences reported by users of four main drug types, and show that we can predict with an F1-score of up to 88 % the drug behind a certain experience. We also perform an analysis of the dominant psycholinguistic processes and dominant emotions associated with each drug type, which sheds light on the characteristics of drug users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2023/>A Practical Perspective on Latent Structured Prediction for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a></a></strong><br><a href=/people/i/iryna-haponchyk/>Iryna Haponchyk</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2023><div class="card-body p-3 small">Latent structured prediction theory proposes powerful methods such as Latent Structural SVM (LSSVM), which can potentially be very appealing for coreference resolution (CR). In contrast, only small work is available, mainly targeting the latent structured perceptron (LSP). In this paper, we carried out a practical study comparing for the first time <a href=https://en.wikipedia.org/wiki/Educational_technology>online learning</a> with LSSVM. We analyze the intricacies that may have made initial attempts to use LSSVM fail, i.e., a huge <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a> and much lower accuracy produced by Kruskal&#8217;s spanning tree algorithm. In this respect, we also propose a new effective feature selection approach for improving system efficiency. The results show that LSP, if correctly parameterized, produces the same performance as LSSVM, being much more efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2024/>On the Need of <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross Validation</a> for Discourse Relation Classification</a></strong><br><a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2024><div class="card-body p-3 small">The task of implicit discourse relation classification has received increased attention in recent years, including two CoNNL shared tasks on the topic. Existing machine learning models for the task train on sections 2-21 of the PDTB and test on section 23, which includes a total of 761 implicit discourse relations. In this paper, we&#8217;d like to make a methodological point, arguing that the standard test set is too small to draw conclusions about whether the inclusion of certain features constitute a genuine improvement, or whether one got lucky with some properties of the <a href=https://en.wikipedia.org/wiki/Test_set>test set</a>, and argue for the adoption of <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>cross validation</a> for the discourse relation classification task by the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2025" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2025/>Using the Output Embedding to Improve <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a></a></strong><br><a href=/people/o/ofir-press/>Ofir Press</a>
|
<a href=/people/l/lior-wolf/>Lior Wolf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2025><div class="card-body p-3 small">We study the topmost weight matrix of neural network language models. We show that this <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a> constitutes a valid <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. When training <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a>, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2026/>Identifying beneficial task relations for <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> in deep neural networks</a></strong><br><a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2026><div class="card-body p-3 small">Multi-task learning (MTL) in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a>. While it has brought significant improvements in a number of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2027 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2027/>Effective search space reduction for spell correction using character neural embeddings</a></strong><br><a href=/people/h/harshit-pande/>Harshit Pande</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2027><div class="card-body p-3 small">We present a novel, unsupervised, and distance measure agnostic method for search space reduction in spell correction using neural character embeddings. The embeddings are learned by skip-gram word2vec training on sequences generated from dictionary words in a phonetic information-retentive manner. We report a very high performance in terms of both success rates and reduction of search space on the Birkbeck spelling error corpus. To the best of our knowledge, this is the first application of <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> to spell correction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2028/>Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2028><div class="card-body p-3 small">The popular skip-gram model induces <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> by exploiting the signal from word-context coocurrence. We offer a new interpretation of <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a> based on exponential family PCA-a form of matrix factorization to generalize the <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram model</a> to tensor factorization. In turn, this lets us train <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> through richer higher-order coocurrences, e.g., triples that include positional information (to incorporate syntax) or <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological information</a> (to share parameters across related words). We experiment on 40 languages and show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves upon <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2029/>Latent Variable Dialogue Models and their Diversity</a></strong><br><a href=/people/k/kris-cao/>Kris Cao</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2029><div class="card-body p-3 small">We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the &#8216;boring output&#8217; issue of deterministic dialogue models. Experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2030 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2030/>Age Group Classification with Speech and Metadata Multimodality Fusion</a></strong><br><a href=/people/d/denys-katerenchuk/>Denys Katerenchuk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2030><div class="card-body p-3 small">Children comprise a significant proportion of TV viewers and it is worthwhile to customize the experience for them. However, identifying who is a child in the audience can be a challenging task. We present initial studies of a novel <a href=https://en.wikipedia.org/wiki/Methodology>method</a> which combines utterances with user metadata. In particular, we develop an ensemble of different machine learning techniques on different subsets of data to improve child detection. Our initial results show an 9.2 % absolute improvement over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, leading to a state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2031 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2031/>Automatically augmenting an emotion dataset improves <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> using audio</a></strong><br><a href=/people/e/egor-lakomkin/>Egor Lakomkin</a>
|
<a href=/people/c/cornelius-weber/>Cornelius Weber</a>
|
<a href=/people/s/stefan-wermter/>Stefan Wermter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2031><div class="card-body p-3 small">In this work, we tackle a problem of <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech emotion classification</a>. One of the issues in the area of affective computation is that the amount of annotated data is very limited. On the other hand, the number of ways that the same emotion can be expressed verbally is enormous due to variability between speakers. This is one of the factors that limits performance and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>. We propose a simple method that extracts <a href=https://en.wikipedia.org/wiki/Sampling_(music)>audio samples</a> from <a href=https://en.wikipedia.org/wiki/Film>movies</a> using textual sentiment analysis. As a result, it is possible to automatically construct a larger dataset of <a href=https://en.wikipedia.org/wiki/Sampling_(signal_processing)>audio samples</a> with positive, negative emotional and neutral speech. We show that pretraining <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> on such a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> yields better results on the challenging EmotiW corpus. This experiment shows a potential benefit of combining textual sentiment analysis with <a href=https://en.wikipedia.org/wiki/Voice_(phonetics)>vocal information</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2033/>Hybrid Dialog State Tracker with ASR Features<span class=acl-fixed-case>ASR</span> Features</a></strong><br><a href=/people/m/miroslav-vodolan/>Miroslav Vodolán</a>
|
<a href=/people/r/rudolf-kadlec/>Rudolf Kadlec</a>
|
<a href=/people/j/jan-kleindienst/>Jan Kleindienst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2033><div class="card-body p-3 small">This paper presents a hybrid dialog state tracker enhanced by trainable Spoken Language Understanding (SLU) for slot-filling dialog systems. Our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> is inspired by previously proposed neural-network-based belief-tracking systems. In addition, we extended some parts of our <a href=https://en.wikipedia.org/wiki/Modular_programming>modular architecture</a> with <a href=https://en.wikipedia.org/wiki/Differentiable_function>differentiable rules</a> to allow <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end training</a>. We hypothesize that these rules allow our <a href=https://en.wikipedia.org/wiki/Music_tracker>tracker</a> to generalize better than pure machine-learning based systems. For evaluation, we used the Dialog State Tracking Challenge (DSTC) 2 dataset-a popular belief tracking testbed with dialogs from restaurant information system. To our knowledge, our hybrid tracker sets a new state-of-the-art result in three out of four categories within the DSTC2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2034 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2034/>Morphological Analysis without Expert Annotation</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/g/grzegorz-kondrak/>Grzegorz Kondrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2034><div class="card-body p-3 small">The task of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a> is to produce a complete list of lemma+tag analyses for a given <a href=https://en.wikipedia.org/wiki/Logical_form>word-form</a>. We propose a discriminative string transduction approach which exploits plain inflection tables and raw text corpora, thus obviating the need for expert annotation. Experiments on four languages demonstrate that our system has much higher coverage than a hand-engineered FST analyzer, and is more accurate than a state-of-the-art morphological tagger.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2035/>Morphological Analysis of the Dravidian Language Family<span class=acl-fixed-case>D</span>ravidian Language Family</a></strong><br><a href=/people/a/arun-kumar/>Arun Kumar</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/l/lluis-padro/>Lluís Padró</a>
|
<a href=/people/a/antoni-oliver/>Antoni Oliver</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2035><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a> are one of the most widely spoken language families in the world, yet there are very few annotated resources available to NLP researchers. To remedy this, we create DravMorph, a <a href=https://en.wikipedia.org/wiki/Speech_corpus>corpus</a> annotated for morphological segmentation and <a href=https://en.wikipedia.org/wiki/Part_of_speech>part-of-speech</a>. Additionally, we exploit novel features and higher-order models to set state-of-the-art results on these corpora on both tasks, beating techniques proposed in the literature by as much as 4 points in segmentation F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2036 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2036/>BabelDomains : Large-Scale Domain Labeling of Lexical Resources<span class=acl-fixed-case>B</span>abel<span class=acl-fixed-case>D</span>omains: Large-Scale Domain Labeling of Lexical Resources</a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2036><div class="card-body p-3 small">In this paper we present BabelDomains, a unified resource which provides lexical items with information about domains of knowledge. We propose an automatic method that uses knowledge from various lexical resources, exploiting both distributional and graph-based clues, to accurately propagate domain information. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> intrinsically on two lexical resources (WordNet and BabelNet), achieving a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> over 80 % in both cases. Finally, we show the potential of BabelDomains in a supervised learning setting, clustering training data by domain for hypernym discovery.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2037 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2037" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2037/>JFLEG : A Fluency Corpus and Benchmark for Grammatical Error Correction<span class=acl-fixed-case>JFLEG</span>: A Fluency Corpus and Benchmark for Grammatical Error Correction</a></strong><br><a href=/people/c/courtney-napoles/>Courtney Napoles</a>
|
<a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2037><div class="card-body p-3 small">We present a new parallel corpus, JHU FLuency-Extended GUG corpus (JFLEG) for developing and evaluating grammatical error correction (GEC). Unlike other <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct <a href=https://en.wikipedia.org/wiki/Error_(linguistics)>grammatical errors</a> but also make the original text more native sounding. We describe the types of corrections made and benchmark four leading GEC systems on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, identifying specific areas in which they do well and how they can improve. JFLEG fulfills the need for a new gold standard to properly assess the current state of <a href=https://en.wikipedia.org/wiki/General_Electric_Company>GEC</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2039 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2039/>The Parallel Meaning Bank : Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations<span class=acl-fixed-case>P</span>arallel <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>B</span>ank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations</a></strong><br><a href=/people/l/lasha-abzianidze/>Lasha Abzianidze</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/k/kilian-evang/>Kilian Evang</a>
|
<a href=/people/h/hessel-haagsma/>Hessel Haagsma</a>
|
<a href=/people/r/rik-van-noord/>Rik van Noord</a>
|
<a href=/people/p/pierre-ludmann/>Pierre Ludmann</a>
|
<a href=/people/d/duc-duy-nguyen/>Duc-Duy Nguyen</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2039><div class="card-body p-3 small">The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four languages (English, German, Italian, and Dutch). Our approach is based on cross-lingual projection : automatically produced (and manually corrected) semantic annotations for English sentences are mapped onto their word-aligned translations, assuming that the translations are meaning-preserving. The <a href=https://en.wikipedia.org/wiki/Semantic_annotation>semantic annotation</a> consists of five main steps : (i) segmentation of the text in sentences and lexical items ; (ii) syntactic parsing with Combinatory Categorial Grammar ; (iii) universal semantic tagging ; (iv) symbolization ; and (v) compositional semantic analysis based on <a href=https://en.wikipedia.org/wiki/Discourse_representation_theory>Discourse Representation Theory</a>. These steps are performed using <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a> trained in a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised manner</a>. The employed annotation models are all language-neutral. Our first results are promising.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2040 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2040/>Cross-lingual tagger evaluation without test data</a></strong><br><a href=/people/z/zeljko-agic/>Željko Agić</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2040><div class="card-body p-3 small">We address the challenge of cross-lingual POS tagger evaluation in absence of manually annotated test data. We put forth and evaluate two dictionary-based metrics. On the tasks of accuracy prediction and system ranking, we reveal that these metrics are reliable enough to approximate test set-based evaluation, and at the same time lean enough to support assessment for truly low-resource languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2042 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2042/>The Content Types Dataset : a New Resource to Explore Semantic and Functional Characteristics of Texts</a></strong><br><a href=/people/r/rachele-sprugnoli/>Rachele Sprugnoli</a>
|
<a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/g/giovanni-moretti/>Giovanni Moretti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2042><div class="card-body p-3 small">This paper presents a new resource, called Content Types Dataset, to promote the analysis of texts as a composition of units with specific semantic and functional roles. By developing this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we also introduce a new NLP task for the automatic classification of Content Types. The annotation scheme and the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> are described together with two sets of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2043/>Continuous N-gram Representations for Authorship Attribution</a></strong><br><a href=/people/y/yunita-sari/>Yunita Sari</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/m/mark-stevenson/>Mark Stevenson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2043><div class="card-body p-3 small">This paper presents work on using continuous representations for <a href=https://en.wikipedia.org/wiki/Attribution_(copyright)>authorship attribution</a>. In contrast to previous work, which uses discrete feature representations, our model learns continuous representations for <a href=https://en.wikipedia.org/wiki/N-gram>n-gram features</a> via a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> jointly with the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification layer</a>. Experimental results demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on two datasets, while producing comparable results on the remaining two.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2044 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2044" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2044/>Reconstructing the house from the ad : <a href=https://en.wikipedia.org/wiki/Structured_prediction>Structured prediction</a> on real estate classifieds</a></strong><br><a href=/people/g/giannis-bekoulis/>Giannis Bekoulis</a>
|
<a href=/people/j/johannes-deleu/>Johannes Deleu</a>
|
<a href=/people/t/thomas-demeester/>Thomas Demeester</a>
|
<a href=/people/c/chris-develder/>Chris Develder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2044><div class="card-body p-3 small">In this paper, we address the (to the best of our knowledge) new problem of extracting a structured description of real estate properties from their natural language descriptions in <a href=https://en.wikipedia.org/wiki/Classified_advertising>classifieds</a>. We survey and present several models to (a) identify important <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> of a property (e.g.,rooms) from classifieds and (b) structure them into a tree format, with the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> representing a <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>part-of relation</a>. Experiments show that a graph-based system deriving the <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> from an initially fully connected entity graph, outperforms a transition-based system starting from only the entity nodes, since it better reconstructs the <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2046 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2046/>Improving ROUGE for Timeline Summarization<span class=acl-fixed-case>ROUGE</span> for Timeline Summarization</a></strong><br><a href=/people/s/sebastian-martschat/>Sebastian Martschat</a>
|
<a href=/people/k/katja-markert/>Katja Markert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2046><div class="card-body p-3 small">Current evaluation metrics for timeline summarization either ignore the temporal aspect of the task or require strict date matching. We introduce variants of ROUGE that allow alignment of daily summaries via temporal distance or <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. We argue for the suitability of these variants in a theoretical analysis and demonstrate it in a battery of task-specific tests.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2047 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2047/>Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization</a></strong><br><a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2047><div class="card-body p-3 small">This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and control the output words based on the estimation in the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2048 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2048/>To Sing like a Mockingbird</a></strong><br><a href=/people/l/lorenzo-gatti/>Lorenzo Gatti</a>
|
<a href=/people/g/gozde-ozbal/>Gözde Özbal</a>
|
<a href=/people/o/oliviero-stock/>Oliviero Stock</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2048><div class="card-body p-3 small">Musical parody, i.e. the act of changing the lyrics of an existing and very well-known song, is a commonly used technique for creating catchy advertising tunes and for mocking people or events. Here we describe a <a href=https://en.wikipedia.org/wiki/System>system</a> for automatically producing a <a href=https://en.wikipedia.org/wiki/Parody_music>musical parody</a>, starting from a <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus of songs</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> can automatically identify characterizing words and concepts related to a novel text, which are taken from the <a href=https://en.wikipedia.org/wiki/News>daily news</a>. These concepts are then used as seeds to appropriately replace part of the original <a href=https://en.wikipedia.org/wiki/Lyrics>lyrics</a> of a song, using <a href=https://en.wikipedia.org/wiki/Metre_(poetry)>metrical</a>, <a href=https://en.wikipedia.org/wiki/Rhyme>rhyming</a> and lexical constraints. Finally, the <a href=https://en.wikipedia.org/wiki/Parody>parody</a> can be sung with a singing speech synthesizer, with no intervention from the user.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2049 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2049/>K-best Iterative Viterbi Parsing<span class=acl-fixed-case>V</span>iterbi Parsing</a></strong><br><a href=/people/k/katsuhiko-hayashi/>Katsuhiko Hayashi</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2049><div class="card-body p-3 small">This paper presents an efficient and optimal parsing algorithm for probabilistic context-free grammars (PCFGs). To achieve faster <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>, our proposal employs a pruning technique to reduce unnecessary edges in the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a>. The key is to conduct repetitively Viterbi inside and outside parsing, while gradually expanding the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> to efficiently compute <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic bounds</a> used for <a href=https://en.wikipedia.org/wiki/Parsing>pruning</a>. Our experimental results using the English Penn Treebank corpus show that the proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is faster than the standard CKY parsing algorithm. In addition, we also show how to extend this <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to extract k-best Viterbi parse trees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2050 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2050/>PP Attachment : Where do We Stand?<span class=acl-fixed-case>PP</span> Attachment: Where do We Stand?</a></strong><br><a href=/people/d/daniel-de-kok/>Daniël de Kok</a>
|
<a href=/people/j/jianqiang-ma/>Jianqiang Ma</a>
|
<a href=/people/c/corina-dima/>Corina Dima</a>
|
<a href=/people/e/erhard-hinrichs/>Erhard Hinrichs</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2050><div class="card-body p-3 small">Prepostitional phrase (PP) attachment is a well known challenge to <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. In this paper, we combine the insights of different works, namely : (1) treating PP attachment as a classification task with an arbitrary number of attachment candidates ; (2) using auxiliary distributions to augment the data beyond the hand-annotated training set ; (3) using topological fields to get information about the distribution of PP attachment throughout clauses and (4) using state-of-the-art techniques such as word embeddings and neural networks. We show that jointly using these <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>techniques</a> leads to substantial improvements. We also conduct a qualitative analysis to gauge where the ceiling of the task is in a realistic setup.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2051 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2051/>Do n’t Stop Me Now ! Using Global Dynamic Oracles to Correct Training Biases of Transition-Based Dependency Parsers</a></strong><br><a href=/people/l/lauriane-aufrant/>Lauriane Aufrant</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2051><div class="card-body p-3 small">This paper formalizes a sound extension of dynamic oracles to global training, in the frame of transition-based dependency parsers. By dispensing with the pre-computation of references, this extension widens the training strategies that can be entertained for such parsers ; we show this by revisiting two standard training procedures, early-update and max-violation, to correct some of their search space sampling biases. Experimentally, on the SPMRL treebanks, this improvement increases the similarity between the train and test distributions and yields performance improvements up to 0.7 UAS, without any <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>computation overhead</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2052/>Joining Hands : Exploiting Monolingual Treebanks for Parsing of Code-mixing Data</a></strong><br><a href=/people/i/irshad-bhat/>Irshad Bhat</a>
|
<a href=/people/r/riyaz-ahmad-bhat/>Riyaz A. Bhat</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2052><div class="card-body p-3 small">In this paper, we propose efficient and less resource-intensive strategies for parsing of code-mixed data. These strategies are not constrained by in-domain annotations, rather they leverage pre-existing monolingual annotated resources for training. We show that these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can produce significantly better results as compared to an <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>informed baseline</a>. Due to lack of an evaluation set for code-mixed structures, we also present a data set of 450 Hindi and English code-mixed tweets of Hindi multilingual speakers for evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2053 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2053/>Multilingual Lexicalized Constituency Parsing with Word-Level Auxiliary Tasks</a></strong><br><a href=/people/m/maximin-coavoux/>Maximin Coavoux</a>
|
<a href=/people/b/benoit-crabbe/>Benoît Crabbé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2053><div class="card-body p-3 small">We introduce a constituency parser based on a bi-LSTM encoder adapted from recent work (Cross and Huang, 2016b ; Kiperwasser and Goldberg, 2016), which can incorporate a lower level character biLSTM (Ballesteros et al., 2015 ; Plank et al., 2016). We model two important interfaces of constituency parsing with auxiliary tasks supervised at the word level : (i) part-of-speech (POS) and morphological tagging, (ii) functional label prediction. On the SPMRL dataset, our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> obtains above state-of-the-art results on constituency parsing without requiring either predicted POS or morphological tags, and outputs labelled dependency trees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2054 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2054/>Be Precise or Fuzzy : Learning the Meaning of Cardinals and Quantifiers from Vision</a></strong><br><a href=/people/s/sandro-pezzelle/>Sandro Pezzelle</a>
|
<a href=/people/m/marco-marelli/>Marco Marelli</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2054><div class="card-body p-3 small">People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or <a href=https://en.wikipedia.org/wiki/Quantifier_(linguistics)>natural language quantifiers</a> (e.g. few, most, all). In humans, these two <a href=https://en.wikipedia.org/wiki/Process_(anatomy)>processes</a> underlie fairly different <a href=https://en.wikipedia.org/wiki/Cognition>cognitive and neural mechanisms</a>. Inspired by this evidence, the present study proposes two <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for learning the objective meaning of cardinals and <a href=https://en.wikipedia.org/wiki/Quantifier_(logic)>quantifiers</a> from visual scenes containing multiple objects. We show that a model capitalizing on a &#8216;fuzzy&#8217; measure of similarity is effective for learning <a href=https://en.wikipedia.org/wiki/Quantifier_(logic)>quantifiers</a>, whereas the learning of exact cardinals is better accomplished when information about number is provided.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2055/>Improving a Strong Neural Parser with Conjunction-Specific Features</a></strong><br><a href=/people/j/jessica-ficler/>Jessica Ficler</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2055><div class="card-body p-3 small">While dependency parsers reach very high overall accuracy, some dependency relations are much harder than others. In particular, dependency parsers perform poorly in coordination construction (i.e., correctly attaching the conj relation). We extend a state-of-the-art dependency parser with conjunction-specific features, focusing on the similarity between the conjuncts head words. Training the extended <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> yields an improvement in conj attachment as well as in overall dependency parsing accuracy on the Stanford dependency conversion of the Penn TreeBank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2057/>Improving Evaluation of Document-level Machine Translation Quality Estimation</a></strong><br><a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/c/carla-parra-escartin/>Carla Parra</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2057><div class="card-body p-3 small">Meaningful conclusions about the relative performance of NLP systems are only possible if the <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold standard</a> employed in a given <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is both valid and reliable. In this paper, we explore the validity of human annotations currently employed in the evaluation of document-level quality estimation for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>. We demonstrate the degree to which MT system rankings are dependent on weights employed in the construction of the <a href=https://en.wikipedia.org/wiki/Gold_standard>gold standard</a>, before proposing direct human assessment as a valid alternative. Experiments show direct assessment (DA) scores for documents to be highly reliable, achieving a correlation of above 0.9 in a self-replication experiment, in addition to a substantial estimated cost reduction through quality controlled crowd-sourcing. The original <a href=https://en.wikipedia.org/wiki/Gold_standard>gold standard</a> based on <a href=https://en.wikipedia.org/wiki/Post-editing>post-edits</a> incurs a 1020 times greater cost than DA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2058 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2058/>Neural Machine Translation by Minimising the Bayes-risk with Respect to Syntactic Translation Lattices<span class=acl-fixed-case>B</span>ayes-risk with Respect to Syntactic Translation Lattices</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/a/adria-de-gispert/>Adrià de Gispert</a>
|
<a href=/people/e/eva-hasler/>Eva Hasler</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2058><div class="card-body p-3 small">We present a novel scheme to combine <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> with traditional <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation (SMT)</a>. Our approach borrows ideas from linearised lattice minimum Bayes-risk decoding for <a href=https://en.wikipedia.org/wiki/Signal-to-noise_ratio>SMT</a>. The NMT score is combined with the <a href=https://en.wikipedia.org/wiki/Bayes_risk>Bayes-risk</a> of the <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translation</a> according the SMT lattice. This makes our approach much more flexible than n-best list or lattice rescoring as the neural decoder is not restricted to the SMT search space. We show an efficient and simple way to integrate risk estimation into the NMT decoder which is suitable for word-level as well as subword-unit-level NMT. We test our method on <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese-English</a> and report significant gains over lattice rescoring on several data sets for both single and ensembled NMT. The MBR decoder produces entirely new hypotheses far beyond simply rescoring the SMT search space or fixing UNKs in the NMT output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2059/>Producing Unseen <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>Morphological Variants</a> in Statistical Machine Translation</a></strong><br><a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/ales-tamchyna/>Aleš Tamchyna</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2059><div class="card-body p-3 small">Translating into <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphologically rich languages</a> is difficult. Although the coverage of lemmas may be reasonable, many morphological variants can not be learned from the training data. We present a statistical translation system that is able to produce these <a href=https://en.wikipedia.org/wiki/Inflection>inflected word forms</a>. Different from most previous work, we do not separate morphological prediction from <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a> into two consecutive steps. Our approach is novel in that it is integrated in decoding and takes advantage of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> from both the source language and the target language sides.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2060" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2060/>How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs<span class=acl-fixed-case>MT</span> Quality with Contrastive Translation Pairs</a></strong><br><a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2060><div class="card-body p-3 small">Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>agreement</a> over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of <a href=https://en.wikipedia.org/wiki/Error>error</a>. We present LingEval97, a large-scale data set of 97000 contrastive translation pairs based on the WMT English-German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2061 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2061/>Neural Machine Translation with Recurrent Attention Modeling</a></strong><br><a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/z/zhiting-hu/>Zhiting Hu</a>
|
<a href=/people/y/yuntian-deng/>Yuntian Deng</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/a/alex-smola/>Alex Smola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2061><div class="card-body p-3 small">Knowing which words have been attended to in previous time steps while generating a <a href=https://en.wikipedia.org/wiki/Translation>translation</a> is a rich source of information for predicting what words will be attended to in the future. We improve upon the attention model of Bahdanau et al. (2014) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> per input word. This <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> easily captures informative features, such as <a href=https://en.wikipedia.org/wiki/Fertility>fertility</a> and regularities in relative distortion. In experiments, we show our parameterization of attention improves translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2062 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2062/>Inducing Embeddings for Rare and Unseen Words by Leveraging Lexical Resources</a></strong><br><a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2062><div class="card-body p-3 small">We put forward an approach that exploits the knowledge encoded in lexical resources in order to induce representations for words that were not encountered frequently during training. Our approach provides an advantage over the past work in that it enables vocabulary expansion not only for <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological variations</a>, but also for infrequent domain specific terms. We performed evaluations in different settings, showing that the technique can provide consistent improvements on multiple benchmarks across domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2063 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2063/>Large-scale evaluation of dependency-based DSMs : Are they worth the effort?<span class=acl-fixed-case>DSM</span>s: Are they worth the effort?</a></strong><br><a href=/people/g/gabriella-lapesa/>Gabriella Lapesa</a>
|
<a href=/people/s/stefan-evert/>Stefan Evert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2063><div class="card-body p-3 small">This paper presents a large-scale evaluation study of dependency-based distributional semantic models. We evaluate dependency-filtered and dependency-structured DSMs in a number of standard semantic similarity tasks, systematically exploring their parameter space in order to give them a fair shot against window-based models. Our results show that properly tuned window-based DSMs still outperform the dependency-based models in most tasks. There appears to be little need for the language-dependent resources and <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> associated with <a href=https://en.wikipedia.org/wiki/Syntactic_analysis>syntactic analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2064 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2064/>How Well Can We Predict Hypernyms from Word Embeddings? A Dataset-Centric Analysis</a></strong><br><a href=/people/i/ivan-sanchez/>Ivan Sanchez</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2064><div class="card-body p-3 small">One key property of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> currently under study is their capacity to encode <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a>. Previous works have used <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a> to recover hypernymy structures from <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. However, the overall results do not clearly show how well we can recover such <a href=https://en.wikipedia.org/wiki/Mathematical_structure>structures</a>. We conduct the first dataset-centric analysis that shows how only the Baroni dataset provides consistent results. We empirically show that a possible reason for its good performance is its alignment to dimensions specific of <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a> : generality and similarity</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2065 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2065/>Cross-Lingual Syntactically Informed Distributed Word Representations</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2065><div class="card-body p-3 small">We develop a novel cross-lingual word representation model which injects syntactic information through dependency-based contexts into a shared cross-lingual word vector space. The model, termed CL-DepEmb, is based on the following assumptions : (1) dependency relations are largely language-independent, at least for related languages and prominent dependency links such as direct objects, as evidenced by the Universal Dependencies project ; (2) word translation equivalents take similar grammatical roles in a sentence and are therefore substitutable within their syntactic contexts. Experiments with several language pairs on word similarity and bilingual lexicon induction, two fundamental semantic tasks emphasising <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>, suggest the usefulness of the proposed syntactically informed cross-lingual word vector spaces. Improvements are observed in both tasks over standard cross-lingual offline mapping baselines trained using the same setup and an equal level of bilingual supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2066 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/E17-2066.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/E17-2066/>Using Word Embedding for Cross-Language Plagiarism Detection</a></strong><br><a href=/people/j/jeremy-ferrero/>Jérémy Ferrero</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/f/frederic-agnes/>Frédéric Agnès</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2066><div class="card-body p-3 small">This paper proposes to use distributed representation of words (word embeddings) in cross-language textual similarity detection. The main contributions of this paper are the following : (a) we introduce new cross-language similarity detection methods based on distributed representation of words ; (b) we combine the different methods proposed to verify their complementarity and finally obtain an overall F1 score of 89.15 % for English-French similarity detection at chunk level (88.5 % at sentence level) on a very challenging corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2067 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2067" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2067/>The Interplay of <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a> in Word Embeddings</a></strong><br><a href=/people/o/oded-avraham/>Oded Avraham</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2067><div class="card-body p-3 small">We explore the ability of word embeddings to capture both semantic and morphological similarity, as affected by the different types of linguistic properties (surface form, <a href=https://en.wikipedia.org/wiki/Lemma_(morphology)>lemma</a>, morphological tag) used to compose the representation of each word. We train several <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, where each uses a different subset of these properties to compose its <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. By evaluating the models on semantic and morphological measures, we reveal some useful insights on the relationship between <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2068 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2068" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2068/>Bag of Tricks for Efficient Text Classification</a></strong><br><a href=/people/a/armand-joulin/>Armand Joulin</a>
|
<a href=/people/e/edouard-grave/>Edouard Grave</a>
|
<a href=/people/p/piotr-bojanowski/>Piotr Bojanowski</a>
|
<a href=/people/t/tomas-mikolov/>Tomas Mikolov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2068><div class="card-body p-3 small">This paper explores a simple and efficient <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> on more than one billion words in less than ten minutes using a standard <a href=https://en.wikipedia.org/wiki/Multi-core_processor>multicore CPU</a>, and classify half a million sentences among 312 K classes in less than a minute.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2069 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2069/>Pulling Out the Stops : Rethinking Stopword Removal for Topic Models</a></strong><br><a href=/people/a/alexandra-schofield/>Alexandra Schofield</a>
|
<a href=/people/m/mans-magnusson/>Måns Magnusson</a>
|
<a href=/people/d/david-mimno/>David Mimno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2069><div class="card-body p-3 small">It is often assumed that <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> benefit from the use of a manually curated stopword list. Constructing this <a href=https://en.wikipedia.org/wiki/List_(abstract_data_type)>list</a> is time-consuming and often subject to user judgments about what kinds of words are important to the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and the application. Although stopword removal clearly affects which word types appear as most probable terms in topics, we argue that this improvement is superficial, and that topic inference benefits little from the practice of removing <a href=https://en.wikipedia.org/wiki/Stopword>stopwords</a> beyond very frequent terms. Removing corpus-specific stopwords after model inference is more transparent and produces similar results to removing those words prior to <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2070 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2070/>Measuring Topic Coherence through Optimal Word Buckets</a></strong><br><a href=/people/n/nitin-ramrakhiyani/>Nitin Ramrakhiyani</a>
|
<a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/g/girish-palshikar/>Girish Palshikar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2070><div class="card-body p-3 small">Measuring topic quality is essential for scoring the learned topics and their subsequent use in <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a> and <a href=https://en.wikipedia.org/wiki/Text_classification>Text classification</a>. To measure quality of Latent Dirichlet Allocation (LDA) based topics learned from text, we propose a novel approach based on grouping of topic words into buckets (TBuckets). A single large bucket signifies a single <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherent theme</a>, in turn indicating high <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>topic coherence</a>. TBuckets uses word embeddings of topic words and employs <a href=https://en.wikipedia.org/wiki/Singular_value_decomposition>singular value decomposition (SVD)</a> and Integer Linear Programming based optimization to create coherent word buckets. TBuckets outperforms the state-of-the-art techniques when evaluated using 3 publicly available datasets and on another one proposed in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2071 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2071/>A Hybrid CNN-RNN Alignment Model for Phrase-Aware Sentence Classification<span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>RNN</span> Alignment Model for Phrase-Aware Sentence Classification</a></strong><br><a href=/people/s/shiou-tian-hsu/>Shiou Tian Hsu</a>
|
<a href=/people/c/changsung-moon/>Changsung Moon</a>
|
<a href=/people/p/paul-jones/>Paul Jones</a>
|
<a href=/people/n/nagiza-samatova/>Nagiza Samatova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2071><div class="card-body p-3 small">The success of sentence classification often depends on understanding both the syntactic and semantic properties of <a href=https://en.wikipedia.org/wiki/Phrase>word-phrases</a>. Recent progress on this task has been based on exploiting the <a href=https://en.wikipedia.org/wiki/Grammar>grammatical structure</a> of sentences but often this structure is difficult to parse and noisy. In this paper, we propose a structure-independent &#8216;Gated Representation Alignment&#8217; (GRA) model that blends a phrase-focused Convolutional Neural Network (CNN) approach with sequence-oriented Recurrent Neural Network (RNN). Our novel alignment mechanism allows the RNN to selectively include phrase information in a word-by-word sentence representation, and to do this without awareness of the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. An empirical evaluation of GRA shows higher prediction accuracy (up to 4.6 %) of fine-grained sentiment ratings, when compared to other structure-independent baselines. We also show comparable results to several structure-dependent methods. Finally, we analyzed the effect of our alignment mechanism and found that this is critical to the effectiveness of the CNN-RNN hybrid.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2072 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2072/>Multivariate Gaussian Document Representation from Word Embeddings for Text Categorization<span class=acl-fixed-case>G</span>aussian Document Representation from Word Embeddings for Text Categorization</a></strong><br><a href=/people/g/giannis-nikolentzos/>Giannis Nikolentzos</a>
|
<a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/f/francois-rousseau/>François Rousseau</a>
|
<a href=/people/y/yannis-stavrakas/>Yannis Stavrakas</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2072><div class="card-body p-3 small">Recently, there has been a lot of activity in learning distributed representations of words in <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a>. Although there are <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> capable of learning high-quality distributed representations of words, how to generate <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representations</a> of the same quality for phrases or documents still remains a challenge. In this paper, we propose to model each document as a multivariate Gaussian distribution based on the distributed representations of its words. We then measure the <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> between two documents based on the similarity of their distributions. Experiments on eight standard text categorization datasets demonstrate the effectiveness of the proposed approach in comparison with state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2073 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2073/>Derivation of Document Vectors from Adaptation of LSTM Language Model<span class=acl-fixed-case>LSTM</span> Language Model</a></strong><br><a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/b/brian-mak/>Brian Mak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2073><div class="card-body p-3 small">In many natural language processing (NLP) tasks, a document is commonly modeled as a <a href=https://en.wikipedia.org/wiki/Bag_of_words>bag of words</a> using the term frequency-inverse document frequency (TF-IDF) vector. One major shortcoming of the frequency-based TF-IDF feature vector is that it ignores word orders that carry syntactic and semantic relationships among the words in a document. This paper proposes a novel distributed vector representation of a document, which will be labeled as DV-LSTM, and is derived from the result of adapting a long short-term memory recurrent neural network language model by the document. DV-LSTM is expected to capture some high-level sequential information in the document, which other current document representations fail to do. It was evaluated in document genre classification in the <a href=https://en.wikipedia.org/wiki/Brown_Corpus>Brown Corpus</a> and the BNC Baby Corpus. The results show that DV-LSTM significantly outperforms TF-IDF vector and paragraph vector (PV-DM) in most cases, and their combinations may further improve the classification performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2074 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2074/>Real-Time Keyword Extraction from Conversations</a></strong><br><a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/a/antoine-tixier/>Antoine Tixier</a>
|
<a href=/people/i/ioannis-nikolentzos/>Ioannis Nikolentzos</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2074><div class="card-body p-3 small">We introduce a novel <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> to extract <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> from meeting speech in real-time. Our approach builds on the graph-of-words representation of text and leverages the k-core decomposition algorithm and properties of submodular functions. We outperform multiple baselines in a real-time scenario emulated from the AMI and ICSI meeting corpora. Evaluation is conducted against both extractive and abstractive gold standard using two standard <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a> and a newer one based on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2077 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2077/>Evaluating Persuasion Strategies and Deep Reinforcement Learning methods for Negotiation Dialogue agents</a></strong><br><a href=/people/s/simon-keizer/>Simon Keizer</a>
|
<a href=/people/m/markus-guhe/>Markus Guhe</a>
|
<a href=/people/h/heriberto-cuayahuitl/>Heriberto Cuayáhuitl</a>
|
<a href=/people/i/ioannis-efstathiou/>Ioannis Efstathiou</a>
|
<a href=/people/k/klaus-peter-engelbrecht/>Klaus-Peter Engelbrecht</a>
|
<a href=/people/m/mihai-dobre/>Mihai Dobre</a>
|
<a href=/people/a/alex-lascarides/>Alex Lascarides</a>
|
<a href=/people/o/oliver-lemon/>Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2077><div class="card-body p-3 small">In this paper we present a comparative evaluation of various <a href=https://en.wikipedia.org/wiki/Negotiation>negotiation strategies</a> within an online version of the game Settlers of Catan. The comparison is based on human subjects playing games against artificial game-playing agents (&#8216;bots&#8217;) which implement different negotiation dialogue strategies, using a chat dialogue interface to negotiate trades. Our results suggest that a negotiation strategy that uses <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion</a>, as well as a strategy that is trained from data using Deep Reinforcement Learning, both lead to an improved win rate against humans, compared to previous rule-based and supervised learning baseline dialogue negotiators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2078 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2078/>Unsupervised Dialogue Act Induction using Gaussian Mixtures<span class=acl-fixed-case>G</span>aussian Mixtures</a></strong><br><a href=/people/t/tomas-brychcin/>Tomáš Brychcín</a>
|
<a href=/people/p/pavel-kral/>Pavel Král</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2078><div class="card-body p-3 small">This paper introduces a new <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> for dialogue act induction. Given the sequence of dialogue utterances, the task is to assign them the labels representing their function in the dialogue. Utterances are represented as real-valued vectors encoding their meaning. We model the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> as <a href=https://en.wikipedia.org/wiki/Hidden_Markov_model>Hidden Markov model</a> with emission probabilities estimated by Gaussian mixtures. We use <a href=https://en.wikipedia.org/wiki/Gibbs_sampling>Gibbs sampling</a> for <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior inference</a>. We present the results on the standard Switchboard-DAMSL corpus. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> achieves promising results compared with strong supervised baselines and outperforms other unsupervised algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2079 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2079/>Grounding Language by Continuous Observation of Instruction Following</a></strong><br><a href=/people/t/ting-han/>Ting Han</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2079><div class="card-body p-3 small">Grounded semantics is typically learnt from utterance-level meaning representations (e.g., successful database retrievals, denoted objects in <a href=https://en.wikipedia.org/wiki/Digital_image>images</a>, moves in a game). We explore learning word and utterance meanings by continuous observation of the actions of an instruction follower (IF). While an instruction giver (IG) provided a verbal description of a configuration of objects, IF recreated it using a <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>GUI</a>. Aligning these GUI actions to sub-utterance chunks allows a simple <a href=https://en.wikipedia.org/wiki/Maximum_entropy_model>maximum entropy model</a> to associate them as chunk meaning better than just providing it with the utterance-final configuration. This shows that <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> useful for incremental (word-by-word) application, as required in natural dialogue, might also be better acquired from incremental settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2081 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2081" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2081/>Efficient, Compositional, Order-sensitive n-gram Embeddings</a></strong><br><a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/p/pushpendre-rastogi/>Pushpendre Rastogi</a>
|
<a href=/people/m/m-patrick-martin/>M. Patrick Martin</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2081><div class="card-body p-3 small">We propose ECO : a new way to generate embeddings for phrases that is Efficient, Compositional, and Order-sensitive. Our method creates decompositional embeddings for words offline and combines them to create new <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for phrases in real time. Unlike other approaches, ECO can create <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for phrases not seen during training. We evaluate ECO on supervised and unsupervised tasks and demonstrate that creating phrase embeddings that are sensitive to <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> can help downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2082 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2082" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2082/>Integrating Semantic Knowledge into <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Embeddings</a> Based on Information Content Measurement</a></strong><br><a href=/people/h/hsin-yang-wang/>Hsin-Yang Wang</a>
|
<a href=/people/w/wei-yun-ma/>Wei-Yun Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2082><div class="card-body p-3 small">Distributional word representations are widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. These <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> are based on an assumption that words with a similar context tend to have a similar meaning. To improve the quality of the context-based embeddings, many researches have explored how to make full use of existing lexical resources. In this paper, we argue that while we incorporate the prior knowledge with context-based embeddings, words with different occurrences should be treated differently. Therefore, we propose to rely on the measurement of information content to control the degree of applying prior knowledge into context-based embeddings-different words would have different learning rates when adjusting their embeddings. In the result, we demonstrate that our <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> get significant improvements on two different tasks : <a href=https://en.wikipedia.org/wiki/Similarity_measure>Word Similarity</a> and <a href=https://en.wikipedia.org/wiki/Analogical_reasoning>Analogical Reasoning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2083 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2083/>Improving Neural Knowledge Base Completion with Cross-Lingual Projections</a></strong><br><a href=/people/p/patrick-klein/>Patrick Klein</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2083><div class="card-body p-3 small">In this paper we present a cross-lingual extension of a neural tensor network model for knowledge base completion. We exploit multilingual synsets from <a href=https://en.wikipedia.org/wiki/BabelNet>BabelNet</a> to translate English triples to other languages and then augment the reference knowledge base with cross-lingual triples. We project monolingual embeddings of different languages to a shared multilingual space and use them for <a href=https://en.wikipedia.org/wiki/Network_topology>network initialization</a> (i.e., as initial concept embeddings). We then train the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> with triples from the cross-lingually augmented knowledge base. Results on WordNet link prediction show that leveraging cross-lingual information yields significant gains over exploiting only monolingual triples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2084 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2084/>Modelling metaphor with attribute-based semantics</a></strong><br><a href=/people/l/luana-bulat/>Luana Bulat</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2084><div class="card-body p-3 small">One of the key problems in computational metaphor modelling is finding the optimal level of abstraction of semantic representations, such that these are able to capture and generalise metaphorical mechanisms. In this paper we present the first metaphor identification method that uses <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> constructed from <a href=https://en.wikipedia.org/wiki/Norm_(philosophy)>property norms</a>. Such <a href=https://en.wikipedia.org/wiki/Social_norm>norms</a> have been previously shown to provide a cognitively plausible representation of concepts in terms of <a href=https://en.wikipedia.org/wiki/Semantics>semantic properties</a>. Our results demonstrate that such property-based semantic representations provide a suitable model of cross-domain knowledge projection in metaphors, outperforming standard distributional models on a metaphor identification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2085 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2085/>When a Red Herring in Not a Red Herring : Using Compositional Methods to Detect Non-Compositional Phrases</a></strong><br><a href=/people/j/julie-weeds/>Julie Weeds</a>
|
<a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/j/jeremy-reffin/>Jeremy Reffin</a>
|
<a href=/people/d/david-weir/>David Weir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2085><div class="card-body p-3 small">Non-compositional phrases such as <a href=https://en.wikipedia.org/wiki/Red_herring>red herring</a> and weakly compositional phrases such as <a href=https://en.wikipedia.org/wiki/Spelling_bee>spelling bee</a> are an integral part of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> (Sag, 2002). They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics. Compositionality detection therefore provides a good testbed for <a href=https://en.wikipedia.org/wiki/Compositing>compositional methods</a>. We compare an integrated compositional distributional approach, using sparse high dimensional representations, with the ad-hoc compositional approach of applying simple composition operations to state-of-the-art neural embeddings.<i>red herring</i> and weakly compositional phrases such as <i>spelling bee</i> are an integral part of natural language (Sag, 2002). They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics. Compositionality detection therefore provides a good testbed for compositional methods. We compare an integrated compositional distributional approach, using sparse high dimensional representations, with the ad-hoc compositional approach of applying simple composition operations to state-of-the-art neural embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2086 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2086/>Applying Multi-Sense Embeddings for German Verbs to Determine Semantic Relatedness and to Detect Non-Literal Language<span class=acl-fixed-case>G</span>erman Verbs to Determine Semantic Relatedness and to Detect Non-Literal Language</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2086><div class="card-body p-3 small">Up to date, the majority of <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational models</a> still determines the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a> between words (or larger linguistic units) on the type level. In this paper, we compare and extend multi-sense embeddings, in order to model and utilise <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> on the token level. We focus on the challenging class of complex verbs, and evaluate the model variants on various semantic tasks : semantic classification ; predicting compositionality ; and detecting non-literal language usage. While there is no overall best model, all models significantly outperform a word2vec single-sense skip baseline, thus demonstrating the need to distinguish between <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> in a distributional semantic model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2087 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2087" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2087/>Negative Sampling Improves Hypernymy Extraction Based on Projection Learning</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/n/nikolay-arefyev/>Nikolay Arefyev</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2087><div class="card-body p-3 small">We present a new approach to extraction of hypernyms based on projection learning and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. In contrast to <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification-based approaches</a>, <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection-based methods</a> require no candidate <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponym-hypernym pairs</a>. While it is natural to use both positive and negative training examples in supervised relation extraction, the impact of positive examples on hypernym prediction was not studied so far. In this paper, we show that explicit negative examples used for <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improve performance compared to the state-of-the-art approach of Fu et al. (2014) on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2088 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2088/>A Dataset for Multi-Target Stance Detection</a></strong><br><a href=/people/p/parinaz-sobhani/>Parinaz Sobhani</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2088><div class="card-body p-3 small">Current <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for stance classification often treat each target independently, but in many <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>, there exist natural dependencies among targets, e.g., stance towards two or more politicians in an election or towards several brands of the same product. In this paper, we focus on the problem of multi-target stance detection. We present a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that we built for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Furthermore, We experiment with several neural models on the dataset and show that they are more effective in jointly modeling the overall position towards two related targets compared to <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>independent predictions</a> and other models of joint learning, such as cascading classification. We make the new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> publicly available, in order to facilitate further research in multi-target stance classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2090 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2090/>Predicting Emotional Word Ratings using Distributional Representations and Signed Clustering</a></strong><br><a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/d/daniel-preotiuc-pietro/>Daniel Preoţiuc-Pietro</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2090><div class="card-body p-3 small">Inferring the emotional content of words is important for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>text-based sentiment analysis</a>, dialogue systems and <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a>, but word ratings are expensive to collect at scale and across languages or domains. We develop a method that automatically extends word-level ratings to unrated words using signed clustering of vector space word representations along with affect ratings. We use our method to determine a word&#8217;s valence and arousal, which determine its position on the circumplex model of affect, the most popular dimensional model of emotion. Our method achieves superior out-of-sample word rating prediction on both affective dimensions across three different languages when compared to state-of-the-art word similarity based methods. Our method can assist building word ratings for new languages and improve downstream tasks such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2091 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2091/>Attention Modeling for Targeted Sentiment</a></strong><br><a href=/people/j/jiangming-liu/>Jiangming Liu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2091><div class="card-body p-3 small">Neural network models have been used for target-dependent sentiment analysis. Previous work focus on learning a target specific representation for a given input sentence which is used for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. However, they do not explicitly model the contribution of each word in a sentence with respect to targeted sentiment polarities. We investigate an attention model to this end. In particular, a vanilla LSTM model is used to induce an attention value of the whole sentence. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is further extended to differentiate left and right contexts given a certain target following previous work. Results show that by using <a href=https://en.wikipedia.org/wiki/Attention>attention</a> to model the contribution of each word with respect to the target, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> gives significantly improved results over two standard <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarks</a>. We report the best <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2092 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2092" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2092/>EmoBank : Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis<span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>B</span>ank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis</a></strong><br><a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2092><div class="card-body p-3 small">We describe EmoBank, a corpus of 10k English sentences balancing multiple genres, which we annotated with dimensional emotion metadata in the Valence-Arousal-Dominance (VAD) representation format. EmoBank excels with a bi-perspectival and bi-representational design. On the one hand, we distinguish between writer&#8217;s and reader&#8217;s emotions, on the other hand, a subset of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> complements dimensional VAD annotations with categorical ones based on Basic Emotions. We find evidence for the supremacy of the reader&#8217;s perspective in terms of IAA and rating intensity, and achieve close-to-human performance when mapping between dimensional and categorical formats.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2093 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2093/>Structural Attention Neural Networks for improved <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a></a></strong><br><a href=/people/f/filippos-kokkinos/>Filippos Kokkinos</a>
|
<a href=/people/a/alexandros-potamianos/>Alexandros Potamianos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2093><div class="card-body p-3 small">We introduce a tree-structured attention neural network for <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentences</a> and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottom-up and top-down information propagation. Also, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> utilizes structural attention to identify the most salient representations during the construction of the <a href=https://en.wikipedia.org/wiki/Syntactic_tree>syntactic tree</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2094/>Ranking Convolutional Recurrent Neural Networks for Purchase Stage Identification on Imbalanced Twitter Data<span class=acl-fixed-case>T</span>witter Data</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/f/francine-chen/>Francine Chen</a>
|
<a href=/people/y/yan-ying-chen/>Yan-Ying Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2094><div class="card-body p-3 small">Users often use <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> to share their interest in products. We propose to identify purchase stages from Twitter data following the AIDA model (Awareness, Interest, Desire, Action). In particular, we define the task of classifying the purchase stage of each tweet in a user&#8217;s tweet sequence. We introduce RCRNN, a Ranking Convolutional Recurrent Neural Network which computes tweet representations using convolution over word embeddings and models a tweet sequence with gated recurrent units. Also, we consider various methods to cope with the imbalanced label distribution in our data and show that a ranking layer outperforms class weights.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2096 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2096/>Reranking Translation Candidates Produced by Several Bilingual Word Similarity Sources</a></strong><br><a href=/people/l/laurent-jakubina/>Laurent Jakubina</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2096><div class="card-body p-3 small">We investigate the <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a> of the output of several distributional approaches on the Bilingual Lexicon Induction task. We show that reranking an n-best list produced by any of those approaches leads to very substantial improvements. We further demonstrate that combining several n-best lists by <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a> is an effective way of further boosting performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2097 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2097/>Lexicalized Reordering for Left-to-Right Hierarchical Phrase-based Translation</a></strong><br><a href=/people/m/maryam-siahbani/>Maryam Siahbani</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2097><div class="card-body p-3 small">Phrase-based and hierarchical phrase-based (Hiero) translation models differ radically in the way reordering is modeled. Lexicalized reordering models play an important role in phrase-based MT and such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have been added to CKY-based decoders for Hiero. Watanabe et al. (2006) proposed a promising decoding algorithm for Hiero (LR-Hiero) that visits input spans in arbitrary order and produces the translation in left to right (LR) order which leads to far fewer language model calls and leads to a considerable speedup in decoding. We introduce a novel shift-reduce algorithm to LR-Hiero to decode with our lexicalized reordering model (LRM) and show that it improves translation quality for Czech-English, Chinese-English and German-English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2099 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2099/>Addressing Problems across Linguistic Levels in SMT : Combining Approaches to Model <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a>, <a href=https://en.wikipedia.org/wiki/Syntax>Syntax</a> and Lexical Choice<span class=acl-fixed-case>SMT</span>: Combining Approaches to Model Morphology, Syntax and Lexical Choice</a></strong><br><a href=/people/m/marion-weller-di-marco/>Marion Weller-Di Marco</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2099><div class="card-body p-3 small">Many errors in phrase-based SMT can be attributed to problems on three linguistic levels : <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological complexity</a> in the target language, structural differences and <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a>. We explore combinations of linguistically motivated approaches to address these problems in English-to-German SMT and show that they are complementary to one another, but also that the popular verbal pre-ordering can cause problems on the morphological and lexical level. A discriminative classifier can overcome these problems, in particular when enriching standard lexical features with <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> geared towards verbal inflection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2100 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2100" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2100/>Machine Translation of Spanish Personal and Possessive Pronouns Using Anaphora Probabilities<span class=acl-fixed-case>S</span>panish Personal and Possessive Pronouns Using Anaphora Probabilities</a></strong><br><a href=/people/n/ngoc-quang-luong/>Ngoc Quang Luong</a>
|
<a href=/people/a/andrei-popescu-belis/>Andrei Popescu-Belis</a>
|
<a href=/people/a/annette-rios-gonzales/>Annette Rios Gonzales</a>
|
<a href=/people/d/don-tuggener/>Don Tuggener</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2100><div class="card-body p-3 small">We implement a fully probabilistic model to combine the hypotheses of a Spanish anaphora resolution system with those of a Spanish-English machine translation system. The probabilities over antecedents are converted into probabilities for the features of translated pronouns, and are integrated with phrase-based MT using an additional translation model for <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>. The system improves the translation of several Spanish personal and possessive pronouns into <a href=https://en.wikipedia.org/wiki/English_language>English</a>, by solving translation divergencies such as &#8216;ella&#8217; vs. &#8216;she&#8217;/&#8216;it&#8217; or &#8216;su&#8217; vs. &#8216;his&#8217;/&#8216;her&#8217;/&#8216;its&#8217;/&#8216;their&#8217;. On a test set with 2,286 <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>, a baseline system correctly translates 1,055 of them, while ours improves this by 41. Moreover, with oracle antecedents, <a href=https://en.wikipedia.org/wiki/Possessive>possessives</a> are translated with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 83 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2101 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2101/>Using Images to Improve Machine-Translating E-Commerce Product Listings.<span class=acl-fixed-case>E</span>-Commerce Product Listings.</a></strong><br><a href=/people/i/iacer-calixto/>Iacer Calixto</a>
|
<a href=/people/d/daniel-stein/>Daniel Stein</a>
|
<a href=/people/e/evgeny-matusov/>Evgeny Matusov</a>
|
<a href=/people/p/pintu-lohar/>Pintu Lohar</a>
|
<a href=/people/s/sheila-castilho/>Sheila Castilho</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2101><div class="card-body p-3 small">In this paper we study the impact of using <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> to machine-translate user-generated e-commerce product listings. We study how a multi-modal Neural Machine Translation (NMT) model compares to two text-only approaches : a conventional state-of-the-art attentional NMT and a Statistical Machine Translation (SMT) model. User-generated product listings often do not constitute <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>grammatical or well-formed sentences</a>. More often than not, they consist of the juxtaposition of short phrases or <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a>. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> end-to-end as well as use text-only and multi-modal NMT models for re-ranking n-best lists generated by an SMT model. We qualitatively evaluate our user-generated training data also analyse how adding synthetic data impacts the results. We evaluate our models quantitatively using BLEU and TER and find that (i) additional synthetic data has a general positive impact on text-only and multi-modal NMT models, and that (ii) using a multi-modal NMT model for re-ranking n-best lists improves TER significantly across different n-best list sizes.<tex-math>n</tex-math>-best lists generated by an SMT model. We qualitatively evaluate our user-generated training data also analyse how adding synthetic data impacts the results. We evaluate our models quantitatively using BLEU and TER and find that (i) additional synthetic data has a general positive impact on text-only and multi-modal NMT models, and that (ii) using a multi-modal NMT model for re-ranking n-best lists improves TER significantly across different n-best list sizes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2105 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2105/>Large-Scale Categorization of Japanese Product Titles Using Neural Attention Models<span class=acl-fixed-case>J</span>apanese Product Titles Using Neural Attention Models</a></strong><br><a href=/people/y/yandi-xia/>Yandi Xia</a>
|
<a href=/people/a/aaron-levine/>Aaron Levine</a>
|
<a href=/people/p/pradipto-das/>Pradipto Das</a>
|
<a href=/people/g/giuseppe-di-fabbrizio/>Giuseppe Di Fabbrizio</a>
|
<a href=/people/k/keiji-shinzato/>Keiji Shinzato</a>
|
<a href=/people/a/ankur-datta/>Ankur Datta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2105><div class="card-body p-3 small">We propose a variant of Convolutional Neural Network (CNN) models, the Attention CNN (ACNN) ; for large-scale categorization of millions of Japanese items into thirty-five product categories. Compared to a state-of-the-art Gradient Boosted Tree (GBT) classifier, the proposed model reduces training time from three weeks to three days while maintaining more than 96 % accuracy. Additionally, our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> characterizes products by imputing attentive focus on word tokens in a language agnostic way. The attention words have been observed to be semantically highly correlated with the predicted categories and give us a choice of automatic feature extraction for <a href=https://en.wikipedia.org/wiki/Downstream_processing>downstream processing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2106 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2106/>Convolutional Neural Networks for Authorship Attribution of Short Texts</a></strong><br><a href=/people/p/prasha-shrestha/>Prasha Shrestha</a>
|
<a href=/people/s/sebastian-sierra/>Sebastian Sierra</a>
|
<a href=/people/f/fabio-a-gonzalez/>Fabio González</a>
|
<a href=/people/m/manuel-montes/>Manuel Montes</a>
|
<a href=/people/p/paolo-rosso/>Paolo Rosso</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2106><div class="card-body p-3 small">We present a model to perform <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship attribution</a> of tweets using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks (CNNs)</a> over <a href=https://en.wikipedia.org/wiki/N-gram>character n-grams</a>. We also present a strategy that improves model interpretability by estimating the importance of input text fragments in the predicted classification. The experimental evaluation shows that text CNNs perform competitively and are able to outperform previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2108 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2108/>On the Relevance of Syntactic and Discourse Features for Author Profiling and Identification</a></strong><br><a href=/people/j/juan-soler-company/>Juan Soler-Company</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2108><div class="card-body p-3 small">The majority of approaches to <a href=https://en.wikipedia.org/wiki/Author_profiling>author profiling</a> and author identification focus mainly on <a href=https://en.wikipedia.org/wiki/Lexicon>lexical features</a>, i.e., on the content of a text. We argue that syntactic and discourse features play a significantly more prominent role than they were given in the past. We show that they achieve state-of-the-art performance in author and gender identification on a <a href=https://en.wikipedia.org/wiki/Text_corpus>literary corpus</a> while keeping the feature set small : the used feature set is composed of only 188 features and still outperforms the winner of the PAN 2014 shared task on author verification in the <a href=https://en.wikipedia.org/wiki/Literary_genre>literary genre</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2109 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2109/>Unsupervised Cross-Lingual Scaling of Political Texts</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/f/federico-nanni/>Federico Nanni</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2109><div class="card-body p-3 small">Political text scaling aims to linearly order parties and politicians across <a href=https://en.wikipedia.org/wiki/Political_dimension>political dimensions</a> (e.g., <a href=https://en.wikipedia.org/wiki/Left&#8211;right_political_spectrum>left-to-right ideology</a>) based on <a href=https://en.wikipedia.org/wiki/Content_(media)>textual content</a> (e.g., <a href=https://en.wikipedia.org/wiki/Public_speaking>politician speeches</a> or party manifestos). Existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> scale texts based on relative word usage and can not be used for cross-lingual analyses. Additionally, there is little quantitative evidence that the output of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> correlates with common political dimensions like left-to-right orientation. Experimental results show that the semantically-informed scaling models better predict the party positions than the existing word-based models in two different political dimensions. Furthermore, the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> exhibit no drop in performance in the cross-lingual compared to monolingual setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2111 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2111/>Multimodal Topic Labelling</a></strong><br><a href=/people/i/ionut-sorodoc/>Ionut Sorodoc</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a>
|
<a href=/people/n/nikolaos-aletras/>Nikolaos Aletras</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2111><div class="card-body p-3 small">Topics generated by topic models are typically presented as a list of topic terms. Automatic topic labelling is the task of generating a succinct label that summarises the theme or subject of a topic, with the intention of reducing the cognitive load of end-users when interpreting these topics. Traditionally, topic label systems focus on a single label modality, e.g. textual labels. In this work we propose a multimodal approach to topic labelling using a simple <a href=https://en.wikipedia.org/wiki/Feedforward_neural_network>feedforward neural network</a>. Given a topic and a candidate image or textual label, our method automatically generates a rating for the label, relative to the topic. Experiments show that this multimodal approach outperforms single-modality topic labelling systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2112 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2112/>Detecting (Un)Important Content for Single-Document News Summarization</a></strong><br><a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/f/forrest-bao/>Forrest Bao</a>
|
<a href=/people/a/ani-nenkova/>Ani Nenkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2112><div class="card-body p-3 small">We present a robust approach for detecting intrinsic sentence importance in <a href=https://en.wikipedia.org/wiki/News>news</a>, by training on two corpora of document-summary pairs. When used for single-document summarization, our approach, combined with the beginning of document heuristic, outperforms a state-of-the-art summarizer and the beginning-of-article baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for <a href=https://en.wikipedia.org/wiki/News>news</a> have not been able to consistently outperform the strong beginning-of-article baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2113 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2113/>F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media<span class=acl-fixed-case>F</span>-Score Driven Max Margin Neural Network for Named Entity Recognition in <span class=acl-fixed-case>C</span>hinese Social Media</a></strong><br><a href=/people/h/hangfeng-he/>Hangfeng He</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2113><div class="card-body p-3 small">We focus on named entity recognition (NER) for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning model</a> based on B-LSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine <a href=https://en.wikipedia.org/wiki/Transition_probability>transition probability</a> with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> in our model. To bridge the gap between label accuracy and <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of NER, we construct a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> which can be directly trained on <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>. When considering the instability of <a href=https://en.wikipedia.org/wiki/F-score>F-score driven method</a> and meaningful information provided by label accuracy, we propose an integrated method to train on both <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> and label accuracy. Our integrated model yields 7.44 % improvement over previous state-of-the-art result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2114 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2114/>Discriminative Information Retrieval for Question Answering Sentence Selection</a></strong><br><a href=/people/t/tongfei-chen/>Tongfei Chen</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2114><div class="card-body p-3 small">We propose a framework for discriminative IR atop linguistic features, trained to improve the recall of answer candidate passage retrieval, the initial step in text-based question answering. We formalize this as an instance of linear feature-based IR, demonstrating a 34%-43 % improvement in <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> for <a href=https://en.wikipedia.org/wiki/Triage>candidate triage</a> for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2115 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2115/>Effective shared representations with <a href=https://en.wikipedia.org/wiki/Multitask_learning>Multitask Learning</a> for Community Question Answering</a></strong><br><a href=/people/d/daniele-bonadiman/>Daniele Bonadiman</a>
|
<a href=/people/a/antonio-uva/>Antonio Uva</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2115><div class="card-body p-3 small">An important asset of using Deep Neural Networks (DNNs) for text applications is their ability to automatically engineering features. Unfortunately, DNNs usually require a lot of training data, especially for highly semantic tasks such as community Question Answering (cQA). In this paper, we tackle the problem of data scarcity by learning the target <a href=https://en.wikipedia.org/wiki/Deep_learning>DNN</a> together with two auxiliary tasks in a multitask learning setting. We exploit the strong semantic connection between selection of comments relevant to (i) new questions and (ii) forum questions. This enables a global representation for comments, new and previous questions. The experiments of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a SemEval challenge dataset for cQA show a 20 % of relative improvement over standard DNNs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2117 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2117/>Temporal information extraction from clinical text</a></strong><br><a href=/people/j/julien-tourille/>Julien Tourille</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2117><div class="card-body p-3 small">In this paper, we present a method for temporal relation extraction from clinical narratives in <a href=https://en.wikipedia.org/wiki/French_language>French</a> and in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We experiment on two comparable corpora, the MERLOT corpus and the THYME corpus, and show that a common approach can be used for both languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2118 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2118/>Neural Temporal Relation Extraction</a></strong><br><a href=/people/d/dmitriy-dligach/>Dmitriy Dligach</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/c/chen-lin/>Chen Lin</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2118><div class="card-body p-3 small">We experiment with neural architectures for temporal relation extraction and establish a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for several scenarios. We find that neural models with only tokens as input outperform state-of-the-art hand-engineered feature-based models, that convolutional neural networks outperform LSTM models, and that encoding relation arguments with XML tags outperforms a traditional position-based encoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2119 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2119/>End-to-End Trainable Attentive Decoder for Hierarchical Entity Classification</a></strong><br><a href=/people/s/sanjeev-karn/>Sanjeev Karn</a>
|
<a href=/people/u/ulli-waltinger/>Ulli Waltinger</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2119><div class="card-body p-3 small">We address fine-grained entity classification and propose a novel attention-based recurrent neural network (RNN) encoder-decoder that generates paths in the type hierarchy and can be trained end-to-end. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better on fine-grained entity classification than prior work that relies on flat or local classifiers that do not directly model hierarchical structure.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>