<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2021.rocling-1.pdf>Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021)</a></h2><p class=lead><a href=/people/l/lung-hao-lee/>Lung-Hao Lee</a>,
<a href=/people/c/chia-hui-chang/>Chia-Hui Chang</a>,
<a href=/people/k/kuan-yu-chen/>Kuan-Yu Chen</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.rocling-1</dd><dt>Month:</dt><dd>October</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Taoyuan, Taiwan</dd><dt>Venue:</dt><dd><a href=/venues/rocling/>ROCLING</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.rocling-1>https://aclanthology.org/2021.rocling-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.rocling-1.pdf>https://aclanthology.org/2021.rocling-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.rocling-1.pdf title="Open PDF of 'Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+33rd+Conference+on+Computational+Linguistics+and+Speech+Processing+%28ROCLING+2021%29" title="Search for 'Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.0/>Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021)</a></strong><br><a href=/people/l/lung-hao-lee/>Lung-Hao Lee</a>
|
<a href=/people/c/chia-hui-chang/>Chia-Hui Chang</a>
|
<a href=/people/k/kuan-yu-chen/>Kuan-Yu Chen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.2/>A Study on Using <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a> to Improve BERT Model for Emotional Classification of <a href=https://en.wikipedia.org/wiki/Chinese_poetry>Chinese Lyrics</a><span class=acl-fixed-case>BERT</span> Model for Emotional Classification of <span class=acl-fixed-case>C</span>hinese Lyrics</a></strong><br><a href=/people/j/jia-yi-liao/>Jia-Yi Liao</a>
|
<a href=/people/y/ya-hsuan-lin/>Ya-Hsuan Lin</a>
|
<a href=/people/k/kuan-cheng-lin/>Kuan-Cheng Lin</a>
|
<a href=/people/j/jia-wei-chang/>Jia-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--2><div class="card-body p-3 small">The explosive growth of <a href=https://en.wikipedia.org/wiki/Music_library>music libraries</a> has made <a href=https://en.wikipedia.org/wiki/Music_information_retrieval>music information retrieval</a> and recommendation a critical issue. Recommendation systems based on music emotion recognition are gradually gaining attention. Most of the studies focus on <a href=https://en.wikipedia.org/wiki/Audio_signal_processing>audio data</a> rather than <a href=https://en.wikipedia.org/wiki/Lyrics>lyrics</a> to build models of music emotion classification. In addition, because of the richness of <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English language resources</a>, most of the existing studies are focused on <a href=https://en.wikipedia.org/wiki/English_language>English lyrics</a> but rarely on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. For this reason, We propose an approach that uses the BERT pretraining model and <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer learning</a> to improve the emotion classification task of Chinese lyrics. The following approaches were used without any specific training for the Chinese lyrics emotional classification task : (a) Using BERT, only can reach 50 % of the classification accuracy. (b) Using BERT with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> of CVAW, CVAP, and CVAT datasets can achieve 71 % classification accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.3/>Nested Named Entity Recognition for Chinese Electronic Health Records with QA-based Sequence Labeling<span class=acl-fixed-case>C</span>hinese Electronic Health Records with <span class=acl-fixed-case>QA</span>-based Sequence Labeling</a></strong><br><a href=/people/y/yu-lun-chiang/>Yu-Lun Chiang</a>
|
<a href=/people/c/chih-hao-lin/>Chih-Hao Lin</a>
|
<a href=/people/c/cheng-lung-sung/>Cheng-Lung Sung</a>
|
<a href=/people/k/keh-yih-su/>Keh-Yih Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--3><div class="card-body p-3 small">This study presents a novel QA-based sequence labeling (QASL) approach to naturally tackle both flat and nested Named Entity Recogntion (NER) tasks on a Chinese Electronic Health Records (CEHRs) dataset. This proposed QASL approach parallelly asks a corresponding natural language question for each specific named entity type, and then identifies those associated NEs of the same specified type with the BIO tagging scheme. The associated nested NEs are then formed by overlapping the results of various types. In comparison with those pure sequence-labeling (SL) approaches, since the given question includes significant prior knowledge about the specified entity type and the capability of extracting NEs with different types, the performance for nested NER task is thus improved, obtaining 90.70 % of F1-score. Besides, in comparison with the pure QA-based approach, our proposed approach retains the SL features, which could extract multiple NEs with the same types without knowing the exact number of NEs in the same passage in advance. Eventually, experiments on our CEHR dataset demonstrate that QASL-based models greatly outperform the SL-based models by 6.12 % to 7.14 % of F1-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.4/>AI Clerk Platform : Information Extraction DIY Platform<span class=acl-fixed-case>AI</span> Clerk Platform : Information Extraction <span class=acl-fixed-case>DIY</span> Platform</a></strong><br><a href=/people/r/ru-yng-chang/>Ru-Yng Chang</a>
|
<a href=/people/w/wen-lun-chen/>Wen-Lun Chen</a>
|
<a href=/people/c/cheng-ju-kao/>Cheng-Ju Kao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--4><div class="card-body p-3 small">Information extraction is a core technology of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, which extracts some meaningful phrases / clauses from unstructured or semistructured content to a particular topic. It can be said to be the core technology of many <a href=https://en.wikipedia.org/wiki/List_of_programming_languages_by_type>language technologies</a> and applications. This paper introduces AI Clerk Platform, which aims to accelerate and improve the entire process and convenience of the development of information extraction tools. AI Clerk Platform provides a friendly and intuitive visualized manual labeling interface, sets suitable semantic label in need, and implements, distributes and controls manual labeling tasks, so that users can complete customized information extraction models without programming and view the automatically predict results of models by three method. AI Clerk Platform further assists in the development of other natural language processing technologies and the derivation of application services.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.6/>Speech Emotion Recognition Based on CNN+LSTM Model<span class=acl-fixed-case>CNN</span>+<span class=acl-fixed-case>LSTM</span> Model</a></strong><br><a href=/people/w/wei-mou/>Wei Mou</a>
|
<a href=/people/p/pei-hsuan-shen/>Pei-Hsuan Shen</a>
|
<a href=/people/c/chu-yun-chu/>Chu-Yun Chu</a>
|
<a href=/people/y/yu-cheng-chiu/>Yu-Cheng Chiu</a>
|
<a href=/people/t/tsung-hsien-yang/>Tsung-Hsien Yang</a>
|
<a href=/people/m/ming-hsiang-su/>Ming-Hsiang Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--6><div class="card-body p-3 small">Due to the popularity of intelligent dialogue assistant services, speech emotion recognition has become more and more important. In the communication between humans and machines, <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> and emotion analysis can enhance the interaction between machines and humans. This study uses the CNN+LSTM model to implement speech emotion recognition (SER) processing and prediction. From the experimental results, it is known that using the CNN+LSTM model achieves better performance than using the traditional NN model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.7/>A Study on Contextualized Language Modeling for Machine Reading Comprehension</a></strong><br><a href=/people/c/chin-ying-wu/>Chin-Ying Wu</a>
|
<a href=/people/y/yung-chang-hsu/>Yung-Chang Hsu</a>
|
<a href=/people/b/berlin-chen/>Berlin Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--7><div class="card-body p-3 small">With the recent breakthrough of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning technologies</a>, research on machine reading comprehension (MRC) has attracted much attention and found its versatile applications in many use cases. MRC is an important natural language processing (NLP) task aiming to assess the ability of a machine to understand natural language expressions, which is typically operationalized by first asking questions based on a given text paragraph and then receiving machine-generated answers in accordance with the given context paragraph and questions. In this paper, we leverage two novel pretrained language models built on top of Bidirectional Encoder Representations from Transformers (BERT), namely BERT-wwm and MacBERT, to develop effective MRC methods. In addition, we also seek to investigate whether additional incorporation of the categorical information about a context paragraph can benefit MRC or not, which is achieved based on performing context paragraph clustering on the training dataset. On the other hand, an ensemble learning approach is proposed to harness the synergistic power of the aforementioned two BERT-based models so as to further promote MRC performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.12/>Discussion on domain generalization in the cross-device speaker verification system</a></strong><br><a href=/people/w/wei-ting-lin/>Wei-Ting Lin</a>
|
<a href=/people/y/yu-jia-zhang/>Yu-Jia Zhang</a>
|
<a href=/people/c/chia-ping-chen/>Chia-Ping Chen</a>
|
<a href=/people/c/chung-li-lu/>Chung-Li Lu</a>
|
<a href=/people/b/bo-cheng-chan/>Bo-Cheng Chan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--12><div class="card-body p-3 small">In this paper, we use domain generalization to improve the performance of the cross-device speaker verification system. Based on a trainable speaker verification system, we use domain generalization algorithms to fine-tune the model parameters. First, we use the VoxCeleb2 dataset to train ECAPA-TDNN as a baseline model. Then, use the CHT-TDSV dataset and the following domain generalization algorithms to fine-tune it : DANN, CDNN, Deep CORAL. Our proposed system tests 10 different scenarios in the NSYSU-TDSV dataset, including a single device and multiple devices. Finally, in the scenario of multiple devices, the best equal error rate decreased from 18.39 in the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> to 8.84. Successfully achieved cross-device identification on the speaker verification system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.rocling-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.13/>Integrated Semantic and Phonetic Post-correction for Chinese Speech Recognition<span class=acl-fixed-case>C</span>hinese Speech Recognition</a></strong><br><a href=/people/y/yi-chang-chen/>Yi-Chang Chen</a>
|
<a href=/people/c/chun-yen-cheng/>Chun-Yen Cheng</a>
|
<a href=/people/c/chien-an-chen/>Chien-An Chen</a>
|
<a href=/people/m/ming-chieh-sung/>Ming-Chieh Sung</a>
|
<a href=/people/y/yi-ren-yeh/>Yi-Ren Yeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--13><div class="card-body p-3 small">Due to the recent advances of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, several works have applied the pre-trained masked language model (MLM) of BERT to the post-correction of speech recognition. However, existing pre-trained models only consider the <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic correction</a> while the <a href=https://en.wikipedia.org/wiki/Phoneme>phonetic features</a> of words is neglected. The semantic-only post-correction will consequently decrease the performance since homophonic errors are fairly common in Chinese ASR. In this paper, we proposed a novel approach to collectively exploit the contextualized representation and the phonetic information between the error and its replacing candidates to alleviate the <a href=https://en.wikipedia.org/wiki/Error_rate>error rate</a> of Chinese ASR. Our experiment results on real world speech recognition datasets showed that our proposed method has evidently lower <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>CER</a> than the baseline model, which utilized a pre-trained BERT MLM as the corrector.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.14/>A Preliminary Study on Environmental Sound Classification Leveraging Large-Scale Pretrained Model and <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>Semi-Supervised Learning</a></a></strong><br><a href=/people/y/you-sheng-tsao/>You-Sheng Tsao</a>
|
<a href=/people/t/tien-hong-lo/>Tien-Hong Lo</a>
|
<a href=/people/j/jiun-ting-li/>Jiun-Ting Li</a>
|
<a href=/people/s/shi-yan-weng/>Shi-Yan Weng</a>
|
<a href=/people/b/berlin-chen/>Berlin Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--14><div class="card-body p-3 small">With the widespread commercialization of smart devices, research on environmental sound classification has gained more and more attention in recent years. In this paper, we set out to make effective use of large-scale audio pretrained model and <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised model training paradigm</a> for environmental sound classification. To this end, an environmental sound classification method is first put forward, whose <a href=https://en.wikipedia.org/wiki/Component_model>component model</a> is built on top a large-scale audio pretrained model. Further, to simulate a low-resource sound classification setting where only limited supervised examples are made available, we instantiate the notion of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> with a recently proposed training algorithm (namely, FixMatch) and a data augmentation method (namely, SpecAugment) to achieve the goal of semi-supervised model training. Experiments conducted on bench-mark dataset UrbanSound8 K reveal that our classification method can lead to an accuracy improvement of 2.4 % in relation to a current <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.15/>Mining Commonsense and Domain Knowledge from Math Word Problems</a></strong><br><a href=/people/s/shih-hung-tsai/>Shih-Hung Tsai</a>
|
<a href=/people/c/chao-chun-liang/>Chao-Chun Liang</a>
|
<a href=/people/h/hsin-min-wang/>Hsin-Min Wang</a>
|
<a href=/people/k/keh-yih-su/>Keh-Yih Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--15><div class="card-body p-3 small">Current neural math solvers learn to incorporate commonsense or domain knowledge by utilizing pre-specified constants or formulas. However, as these constants and formulas are mainly human-specified, the generalizability of the <a href=https://en.wikipedia.org/wiki/Solver>solvers</a> is limited. In this paper, we propose to explicitly retrieve the required knowledge from math problemdatasets. In this way, we can determinedly characterize the required knowledge andimprove the explainability of solvers. Our two <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> take the problem text andthe <a href=https://en.wikipedia.org/wiki/Equation_solving>solution equations</a> as input. Then, they try to deduce the required commonsense and domain knowledge by integrating information from both parts. We construct two math datasets and show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that they can retrieve the required knowledge for <a href=https://en.wikipedia.org/wiki/Problem_solving>problem-solving</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.22/>A BERT-based Siamese-structured Retrieval Model<span class=acl-fixed-case>BERT</span>-based <span class=acl-fixed-case>S</span>iamese-structured Retrieval Model</a></strong><br><a href=/people/h/hung-yun-chiang/>Hung-Yun Chiang</a>
|
<a href=/people/k/kuan-yu-chen/>Kuan-Yu Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--22><div class="card-body p-3 small">Due to the development of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, the natural language processing tasks have made great progresses by leveraging the bidirectional encoder representations from Transformers (BERT). The goal of <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> is to search the most relevant results for the user&#8217;s query from a large set of documents. Although BERT-based retrieval models have shown excellent results in many studies, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> usually suffer from the need for large amounts of computations and/or additional storage spaces. In view of the flaws, a BERT-based Siamese-structured retrieval model (BESS) is proposed in this paper. BESS not only inherits the merits of pre-trained language models, but also can generate extra information to compensate the original query automatically. Besides, the <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning strategy</a> is introduced to make the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> more robust. Accordingly, we evaluate BESS on three public-available corpora, and the experimental results demonstrate the efficiency of the proposed retrieval model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.27/>Using Valence and Arousal-infused Bi-LSTM for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> in Social Media Product Reviews<span class=acl-fixed-case>B</span>i-<span class=acl-fixed-case>LSTM</span> for Sentiment Analysis in Social Media Product Reviews</a></strong><br><a href=/people/y/yu-ya-cheng/>Yu-Ya Cheng</a>
|
<a href=/people/w/wen-chao-yeh/>Wen-Chao Yeh</a>
|
<a href=/people/y/yan-ming-chen/>Yan-Ming Chen</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--27><div class="card-body p-3 small">With the popularity of the current Internet age, online social platforms have provided a bridge for communication between private companies, public organizations, and the public. The purpose of this research is to understand the user&#8217;s experience of the product by analyzing product review data in different fields. We propose a BiLSTM-based neural network which infused rich emotional information. In addition to consider <a href=https://en.wikipedia.org/wiki/Valence_(psychology)>Valence</a> and Arousal which is the smallest morpheme of emotional information, the dependence relationship between texts is also integrated into the deep learning model to analyze the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>. The experimental results show that this <a href=https://en.wikipedia.org/wiki/Research>research</a> can achieve good performance in predicting the vocabulary Valence and Arousal. In addition, the integration of VA and dependency information into the BiLSTM model can have excellent performance for social text sentiment analysis, which verifies that this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is effective in emotion recognition of social medial short text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.29/>Aggregating User-Centric and Post-Centric Sentiments from <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> for Topical Stance Prediction</a></strong><br><a href=/people/j/jenq-haur-wang/>Jenq-Haur Wang</a>
|
<a href=/people/k/kuan-ting-chen/>Kuan-Ting Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--29><div class="card-body p-3 small">Conventional <a href=https://en.wikipedia.org/wiki/Opinion_poll>opinion polls</a> were usually conducted via questionnaires or phone interviews, which are time-consuming and error-prone. With the advances in <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking platforms</a>, it&#8217;s easier for the general public to express their opinions on popular topics. Given the huge amount of user opinions, it would be useful if we can automatically collect and aggregate the overall topical stance for a specific topic. In this paper, we propose to predict topical stances from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> by concept expansion, sentiment classification, and stance aggregation based on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. For concept expansion of a given topic, related posts are collected from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and clustered by <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Then, major <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> are extracted by <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> and named entity recognition methods. For sentiment classification and aggregation, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>machine learning methods</a> are used to train sentiment lexicon with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Then, the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment scores</a> from user-centric and post-centric views are aggregated as the total stance on the topic. In the experiments, we evaluated the performance of our proposed approach using <a href=https://en.wikipedia.org/wiki/Social_media>social media data</a> from <a href=https://en.wikipedia.org/wiki/Internet_forum>online forums</a>. The experimental results for 2016 Taiwan Presidential Election showed that our proposed method can effectively expand keywords and aggregate topical stances from the public for accurate prediction of election results. The best performance is 0.52 % in terms of <a href=https://en.wikipedia.org/wiki/Mean_absolute_error>mean absolute error (MAE)</a>. Further investigation is needed to evaluate the performance of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in larger scales.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.31/>Hidden Advertorial Detection on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> in Chinese<span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/m/meng-ching-ho/>Meng-Ching Ho</a>
|
<a href=/people/c/ching-yun-chuang/>Ching-Yun Chuang</a>
|
<a href=/people/y/yi-chun-hsu/>Yi-Chun Hsu</a>
|
<a href=/people/y/yu-yun-chang/>Yu-Yun Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--31><div class="card-body p-3 small">Nowadays, there are a lot of <a href=https://en.wikipedia.org/wiki/Advertising>advertisements</a> hiding as normal posts or experience sharing in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. There is little research of advertorial detection on <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin Chinese texts</a>. This paper thus aimed to focus on hidden advertorial detection of <a href=https://en.wikipedia.org/wiki/Online_advertising>online posts</a> in <a href=https://en.wikipedia.org/wiki/Taiwanese_Mandarin>Taiwan Mandarin Chinese</a>. We inspected seven <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual features</a> based on <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic theories</a> in <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse level</a>. These features can be further grouped into three schemas under the general advertorial writing structure. We further implemented these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to train a multi-task BERT model to detect <a href=https://en.wikipedia.org/wiki/Advertorial>advertorials</a>. The results suggested that specific <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> would help extract <a href=https://en.wikipedia.org/wiki/Advertorial>advertorials</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.32/>Automatic Extraction of English Grammar Pattern Correction Rules<span class=acl-fixed-case>E</span>nglish Grammar Pattern Correction Rules</a></strong><br><a href=/people/k/kuan-yu-shen/>Kuan-Yu Shen</a>
|
<a href=/people/y/yi-chien-lin/>Yi-Chien Lin</a>
|
<a href=/people/j/jason-s-chang/>Jason S. Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--32><div class="card-body p-3 small">We introduce a method for generating error-correction rules for grammar pattern errors in a given annotated learner corpus. In our approach, annotated edits in the learner corpus are converted into edit rules for correcting common writing errors. The method involves automatic extraction of grammar patterns, and automatic alignment of the erroneous patterns and correct patterns. At run-time, <a href=https://en.wikipedia.org/wiki/Pattern_recognition>grammar patterns</a> are extracted from the grammatically correct sentences, and correction rules are retrieved by aligning the extracted <a href=https://en.wikipedia.org/wiki/Pattern_recognition>grammar patterns</a> with the erroneous patterns. Using the proposed method, we generate 1,499 high-quality correction rules related to 232 <a href=https://en.wikipedia.org/wiki/Headword>headwords</a>. The method can be used to assist <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>ESL students</a> in avoiding <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a>, and aid teachers in correcting students&#8217; essays. Additionally, the method can be used in the compilation of collocation error dictionaries and the construction of grammar error correction systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.39/>Learning to Find Translation of Grammar Patterns in Parallel Corpus</a></strong><br><a href=/people/k/kai-wen-tuan/>Kai-Wen Tuan</a>
|
<a href=/people/y/yi-jyun-chen/>Yi-Jyun Chen</a>
|
<a href=/people/y/yi-chien-lin/>Yi-Chien Lin</a>
|
<a href=/people/c/chun-ho-kwok/>Chun-Ho Kwok</a>
|
<a href=/people/h/hai-lun-tu/>Hai-Lun Tu</a>
|
<a href=/people/j/jason-s-chang/>Jason S. Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--39><div class="card-body p-3 small">We introduce a method for assisting English as Second Language (ESL) learners by providing translations of Collins COBUILD grammar patterns(GP) for a given word. In our approach, bilingual parallel corpus is transformed into bilingual GP pairs aimed at providing native language support for learning word usage through GPs. The method involves automatically parsing sentences to extract GPs, automatically generating translation GP pairs from bilingual sentences, and automatically extracting common bilingual GPs. At run-time, the target word is used for lookup GPs and translations, and the retrieved common GPs and their example sentences are shown to the user. We present a prototype phrase search engine, Linggle GPTrans, that implements the methods to assist <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>ESL learners</a>. Preliminary evaluation on a set of more than 300 GP-translation pairs shows that the methods achieve 91 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.49/>SoochowDS at ROCLING-2021 Shared Task : Text Sentiment Analysis Using BERT and LSTM<span class=acl-fixed-case>S</span>oochow<span class=acl-fixed-case>DS</span> at <span class=acl-fixed-case>ROCLING</span>-2021 Shared Task: Text Sentiment Analysis Using <span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/r/ruei-cyuan-su/>Ruei-Cyuan Su</a>
|
<a href=/people/s/sig-seong-chong/>Sig-Seong Chong</a>
|
<a href=/people/t/tzu-en-su/>Tzu-En Su</a>
|
<a href=/people/m/ming-hsiang-su/>Ming-Hsiang Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--49><div class="card-body p-3 small">In this shared task, this paper proposes a method to combine the BERT-based word vector model and the LSTM prediction model to predict the Valence and Arousal values in the text. Among them, the BERT-based word vector is 768-dimensional, and each word vector in the sentence is sequentially fed to the LSTM model for prediction. The experimental results show that the performance of our proposed method is better than the results of the <a href=https://en.wikipedia.org/wiki/Lasso_regression>Lasso Regression model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.50/>NCU-NLP at ROCLING-2021 Shared Task : Using MacBERT Transformers for Dimensional Sentiment Analysis<span class=acl-fixed-case>NCU</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>ROCLING</span>-2021 Shared Task: Using <span class=acl-fixed-case>M</span>ac<span class=acl-fixed-case>BERT</span> Transformers for Dimensional Sentiment Analysis</a></strong><br><a href=/people/m/man-chen-hung/>Man-Chen Hung</a>
|
<a href=/people/c/chao-yi-chen/>Chao-Yi Chen</a>
|
<a href=/people/p/pin-jung-chen/>Pin-Jung Chen</a>
|
<a href=/people/l/lung-hao-lee/>Lung-Hao Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--50><div class="card-body p-3 small">We use the MacBERT transformers and fine-tune them to ROCLING-2021 shared tasks using the CVAT and CVAS data. We compare the performance of MacBERT with the other two transformers BERT and RoBERTa in the valence and arousal dimensions, respectively. MAE and correlation coefficient (r) were used as evaluation metrics. On ROCLING-2021 test set, our used MacBERT model achieves 0.611 of <a href=https://en.wikipedia.org/wiki/Major_histocompatibility_complex>MAE</a> and 0.904 of r in the <a href=https://en.wikipedia.org/wiki/Valence_(psychology)>valence dimensions</a> ; and 0.938 of <a href=https://en.wikipedia.org/wiki/Major_histocompatibility_complex>MAE</a> and 0.549 of r in the arousal dimension.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.rocling-1.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--rocling-1--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.rocling-1.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.rocling-1.51/>ROCLING-2021 Shared Task : <a href=https://en.wikipedia.org/wiki/Dimensional_analysis>Dimensional Sentiment Analysis</a> for Educational Texts<span class=acl-fixed-case>ROCLING</span>-2021 Shared Task: Dimensional Sentiment Analysis for Educational Texts</a></strong><br><a href=/people/l/liang-chih-yu/>Liang-Chih Yu</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/b/bo-peng/>Bo Peng</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--rocling-1--51><div class="card-body p-3 small">This paper presents the ROCLING 2021 shared task on dimensional sentiment analysis for educational texts which seeks to identify a real-value sentiment score of self-evaluation comments written by Chinese students in the both valence and arousal dimensions. Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and <a href=https://en.wikipedia.org/wiki/Arousal>arousal</a> represents the degree of excitement and calm. Of the 7 teams registered for this shared task for two-dimensional sentiment analysis, 6 submitted results. We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques for the educational domain. All data sets with gold standards and scoring script are made publicly available to researchers.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>