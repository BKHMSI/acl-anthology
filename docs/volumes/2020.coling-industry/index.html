<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 28th International Conference on Computational Linguistics: Industry Track - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the 28th International Conference on Computational Linguistics: Industry Track</h2><p class=lead><a href=/people/a/ann-clifton/>Ann Clifton</a>,
<a href=/people/c/courtney-napoles/>Courtney Napoles</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.coling-industry</dd><dt>Month:</dt><dd>December</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venue:</dt><dd><a href=/venues/coling/>COLING</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>International Committee on Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.coling-industry>https://aclanthology.org/2020.coling-industry</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+28th+International+Conference+on+Computational+Linguistics%3A+Industry+Track" title="Search for 'Proceedings of the 28th International Conference on Computational Linguistics: Industry Track' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.0/>Proceedings of the 28th International Conference on Computational Linguistics: Industry Track</a></strong><br><a href=/people/a/ann-clifton/>Ann Clifton</a>
|
<a href=/people/c/courtney-napoles/>Courtney Napoles</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.4/>Query Distillation : BERT-based Distillation for Ensemble Ranking<span class=acl-fixed-case>BERT</span>-based Distillation for Ensemble Ranking</a></strong><br><a href=/people/w/wangshu-zhang/>Wangshu Zhang</a>
|
<a href=/people/j/junhong-liu/>Junhong Liu</a>
|
<a href=/people/z/zujie-wen/>Zujie Wen</a>
|
<a href=/people/y/yafang-wang/>Yafang Wang</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--4><div class="card-body p-3 small">Recent years have witnessed substantial progress in the development of neural ranking networks, but also an increasingly heavy computational burden due to growing numbers of parameters and the adoption of model ensembles. Knowledge Distillation (KD) is a common solution to balance the effectiveness and efficiency. However, it is not straightforward to apply KD to ranking problems. Ranking Distillation (RD) has been proposed to address this issue, but only shows effectiveness on recommendation tasks. We present a novel two-stage distillation method for ranking problems that allows a smaller student model to be trained while benefitting from the better performance of the teacher model, providing better control of the inference latency and computational burden. We design a novel BERT-based ranking model structure for list-wise ranking to serve as our student model. All ranking candidates are fed to the BERT model simultaneously, such that the self-attention mechanism can enable joint inference to rank the document list. Our experiments confirm the advantages of our method, not just with regard to the inference latency but also in terms of higher-quality rankings compared to the original teacher model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.8/>Interactive Question Clarification in Dialogue via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/x/xiang-hu/>Xiang Hu</a>
|
<a href=/people/z/zujie-wen/>Zujie Wen</a>
|
<a href=/people/y/yafang-wang/>Yafang Wang</a>
|
<a href=/people/x/xiaolong-li/>Xiaolong Li</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--8><div class="card-body p-3 small">Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human interaction</a>, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement model</a> to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> with a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep policy network</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on real-world user clicks and demonstrate significant improvements across several different experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.9/>Towards building a Robust Industry-scale Question Answering System</a></strong><br><a href=/people/r/rishav-chakravarti/>Rishav Chakravarti</a>
|
<a href=/people/a/anthony-ferritto/>Anthony Ferritto</a>
|
<a href=/people/b/bhavani-iyer/>Bhavani Iyer</a>
|
<a href=/people/l/lin-pan/>Lin Pan</a>
|
<a href=/people/r/radu-florian/>Radu Florian</a>
|
<a href=/people/s/salim-roukos/>Salim Roukos</a>
|
<a href=/people/a/avirup-sil/>Avi Sil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--9><div class="card-body p-3 small">Industry-scale NLP systems necessitate two <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Robustness : zero-shot transfer learning (ZSTL) performance has to be commendable and 2. Efficiency : systems have to train efficiently and respond instantaneously. In this paper, we introduce the development of a production model called GAAMA (Go Ahead Ask Me Anything) which possess the above two characteristics. For <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> trains on the recently introduced Natural Questions (NQ) dataset. NQ poses additional challenges over older datasets like SQuAD : (a) QA systems need to read and comprehend an entire Wikipedia article rather than a small passage, and (b) NQ does not suffer from <a href=https://en.wikipedia.org/wiki/Observation_bias>observation bias</a> during construction, resulting in less lexical overlap between the question and the article. GAAMA consists of Attention-over-Attention, diversity among attention heads, hierarchical transfer learning, and synthetic data augmentation while being computationally inexpensive. Building on top of the powerful BERTQA model, GAAMA provides a 2.0 % absolute boost in F1 over the industry-scale state-of-the-art (SOTA) system on NQ. Further, we show that GAAMA transfers zero-shot to unseen real life and important domains as it yields respectable performance on two benchmarks : the BioASQ and the newly introduced CovidQA datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.18/>Learning Domain Terms-Empirical Methods to Enhance Enterprise Text Analytics Performance</a></strong><br><a href=/people/g/gargi-roy/>Gargi Roy</a>
|
<a href=/people/l/lipika-dey/>Lipika Dey</a>
|
<a href=/people/m/mohammad-shakir/>Mohammad Shakir</a>
|
<a href=/people/t/tirthankar-dasgupta/>Tirthankar Dasgupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--18><div class="card-body p-3 small">Performance of standard text analytics algorithms are known to be substantially degraded on consumer generated data, which are often very noisy. These algorithms also do not work well on enterprise data which has a very different nature from <a href=https://en.wikipedia.org/wiki/News_aggregator>News repositories</a>, storybooks or Wikipedia data. Text cleaning is a mandatory step which aims at <a href=https://en.wikipedia.org/wiki/Noise_reduction>noise removal</a> and <a href=https://en.wikipedia.org/wiki/Noise_reduction>correction</a> to improve performance. However, enterprise data need special cleaning methods since it contains many domain terms which appear to be noise against a standard <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a>, but in reality are not so. In this work we present detailed analysis of characteristics of enterprise data and suggest <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> for cleaning these repositories after domain terms have been automatically segregated from true noise terms. Noise terms are thereafter corrected in a contextual fashion. The effectiveness of the method is established through careful manual evaluation of error corrections over several standard <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a>, including those available for hate speech detection, where there is deliberate distortion to avoid detection. We also share results to show enhancement in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> after noise correction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-industry.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-industry--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-industry.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-industry.20/>ScopeIt : Scoping Task Relevant Sentences in Documents<span class=acl-fixed-case>S</span>cope<span class=acl-fixed-case>I</span>t: Scoping Task Relevant Sentences in Documents</a></strong><br><a href=/people/b/barun-patra/>Barun Patra</a>
|
<a href=/people/v/vishwas-suryanarayanan/>Vishwas Suryanarayanan</a>
|
<a href=/people/c/chala-fufa/>Chala Fufa</a>
|
<a href=/people/p/pamela-bhattacharya/>Pamela Bhattacharya</a>
|
<a href=/people/c/charles-c-lee/>Charles Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-industry--20><div class="card-body p-3 small">A prominent problem faced by conversational agents working with large documents (Eg : email-based assistants) is the frequent presence of information in the document that is irrelevant to the assistant. This in turn makes it harder for the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> to accurately detect intents, extract entities relevant to those intents and perform the desired action. To address this issue we present a neural model for scoping relevant information for the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> from a large document. We show that when used as the first step in a popularly used email-based assistant for helping users schedule meetings, our proposed model helps improve the performance of the intent detection and entity extraction tasks required by the agent for correctly scheduling meetings : across a suite of 6 downstream tasks, by using our proposed method, we observe an average gain of 35 % in precision without any drop in recall. Additionally, we demonstrate that the same approach can be used for component level analysis in large documents, such as signature block identification.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>