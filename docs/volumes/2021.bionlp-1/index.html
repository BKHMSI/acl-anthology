<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 20th Workshop on Biomedical Language Processing - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2021.bionlp-1.pdf>Proceedings of the 20th Workshop on Biomedical Language Processing</a></h2><p class=lead><a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a>,
<a href=/people/k/k-bretonnel-cohen/>Kevin Bretonnel Cohen</a>,
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a>,
<a href=/people/j/junichi-tsujii/>Junichi Tsujii</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.bionlp-1</dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/bionlp/>BioNLP</a>
| <a href=/venues/naacl/>NAACL</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigbiomed/>SIGBIOMED</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.bionlp-1>https://aclanthology.org/2021.bionlp-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.bionlp-1.pdf>https://aclanthology.org/2021.bionlp-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.bionlp-1.pdf title="Open PDF of 'Proceedings of the 20th Workshop on Biomedical Language Processing'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+20th+Workshop+on+Biomedical+Language+Processing" title="Search for 'Proceedings of the 20th Workshop on Biomedical Language Processing' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.0/>Proceedings of the 20th Workshop on Biomedical Language Processing</a></strong><br><a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a>
|
<a href=/people/k/k-bretonnel-cohen/>Kevin Bretonnel Cohen</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a>
|
<a href=/people/j/junichi-tsujii/>Junichi Tsujii</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bionlp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.3/>Scalable Few-Shot Learning of Robust Biomedical Name Representations</a></strong><br><a href=/people/p/pieter-fivez/>Pieter Fivez</a>
|
<a href=/people/s/simon-suster/>Simon Suster</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--3><div class="card-body p-3 small">Recent research on robust representations of biomedical names has focused on modeling large amounts of fine-grained conceptual distinctions using complex neural encoders. In this paper, we explore the opposite <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> : training a simple encoder architecture using only small sets of <a href=https://en.wikipedia.org/wiki/Name>names</a> sampled from high-level biomedical concepts. Our encoder post-processes pretrained representations of biomedical names, and is effective for various types of input representations, both domain-specific or unsupervised. We validate our proposed few-shot learning approach on multiple biomedical relatedness benchmarks, and show that it allows for continual learning, where we accumulate information from various conceptual hierarchies to consistently improve encoder performance. Given these findings, we propose our approach as a low-cost alternative for exploring the impact of conceptual distinctions on robust biomedical name representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.4/>SAFFRON : tranSfer leArning For Food-disease RelatiOn extractioN<span class=acl-fixed-case>SAFFRON</span>: tran<span class=acl-fixed-case>S</span>fer le<span class=acl-fixed-case>A</span>rning For Food-disease <span class=acl-fixed-case>R</span>elati<span class=acl-fixed-case>O</span>n extractio<span class=acl-fixed-case>N</span></a></strong><br><a href=/people/g/gjorgjina-cenikj/>Gjorgjina Cenikj</a>
|
<a href=/people/t/tome-eftimov/>Tome Eftimov</a>
|
<a href=/people/b/barbara-korousic-seljak/>Barbara Koroušić Seljak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--4><div class="card-body p-3 small">The accelerating growth of <a href=https://en.wikipedia.org/wiki/Big_data>big data</a> in the biomedical domain, with an endless amount of electronic health records and more than 30 million citations and abstracts in <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a>, introduces the need for automatic structuring of textual biomedical data. In this paper, we develop a method for detecting relations between food and disease entities from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>raw text</a>. Due to the lack of annotated data on <a href=https://en.wikipedia.org/wiki/Food>food</a> with respect to health, we explore the feasibility of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> by training BERT-based models on existing datasets annotated for the presence of cause and treat relations among different types of biomedical entities, and using them to recognize the same relations between <a href=https://en.wikipedia.org/wiki/Food>food and disease entities</a> in a dataset created for the purposes of this study. The best <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve macro averaged F1 scores of 0.847 and 0.900 for the <a href=https://en.wikipedia.org/wiki/Causality>cause and treat relations</a>, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.8/>Overview of the MEDIQA 2021 Shared Task on <a href=https://en.wikipedia.org/wiki/Summarization>Summarization</a> in the Medical Domain<span class=acl-fixed-case>MEDIQA</span> 2021 Shared Task on Summarization in the Medical Domain</a></strong><br><a href=/people/a/asma-ben-abacha/>Asma Ben Abacha</a>
|
<a href=/people/y/yassine-mrabet/>Yassine Mrabet</a>
|
<a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/c/chaitanya-shivade/>Chaitanya Shivade</a>
|
<a href=/people/c/curtis-langlotz/>Curtis Langlotz</a>
|
<a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--8><div class="card-body p-3 small">The MEDIQA 2021 shared tasks at the BioNLP 2021 workshop addressed three tasks on <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> for medical text : (i) a question summarization task aimed at exploring new approaches to understanding complex real-world consumer health queries, (ii) a multi-answer summarization task that targeted aggregation of multiple relevant answers to a biomedical question into one concise and relevant answer, and (iii) a radiology report summarization task addressing the development of clinically relevant impressions from radiology report findings. Thirty-five teams participated in these shared tasks with sixteen working notes submitted (fifteen accepted) describing a wide variety of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> developed and tested on the shared and external datasets. In this paper, we describe the tasks, the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> and techniques developed by various teams, the results of the <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>, and a study of correlations among various summarization evaluation measures. We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.9/>WBI at MEDIQA 2021 : Summarizing Consumer Health Questions with Generative Transformers<span class=acl-fixed-case>WBI</span> at <span class=acl-fixed-case>MEDIQA</span> 2021: Summarizing Consumer Health Questions with Generative Transformers</a></strong><br><a href=/people/m/mario-sanger/>Mario Sänger</a>
|
<a href=/people/l/leon-weber/>Leon Weber</a>
|
<a href=/people/u/ulf-leser/>Ulf Leser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--9><div class="card-body p-3 small">This paper describes our contribution for the MEDIQA-2021 Task 1 question summarization competition. We model the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> as conditional generation problem. Our concrete pipeline performs a finetuning of the large pretrained generative transformers PEGASUS (Zhang et al.,2020a) and BART (Lewis et al.,2020). We used the resulting <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> as strong baselines and experimented with (i) integrating structured knowledge via entity embeddings, (ii) ensembling multiple generative models with the generator-discriminator framework and (iii) disentangling summarization and interrogative prediction to achieve further improvements. Our best performing model, a fine-tuned vanilla PEGASUS, reached the second place in the competition with an ROUGE-2-F1 score of 15.99. We observed that all of our additional measures hurt performance (up to 5.2 pp) on the official test set. In course of a post-hoc experimental analysis which uses a larger validation set results indicate slight performance improvements through the proposed extensions. However, further analysis is need to provide stronger evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.11/>BDKG at MEDIQA 2021 : System Report for the Radiology Report Summarization Task<span class=acl-fixed-case>BDKG</span> at <span class=acl-fixed-case>MEDIQA</span> 2021: System Report for the Radiology Report Summarization Task</a></strong><br><a href=/people/s/songtai-dai/>Songtai Dai</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/y/yong-zhu/>Yong Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--11><div class="card-body p-3 small">This paper presents our winning system at the Radiology Report Summarization track of the MEDIQA 2021 shared task. Radiology report summarization automatically summarizes radiology findings into free-text impressions. This year&#8217;s task emphasizes the generalization and transfer ability of participating systems. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is built upon a pre-trained Transformer encoder-decoder architecture, i.e., <a href=https://en.wikipedia.org/wiki/PEGASUS>PEGASUS</a>, deployed with an additional domain adaptation module to particularly handle the transfer and generalization issue. Heuristics like <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble</a> and <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a> are also used. Our system is conceptually simple yet highly effective, achieving a ROUGE-2 score of 0.436 on test set and ranked the 1st place among all participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.12/>damo_nlp at MEDIQA 2021 : Knowledge-based Preprocessing and Coverage-oriented Reranking for Medical Question Summarization<span class=acl-fixed-case>MEDIQA</span> 2021: Knowledge-based Preprocessing and Coverage-oriented Reranking for Medical Question Summarization</a></strong><br><a href=/people/y/yifan-he/>Yifan He</a>
|
<a href=/people/m/mosha-chen/>Mosha Chen</a>
|
<a href=/people/s/songfang-huang/>Songfang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--12><div class="card-body p-3 small">Medical question summarization is an important but difficult task, where the input is often complex and erroneous while annotated data is expensive to acquire. We report our participation in the MEDIQA 2021 question summarization task in which we are required to address these challenges. We start from pre-trained conditional generative language models, use <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> to help correct input errors, and rerank single system outputs to boost coverage. Experimental results show significant improvement in string-based metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bionlp-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.13/>Stress Test Evaluation of Biomedical Word Embeddings</a></strong><br><a href=/people/v/vladimir-araujo/>Vladimir Araujo</a>
|
<a href=/people/a/andres-carvallo/>Andrés Carvallo</a>
|
<a href=/people/c/carlos-aspillaga/>Carlos Aspillaga</a>
|
<a href=/people/c/camilo-thorne/>Camilo Thorne</a>
|
<a href=/people/d/denis-parra/>Denis Parra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--13><div class="card-body p-3 small">The success of pretrained word embeddings has motivated their use in the biomedical domain, with contextualized embeddings yielding remarkable results in several biomedical NLP tasks. However, there is a lack of research on quantifying their behavior under <a href=https://en.wikipedia.org/wiki/Stress_(biology)>severe stress scenarios</a>. In this work, we systematically evaluate three language models with adversarial examples automatically constructed tests that allow us to examine how robust the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are. We propose two types of stress scenarios focused on the biomedical named entity recognition (NER) task, one inspired by spelling errors and another based on the use of synonyms for medical terms. Our experiments with three <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> show that the performance of the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> decreases considerably, in addition to revealing their weaknesses and strengths. Finally, we show that adversarial training causes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to improve their robustness and even to exceed the original performance in some cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.16/>BioELECTRA : Pretrained Biomedical text Encoder using Discriminators<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>ELECTRA</span>:Pretrained Biomedical text Encoder using Discriminators</a></strong><br><a href=/people/k/kamal-raj-kanakarajan/>Kamal raj Kanakarajan</a>
|
<a href=/people/b/bhuvana-kundumani/>Bhuvana Kundumani</a>
|
<a href=/people/m/malaikannan-sankarasubbu/>Malaikannan Sankarasubbu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--16><div class="card-body p-3 small">Recent advancements in pretraining strategies in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> have shown a significant improvement in the performance of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on various text mining tasks. We apply &#8216;replaced token detection&#8217; pretraining technique proposed by ELECTRA and pretrain a biomedical language model from scratch using biomedical text and vocabulary. We introduce BioELECTRA, a biomedical domain-specific language encoder model that adapts ELECTRA for the Biomedical domain. WE evaluate our model on the BLURB and BLUE biomedical NLP benchmarks. BioELECTRA outperforms the previous models and achieves state of the art (SOTA) on all the 13 datasets in BLURB benchmark and on all the 4 Clinical datasets from BLUE Benchmark across 7 different NLP tasks. BioELECTRA pretrained on PubMed and PMC full text articles performs very well on Clinical datasets as well. BioELECTRA achieves new SOTA 86.34%(1.39 % accuracy improvement) on MedNLI and 64 % (2.98 % accuracy improvement) on PubMedQA dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bionlp-1.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.17/>Word centrality constrained representation for keyphrase extraction</a></strong><br><a href=/people/z/zelalem-gero/>Zelalem Gero</a>
|
<a href=/people/j/joyce-ho/>Joyce Ho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--17><div class="card-body p-3 small">To keep pace with the increased generation and digitization of documents, automated methods that can improve <a href=https://en.wikipedia.org/wiki/Search_engine_technology>search</a>, discovery and mining of the vast body of literature are essential. Keyphrases provide a concise representation by identifying salient concepts in a document. Various <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised approaches</a> model <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>keyphrase extraction</a> using local context to predict the label for each token and perform much better than the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised counterparts</a>. Unfortunately, this <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> fails for short documents where the context is unclear. Moreover, <a href=https://en.wikipedia.org/wiki/Keyword_(linguistics)>keyphrases</a>, which are usually the gist of a document, need to be the central theme. We propose a new extraction model that introduces a <a href=https://en.wikipedia.org/wiki/Centrality>centrality constraint</a> to enrich the word representation of a Bidirectional long short-term memory. Performance evaluation on 2 publicly available datasets demonstrate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms existing state-of-the art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.18/>End-to-end Biomedical Entity Linking with Span-based Dictionary Matching</a></strong><br><a href=/people/s/shogo-ujiie/>Shogo Ujiie</a>
|
<a href=/people/h/hayate-iso/>Hayate Iso</a>
|
<a href=/people/s/shuntaro-yada/>Shuntaro Yada</a>
|
<a href=/people/s/shoko-wakamiya/>Shoko Wakamiya</a>
|
<a href=/people/e/eiji-aramaki/>Eiji Aramaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--18><div class="card-body p-3 small">Disease name recognition and <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> is a fundamental process in <a href=https://en.wikipedia.org/wiki/Biomedical_text_mining>biomedical text mining</a>. Recently, neural joint learning of both <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> has been proposed to utilize the mutual benefits. While this approach achieves high performance, disease concepts that do not appear in the training dataset can not be accurately predicted. This study introduces a novel <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end approach</a> that combines span representations with dictionary-matching features to address this problem. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> handles unseen concepts by referring to a <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a> while maintaining the performance of neural network-based models. Experiments using two major datasaets demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved competitive results with strong baselines, especially for unseen concepts during training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.26/>Context-aware query design combines knowledge and data for efficient reading and reasoning</a></strong><br><a href=/people/e/emilee-holtzapple/>Emilee Holtzapple</a>
|
<a href=/people/b/brent-cochran/>Brent Cochran</a>
|
<a href=/people/n/natasa-miskov-zivanov/>Natasa Miskov-Zivanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--26><div class="card-body p-3 small">The amount of biomedical literature has vastly increased over the past few decades. As a result, the sheer quantity of accessible information is overwhelming, and complicates manual information retrieval. Automated methods seek to speed up <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> from <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical literature</a>. However, such automated methods are still too time-intensive to survey all existing biomedical literature. We present a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for automatically generating literature queries that select relevant papers based on <a href=https://en.wikipedia.org/wiki/Data>biological data</a>. By using <a href=https://en.wikipedia.org/wiki/Gene_expression>differentially expressed genes</a> to inform our literature searches, we focus <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> on mechanistic signaling details that are crucial for the disease or context of interest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.27/>Measuring the relative importance of full text sections for <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> from <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific literature</a>.</a></strong><br><a href=/people/l/lana-yeganova/>Lana Yeganova</a>
|
<a href=/people/w/won-gyu-kim/>Won Gyu Kim</a>
|
<a href=/people/d/donald-c-comeau/>Donald Comeau</a>
|
<a href=/people/w/w-john-wilbur/>W John Wilbur</a>
|
<a href=/people/z/zhiyong-lu/>Zhiyong Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--27><div class="card-body p-3 small">With the growing availability of full-text articles, integrating <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>abstracts</a> and full texts of documents into a unified representation is essential for comprehensive search of scientific literature. However, previous studies have shown that navely merging <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>abstracts</a> with full texts of articles does not consistently yield better performance. Balancing the contribution of query terms appearing in the abstract and in sections of different importance in full text articles remains a challenge both with traditional bag-of-words IR approaches and for neural retrieval methods. In this work we establish the connection between the BM25 score of a query term appearing in a section of a full text document and the probability of that document being clicked or identified as relevant. Probability is computed using Pool Adjacent Violators (PAV), an isotonic regression algorithm, providing a <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimate</a> based on the observed data. Using this probabilistic transformation of BM25 scores we show an improved performance on the PubMed Click dataset developed and presented in this study, as well as the 2007 TREC Genomics collection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.31/>SB_NITK at MEDIQA 2021 : Leveraging Transfer Learning for Question Summarization in Medical Domain<span class=acl-fixed-case>SB</span>_<span class=acl-fixed-case>NITK</span> at <span class=acl-fixed-case>MEDIQA</span> 2021: Leveraging Transfer Learning for Question Summarization in Medical Domain</a></strong><br><a href=/people/s/spandana-balumuri/>Spandana Balumuri</a>
|
<a href=/people/s/sony-bachina/>Sony Bachina</a>
|
<a href=/people/s/sowmya-kamath-s/>Sowmya Kamath S</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--31><div class="card-body p-3 small">Recent strides in the healthcare domain, have resulted in vast quantities of <a href=https://en.wikipedia.org/wiki/Streaming_data>streaming data</a> available for use for building intelligent knowledge-based applications. However, the challenges introduced to the huge volume, velocity of generation, variety and variability of this medical data have to be adequately addressed. In this paper, we describe the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and results for our submission at MEDIQA 2021 Question Summarization shared task. In order to improve the performance of summarization of consumer health questions, our method explores the use of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to utilize the knowledge of NLP transformers like BART, T5 and <a href=https://en.wikipedia.org/wiki/PEGASUS>PEGASUS</a>. The proposed models utilize the knowledge of pre-trained NLP transformers to achieve improved results when compared to conventional deep learning models such as LSTM, RNN etc. Our team SB_NITK ranked 12th among the total 22 submissions in the official final rankings. Our BART based model achieved a ROUGE-2 F1 score of 0.139.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bionlp-1.33" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.33/>QIAI at MEDIQA 2021 : Multimodal Radiology Report Summarization<span class=acl-fixed-case>QIAI</span> at <span class=acl-fixed-case>MEDIQA</span> 2021: Multimodal Radiology Report Summarization</a></strong><br><a href=/people/j/jean-benoit-delbrouck/>Jean-Benoit Delbrouck</a>
|
<a href=/people/c/cassie-zhang/>Cassie Zhang</a>
|
<a href=/people/d/daniel-rubin/>Daniel Rubin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--33><div class="card-body p-3 small">This paper describes the solution of the QIAI lab sent to the Radiology Report Summarization (RRS) challenge at MEDIQA 2021. This paper aims to investigate whether using <a href=https://en.wikipedia.org/wiki/Multimodality>multimodality</a> during training improves the summarizing performances of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> at test-time. Our preliminary results shows that taking advantage of the visual features from the <a href=https://en.wikipedia.org/wiki/X-ray>x-rays</a> associated to the radiology reports leads to higher evaluation metrics compared to a text-only baseline system. These improvements are reported according to the automatic evaluation metrics METEOR, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and ROUGE scores. Our experiments can be fully replicated at the following address : https:// github.com/jbdel/vilmedic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.37/>MNLP at MEDIQA 2021 : Fine-Tuning PEGASUS for Consumer Health Question Summarization<span class=acl-fixed-case>MNLP</span> at <span class=acl-fixed-case>MEDIQA</span> 2021: Fine-Tuning <span class=acl-fixed-case>PEGASUS</span> for Consumer Health Question Summarization</a></strong><br><a href=/people/j/jooyeon-lee/>Jooyeon Lee</a>
|
<a href=/people/h/huong-dang/>Huong Dang</a>
|
<a href=/people/o/ozlem-uzuner/>Ozlem Uzuner</a>
|
<a href=/people/s/sam-henry/>Sam Henry</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--37><div class="card-body p-3 small">This paper details a Consumer Health Question (CHQ) summarization model submitted to MEDIQA 2021 for shared task 1 : Question Summarization. Many CHQs are composed of multiple sentences with typos or unnecessary information, which can interfere with automated question answering systems. Question summarization mitigates this issue by removing this unnecessary information, aiding automated systems in generating a more accurate summary. Our summarization approach focuses on applying multiple pre-processing techniques, including question focus identification on the input and the development of an ensemble method to combine question focus with an abstractive summarization method. We use the state-of-art abstractive summarization model, PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization), to generate abstractive summaries. Our experiments show that using our ensemble method, which combines abstractive summarization with question focus identification, improves performance over using <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> alone. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> shows a ROUGE-2 F-measure of 11.14 % against the official test dataset.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>