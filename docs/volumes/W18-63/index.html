<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Third Conference on Machine Translation: Research Papers - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W18-63.pdf>Proceedings of the Third Conference on Machine Translation: Research Papers</a></h2><p class=lead><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>,
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>,
<a href=/people/c/christian-federmann/>Christian Federmann</a>,
<a href=/people/m/mark-fishel/>Mark Fishel</a>,
<a href=/people/y/yvette-graham/>Yvette Graham</a>,
<a href=/people/b/barry-haddow/>Barry Haddow</a>,
<a href=/people/m/matthias-huck/>Matthias Huck</a>,
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>,
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>,
<a href=/people/c/christof-monz/>Christof Monz</a>,
<a href=/people/m/matteo-negri/>Matteo Negri</a>,
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>,
<a href=/people/m/mariana-neves/>Mariana Neves</a>,
<a href=/people/m/matt-post/>Matt Post</a>,
<a href=/people/l/lucia-specia/>Lucia Specia</a>,
<a href=/people/m/marco-turchi/>Marco Turchi</a>,
<a href=/people/k/karin-verspoor/>Karin Verspoor</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W18-63</dd><dt>Month:</dt><dd>October</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Brussels, Belgium</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/wmt/>WMT</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigmt/>SIGMT</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W18-63>https://aclanthology.org/W18-63</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W18-63.pdf>https://aclanthology.org/W18-63.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W18-63.pdf title="Open PDF of 'Proceedings of the Third Conference on Machine Translation: Research Papers'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Third+Conference+on+Machine+Translation%3A+Research+Papers" title="Search for 'Proceedings of the Third Conference on Machine Translation: Research Papers' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6300/>Proceedings of the Third Conference on Machine Translation: Research Papers</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6301/>Scaling Neural Machine Translation</a></strong><br><a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/s/sergey-edunov/>Sergey Edunov</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6301><div class="card-body p-3 small">Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT&#8217;14 English-German translation, we match the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of Vaswani et al. (2017) in under 5 hours when training on 8 <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPUs</a> and we obtain a new state of the art of 29.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT&#8217;14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6302/>Character-level Chinese-English Translation through ASCII Encoding<span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish Translation through <span class=acl-fixed-case>ASCII</span> Encoding</a></strong><br><a href=/people/n/nikola-i-nikolov/>Nikola I. Nikolov</a>
|
<a href=/people/y/yuhuang-hu/>Yuhuang Hu</a>
|
<a href=/people/m/mi-xue-tan/>Mi Xue Tan</a>
|
<a href=/people/r/richard-h-r-hahnloser/>Richard H.R. Hahnloser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6302><div class="card-body p-3 small">Character-level Neural Machine Translation (NMT) models have recently achieved impressive results on many language pairs. They mainly do well for <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European language pairs</a>, where the languages share the same <a href=https://en.wikipedia.org/wiki/Writing_system>writing system</a>. However, for <a href=https://en.wikipedia.org/wiki/Translation>translating</a> between <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, the gap between the two different <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a> poses a major challenge because of a lack of systematic correspondence between the individual linguistic units. In this paper, we enable character-level NMT for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, by breaking down <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a> into linguistic units similar to that of <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European languages</a>. We use the Wubi encoding scheme, which preserves the original shape and semantic information of the characters, while also being reversible. We show promising results from training Wubi-based models on the character- and subword-level with recurrent as well as convolutional models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6304/>An Analysis of Attention Mechanisms : The Case of <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a> in Neural Machine Translation</a></strong><br><a href=/people/g/gongbo-tang/>Gongbo Tang</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6304><div class="card-body p-3 small">Recent work has shown that the encoder-decoder attention mechanisms in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> are different from the <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a> in <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> pay more attention to <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context tokens</a> when translating <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguous words</a>. We explore the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention distribution patterns</a> when translating ambiguous nouns. Counterintuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns. We conclude that <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is not the main mechanism used by NMT models to incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> for WSD. The experimental results suggest that NMT models learn to encode <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> necessary for WSD in the encoder hidden states. For the attention mechanism in Transformer models, we reveal that the first few layers gradually learn to align source and target tokens and the last few layers learn to extract <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from the related but unaligned context tokens.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6305/>Discourse-Related Language Contrasts in English-Croatian Human and Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>C</span>roatian Human and Machine Translation</a></strong><br><a href=/people/m/margita-sostaric/>Margita Šoštarić</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6305><div class="card-body p-3 small">We present an analysis of a number of <a href=https://en.wikipedia.org/wiki/Coreference>coreference phenomena</a> in English-Croatian human and machine translations. The aim is to shed light on the differences in the way these structurally different languages make use of discourse information and provide insights for discourse-aware machine translation system development. The phenomena are automatically identified in parallel data using <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> produced by <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> and word alignment tools, enabling us to pinpoint patterns of interest in both languages. We make the <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> more fine-grained by including three corpora pertaining to three different registers. In a second step, we create a <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> with the challenging linguistic constructions and use it to evaluate the performance of three MT systems. We show that both SMT and NMT systems struggle with handling these discourse phenomena, even though NMT tends to perform somewhat better than SMT. By providing an overview of patterns frequently occurring in actual language use, as well as by pointing out the weaknesses of current MT systems that commonly mistranslate them, we hope to contribute to the effort of resolving the issue of discourse phenomena in MT applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6306/>Coreference and Coherence in Neural Machine Translation : A Study Using Oracle Experiments</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6306><div class="card-body p-3 small">Cross-sentence context can provide valuable information in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> and is critical for translation of anaphoric pronouns and for providing consistent translations. In this paper, we devise simple oracle experiments targeting <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and <a href=https://en.wikipedia.org/wiki/Coherence_(physics)>coherence</a>. Oracles are an easy way to evaluate the effect of different discourse-level phenomena in NMT using BLEU and eliminate the necessity to manually define challenge sets for this purpose. We propose two context-aware NMT models and compare them against <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> working on a concatenation of consecutive sentences. Concatenation models perform better, but are computationally expensive. We show that NMT models taking advantage of context oracle signals can achieve considerable gains in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, of up to 7.02 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and 1.89 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> on subtitles translation. Access to strong signals allows us to make clear comparisons between context-aware models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6307" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6307/>A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/mathias-muller/>Mathias Müller</a>
|
<a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/e/elena-voita/>Elena Voita</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6307><div class="card-body p-3 small">The translation of pronouns presents a special challenge to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to this day, since <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> often requires context outside the current sentence. Recent work on <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. However, <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that quantify the overall translation quality are ill-equipped to measure gains from additional context. We argue that a different kind of <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is needed to assess how well <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> translate inter-sentential phenomena such as <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>. This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns. Furthermore, we perform experiments with several context-aware models. We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on our contrastive test set. Our experiments also show the effectiveness of parameter tying for multi-encoder architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6309/>A neural interlingua for multilingual machine translation</a></strong><br><a href=/people/y/yichao-lu/>Yichao Lu</a>
|
<a href=/people/p/phillip-keung/>Phillip Keung</a>
|
<a href=/people/f/faisal-ladhak/>Faisal Ladhak</a>
|
<a href=/people/v/vikas-bhardwaj/>Vikas Bhardwaj</a>
|
<a href=/people/s/shaonan-zhang/>Shaonan Zhang</a>
|
<a href=/people/j/jason-sun/>Jason Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6309><div class="card-body p-3 small">We incorporate an explicit neural interlingua into a multilingual encoder-decoder neural machine translation (NMT) architecture. We demonstrate that our model learns a language-independent representation by performing direct zero-shot translation (without using pivot translation), and by using the source sentence embeddings to create an English Yelp review classifier that, through the mediation of the neural interlingua, can also classify French and German reviews. Furthermore, we show that, despite using a smaller number of parameters than a pairwise collection of bilingual NMT models, our approach produces comparable BLEU scores for each language pair in WMT15.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6310/>Improving Neural Language Models with Weight Norm Initialization and Regularization</a></strong><br><a href=/people/c/christian-herold/>Christian Herold</a>
|
<a href=/people/y/yingbo-gao/>Yingbo Gao</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6310><div class="card-body p-3 small">Embedding and projection matrices are commonly used in neural language models (NLM) as well as in other sequence processing networks that operate on large vocabularies. We examine such <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrices</a> in fine-tuned language models and observe that a NLM learns word vectors whose norms are related to the word frequencies. We show that by initializing the weight norms with scaled log word counts, together with other techniques, lower perplexities can be obtained in early epochs of training. We also introduce a weight norm regularization loss term, whose hyperparameters are tuned via a <a href=https://en.wikipedia.org/wiki/Grid_search>grid search</a>. With this <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a>, we are able to significantly improve perplexities on two word-level language modeling tasks (without dynamic evaluation): from 54.44 to 53.16 on <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank (PTB)</a> and from 61.45 to 60.13 on <a href=https://en.wikipedia.org/wiki/WikiText>WikiText-2 (WT2)</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6311" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6311/>Contextual Neural Model for Translating Bilingual Multi-Speaker Conversations</a></strong><br><a href=/people/s/sameen-maruf/>Sameen Maruf</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6311><div class="card-body p-3 small">Recent works in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> have begun to explore document translation. However, translating online multi-speaker conversations is still an open problem. In this work, we propose the task of translating Bilingual Multi-Speaker Conversations, and explore neural architectures which exploit both source and target-side conversation histories for this task. To initiate an evaluation for this task, we introduce <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> extracted from <a href=https://en.wikipedia.org/wiki/Europarl>Europarl v7</a> and OpenSubtitles2016. Our experiments on four language-pairs confirm the significance of leveraging conversation history, both in terms of <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6313" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6313/>Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation</a></strong><br><a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/r/rebecca-marvin/>Rebecca Marvin</a>
|
<a href=/people/p/paul-mcnamee/>Paul McNamee</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/t/tim-anderson/>Tim Anderson</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6313><div class="card-body p-3 small">To better understand the effectiveness of continued training, we analyze the major components of a neural machine translation system (the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, <a href=https://en.wikipedia.org/wiki/Code>decoder</a>, and each embedding space) and consider each component&#8217;s contribution to, and capacity for, domain adaptation. We find that freezing any single component during continued training has minimal impact on performance, and that performance is surprisingly good when a single component is adapted while holding the rest of the model fixed. We also find that continued training does not move the model very far from the out-of-domain model, compared to a sensitivity analysis metric, suggesting that the out-of-domain model can provide a good generic initialization for the new domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6314/>Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection</a></strong><br><a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a>
|
<a href=/people/m/macduff-hughes/>Macduff Hughes</a>
|
<a href=/people/t/tetsuji-nakagawa/>Tetsuji Nakagawa</a>
|
<a href=/people/c/ciprian-chelba/>Ciprian Chelba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6314><div class="card-body p-3 small">Measuring domain relevance of data and identifying or selecting well-fit domain data for machine translation (MT) is a well-studied topic, but denoising is not yet. Denoising is concerned with a different type of <a href=https://en.wikipedia.org/wiki/Data_quality>data quality</a> and tries to reduce the negative impact of data noise on MT training, in particular, neural MT (NMT) training. This paper generalizes methods for measuring and selecting data for domain MT and applies them to denoising NMT training. The proposed approach uses trusted data and a denoising curriculum realized by online data selection. Intrinsic and extrinsic evaluations of the approach show its significant effectiveness for NMT to train on data with severe noise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6315/>Using Monolingual Data in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> : a Systematic Study</a></strong><br><a href=/people/f/franck-burlot/>Franck Burlot</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6315><div class="card-body p-3 small">Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (Phrase-Based MT) is the way monolingual target data, which often abounds, is used in these two paradigms. While Phrase-Based MT can seamlessly integrate very large language models trained on billions of sentences, the best option for Neural MT developers seems to be the generation of artificial parallel data through back-translation-a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6319 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-6319.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-6319/>A Call for Clarity in Reporting BLEU Scores<span class=acl-fixed-case>BLEU</span> Scores</a></strong><br><a href=/people/m/matt-post/>Matt Post</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6319><div class="card-body p-3 small">The field of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to the <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a>, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These <a href=https://en.wikipedia.org/wiki/Parameter>parameters</a> are often not reported or are hard to find, and consequently, BLEU scores between papers can not be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6320/>Exploring gap filling as a cheaper alternative to reading comprehension questionnaires when evaluating <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> for gisting</a></strong><br><a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6320><div class="card-body p-3 small">A popular application of machine translation (MT) is gisting : MT is consumed as is to make sense of text in a foreign language. Evaluation of the usefulness of MT for gisting is surprisingly uncommon. The classical method uses reading comprehension questionnaires (RCQ), in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. Recently, gap-filling (GF), a form of cloze testing, has been proposed as a cheaper alternative to RCQ. In GF, certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. This paper reports, for the first time, a comparative evaluation, using both RCQ and GF, of translations from multiple MT systems for the same foreign texts, and a systematic study on the effect of variables such as gap density, gap-selection strategies, and document context in GF. The main findings of the study are : (a) both <a href=https://en.wikipedia.org/wiki/Questionnaire_construction>RCQ</a> and GF clearly identify MT to be useful ; (b) global RCQ and GF rankings for the MT systems are mostly in agreement ; (c) GF scores vary very widely across informants, making comparisons among MT systems hard, and (d) unlike <a href=https://en.wikipedia.org/wiki/Questionnaire_construction>RCQ</a>, which is framed around documents, GF evaluation can be framed at the sentence level. These findings support the use of <a href=https://en.wikipedia.org/wiki/Glucosamine>GF</a> as a cheaper alternative to <a href=https://en.wikipedia.org/wiki/Carboxylic_acid>RCQ</a>.<i>gisting</i>: MT is consumed <i>as is</i> to make sense of text in a foreign language. Evaluation of the usefulness of MT for gisting is surprisingly uncommon. The classical method uses <i>reading comprehension questionnaires</i> (RCQ), in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. Recently, <i>gap-filling</i> (GF), a form of <i>cloze</i> testing, has been proposed as a cheaper alternative to RCQ. In GF, certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. This paper reports, for the first time, a comparative evaluation, using both RCQ and GF, of translations from multiple MT systems for the same foreign texts, and a systematic study on the effect of variables such as gap density, gap-selection strategies, and document context in GF. The main findings of the study are: (a) both RCQ and GF clearly identify MT to be useful; (b) global RCQ and GF rankings for the MT systems are mostly in agreement; (c) GF scores vary very widely across informants, making comparisons among MT systems hard, and (d) unlike RCQ, which is framed around documents, GF evaluation can be framed at the sentence level. These findings support the use of GF as a cheaper alternative to RCQ.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6321/>Simple Fusion : Return of the <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a></a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/j/james-cross/>James Cross</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6321><div class="card-body p-3 small">Neural Machine Translation (NMT) typically leverages <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> in training through backtranslation. We investigate an alternative simple method to use monolingual data for NMT training : We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch. To achieve that, we train the translation model to predict the <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>residual probability</a> of the training data added to the prediction of the LM. This enables the TM to focus its capacity on modeling the source sentence since it can rely on the LM for <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. We show that our method outperforms previous approaches to integrate LMs into NMT while the architecture is simpler as it does not require gating networks to balance TM and LM. We observe gains of between +0.24 and +2.36 BLEU on all four test sets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top of ensembles without LM. We compare our method with alternative ways to utilize monolingual data such as backtranslation, shallow fusion, and <a href=https://en.wikipedia.org/wiki/Cold_fusion>cold fusion</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6324/>Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation</a></strong><br><a href=/people/z/zhong-zhou/>Zhong Zhou</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6324><div class="card-body p-3 small">We work on <a href=https://en.wikipedia.org/wiki/Translation>translation</a> from rich-resource languages to low-resource languages. The main challenges we identify are the lack of low-resource language data, effective methods for cross-lingual transfer, and the variable-binding problem that is common in neural systems. We build a <a href=https://en.wikipedia.org/wiki/Machine_translation>translation system</a> that addresses these challenges using eight <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European language families</a> as our test ground. Firstly, we add the source and the target family labels and study intra-family and inter-family influences for effective cross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for English-Swedish translation using eight families compared to the single-family multi-source multi-target baseline. Moreover, we find that training on two neighboring families closest to the low-resource language is often enough. Secondly, we construct an ablation study and find that reasonably good results can be achieved even with considerably less target data. Thirdly, we address the variable-binding problem by building an order-preserving named entity translation model. We obtain 60.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in qualitative evaluation where our translations are akin to human translations in a preliminary study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6325 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6325/>Trivial Transfer Learning for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6325><div class="card-body p-3 small">Transfer learning has been proven as an effective technique for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> under low-resource conditions. Existing methods require a common target language, <a href=https://en.wikipedia.org/wiki/Language_family>language relatedness</a>, or specific training tricks and regimes. We present a simple transfer learning method, where we first train a parent model for a high-resource language pair and then continue the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> on a low-resource pair only by replacing the training corpus. This <a href=https://en.wikipedia.org/wiki/Child_model>child model</a> performs significantly better than the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> trained for low-resource pair only. We are the first to show this for targeting different languages, and we observe the improvements even for unrelated languages with different alphabets.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>