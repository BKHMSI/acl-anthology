<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W17-53.pdf>Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for <span class=acl-fixed-case>NLP</span></a></h2><p class=lead><a href=/people/s/samuel-bowman/>Samuel Bowman</a>,
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>,
<a href=/people/f/felix-hill/>Felix Hill</a>,
<a href=/people/a/angeliki-lazaridou/>Angeliki Lazaridou</a>,
<a href=/people/o/omer-levy/>Omer Levy</a>,
<a href=/people/r/roi-reichart/>Roi Reichart</a>,
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W17-53</dd><dt>Month:</dt><dd>September</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Copenhagen, Denmark</dd><dt>Venues:</dt><dd><a href=/venues/repeval/>RepEval</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W17-53>https://aclanthology.org/W17-53</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/W17-53 title="To the current version of the paper by DOI">10.18653/v1/W17-53</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W17-53.pdf>https://aclanthology.org/W17-53.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W17-53.pdf title="Open PDF of 'Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2nd+Workshop+on+Evaluating+Vector+Space+Representations+for+NLP" title="Search for 'Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5300/>Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/s/samuel-bowman/>Samuel Bowman</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/f/felix-hill/>Felix Hill</a>
|
<a href=/people/a/angeliki-lazaridou/>Angeliki Lazaridou</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5301/>The RepEval 2017 Shared Task : Multi-Genre Natural Language Inference with Sentence Representations<span class=acl-fixed-case>R</span>ep<span class=acl-fixed-case>E</span>val 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations</a></strong><br><a href=/people/n/nikita-nangia/>Nikita Nangia</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/a/angeliki-lazaridou/>Angeliki Lazaridou</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5301><div class="card-body p-3 small">This paper presents the results of the RepEval 2017 Shared Task, which evaluated neural network sentence representation learning models on the Multi-Genre Natural Language Inference corpus (MultiNLI) recently introduced by Williams et al. All of the five participating teams beat the bidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in Williams et al. The best single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> used stacked BiLSTMs with residual connections to extract <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence features</a> and reached 74.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the genre-matched test set. Surprisingly, the results of the competition were fairly consistent across the genre-matched and genre-mismatched test sets, and across subsets of the test data representing a variety of linguistic phenomena, suggesting that all of the submitted systems learned reasonably domain-independent representations for sentence meaning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5302/>Traversal-Free Word Vector Evaluation in Analogy Space</a></strong><br><a href=/people/x/xiaoyin-che/>Xiaoyin Che</a>
|
<a href=/people/n/nico-ring/>Nico Ring</a>
|
<a href=/people/w/willi-raschkowski/>Willi Raschkowski</a>
|
<a href=/people/h/haojin-yang/>Haojin Yang</a>
|
<a href=/people/c/christoph-meinel/>Christoph Meinel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5302><div class="card-body p-3 small">In this paper, we propose an alternative evaluating metric for word analogy questions (A to B is as C to D) in word vector evaluation. Different from the traditional method which predicts the fourth word by the given three, we measure the similarity directly on the relations of two pairs of given words, just as shifting the relation vectors into a new analogy space. Cosine and Euclidean distances are then calculated as measurements. Observation and experiments shows the proposed analogy space evaluation could offer a more comprehensive evaluating result on word vectors with word analogy questions. Meanwhile, <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> are remarkably reduced by avoiding traversing the vocabulary.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5303/>Hypothesis Testing based Intrinsic Evaluation of Word Embeddings</a></strong><br><a href=/people/n/nishant-gurnani/>Nishant Gurnani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5303><div class="card-body p-3 small">We introduce the cross-match test-an exact, distribution free, high-dimensional hypothesis test as an intrinsic evaluation metric for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We show that <a href=https://en.wikipedia.org/wiki/Cross-matching>cross-match</a> is an effective means of measuring the distributional similarity between different <a href=https://en.wikipedia.org/wiki/Vector_space>vector representations</a> and of evaluating the <a href=https://en.wikipedia.org/wiki/Statistical_significance>statistical significance</a> of different vector embedding models. Additionally, we find that <a href=https://en.wikipedia.org/wiki/Cross-matching>cross-match</a> can be used to provide a quantitative measure of linguistic similarity for selecting bridge languages for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We demonstrate that the results of the <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>hypothesis test</a> align with our expectations and note that the framework of two sample hypothesis testing is not limited to <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and can be extended to all <a href=https://en.wikipedia.org/wiki/Vector_space>vector representations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5304/>Evaluation of word embeddings against <a href=https://en.wikipedia.org/wiki/Cognition>cognitive processes</a> : primed reaction times in lexical decision and naming tasks</a></strong><br><a href=/people/j/jeremy-auguste/>Jeremy Auguste</a>
|
<a href=/people/a/arnaud-rey/>Arnaud Rey</a>
|
<a href=/people/b/benoit-favre/>Benoit Favre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5304><div class="card-body p-3 small">This work presents a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for word similarity evaluation grounded on cognitive sciences experimental data. Word pair similarities are compared to <a href=https://en.wikipedia.org/wiki/Mental_chronometry>reaction times</a> of subjects in large scale lexical decision and naming tasks under <a href=https://en.wikipedia.org/wiki/Semantic_priming>semantic priming</a>. Results show that GloVe embeddings lead to significantly higher correlation with experimental measurements than other controlled and off-the-shelf embeddings, and that the choice of a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training corpus</a> is less important than that of the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>. Comparison of rankings with other datasets shows that the cognitive phenomenon covers more aspects than simply <a href=https://en.wikipedia.org/wiki/Word_sense>word relatedness</a> or <a href=https://en.wikipedia.org/wiki/Similarity_(psychology)>similarity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5305/>Playing with Embeddings : Evaluating embeddings for Robot Language Learning through MUD Games<span class=acl-fixed-case>MUD</span> Games</a></strong><br><a href=/people/a/anmol-gulati/>Anmol Gulati</a>
|
<a href=/people/k/kumar-krishna-agrawal/>Kumar Krishna Agrawal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5305><div class="card-body p-3 small">Acquiring language provides a ubiquitous mode of communication, across humans and robots. To this effect, distributional representations of words based on co-occurrence statistics, have provided significant advancements ranging across <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension</a>. In this paper, we study the suitability of using general purpose word-embeddings for language learning in robots. We propose using text-based games as a proxy to evaluating <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> on real robots. Based in a risk-reward setting, we review the effectiveness of the embeddings in navigating tasks in fantasy games, as an approximation to their performance on more complex scenarios, like language assisted robot navigation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5306/>Recognizing Textual Entailment in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> Using Word Embeddings<span class=acl-fixed-case>T</span>witter Using Word Embeddings</a></strong><br><a href=/people/o/octavia-maria-sulea/>Octavia-Maria Şulea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5306><div class="card-body p-3 small">In this paper, we investigate the application of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning techniques</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to the task of Recognizing Textual Entailment (RTE) in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a>. We look at a manually labeled dataset consisting of user generated short texts posted on Twitter (tweets) and related to four recent media events (the <a href=https://en.wikipedia.org/wiki/Charlie_Hebdo_shooting>Charlie Hebdo shooting</a>, the Ottawa shooting, the Sydney Siege, and the German Wings crash) and test to what extent neural techniques and embeddings are able to distinguish between tweets that entail or contradict each other or that claim unrelated things. We obtain comparable results to the state of the art in a train-test setting, but we show that, due to the noisy aspect of the data, results plummet in an evaluation strategy crafted to better simulate a real-life train-test scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5309" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5309/>Character-level Intra Attention Network for Natural Language Inference</a></strong><br><a href=/people/h/han-yang/>Han Yang</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5309><div class="card-body p-3 small">Natural language inference (NLI) is a central problem in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>. End-to-end artificial neural networks have reached state-of-the-art performance in NLI field recently. In this paper, we propose Character-level Intra Attention Network (CIAN) for the NLI task. In our model, we use the character-level convolutional network to replace the standard word embedding layer, and we use the intra attention to capture the intra-sentence semantics. The proposed CIAN model provides improved results based on a newly published MNLI corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5310" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5310/>Refining <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>Raw Sentence Representations</a> for Textual Entailment Recognition via <a href=https://en.wikipedia.org/wiki/Attention>Attention</a></a></strong><br><a href=/people/j/jorge-balazs/>Jorge Balazs</a>
|
<a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/p/pablo-loyola/>Pablo Loyola</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5310><div class="card-body p-3 small">In this paper we present the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> used by the team Rivercorners for the 2017 RepEval shared task. First, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> separately encodes a pair of sentences into variable-length representations by using a bidirectional LSTM. Later, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> creates fixed-length raw representations by means of simple aggregation functions, which are then refined using an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. Finally it combines the refined <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> of both sentences into a single vector to be used for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. With this model we obtained test accuracies of 72.057 % and 72.055 % in the matched and mismatched evaluation tracks respectively, outperforming the LSTM baseline, and obtaining performances similar to a model that relies on shared information between sentences (ESIM). When using an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> both accuracies increased to 72.247 % and 72.827 % respectively.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>