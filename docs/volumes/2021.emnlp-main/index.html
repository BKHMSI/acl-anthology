<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</h2><p class=lead><a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>,
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>,
<a href=/people/l/lucia-specia/>Lucia Specia</a>,
<a href=/people/w/wen-tau-yih/>Scott Wen-tau Yih</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.emnlp-main</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online and Punta Cana, Dominican Republic</dd><dt>Venue:</dt><dd><a href=/venues/emnlp/>EMNLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.emnlp-main>https://aclanthology.org/2021.emnlp-main</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2021+Conference+on+Empirical+Methods+in+Natural+Language+Processing" title="Search for 'Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.0/>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></strong><br><a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/w/wen-tau-yih/>Scott Wen-tau Yih</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.11/>Multiplex Graph Neural Network for Extractive Text Summarization</a></strong><br><a href=/people/b/baoyu-jing/>Baoyu Jing</a>
|
<a href=/people/z/zeyu-you/>Zeyu You</a>
|
<a href=/people/t/tao-yang/>Tao Yang</a>
|
<a href=/people/w/wei-fan/>Wei Fan</a>
|
<a href=/people/h/hanghang-tong/>Hanghang Tong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--11><div class="card-body p-3 small">Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a> plays an important role. Recent studies have leveraged <a href=https://en.wikipedia.org/wiki/Graph_theory>graph neural networks</a> to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> and natural connection relationships), nor model intra-sentential relationships (e.g, <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntactic relationship</a> among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the CNN / DailyMail benchmark dataset to demonstrate effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.14/>Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context</a></strong><br><a href=/people/x/xinnian-liang/>Xinnian Liang</a>
|
<a href=/people/s/shuangzhi-wu/>Shuangzhi Wu</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--14><div class="card-body p-3 small">Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we propose a novel method for UKE, where local and global contexts are jointly modeled. From a global view, we calculate the similarity between a certain phrase and the whole document in the <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> as transitional embedding based models do. In terms of the local view, we first build a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> based on the document where phrases are regarded as vertices and the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> are similarities between vertices. Then, we proposed a new centrality computation method to capture local salient information based on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>. Finally, we further combine the modeling of global and local context for <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a>. We evaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010) and compare with existing state-of-the-art models. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms most <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> while generalizing better on input documents with different domains and length. Additional ablation study shows that both the local and global information is crucial for unsupervised keyphrase extraction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.17/>A Partition Filter Network for Joint Entity and Relation Extraction</a></strong><br><a href=/people/z/zhiheng-yan/>Zhiheng Yan</a>
|
<a href=/people/c/chong-zhang/>Chong Zhang</a>
|
<a href=/people/j/jinlan-fu/>Jinlan Fu</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--17><div class="card-body p-3 small">In joint entity and relation extraction, existing work either sequentially encode task-specific features, leading to an imbalance in inter-task feature interaction where <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted later have no direct contact with those that come first. Or they encode entity features and relation features in a parallel manner, meaning that feature representation learning for each task is largely independent of each other except for input sharing. We propose a partition filter network to model two-way interaction between tasks properly, where <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature encoding</a> is decomposed into two steps : partition and <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filter</a>. In our <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, we leverage two gates : entity and relation gate, to segment <a href=https://en.wikipedia.org/wiki/Neuron>neurons</a> into two task partitions and one shared partition. The shared partition represents inter-task information valuable to both tasks and is evenly shared across two tasks to ensure proper two-way interaction. The task partitions represent intra-task information and are formed through concerted efforts of both gates, making sure that encoding of task-specific features is dependent upon each other. Experiment results on six public datasets show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs significantly better than previous approaches. In addition, contrary to what previous work has claimed, our auxiliary experiments suggest that relation prediction is contributory to named entity prediction in a non-negligible way. The source code can be found at https://github.com/Coopercoppers/PFN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.19.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.19/>Beta Distribution Guided Aspect-aware Graph for Aspect Category Sentiment Analysis with Affective Knowledge</a></strong><br><a href=/people/b/bin-liang/>Bin Liang</a>
|
<a href=/people/h/hang-su/>Hang Su</a>
|
<a href=/people/r/rongdi-yin/>Rongdi Yin</a>
|
<a href=/people/l/lin-gui/>Lin Gui</a>
|
<a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/q/qin-zhao/>Qin Zhao</a>
|
<a href=/people/x/xiaoqi-yu/>Xiaoqi Yu</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--19><div class="card-body p-3 small">In this paper, we investigate the Aspect Category Sentiment Analysis (ACSA) task from a novel perspective by exploring a Beta Distribution guided aspect-aware graph construction based on external knowledge. That is, we are no longer entangled about how to laboriously search the sentiment clues for coarse-grained aspects from the context, but how to preferably find the words highly related to the <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> in the context and determine their importance based on the public knowledge base. In this way, the contextual sentiment clues can be explicitly tracked in ACSA for the <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> in the light of these aspect-related words. To be specific, we first regard each aspect as a pivot to derive aspect-aware words that are highly related to the <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a> from external affective commonsense knowledge. Then, we employ <a href=https://en.wikipedia.org/wiki/Beta_distribution>Beta Distribution</a> to educe the aspect-aware weight, which reflects the importance to the aspect, for each aspect-aware word. Afterward, the aspect-aware words are served as the substitutes of the coarse-grained aspect to construct <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> for leveraging the aspect-related contextual sentiment dependencies in ACSA. Experiments on 6 benchmark datasets show that our approach significantly outperforms the state-of-the-art baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.21/>Improving Multimodal fusion via Mutual Dependency Maximisation</a></strong><br><a href=/people/p/pierre-colombo/>Pierre Colombo</a>
|
<a href=/people/e/emile-chapuis/>Emile Chapuis</a>
|
<a href=/people/m/matthieu-labeau/>Matthieu Labeau</a>
|
<a href=/people/c/chloe-clavel/>Chloé Clavel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--21><div class="card-body p-3 small">Multimodal sentiment analysis is a trending area of research, and multimodal fusion is one of its most active topic. Acknowledging humans communicate through a variety of <a href=https://en.wikipedia.org/wiki/Communication_channel>channels</a> (i.e visual, acoustic, linguistic), multimodal systems aim at integrating different <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal representations</a> into a synthetic one. So far, a consequent effort has been made on developing <a href=https://en.wikipedia.org/wiki/Computer_architecture>complex architectures</a> allowing the fusion of these <a href=https://en.wikipedia.org/wiki/Modularity>modalities</a>. However, such systems are mainly trained by minimising simple <a href=https://en.wikipedia.org/wiki/Loss_function>losses</a> such as L_1 or <a href=https://en.wikipedia.org/wiki/Cross-entropy>cross-entropy</a>. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to 4.3 on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets : CMU-MOSI and CMU-MOSEI. Our method not only achieves a new SOTA on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> but also produces <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.<tex-math>L_1</tex-math> or cross-entropy. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to 4.3 on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets: CMU-MOSI and CMU-MOSEI. Our method not only achieves a new SOTA on both datasets but also produces representations that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.23/>Progressive Self-Training with <a href=https://en.wikipedia.org/wiki/Discriminator>Discriminator</a> for Aspect Term Extraction</a></strong><br><a href=/people/q/qianlong-wang/>Qianlong Wang</a>
|
<a href=/people/z/zhiyuan-wen/>Zhiyuan Wen</a>
|
<a href=/people/q/qin-zhao/>Qin Zhao</a>
|
<a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--23><div class="card-body p-3 small">Aspect term extraction aims to extract <a href=https://en.wikipedia.org/wiki/Aspect_(grammar)>aspect terms</a> from a review sentence that users have expressed opinions on. One of the remaining challenges for aspect term extraction resides in the lack of sufficient <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a>. While self-training is potentially an effective method to address this issue, the pseudo-labels it yields on unlabeled data could induce <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>. In this paper, we use two means to alleviate the noise in the pseudo-labels. One is that inspired by the curriculum learning, we refine the conventional self-training to progressive self-training. Specifically, the base model infers pseudo-labels on a progressive subset at each iteration, where samples in the <a href=https://en.wikipedia.org/wiki/Subset>subset</a> become harder and more numerous as the iteration proceeds. The other is that we use a <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> to filter the noisy pseudo-labels. Experimental results on four SemEval datasets show that our model significantly outperforms the previous baselines and achieves state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.24" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.24/>Reinforced Counterfactual Data Augmentation for Dual Sentiment Classification</a></strong><br><a href=/people/h/hao-chen/>Hao Chen</a>
|
<a href=/people/r/rui-xia/>Rui Xia</a>
|
<a href=/people/j/jianfei-yu/>Jianfei Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--24><div class="card-body p-3 small">Data augmentation and adversarial perturbation approaches have recently achieved promising results in solving the over-fitting problem in many natural language processing (NLP) tasks including sentiment classification. However, existing studies aimed to improve the generalization ability by augmenting the training data with synonymous examples or adding random noises to <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, which can not address the spurious association problem. In this work, we propose an end-to-end reinforcement learning framework, which jointly performs counterfactual data generation and dual sentiment classification. Our approach has three characteristics:1) the generator automatically generates massive and diverse antonymous sentences ; 2) the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> contains a original-side sentiment predictor and an antonymous-side sentiment predictor, which jointly evaluate the quality of the generated sample and help the generator iteratively generate higher-quality antonymous samples ; 3) the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> is directly used as the final sentiment classifier without the need to build an extra one. Extensive experiments show that our approach outperforms strong data augmentation baselines on several benchmark sentiment classification datasets. Further analysis confirms our approach&#8217;s advantages in generating more diverse training samples and solving the spurious association problem in sentiment classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.27.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.27/>(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys</a></strong><br><a href=/people/k/kenneth-joseph/>Kenneth Joseph</a>
|
<a href=/people/s/sarah-shugars/>Sarah Shugars</a>
|
<a href=/people/r/ryan-gallagher/>Ryan Gallagher</a>
|
<a href=/people/j/jon-green/>Jon Green</a>
|
<a href=/people/a/alexi-quintana-mathe/>Alexi Quintana Mathé</a>
|
<a href=/people/z/zijian-an/>Zijian An</a>
|
<a href=/people/d/david-lazer/>David Lazer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--27><div class="card-body p-3 small">Stance detection, which aims to determine whether an individual is for or against a target concept, promises to uncover <a href=https://en.wikipedia.org/wiki/Public_opinion>public opinion</a> from large streams of <a href=https://en.wikipedia.org/wiki/Social_media>social media data</a>. Yet even human annotation of social media content does not always capture stance as measured by <a href=https://en.wikipedia.org/wiki/Opinion_poll>public opinion polls</a>. We demonstrate this by directly comparing an individual&#8217;s self-reported stance to the stance inferred from their social media data. Leveraging a longitudinal public opinion survey with respondent Twitter handles, we conducted this comparison for 1,129 individuals across four salient targets. We find that <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> is high for both Pro&#8217;&#8217; and Anti&#8217;&#8217; stance classifications but precision is variable in a number of cases. We identify three factors leading to the disconnect between text and author stance : temporal inconsistencies, differences in constructs, and <a href=https://en.wikipedia.org/wiki/Observational_error>measurement errors</a> from both survey respondents and annotators. By presenting a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for assessing the limitations of stance detection models, this work provides important insight into what stance detection truly measures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.30/>Distilling <a href=https://en.wikipedia.org/wiki/Context_(language_use)>Linguistic Context</a> for Language Model Compression</a></strong><br><a href=/people/g/geondo-park/>Geondo Park</a>
|
<a href=/people/g/gyeongman-kim/>Gyeongman Kim</a>
|
<a href=/people/e/eunho-yang/>Eunho Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--30><div class="card-body p-3 small">A computationally expensive and memory intensive <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> lies behind the recent success of language representation learning. Knowledge distillation, a major technique for deploying such a vast <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> in resource-scarce environments, transfers the knowledge on individual word representations learned without restrictions. In this paper, inspired by the recent observations that language representations are relatively positioned and have more semantic knowledge as a whole, we present a new knowledge distillation objective for language representation learning that transfers the contextual knowledge via two types of relationships across representations : Word Relation and Layer Transforming Relation. Unlike other recent <a href=https://en.wikipedia.org/wiki/Distillation>distillation techniques</a> for the <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>, our contextual distillation does not have any restrictions on architectural changes between teacher and student. We validate the effectiveness of our method on challenging benchmarks of language understanding tasks, not only in architectures of various sizes but also in combination with DynaBERT, the recently proposed adaptive size pruning method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.31" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.31/>Dynamic Knowledge Distillation for Pre-trained Language Models</a></strong><br><a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/s/shuhuai-ren/>Shuhuai Ren</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--31><div class="card-body p-3 small">Knowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency. We explore the dynamical adjustments on three aspects : teacher model adoption, data selection, and KD objective adaptation. Experimental results show that (1) proper selection of teacher model can boost the performance of student model ; (2) conducting KD with 10 % informative instances achieves comparable performance while greatly accelerates the training ; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective. We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.36" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.36/>Graph Based Network with Contextualized Representations of Turns in Dialogue</a></strong><br><a href=/people/b/bongseok-lee/>Bongseok Lee</a>
|
<a href=/people/y/yong-suk-choi/>Yong Suk Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--36><div class="card-body p-3 small">Dialogue-based relation extraction (RE) aims to extract relation(s) between two arguments that appear in a dialogue. Because <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> have the characteristics of high personal pronoun occurrences and low <a href=https://en.wikipedia.org/wiki/Information_density>information density</a>, and since most relational facts in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> are not supported by any single sentence, dialogue-based relation extraction requires a comprehensive understanding of dialogue. In this paper, we propose the TUrn COntext awaRE Graph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way people understand dialogues. In addition, we propose a novel approach which treats the task of emotion recognition in conversations (ERC) as a dialogue-based RE. Experiments on a dialogue-based RE dataset and three ERC datasets demonstrate that our model is very effective in various dialogue-based natural language understanding tasks. In these experiments, TUCORE-GCN outperforms the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art models</a> on most of the <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. Our code is available at https://github.com/BlackNoodle/TUCORE-GCN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.37/>Automatically Exposing Problems with Neural Dialog Models</a></strong><br><a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/k/kenji-sagae/>Kenji Sagae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--37><div class="card-body p-3 small">Neural dialog models are known to suffer from problems such as generating unsafe and inconsistent responses. Even though these <a href=https://en.wikipedia.org/wiki/Problem_solving>problems</a> are crucial and prevalent, they are mostly manually identified by model designers through interactions. Recently, some research instructs crowdworkers to goad the bots into triggering such problems. However, humans leverage superficial clues such as <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, while leaving systematic problems undercover. In this paper, we propose two methods including <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to automatically trigger a dialog model into generating problematic responses. We show the effect of our methods in exposing safety and contradiction issues with state-of-the-art dialog models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.38" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.38/>Event Coreference Data (Almost) for Free : Mining Hyperlinks from Online News<span class=acl-fixed-case>E</span>vent Coreference Data (Almost) for Free: <span class=acl-fixed-case>M</span>ining Hyperlinks from Online News</a></strong><br><a href=/people/m/michael-bugert/>Michael Bugert</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--38><div class="card-body p-3 small">Cross-document event coreference resolution (CDCR) is the task of identifying which event mentions refer to the same events throughout a collection of documents. Annotating CDCR data is an arduous and expensive process, explaining why existing corpora are small and lack domain coverage. To overcome this bottleneck, we automatically extract event coreference data from <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> in online news : When referring to a significant real-world event, writers often add a <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlink</a> to another article covering this event. We demonstrate that collecting <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> which point to the same article(s) produces extensive and high-quality CDCR data and create a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 2 M documents and 2.7 M silver-standard event mentions called HyperCoref. We evaluate a state-of-the-art system on three CDCR corpora and find that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on small subsets of HyperCoref are highly competitive, with performance similar to <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on gold-standard data. With our work, we free CDCR research from depending on costly human-annotated training data and open up possibilities for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.39/>Inducing Stereotypical Character Roles from Plot Structure</a></strong><br><a href=/people/l/labiba-jahan/>Labiba Jahan</a>
|
<a href=/people/r/rahul-mittal/>Rahul Mittal</a>
|
<a href=/people/m/mark-finlayson/>Mark Finlayson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--39><div class="card-body p-3 small">Stereotypical character roles-also known as archetypes or dramatis personae-play an important function in <a href=https://en.wikipedia.org/wiki/Narrative>narratives</a> : they facilitate efficient communication with bundles of default characteristics and associations and ease understanding of those characters&#8217; roles in the overall narrative. We present a fully unsupervised k-means clustering approach for learning stereotypical roles given only structural plot information. We demonstrate the technique on Vladimir Propp&#8217;s structural theory of Russian folktales (captured in the extended ProppLearner corpus, with 46 tales), showing that our approach can induce six out of seven of Propp&#8217;s dramatis personae with F1 measures of up to 0.70 (0.58 average), with an additional category for minor characters. We have explored various feature sets and variations of a cluster evaluation method. The best-performing feature set comprises <a href=https://en.wikipedia.org/wiki/Plot_(graphics)>plot functions</a>, <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>unigrams</a>, tf-idf weights, and embeddings over coreference chain heads. Roles that are mentioned more often (Hero, Villain), or have clearly distinct plot patterns (Princess) are more strongly differentiated than less frequent or distinct roles (Dispatcher, Helper, Donor). Detailed error analysis suggests that the quality of the coreference chain and plot functions annotations are critical for this task. We provide all our data and code for reproducibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.43.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.43" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.43/>Adversarial Scrubbing of Demographic Information for Text Classification</a></strong><br><a href=/people/s/somnath-basu-roy-chowdhury/>Somnath Basu Roy Chowdhury</a>
|
<a href=/people/s/sayan-ghosh/>Sayan Ghosh</a>
|
<a href=/people/y/yiyuan-li/>Yiyuan Li</a>
|
<a href=/people/j/junier-oliva/>Junier Oliva</a>
|
<a href=/people/s/shashank-srivastava/>Shashank Srivastava</a>
|
<a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--43><div class="card-body p-3 small">Contextual representations learned by language models can often encode undesirable attributes, like demographic associations of the users, while being trained for an unrelated target task. We aim to scrub such undesirable attributes and learn fair representations while maintaining performance on the target <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. In this paper, we present an adversarial learning framework Adversarial Scrubber (AdS), to debias contextual representations. We perform theoretical analysis to show that our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> converges without leaking <a href=https://en.wikipedia.org/wiki/Demography>demographic information</a> under certain conditions. We extend previous evaluation techniques by evaluating <a href=https://en.wikipedia.org/wiki/Debiasing>debiasing</a> performance using Minimum Description Length (MDL) probing. Experimental evaluations on 8 datasets show that AdS generates <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> with minimal information about <a href=https://en.wikipedia.org/wiki/Demography>demographic attributes</a> while being maximally informative about the target task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.44/>Open-domain clarification question generation without question examples</a></strong><br><a href=/people/j/julia-white/>Julia White</a>
|
<a href=/people/g/gabriel-poesia/>Gabriel Poesia</a>
|
<a href=/people/r/robert-hawkins/>Robert Hawkins</a>
|
<a href=/people/d/dorsa-sadigh/>Dorsa Sadigh</a>
|
<a href=/people/n/noah-goodman/>Noah Goodman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--44><div class="card-body p-3 small">An overarching goal of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> is to enable machines to communicate seamlessly with humans. However, <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> can be ambiguous or unclear. In cases of <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a>, humans engage in an interactive process known as repair : asking questions and seeking clarification until their uncertainty is resolved. We propose a framework for building a visually grounded question-asking model capable of producing polar (yes-no) clarification questions to resolve misunderstandings in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>. Our model uses an expected information gain objective to derive informative questions from an off-the-shelf image captioner without requiring any supervised question-answer data. We demonstrate our model&#8217;s ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human answerers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.45" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.45/>Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting</a></strong><br><a href=/people/w/wangchunshu-zhou/>Wangchunshu Zhou</a>
|
<a href=/people/t/tao-ge/>Tao Ge</a>
|
<a href=/people/c/canwen-xu/>Canwen Xu</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--45><div class="card-body p-3 small">In this paper, we propose Sequence Span Rewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the original text-infilling objective. Compared to the prevalent text infilling objectives for Seq2Seq pre-training, SSR is naturally more consistent with many downstream generation tasks that require <a href=https://en.wikipedia.org/wiki/Rewriting>sentence rewriting</a> (e.g., <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question generation</a>, <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>grammatical error correction</a>, and paraphrase generation). We conduct extensive experiments by using SSR to improve the typical Seq2Seq pre-trained model T5 in a continual pre-training setting and show substantial improvements over T5 on various natural language generation tasks.<b>S</b>equence <b>S</b>pan <b>R</b>ewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the original text-infilling objective. Compared to the prevalent text infilling objectives for Seq2Seq pre-training, SSR is naturally more consistent with many downstream generation tasks that require sentence rewriting (e.g., text summarization, question generation, grammatical error correction, and paraphrase generation). We conduct extensive experiments by using SSR to improve the typical Seq2Seq pre-trained model T5 in a continual pre-training setting and show substantial improvements over T5 on various natural language generation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.50" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.50/>Artificial Text Detection via Examining the Topology of Attention Maps</a></strong><br><a href=/people/l/laida-kushnareva/>Laida Kushnareva</a>
|
<a href=/people/d/daniil-cherniavskii/>Daniil Cherniavskii</a>
|
<a href=/people/v/vladislav-mikhailov/>Vladislav Mikhailov</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a>
|
<a href=/people/s/serguei-barannikov/>Serguei Barannikov</a>
|
<a href=/people/a/alexander-bernstein/>Alexander Bernstein</a>
|
<a href=/people/i/irina-piontkovskaya/>Irina Piontkovskaya</a>
|
<a href=/people/d/dmitri-piontkovski/>Dmitri Piontkovski</a>
|
<a href=/people/e/evgeny-burnaev/>Evgeny Burnaev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--50><div class="card-body p-3 small">The impressive capabilities of recent <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text detection, they still lack interpretability and <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> towards unseen <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data Analysis (TDA) which is currently understudied in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. We empirically show that the features derived from the BERT model outperform count- and neural-based baselines up to 10 % on three common datasets, and tend to be the most robust towards unseen GPT-style generation models as opposed to existing methods. The probing analysis of the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> reveals their sensitivity to the surface and syntactic properties. The results demonstrate that TDA is a promising line with respect to <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>, specifically the ones that incorporate surface and structural information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.52/>Conditional Poisson Stochastic Beams<span class=acl-fixed-case>P</span>oisson Stochastic Beams</a></strong><br><a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/a/afra-amini/>Afra Amini</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--52><div class="card-body p-3 small">Beam search is the default decoding strategy for many sequence generation tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. The set of approximate K-best items returned by the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is a useful summary of the distribution for many applications ; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> into a <a href=https://en.wikipedia.org/wiki/Stochastic_process>stochastic process</a> : Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et al. (2019)&#8217;s stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.54.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.54" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.54/>Moral Stories : Situated Reasoning about Norms, Intents, Actions, and their Consequences</a></strong><br><a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--54><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Social_environment>social settings</a>, much of human behavior is governed by <a href=https://en.wikipedia.org/wiki/Social_norm>unspoken rules of conduct</a> rooted in <a href=https://en.wikipedia.org/wiki/Social_norm>societal norms</a>. For artificial systems to be fully integrated into <a href=https://en.wikipedia.org/wiki/Social_environment>social environments</a>, adherence to such <a href=https://en.wikipedia.org/wiki/Social_norm>norms</a> is a central prerequisite. To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. Moreover, we examine if models can anticipate likely consequences of actions that either observe or violate known norms, or explain why certain actions are preferable by generating relevant norm hypotheses. For this purpose, we introduce Moral Stories, a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.56" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.56/>Injecting Entity Types into Entity-Guided Text Generation</a></strong><br><a href=/people/x/xiangyu-dong/>Xiangyu Dong</a>
|
<a href=/people/w/wenhao-yu/>Wenhao Yu</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--56><div class="card-body p-3 small">Recent successes in deep generative modeling have led to significant advances in natural language generation (NLG). Incorporating entities into neural generation models has demonstrated great improvements by assisting to infer the summary topic and to generate coherent content. To enhance the role of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity</a> in NLG, in this paper, we aim to model the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity type</a> in the decoding phase to generate contextual words accurately. We develop a novel NLG model to produce a target sequence based on a given list of entities. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> has a multi-step decoder that injects the entity types into the process of entity mention generation. Experiments on two public news datasets demonstrate type injection performs better than existing type embedding concatenation baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.59/>The Impact of <a href=https://en.wikipedia.org/wiki/Positional_notation>Positional Encodings</a> on Multilingual Compression</a></strong><br><a href=/people/v/vinit-ravishankar/>Vinit Ravishankar</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--59><div class="card-body p-3 small">In order to preserve word-order information in a non-autoregressive setting, transformer architectures tend to include positional knowledge, by (for instance) adding positional encodings to token embeddings. Several modifications have been proposed over the sinusoidal positional encodings used in the original transformer architecture ; these include, for instance, separating position encodings and token embeddings, or directly modifying attention weights based on the distance between word pairs. We first show that surprisingly, while these modifications tend to improve monolingual language models, none of them result in better multilingual language models. We then answer why that is : sinusoidal encodings were explicitly designed to facilitate compositionality by allowing <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>linear projections</a> over arbitrary time steps. Higher variances in multilingual training distributions requires higher compression, in which case, <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a> becomes indispensable. Learned absolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal embeddings in multilingual settings, but more complex positional encoding architectures lack the <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> to effectively learn cross-lingual alignment. In other words, while sinusoidal positional encodings were designed for monolingual applications, they are particularly useful in multilingual language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.60/>Disentangling Representations of Text by Masking Transformers</a></strong><br><a href=/people/x/xiongyi-zhang/>Xiongyi Zhang</a>
|
<a href=/people/j/jan-willem-van-de-meent/>Jan-Willem van de Meent</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--60><div class="card-body p-3 small">Representations from large pretrained models such as BERT encode a range of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspects. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation ; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, toxicity from dialect in Tweets, and <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> from <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as and often better than previously proposed methods based on variational autoencoders and adversarial training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.62/>Do Long-Range Language Models Actually Use Long-Range Context?</a></strong><br><a href=/people/s/simeng-sun/>Simeng Sun</a>
|
<a href=/people/k/kalpesh-krishna/>Kalpesh Krishna</a>
|
<a href=/people/a/andrew-mattarella-micke/>Andrew Mattarella-Micke</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--62><div class="card-body p-3 small">Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> of the past. However, the ways in which such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8 K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2 K tokens) to these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to <a href=https://en.wikipedia.org/wiki/Textbook>textbooks</a> or magazines).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.63" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.63/>The World of an Octopus : How Reporting Bias Influences a <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a>’s Perception of Color<span class=acl-fixed-case>T</span>he <span class=acl-fixed-case>W</span>orld of an <span class=acl-fixed-case>O</span>ctopus: <span class=acl-fixed-case>H</span>ow <span class=acl-fixed-case>R</span>eporting <span class=acl-fixed-case>B</span>ias <span class=acl-fixed-case>I</span>nfluences a <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>M</span>odel’s <span class=acl-fixed-case>P</span>erception of <span class=acl-fixed-case>C</span>olor</a></strong><br><a href=/people/c/cory-paik/>Cory Paik</a>
|
<a href=/people/s/stephane-aroca-ouellette/>Stéphane Aroca-Ouellette</a>
|
<a href=/people/a/alessandro-roncone/>Alessandro Roncone</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--63><div class="card-body p-3 small">Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that <a href=https://en.wikipedia.org/wiki/Reporting_bias>reporting bias</a>, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects ; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human&#8217;s perception of color ; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that <a href=https://en.wikipedia.org/wiki/Reporting_bias>reporting bias</a> negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.66" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.66/>Semantic Novelty Detection in Natural Language Descriptions</a></strong><br><a href=/people/n/nianzu-ma/>Nianzu Ma</a>
|
<a href=/people/a/alexander-politowicz/>Alexander Politowicz</a>
|
<a href=/people/s/sahisnu-mazumder/>Sahisnu Mazumder</a>
|
<a href=/people/j/jiahua-chen/>Jiahua Chen</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/e/eric-robertson/>Eric Robertson</a>
|
<a href=/people/s/scott-grigsby/>Scott Grigsby</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--66><div class="card-body p-3 small">This paper proposes to study a fine-grained semantic novelty detection task, which can be illustrated with the following example. It is normal that a person walks a dog in the park, but if someone says A man is walking a chicken in the park, it is novel. Given a set of natural language descriptions of normal scenes, we want to identify descriptions of novel scenes. We are not aware of any existing work that solves the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. Although existing novelty or anomaly detection algorithms are applicable, since they are usually topic-based, they perform poorly on our fine-grained semantic novelty detection task. This paper proposes an effective <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> (called GAT-MA) to solve the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> and also contributes a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Experimental evaluation shows that GAT-MA outperforms 11 <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by large margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.67/>Jump-Starting Item Parameters for Adaptive Language Tests</a></strong><br><a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/k/kevin-p-yancey/>Kevin P. Yancey</a>
|
<a href=/people/g/geoffrey-t-laflair/>Geoffrey T. LaFlair</a>
|
<a href=/people/j/jesse-egbert/>Jesse Egbert</a>
|
<a href=/people/m/manqian-liao/>Manqian Liao</a>
|
<a href=/people/b/burr-settles/>Burr Settles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--67><div class="card-body p-3 small">A challenge in designing high-stakes language assessments is calibrating the test item difficulties, either a priori or from limited pilot test data. While prior work has addressed &#8216;cold start&#8217; estimation of item difficulties without piloting, we devise a multi-task generalized linear model with BERT features to jump-start these estimates, rapidly improving their quality with as few as 500 test-takers and a small sample of item exposures (6 each) from a large item bank (4,000 items). Our joint model provides a principled way to compare test-taker proficiency, item difficulty, and language proficiency frameworks like the Common European Framework of Reference (CEFR). This also enables new item difficulty estimates without piloting them first, which in turn limits item exposure and thus enhances test item security. Finally, using operational data from the Duolingo English Test, a high-stakes English proficiency test, we find that the difficulty estimates derived using this method correlate strongly with lexico-grammatical features that correlate with reading complexity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.68/>Voice Query Auto Completion</a></strong><br><a href=/people/r/raphael-tang/>Raphael Tang</a>
|
<a href=/people/k/karun-kumar/>Karun Kumar</a>
|
<a href=/people/k/kendra-chalkley/>Kendra Chalkley</a>
|
<a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/l/liming-zhang/>Liming Zhang</a>
|
<a href=/people/w/wenyan-li/>Wenyan Li</a>
|
<a href=/people/g/gefei-yang/>Gefei Yang</a>
|
<a href=/people/y/yajie-mao/>Yajie Mao</a>
|
<a href=/people/j/junho-shin/>Junho Shin</a>
|
<a href=/people/g/geoffrey-craig-murray/>Geoffrey Craig Murray</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--68><div class="card-body p-3 small">Query auto completion (QAC) is the task of predicting a search engine user&#8217;s final query from their intermediate, incomplete query. In this paper, we extend <a href=https://en.wikipedia.org/wiki/Speech_recognition>QAC</a> to the streaming voice search setting, where automatic speech recognition systems produce intermediate transcriptions as users speak. Naively applying existing methods fails because the intermediate transcriptions often do n&#8217;t form prefixes or even substrings of the final transcription. To address this issue, we propose to condition QAC approaches on intermediate transcriptions to complete voice queries. We evaluate our models on a speech-enabled smart television with real-life voice search traffic, finding that this ASR-aware conditioning improves the completion quality. Our best <a href=https://en.wikipedia.org/wiki/Methodology>method</a> obtains an 18 % relative improvement in mean reciprocal rank over previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.69" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.69/>CoPHE : A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>PHE</span>: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification</a></strong><br><a href=/people/m/matus-falis/>Matúš Falis</a>
|
<a href=/people/h/hang-dong/>Hang Dong</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/b/beatrice-alex/>Beatrice Alex</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--69><div class="card-body p-3 small">Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in <a href=https://en.wikipedia.org/wiki/Prior_art>prior art</a> is evaluated with standard <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation of the structured label space in <a href=https://en.wikipedia.org/wiki/Prior_art>prior art</a>, and propose an alternative <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> based on the depth of the ontology. We propose a set of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> with previously used <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological representation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.70/>Learning Universal Authorship Representations</a></strong><br><a href=/people/r/rafael-a-rivera-soto/>Rafael A. Rivera-Soto</a>
|
<a href=/people/o/olivia-elizabeth-miano/>Olivia Elizabeth Miano</a>
|
<a href=/people/j/juanita-ordonez/>Juanita Ordonez</a>
|
<a href=/people/b/barry-y-chen/>Barry Y. Chen</a>
|
<a href=/people/a/aleem-khan/>Aleem Khan</a>
|
<a href=/people/m/marcus-bishop/>Marcus Bishop</a>
|
<a href=/people/n/nicholas-andrews/>Nicholas Andrews</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--70><div class="card-body p-3 small">Determining whether two documents were composed by the same author, also known as authorship verification, has traditionally been tackled using <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>. Recently, authorship representations learned using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> have been found to outperform alternatives, particularly in large-scale settings involving hundreds of thousands of authors. But do such <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> learned in a particular domain transfer to other domains? Or are these <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> inherently entangled with domain-specific features? To study these questions, we conduct the first large-scale study of cross-domain transfer for authorship verification considering zero-shot transfers involving three disparate domains : <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon reviews</a>, <a href=https://en.wikipedia.org/wiki/Fan_fiction>fanfiction short stories</a>, and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit comments</a>. We find that although a surprising degree of transfer is possible between certain domains, it is not so successful between others. We examine properties of these domains that influence <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and propose simple but effective methods to improve transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.71" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.71/>Predicting emergent linguistic compositions through time : Syntactic frame extension via multimodal chaining</a></strong><br><a href=/people/l/lei-yu/>Lei Yu</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--71><div class="card-body p-3 small">Natural language relies on a finite lexicon to express an unbounded set of emerging ideas. One result of this tension is the formation of new compositions, such that existing linguistic units can be combined with emerging items into novel expressions. We develop a framework that exploits the cognitive mechanisms of chaining and multimodal knowledge to predict emergent compositional expressions through time. We present the syntactic frame extension model (SFEM) that draws on the theory of chaining and knowledge from percept, concept, and language to infer how verbs extend their frames to form new compositions with existing and novel nouns. We evaluate SFEM rigorously on the 1) modalities of knowledge and 2) categorization models of chaining, in a syntactically parsed English corpus over the past 150 years. We show that multimodal SFEM predicts newly emerged verb syntax and arguments substantially better than competing models using purely linguistic or unimodal knowledge. We find support for an exemplar view of chaining as opposed to a prototype view and reveal how the joint approach of multimodal chaining may be fundamental to the creation of literal and figurative language uses including <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a> and <a href=https://en.wikipedia.org/wiki/Metonymy>metonymy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.74/>Revisiting the Uniform Information Density Hypothesis<span class=acl-fixed-case>U</span>niform <span class=acl-fixed-case>I</span>nformation <span class=acl-fixed-case>D</span>ensity Hypothesis</a></strong><br><a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/p/patrick-haller/>Patrick Haller</a>
|
<a href=/people/l/lena-jager/>Lena Jäger</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--74><div class="card-body p-3 small">The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on <a href=https://en.wikipedia.org/wiki/Language_production>language production</a> have been well explored, the <a href=https://en.wikipedia.org/wiki/Hypothesis>hypothesis</a> potentially makes predictions about <a href=https://en.wikipedia.org/wiki/Sentence_processing>language comprehension</a> and <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>linguistic acceptability</a> as well. Further, it is unclear how uniformity in a linguistic signalor lack thereofshould be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID&#8217;s predictions. For <a href=https://en.wikipedia.org/wiki/Acceptability>acceptability judgments</a>, we find clearer evidence that non-uniformity in <a href=https://en.wikipedia.org/wiki/Information_density>information density</a> is predictive of lower <a href=https://en.wikipedia.org/wiki/Acceptability>acceptability</a>. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or documenta finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.76/>Monitoring geometrical properties of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for detecting the emergence of new topics.</a></strong><br><a href=/people/c/clement-christophe/>Clément Christophe</a>
|
<a href=/people/j/julien-velcin/>Julien Velcin</a>
|
<a href=/people/j/jairo-cugliari/>Jairo Cugliari</a>
|
<a href=/people/m/manel-boumghar/>Manel Boumghar</a>
|
<a href=/people/p/philippe-suignard/>Philippe Suignard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--76><div class="card-body p-3 small">Slow emerging topic detection is a task between event detection, where we aggregate behaviors of different words on short period of time, and <a href=https://en.wikipedia.org/wiki/Evolutionary_linguistics>language evolution</a>, where we monitor their long term evolution. In this work, we tackle the problem of early detection of slowly emerging new topics. To this end, we gather evidence of <a href=https://en.wikipedia.org/wiki/Weak_signals>weak signals</a> at the word level. We propose to monitor the behavior of words representation in an <a href=https://en.wikipedia.org/wiki/Embedding>embedding space</a> and use one of its <a href=https://en.wikipedia.org/wiki/Geometry>geometrical properties</a> to characterize the <a href=https://en.wikipedia.org/wiki/Emergence>emergence of topics</a>. As <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is typically hard for this kind of task, we present a framework for quantitative evaluation and show positive results that outperform state-of-the-art methods. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is evaluated on two public datasets of press and scientific articles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.80/>Neural Attention-Aware Hierarchical Topic Model</a></strong><br><a href=/people/y/yuan-jin/>Yuan Jin</a>
|
<a href=/people/h/he-zhao/>He Zhao</a>
|
<a href=/people/m/ming-liu/>Ming Liu</a>
|
<a href=/people/l/lan-du/>Lan Du</a>
|
<a href=/people/w/wray-buntine/>Wray Buntine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--80><div class="card-body p-3 small">Neural topic models (NTMs) apply <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> to topic modelling. Despite their success, NTMs generally ignore two important aspects : (1) only document-level word count information is utilized for the training, while more fine-grained sentence-level information is ignored, and (2) external semantic knowledge regarding documents, sentences and words are not exploited for the training. To address these issues, we propose a variational autoencoder (VAE) NTM model that jointly reconstructs the sentence and document word counts using combinations of bag-of-words (BoW) topical embeddings and pre-trained semantic embeddings. The pre-trained embeddings are first transformed into a common latent topical space to align their semantics with the BoW embeddings. Our model also features hierarchical KL divergence to leverage embeddings of each document to regularize those of their sentences, paying more attention to semantically relevant sentences. Both quantitative and qualitative experiments have shown the efficacy of our model in 1) lowering the reconstruction errors at both the sentence and document levels, and 2) discovering more coherent topics from real-world datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.81/>Relational World Knowledge Representation in Contextual Language Models : A Review<span class=acl-fixed-case>R</span>elational <span class=acl-fixed-case>W</span>orld <span class=acl-fixed-case>K</span>nowledge <span class=acl-fixed-case>R</span>epresentation in <span class=acl-fixed-case>C</span>ontextual <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>M</span>odels: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>R</span>eview</a></strong><br><a href=/people/t/tara-safavi/>Tara Safavi</a>
|
<a href=/people/d/danai-koutra/>Danai Koutra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--81><div class="card-body p-3 small">Relational knowledge bases (KBs) are commonly used to represent <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> in machines. However, while advantageous for their high degree of <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> and <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant human efforts to engineer and maintain. In this review, we take a natural language processing perspective to these limitations, examining how they may be addressed in part by training deep contextual language models (LMs) to internalize and express relational knowledge in more flexible forms. We propose to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision. Our contributions are threefold : (1) We provide a high-level, extensible taxonomy for <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> in LMs ; (2) Within our <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a>, we highlight notable models, evaluation tasks, and findings, in order to provide an up-to-date review of current knowledge representation capabilities in LMs ; and (3) We suggest future research directions that build upon the complementary aspects of LMs and KBs as knowledge representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.84" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.84/>Contrastive Out-of-Distribution Detection for Pretrained Transformers</a></strong><br><a href=/people/w/wenxuan-zhou/>Wenxuan Zhou</a>
|
<a href=/people/f/fangyu-liu/>Fangyu Liu</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--84><div class="card-body p-3 small">Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in real-world scenarios, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> often faces out-of-distribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> should identify such instances, and then either reject them during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> or pass them over to <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that handle another <a href=https://en.wikipedia.org/wiki/Probability_distribution>distribution</a>. In this paper, we develop an unsupervised OOD detection method, in which only the in-distribution (ID) data are used in training. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations, such that OOD instances can be better differentiated from ID ones. These OOD instances can then be accurately detected using the <a href=https://en.wikipedia.org/wiki/Mahalanobis_distance>Mahalanobis distance</a> in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model&#8217;s penultimate layer</a>. We experiment with comprehensive settings and achieve near-perfect OOD detection performance, outperforming <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> drastically. We further investigate the rationales behind the improvement, finding that more compact representations through margin-based contrastive learning bring the improvement. We release our code to the community for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.85/>MindCraft : Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks<span class=acl-fixed-case>M</span>ind<span class=acl-fixed-case>C</span>raft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks</a></strong><br><a href=/people/c/cristian-paul-bara/>Cristian-Paul Bara</a>
|
<a href=/people/s/sky-ch-wang/>Sky CH-Wang</a>
|
<a href=/people/j/joyce-chai/>Joyce Chai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--85><div class="card-body p-3 small">An ideal integration of <a href=https://en.wikipedia.org/wiki/Autonomous_agent>autonomous agents</a> in a human world implies that they are able to collaborate on human terms. In particular, <a href=https://en.wikipedia.org/wiki/Theory_of_mind>theory of mind</a> plays an important role in maintaining common ground during <a href=https://en.wikipedia.org/wiki/Collaboration>human collaboration</a> and <a href=https://en.wikipedia.org/wiki/Communication>communication</a>. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners&#8217; beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.87" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.87/>Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking</a></strong><br><a href=/people/n/nikita-moghe/>Nikita Moghe</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--87><div class="card-body p-3 small">Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pretrained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual representations through parallel ontologies. In this work, we enhance the transfer learning process by intermediate fine-tuning of pretrained multilingual models, where the multilingual models are fine-tuned with different but related data and/or tasks. Specifically, we use parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks suitable for downstream dialogue tasks. We use only 200 K lines of parallel data for intermediate fine-tuning which is already available for 1782 language pairs. We test our approach on the cross-lingual dialogue state tracking task for the parallel MultiWoZ (English-Chinese, Chinese-English) and Multilingual WoZ (English-German, English-Italian) datasets. We achieve impressive improvements (20 % on joint goal accuracy) on the parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla baseline with only 10 % of the target language task data and zero-shot setup respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.88/>ConvFiT : Conversational Fine-Tuning of Pretrained Language Models<span class=acl-fixed-case>ConvFiT:</span> <span class=acl-fixed-case>C</span>onversational Fine-Tuning of Pretrained Language Models</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/s/samuel-coope/>Samuel Coope</a>
|
<a href=/people/d/daniela-gerz/>Daniela Gerz</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--88><div class="card-body p-3 small">Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose ConvFiT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and task-specialised sentence encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not required, and that LMs can be quickly transformed into effective conversational encoders with much smaller amounts of unannotated data ; 2) pretrained LMs can be fine-tuned into task-specialised sentence encoders, optimised for the fine-grained semantics of a particular task. Consequently, such specialised sentence encoders allow for treating ID as a simple semantic similarity task based on interpretable nearest neighbours retrieval. We validate the robustness and versatility of the ConvFiT framework with such similarity-based inference on the standard ID evaluation sets : ConvFiT-ed LMs achieve state-of-the-art ID performance across the board, with particular gains in the most challenging, few-shot setups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.91/>Feedback Attribution for Counterfactual Bandit Learning in Multi-Domain Spoken Language Understanding</a></strong><br><a href=/people/t/tobias-falke/>Tobias Falke</a>
|
<a href=/people/p/patrick-lehnen/>Patrick Lehnen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--91><div class="card-body p-3 small">With counterfactual bandit learning, models can be trained based on positive and negative feedback received for historical predictions, with no labeled data needed. Such <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> is often available in real-world dialog systems, however, the <a href=https://en.wikipedia.org/wiki/Modular_programming>modularized architecture</a> commonly used in large-scale systems prevents the direct application of such <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>. In this paper, we study the feedback attribution problem that arises when using counterfactual bandit learning for multi-domain spoken language understanding. We introduce an experimental setup to simulate the problem on small-scale public datasets, propose attribution methods inspired by multi-agent reinforcement learning and evaluate them against multiple baselines. We find that while directly using overall feedback leads to disastrous performance, our proposed attribution methods can allow training competitive models from user feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.93" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.93/>Extend, do n’t rebuild : Phrasing conditional graph modification as autoregressive sequence labelling</a></strong><br><a href=/people/l/leon-weber/>Leon Weber</a>
|
<a href=/people/j/jannes-munchmeyer/>Jannes Münchmeyer</a>
|
<a href=/people/s/samuele-garda/>Samuele Garda</a>
|
<a href=/people/u/ulf-leser/>Ulf Leser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--93><div class="card-body p-3 small">Deriving and modifying graphs from natural language text has become a versatile basis technology for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> with applications in many subfields, such as <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> or knowledge graph construction. A recent work used this technique for modifying scene graphs (He et al. 2020), by first encoding the original <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> and then generating the modified one based on this <a href=https://en.wikipedia.org/wiki/Code>encoding</a>. In this work, we show that we can considerably increase performance on this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> by phrasing it as graph extension instead of graph generation. We propose the first <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for the resulting graph extension problem based on autoregressive sequence labelling. On three scene graph modification data sets, this formulation leads to improvements in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> between 13 and 24 percentage points. Furthermore, we introduce a novel <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> from the <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedical domain</a> which has much larger linguistic variability and more complex <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> than the scene graph modification data sets. For this <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>, the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the art</a> fails to generalize, while our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can produce meaningful predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.95" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.95/>Learning Logic Rules for Document-Level Relation Extraction</a></strong><br><a href=/people/d/dongyu-ru/>Dongyu Ru</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/j/jiangtao-feng/>Jiangtao Feng</a>
|
<a href=/people/l/lin-qiu/>Lin Qiu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/w/weinan-zhang/>Weinan Zhang</a>
|
<a href=/people/y/yong-yu/>Yong Yu</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--95><div class="card-body p-3 small">Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> learned through (graph) neural networks, which makes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> less transparent. To tackle this challenge, in this paper, we propose LogiRE, a novel probabilistic model for document-level relation extraction by learning logic rules. LogiRE treats <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> and consists of two <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> : a rule generator and a relation extractor. The rule generator is to generate <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> potentially contributing to final predictions, and the relation extractor outputs final predictions based on the generated <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a>. Those two <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> can be efficiently optimized with the expectation-maximization (EM) algorithm. By introducing <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> into <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, LogiRE can explicitly capture long-range dependencies as well as enjoy better interpretation. Empirical results show that significantly outperforms several strong baselines in terms of relation performance and <a href=https://en.wikipedia.org/wiki/Consistency>logical consistency</a>. Our code is available at https://github.com/rudongyu/LogiRE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.96.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--96 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.96 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.96.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.96" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.96/>A Large-Scale Dataset for Empathetic Response Generation</a></strong><br><a href=/people/a/anuradha-welivita/>Anuradha Welivita</a>
|
<a href=/people/y/yubo-xie/>Yubo Xie</a>
|
<a href=/people/p/pearl-pu/>Pearl Pu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--96><div class="card-body p-3 small">Recent development in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> shows a strong trend towards refining pre-trained models with a domain-specific dataset. This is especially the case for response generation where <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> plays an important role. However, existing empathetic datasets remain small, delaying research efforts in this area, for example, the development of emotion-aware chatbots. One main technical challenge has been the cost of manually annotating dialogues with the right emotion labels. In this paper, we describe a large-scale silver dataset consisting of 1 M dialogues annotated with 32 fine-grained emotions, eight empathetic response intents, and the Neutral category. To achieve this goal, we have developed a novel data curation pipeline starting with a small seed of manually annotated data and eventually scaling it to a satisfactory size. We compare its quality against a state-of-the-art gold dataset using both offline experiments and visual validation methods. The resultant <a href=https://en.wikipedia.org/wiki/Subroutine>procedure</a> can be used to create similar <a href=https://en.wikipedia.org/wiki/Data_set_(IBM_mainframe)>datasets</a> in the same domain as well as in other domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.97.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--97 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.97 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.97/>The Perils of Using <a href=https://en.wikipedia.org/wiki/Mechanical_Turk>Mechanical Turk</a> to Evaluate Open-Ended Text Generation<span class=acl-fixed-case>M</span>echanical <span class=acl-fixed-case>T</span>urk to Evaluate Open-Ended Text Generation</a></strong><br><a href=/people/m/marzena-karpinska/>Marzena Karpinska</a>
|
<a href=/people/n/nader-akoury/>Nader Akoury</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--97><div class="card-body p-3 small">Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English teachers</a> provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.98/>Documenting Large Webtext Corpora : A Case Study on the Colossal Clean Crawled Corpus</a></strong><br><a href=/people/j/jesse-dodge/>Jesse Dodge</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/a/ana-marasovic/>Ana Marasović</a>
|
<a href=/people/w/william-agnew/>William Agnew</a>
|
<a href=/people/g/gabriel-ilharco/>Gabriel Ilharco</a>
|
<a href=/people/d/dirk-groeneveld/>Dirk Groeneveld</a>
|
<a href=/people/m/margaret-mitchell/>Margaret Mitchell</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--98><div class="card-body p-3 small">Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4 ; Raffel et al., 2020), a dataset created by applying a set of <a href=https://en.wikipedia.org/wiki/Filter_(software)>filters</a> to a single snapshot of <a href=https://en.wikipedia.org/wiki/Common_Crawl>Common Crawl</a>. We begin by investigating where the <a href=https://en.wikipedia.org/wiki/Data>data</a> came from, and find a significant amount of text from unexpected sources like <a href=https://en.wikipedia.org/wiki/Patent>patents</a> and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>) and evaluation examples from other benchmark NLP datasets. To understand the impact of the <a href=https://en.wikipedia.org/wiki/Content-control_software>filters</a> applied to create this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about <a href=https://en.wikipedia.org/wiki/Minority_group>minority individuals</a>. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.103/>Conundrums in Event Coreference Resolution : Making Sense of the State of the Art</a></strong><br><a href=/people/j/jing-lu/>Jing Lu</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--103><div class="card-body p-3 small">Despite recent promising results on the application of span-based models for event reference interpretation, there is a lack of understanding of what has been improved. We present an empirical analysis of a state-of-the-art span-based event reference systems with the goal of providing the general NLP audience with a better understanding of the state of the art and reference researchers with directions for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.109/>Fast, Effective, and Self-Supervised : Transforming Masked Language Models into Universal Lexical and Sentence Encoders</a></strong><br><a href=/people/f/fangyu-liu/>Fangyu Liu</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--109><div class="card-body p-3 small">Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on <a href=https://en.wikipedia.org/wiki/Natural_language_understanding>NLI</a>, sentence similarity, or <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing tasks</a> using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts <a href=https://en.wikipedia.org/wiki/Machine_learning>MLMs</a> (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentence-level tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.110" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.110/>RuleBERT : Teaching Soft Rules to Pre-Trained Language Models<span class=acl-fixed-case>R</span>ule<span class=acl-fixed-case>BERT</span>: Teaching Soft Rules to Pre-Trained Language Models</a></strong><br><a href=/people/m/mohammed-saeed/>Mohammed Saeed</a>
|
<a href=/people/n/naser-ahmadi/>Naser Ahmadi</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/p/paolo-papotti/>Paolo Papotti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--110><div class="card-body p-3 small">While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, and we propose a revised <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> that enables the PLM to learn how to predict precise probabilities for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.111/>Stepmothers are mean and academics are pretentious : What do pretrained language models learn about you?</a></strong><br><a href=/people/r/rochelle-choenni/>Rochelle Choenni</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a>
|
<a href=/people/r/robert-van-rooij/>Robert van Rooij</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--111><div class="card-body p-3 small">In this paper, we investigate what types of <a href=https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States>stereotypical information</a> are captured by pretrained language models. We present the first dataset comprising stereotypical attributes of a range of social groups and propose a method to elicit <a href=https://en.wikipedia.org/wiki/Stereotype>stereotypes</a> encoded by pretrained language models in an unsupervised fashion. Moreover, we link the emergent stereotypes to their manifestation as basic emotions as a means to study their emotional effects in a more generalized manner. To demonstrate how our methods can be used to analyze emotion and stereotype shifts due to linguistic experience, we use <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on news sources as a case study. Our experiments expose how attitudes towards different social groups vary across models and how quickly emotions and stereotypes can shift at the fine-tuning stage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.114/>When <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> meets NLP : The devil is in the detail<span class=acl-fixed-case>NLP</span>: The devil is in the detail</a></strong><br><a href=/people/i/ivan-habernal/>Ivan Habernal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--114><div class="card-body p-3 small">Differential privacy provides a formal approach to privacy of individuals. Applications of <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> in various scenarios, such as protecting users&#8217; original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private auto-encoder for <a href=https://en.wikipedia.org/wiki/Rewriting>text rewriting</a> (Krishna et al, 2021). ADePT achieves promising results on downstream tasks while providing tight <a href=https://en.wikipedia.org/wiki/Privacy>privacy guarantees</a>. Our proof reveals that ADePT is not differentially private, thus rendering the experimental results unsubstantiated. We also quantify the impact of the error in its private mechanism, showing that the true sensitivity is higher by at least factor 6 in an optimistic case of a very small encoder&#8217;s dimension and that the amount of utterances that are not privatized could easily reach 100 % of the entire dataset. Our intention is neither to criticize the authors, nor the peer-reviewing process, but rather point out that if differential privacy applications in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> rely on formal guarantees, these should be outlined in full and put under detailed scrutiny.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.115/>Achieving Model Robustness through Discrete Adversarial Training</a></strong><br><a href=/people/m/maor-ivgi/>Maor Ivgi</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--115><div class="card-body p-3 small">Discrete adversarial attacks are symbolic perturbations to a language input that preserve the output label but lead to a prediction error. While such attacks have been extensively explored for the purpose of evaluating model robustness, their utility for improving <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> has been limited to offline augmentation only. Concretely, given a trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, attacks are used to generate perturbed (adversarial) examples, and the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is re-trained exactly once. In this work, we address this gap and leverage discrete attacks for online augmentation, where adversarial examples are generated at every training step, adapting to the changing nature of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We propose (i) a new discrete attack, based on <a href=https://en.wikipedia.org/wiki/Best-first_search>best-first search</a>, and (ii) random sampling attacks that unlike prior work are not based on expensive search-based procedures. Surprisingly, we find that <a href=https://en.wikipedia.org/wiki/Simple_random_sample>random sampling</a> leads to impressive gains in <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>, outperforming the commonly-used offline augmentation, while leading to a speedup at training time of ~10x. Furthermore, online augmentation with search-based attacks justifies the higher <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training cost</a>, significantly improving <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> on three datasets. Last, we show that our new <a href=https://en.wikipedia.org/wiki/Attack_(computing)>attack</a> substantially improves <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> compared to <a href=https://en.wikipedia.org/wiki/Attack_(computing)>prior methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.118/>How much pretraining data do <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> need to learn syntax?</a></strong><br><a href=/people/l/laura-perez-mayos/>Laura Pérez-Mayos</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--118><div class="card-body p-3 small">Transformers-based pretrained language models achieve outstanding results in many well-known NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. We explore this impact on the syntactic capabilities of RoBERTa, using <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on incremental sizes of raw text data. First, we use syntactic structural probes to determine whether models pretrained on more data encode a higher amount of syntactic information. Second, we perform a targeted syntactic evaluation to analyze the impact of pretraining data size on the syntactic generalization performance of the models. Third, we compare the performance of the different models on three downstream applications : <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, dependency parsing and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.120" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.120/>Contrastive Explanations for Model Interpretability</a></strong><br><a href=/people/a/alon-jacovi/>Alon Jacovi</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/s/shauli-ravfogel/>Shauli Ravfogel</a>
|
<a href=/people/y/yanai-elazar/>Yanai Elazar</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--120><div class="card-body p-3 small">Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that differentiate two potential decisions are captured. Our modification allows model behavior to consider only contrastive reasoning, and uncover which aspects of the input are useful for and against particular decisions. Our contrastive explanations can additionally answer for which label, and against which alternative label, is a given input feature useful. We produce contrastive explanations via both high-level abstract concept attribution and low-level input token / span attribution for two NLP classification benchmarks. Our findings demonstrate the ability of label-contrastive explanations to provide fine-grained interpretability of model decisions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--121 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.121/>On the Transferability of Adversarial Attacks against Neural Text Classifier</a></strong><br><a href=/people/l/liping-yuan/>Liping Yuan</a>
|
<a href=/people/x/xiaoqing-zheng/>Xiaoqing Zheng</a>
|
<a href=/people/y/yi-zhou/>Yi Zhou</a>
|
<a href=/people/c/cho-jui-hsieh/>Cho-Jui Hsieh</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--121><div class="card-body p-3 small">Deep neural networks are vulnerable to adversarial attacks, where a small perturbation to an input alters the model prediction. In many cases, malicious inputs intentionally crafted for one <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can fool another <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. In this paper, we present the first study to systematically investigate the transferability of adversarial examples for text classification models and explore how various factors, including <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a>, <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization scheme</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, and model capacity, affect the transferability of adversarial examples. Based on these studies, we propose a <a href=https://en.wikipedia.org/wiki/Genetic_algorithm>genetic algorithm</a> to find an ensemble of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that can be used to induce adversarial examples to fool almost all existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Such adversarial examples reflect the defects of the learning process and the data bias in the training set. Finally, we derive word replacement rules that can be used for model diagnostics from these adversarial examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.125/>mT6 : Multilingual Pretrained Text-to-Text Transformer with Translation Pairs<span class=acl-fixed-case>T</span>6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs</a></strong><br><a href=/people/z/zewen-chi/>Zewen Chi</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/s/saksham-singhal/>Saksham Singhal</a>
|
<a href=/people/x/xian-ling-mao/>Xian-Ling Mao</a>
|
<a href=/people/h/he-yan-huang/>Heyan Huang</a>
|
<a href=/people/x/xia-song/>Xia Song</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--125><div class="card-body p-3 small">Multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts, which has shown promising results on many cross-lingual tasks. In this paper, we improve multilingual text-to-text transfer Transformer with translation pairs (mT6). Specifically, we explore three cross-lingual text-to-text pre-training tasks, namely, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, translation pair span corruption, and translation span corruption. In addition, we propose a partially non-autoregressive objective for text-to-text pre-training. We evaluate the methods on seven multilingual benchmark datasets, including sentence classification, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and abstractive summarization. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/MT6>mT6</a> improves cross-lingual transferability over mT5.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.127" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.127/>Speechformer : Reducing <a href=https://en.wikipedia.org/wiki/Information_loss>Information Loss</a> in Direct Speech Translation</a></strong><br><a href=/people/s/sara-papi/>Sara Papi</a>
|
<a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--127><div class="card-body p-3 small">Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a>. However, <a href=https://en.wikipedia.org/wiki/Transformer>Transformer&#8217;s quadratic complexity</a> with respect to the input sequence length prevents its adoption as is with audio signals, which are typically represented by long sequences. Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features. Therefore, potentially useful <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a> is not accessible to higher-level layers in the <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>. To solve this issue, we propose Speechformer, an architecture that, thanks to reduced memory usage in the attention layers, avoids the initial <a href=https://en.wikipedia.org/wiki/Lossy_compression>lossy compression</a> and aggregates information only at a higher level according to more informed linguistic criteria. Experiments on three language pairs (ende / es / nl) show the efficacy of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and of up to 4.0 BLEU in a low resource scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.133" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.133/>Effects of Parameter Norm Growth During Transformer Training : Inductive Bias from Gradient Descent</a></strong><br><a href=/people/w/william-merrill/>William Merrill</a>
|
<a href=/people/v/vivek-ramanujan/>Vivek Ramanujan</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--133><div class="card-body p-3 small">The capacity of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> like the widely adopted <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a> is known to be very high. Evidence is emerging that they learn successfully due to <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude (_ 2 norm) during <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, and its implications for the <a href=https://en.wikipedia.org/wiki/Emergence>emergent representations</a> within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> approximates a discretized network with saturated activation functions. Such saturated networks are known to have a reduced capacity compared to the full network family that can be described in terms of <a href=https://en.wikipedia.org/wiki/Formal_language>formal languages</a> and <a href=https://en.wikipedia.org/wiki/Automata_theory>automata</a>. Our results suggest saturation is a new characterization of an <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> implicit in GD of particular interest for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.<tex-math>\\ell_2</tex-math> norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such &#8220;saturated&#8221; networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.136/>Knowledge-Aware Meta-learning for Low-Resource Text Classification</a></strong><br><a href=/people/h/huaxiu-yao/>Huaxiu Yao</a>
|
<a href=/people/y/ying-xin-wu/>Ying-xin Wu</a>
|
<a href=/people/m/maruan-al-shedivat/>Maruan Al-Shedivat</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--136><div class="card-body p-3 small">Meta-learning has achieved great success in leveraging the historical learned knowledge to facilitate the learning process of the new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, merely learning the knowledge from the historical tasks, adopted by current meta-learning algorithms, may not generalize well to testing tasks when they are not well-supported by training tasks. This paper studies a low-resource text classification problem and bridges the gap between meta-training and meta-testing tasks by leveraging the external knowledge bases. Specifically, we propose KGML to introduce additional <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> for each sentence learned from the extracted sentence-specific knowledge graph. The extensive experiments on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> demonstrate the effectiveness of KGML under both supervised adaptation and unsupervised adaptation settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--138 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.138.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.138" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.138/>Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning</a></strong><br><a href=/people/s/seonghyeon-ye/>Seonghyeon Ye</a>
|
<a href=/people/j/jiseon-kim/>Jiseon Kim</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--138><div class="card-body p-3 small">We introduce EfficientCL, a memory-efficient continual pretraining method that applies contrastive learning with novel <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> and curriculum learning. For <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, we stack two types of operation sequentially : cutoff and PCA jittering. While pretraining steps proceed, we apply curriculum learning by incrementing the augmentation degree for each difficulty step. After <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> is finished, contrastive learning is applied on projected embeddings of original and augmented examples. When finetuned on GLUE benchmark, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms baseline models, especially for sentence-level tasks. Additionally, this improvement is capable with only 70 % of <a href=https://en.wikipedia.org/wiki/Computer_data_storage>computational memory</a> compared to the baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.140/>DIALKI : Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization<span class=acl-fixed-case>DIALKI</span>: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization</a></strong><br><a href=/people/z/zeqiu-wu/>Zeqiu Wu</a>
|
<a href=/people/b/bo-ru-lu/>Bo-Ru Lu</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--140><div class="card-body p-3 small">Identifying relevant knowledge to be used in conversational systems that are grounded in long documents is critical to effective response generation. We introduce a knowledge identification model that leverages the document structure to provide dialogue-contextualized passage encodings and better locate knowledge relevant to the conversation. An auxiliary loss captures the history of dialogue-document connections. We demonstrate the effectiveness of our model on two document-grounded conversational datasets and provide analyses showing generalization to unseen documents and long dialogue contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.141" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.141/>Iconary : A Pictionary-Based Game for Testing Multimodal Communication with Drawings and Text</a></strong><br><a href=/people/c/christopher-clark/>Christopher Clark</a>
|
<a href=/people/j/jordi-salvador/>Jordi Salvador</a>
|
<a href=/people/d/dustin-schwenk/>Dustin Schwenk</a>
|
<a href=/people/d/derrick-bonafilia/>Derrick Bonafilia</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/e/eric-kolve/>Eric Kolve</a>
|
<a href=/people/a/alvaro-herrasti/>Alvaro Herrasti</a>
|
<a href=/people/j/jonghyun-choi/>Jonghyun Choi</a>
|
<a href=/people/s/sachin-mehta/>Sachin Mehta</a>
|
<a href=/people/s/sam-skjonsberg/>Sam Skjonsberg</a>
|
<a href=/people/c/carissa-schoenick/>Carissa Schoenick</a>
|
<a href=/people/a/aaron-sarnat/>Aaron Sarnat</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a>
|
<a href=/people/a/aniruddha-kembhavi/>Aniruddha Kembhavi</a>
|
<a href=/people/o/oren-etzioni/>Oren Etzioni</a>
|
<a href=/people/a/ali-farhadi/>Ali Farhadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--141><div class="card-body p-3 small">Communicating with humans is challenging for <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AIs</a> because it requires a shared understanding of the world, complex <a href=https://en.wikipedia.org/wiki/Semantics_(computer_science)>semantics</a> (e.g., <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> or analogies), and at times <a href=https://en.wikipedia.org/wiki/Gesture_recognition>multi-modal gestures</a> (e.g., pointing with a finger, or an arrow in a diagram). We investigate these challenges in the context of Iconary, a collaborative game of drawing and guessing based on <a href=https://en.wikipedia.org/wiki/Pictionary>Pictionary</a>, that poses a novel challenge for the research community. In Iconary, a Guesser tries to identify a phrase that a Drawer is drawing by composing <a href=https://en.wikipedia.org/wiki/Icon_(computing)>icons</a>, and the Drawer iteratively revises the drawing to help the Guesser in response. This back-and-forth often uses <a href=https://en.wikipedia.org/wiki/Canonical_criticism>canonical scenes</a>, <a href=https://en.wikipedia.org/wiki/Visual_metaphor>visual metaphor</a>, or <a href=https://en.wikipedia.org/wiki/Iconography>icon compositions</a> to express challenging words, making it an ideal test for mixing language and visual / symbolic communication in <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a>. We propose <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> to play Iconary and train them on over 55,000 games between human players. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are skillful players and are able to employ world knowledge in language models to play with words unseen during training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--142 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.142" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.142/>Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems</a></strong><br><a href=/people/f/fei-mi/>Fei Mi</a>
|
<a href=/people/w/wanhao-zhou/>Wanhao Zhou</a>
|
<a href=/people/l/lingjing-kong/>Lingjing Kong</a>
|
<a href=/people/f/fengyu-cai/>Fengyu Cai</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/b/boi-faltings/>Boi Faltings</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--142><div class="card-body p-3 small">As the labeling cost for different <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> in task-oriented dialog (ToD) systems is expensive, a major challenge is to train different <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> with the least amount of labeled data. Recently, large-scale pre-trained language models, have shown promising results for few-shot learning in ToD. In this paper, we devise a self-training approach to utilize the abundant unlabeled dialog data to further improve state-of-the-art pre-trained models in few-shot learning scenarios for ToD systems. Specifically, we propose a self-training approach that iteratively labels the most confident unlabeled data to train a stronger Student model. Moreover, a new text augmentation technique (GradAug) is proposed to better train the Student by replacing non-crucial tokens using a masked language model. We conduct extensive experiments and present analyses on four downstream tasks in ToD, including intent classification, dialog state tracking, dialog act prediction, and response selection. Empirical results demonstrate that the proposed self-training approach consistently improves state-of-the-art pre-trained models (BERT, ToD-BERT) when only a small number of labeled data are available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.143/>Contextual Rephrase Detection for Reducing Friction in Dialogue Systems</a></strong><br><a href=/people/z/zhuoyi-wang/>Zhuoyi Wang</a>
|
<a href=/people/s/saurabh-gupta/>Saurabh Gupta</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/x/xing-fan/>Xing Fan</a>
|
<a href=/people/d/dingcheng-li/>Dingcheng Li</a>
|
<a href=/people/a/alexander-hanbo-li/>Alexander Hanbo Li</a>
|
<a href=/people/c/chenlei-guo/>Chenlei Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--143><div class="card-body p-3 small">For voice assistants like <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a>, <a href=https://en.wikipedia.org/wiki/Google_Assistant>Google Assistant</a>, and <a href=https://en.wikipedia.org/wiki/Siri>Siri</a>, correctly interpreting users&#8217; intentions is of utmost importance. However, users sometimes experience friction with these assistants, caused by errors from different system components or <a href=https://en.wikipedia.org/wiki/User_error>user errors</a> such as slips of the tongue. Users tend to rephrase their queries until they get a satisfactory response. Rephrase detection is used to identify the rephrases and has long been treated as a task with pairwise input, which does not fully utilize the contextual information (e.g. users&#8217; implicit feedback). To this end, we propose a contextual rephrase detection model ContReph to automatically identify rephrases from multi-turn dialogues. We showcase how to leverage the dialogue context and user-agent interaction signals, including the user&#8217;s implicit feedback and the time gap between different turns, which can help significantly outperform the pairwise rephrase detection models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--146 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.146" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.146/>AttentionRank : Unsupervised Keyphrase Extraction using Self and Cross Attentions<span class=acl-fixed-case>A</span>ttention<span class=acl-fixed-case>R</span>ank: Unsupervised Keyphrase Extraction using Self and Cross Attentions</a></strong><br><a href=/people/h/haoran-ding/>Haoran Ding</a>
|
<a href=/people/x/xiao-luo/>Xiao Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--146><div class="card-body p-3 small">Keyword or keyphrase extraction is to identify words or phrases presenting the main topics of a document. This paper proposes the AttentionRank, a hybrid attention model, to identify keyphrases from a document in an unsupervised manner. AttentionRank calculates self-attention and cross-attention using a pre-trained language model. The self-attention is designed to determine the importance of a candidate within the context of a sentence. The cross-attention is calculated to identify the semantic relevance between a candidate and sentences within a document. We evaluate the AttentionRank on three publicly available datasets against seven <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. The results show that the AttentionRank is an effective and robust unsupervised keyphrase extraction model on both long and short documents. Source code is available on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--149 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.149" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.149/>Everything Is All It Takes : A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction</a></strong><br><a href=/people/m/mahsa-yarmohammadi/>Mahsa Yarmohammadi</a>
|
<a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/m/marc-marone/>Marc Marone</a>
|
<a href=/people/h/haoran-xu/>Haoran Xu</a>
|
<a href=/people/s/seth-ebner/>Seth Ebner</a>
|
<a href=/people/g/guanghui-qin/>Guanghui Qin</a>
|
<a href=/people/y/yunmo-chen/>Yunmo Chen</a>
|
<a href=/people/j/jialiang-guo/>Jialiang Guo</a>
|
<a href=/people/c/craig-harman/>Craig Harman</a>
|
<a href=/people/k/kenton-murray/>Kenton Murray</a>
|
<a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--149><div class="card-body p-3 small">Zero-shot cross-lingual information extraction (IE) describes the construction of an IE model for some target language, given existing annotations exclusively in some other language, typically <a href=https://en.wikipedia.org/wiki/English_language>English</a>. While the advance of pretrained multilingual encoders suggests an easy optimism of train on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, run on any language, we find through a thorough exploration and extension of techniques that a combination of approaches, both new and old, leads to better performance than any one cross-lingual strategy in particular. We explore techniques including <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>data projection</a> and self-training, and how different pretrained encoders impact them. We use English-to-Arabic IE as our initial example, demonstrating strong performance in this setting for event extraction, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, and dependency parsing. We then apply <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>data projection</a> and self-training to three tasks across eight target languages. Because no single set of techniques performs the best across all tasks, we encourage practitioners to explore various configurations of the techniques described in this work when seeking to improve on zero-shot training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.151.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.151" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.151/>Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search</a></strong><br><a href=/people/j/jialu-wang/>Jialu Wang</a>
|
<a href=/people/y/yang-liu-umich/>Yang Liu</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--151><div class="card-body p-3 small">Internet search affects people&#8217;s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in <a href=https://en.wikipedia.org/wiki/Image_retrieval>image search</a> in this work : the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> suffer from severe <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a>. Therefore, we introduce two novel debiasing approaches : an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30 K benchmarks show that our methods significantly reduce the gender bias in image search models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.152" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.152/>Style Pooling : Automatic Text Style Obfuscation for Improved Classification Fairness</a></strong><br><a href=/people/f/fatemehsadat-mireshghallah/>Fatemehsadat Mireshghallah</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--152><div class="card-body p-3 small">Text style can reveal sensitive attributes of the author (e.g. age and race) to the reader, which can, in turn, lead to privacy violations and bias in both human and algorithmic decisions based on text. For example, the style of writing in job applications might reveal protected attributes of the candidate which could lead to bias in <a href=https://en.wikipedia.org/wiki/Recruitment>hiring decisions</a>, regardless of whether <a href=https://en.wikipedia.org/wiki/Recruitment>hiring decisions</a> are made algorithmically or by humans. We propose a VAE-based framework that obfuscates stylistic features of human-generated text through style transfer, by automatically re-writing the text itself. Critically, our framework operationalizes the notion of obfuscated style in a flexible way that enables two distinct notions of obfuscated style : (1) a minimal notion that effectively intersects the various styles seen in training, and (2) a maximal notion that seeks to obfuscate by adding stylistic features of all sensitive attributes to text, in effect, computing a union of styles. Our style-obfuscation framework can be used for multiple purposes, however, we demonstrate its effectiveness in improving the fairness of downstream classifiers. We also conduct a comprehensive study on style-pooling&#8217;s effect on <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, semantic consistency, and attribute removal from text, in two and three domain style transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--153 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.153" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.153/>Modeling Disclosive Transparency in NLP Application Descriptions<span class=acl-fixed-case>NLP</span> Application Descriptions</a></strong><br><a href=/people/m/michael-saxon/>Michael Saxon</a>
|
<a href=/people/s/sharon-levy/>Sharon Levy</a>
|
<a href=/people/x/xinyi-wang/>Xinyi Wang</a>
|
<a href=/people/a/alon-albalak/>Alon Albalak</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--153><div class="card-body p-3 small">Broader disclosive transparencytruth and clarity in communication regarding the function of AI systemsis widely considered desirable. Unfortunately, it is a nebulous concept, difficult to both define and quantify. This is problematic, as previous work has demonstrated possible trade-offs and negative consequences to disclosive transparency, such as a confusion effect, where too much information clouds a reader&#8217;s understanding of what a system description means. Disclosive transparency&#8217;s subjective nature has rendered deep study into these problems and their remedies difficult. To improve this state of affairs, We introduce neural language model-based probabilistic metrics to directly model disclosive transparency, and demonstrate that they correlate with user and expert opinions of system transparency, making them a valid objective proxy. Finally, we demonstrate the use of these metrics in a pilot study quantifying the relationships between <a href=https://en.wikipedia.org/wiki/Transparency_(behavior)>transparency</a>, <a href=https://en.wikipedia.org/wiki/Confusion>confusion</a>, and user perceptions in a corpus of real NLP system descriptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.155/>Fairness-aware Class Imbalanced Learning</a></strong><br><a href=/people/s/shivashankar-subramanian/>Shivashankar Subramanian</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--155><div class="card-body p-3 small">Class imbalance is a common challenge in many NLP tasks, and has clear connections to <a href=https://en.wikipedia.org/wiki/Bias>bias</a>, in that bias in training data often leads to higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.157/>Local Word Discovery for Interactive Transcription</a></strong><br><a href=/people/w/william-lane/>William Lane</a>
|
<a href=/people/s/steven-bird/>Steven Bird</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--157><div class="card-body p-3 small">Human expertise and the participation of speech communities are essential factors in the success of technologies for low-resource languages. Accordingly, we propose a new computational task which is tuned to the available knowledge and interests in an Indigenous community, and which supports the construction of high quality texts and lexicons. The task is illustrated for <a href=https://en.wikipedia.org/wiki/Kunwinjku_language>Kunwinjku</a>, a morphologically-complex Australian language. We combine a finite state implementation of a published <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammar</a> with a partial lexicon, and apply this to a noisy phone representation of the signal. We locate known lexemes in the signal and use the morphological transducer to build these out into hypothetical, morphologically-complex words for human validation. We show that applying a single iteration of this method results in a relative transcription density gain of 17 %. Further, we find that 75 % of breath groups in the test set receive at least one correct partial or full-word suggestion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--158 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.158/>Segment, Mask, and Predict : Augmenting Chinese Word Segmentation with Self-Supervision<span class=acl-fixed-case>C</span>hinese Word Segmentation with Self-Supervision</a></strong><br><a href=/people/m/mieradilijiang-maimaiti/>Mieradilijiang Maimaiti</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a>
|
<a href=/people/y/yuanhang-zheng/>Yuanhang Zheng</a>
|
<a href=/people/g/gang-chen/>Gang Chen</a>
|
<a href=/people/k/kaiyu-huang/>Kaiyu Huang</a>
|
<a href=/people/j/ji-zhang/>Ji Zhang</a>
|
<a href=/people/h/huanbo-luan/>Huanbo Luan</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--158><div class="card-body p-3 small">Recent state-of-the-art (SOTA) effective neural network methods and fine-tuning methods based on pre-trained models (PTM) have been used in Chinese word segmentation (CWS), and they achieve great results. However, previous works focus on training the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> with the fixed corpus at every iteration. The intermediate generated information is also valuable. Besides, the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of the previous <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural methods</a> is limited by the large-scale annotated data. There are a few noises in the annotated corpus. Limited efforts have been made by previous studies to deal with such problems. In this work, we propose a self-supervised CWS approach with a straightforward and effective <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>. First, we train a word segmentation model and use it to generate the segmentation results. Then, we use a revised masked language model (MLM) to evaluate the quality of the segmentation results based on the predictions of the MLM. Finally, we leverage the evaluations to aid the training of the segmenter by improved minimum risk training. Experimental results show that our approach outperforms previous methods on 9 different CWS datasets with single criterion training and multiple criteria training and achieves better robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.160/>Fast WordPiece Tokenization<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>P</span>iece Tokenization</a></strong><br><a href=/people/x/xinying-song/>Xinying Song</a>
|
<a href=/people/a/alex-salcianu/>Alex Salcianu</a>
|
<a href=/people/y/yang-song/>Yang Song</a>
|
<a href=/people/d/dave-dopson/>Dave Dopson</a>
|
<a href=/people/d/denny-zhou/>Denny Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--160><div class="card-body p-3 small">Tokenization is a fundamental preprocessing step for almost all NLP tasks. In this paper, we propose efficient <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for the WordPiece tokenization used in BERT, from <a href=https://en.wikipedia.org/wiki/Lexical_analysis>single-word tokenization</a> to <a href=https://en.wikipedia.org/wiki/Lexical_analysis>general text (e.g., sentence) tokenization</a>. When tokenizing a single word, WordPiece uses a longest-match-first strategy, known as <a href=https://en.wikipedia.org/wiki/Maximum_matching>maximum matching</a>. The best known algorithms so far are O(n2) (where n is the input length) or O(nm) (where m is the maximum vocabulary token length). We propose a novel <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> whose tokenization complexity is strictly O(n). Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is inspired by the <a href=https://en.wikipedia.org/wiki/Aho&#8211;Corasick_algorithm>Aho-Corasick algorithm</a>. We introduce additional linkages on top of the <a href=https://en.wikipedia.org/wiki/Trie>trie</a> built from the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a>, allowing smart transitions when the <a href=https://en.wikipedia.org/wiki/Trie>trie matching</a> can not continue. For general text, we further propose an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that combines pre-tokenization (splitting the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> is 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text on average for general text tokenization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--161 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.161/>You should evaluate your <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> on marginal likelihood over tokenisations</a></strong><br><a href=/people/k/kris-cao/>Kris Cao</a>
|
<a href=/people/l/laura-rimell/>Laura Rimell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--161><div class="card-body p-3 small">Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, <a href=https://en.wikipedia.org/wiki/Formal_language>language models</a> should be evaluated on their <a href=https://en.wikipedia.org/wiki/Marginal_likelihood>marginal likelihood</a> over <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenisations</a>. We compare different estimators for the <a href=https://en.wikipedia.org/wiki/Marginal_likelihood>marginal likelihood</a> based on <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling</a>, and show that it is feasible to estimate the <a href=https://en.wikipedia.org/wiki/Marginal_likelihood>marginal likelihood</a> with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in <a href=https://en.wikipedia.org/wiki/Uncertainty>perplexity</a> to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.162/>Broaden the Vision : Geo-Diverse Visual Commonsense Reasoning</a></strong><br><a href=/people/d/da-yin/>Da Yin</a>
|
<a href=/people/l/liunian-harold-li/>Liunian Harold Li</a>
|
<a href=/people/z/ziniu-hu/>Ziniu Hu</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--162><div class="card-body p-3 small">Commonsense is defined as the knowledge on which everyone agrees. However, certain types of <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models&#8217; ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on <a href=https://en.wikipedia.org/wiki/Videocassette_recorder>VCR</a>, a standard benchmark with images primarily from Western regions. We then evaluate how well the trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including <a href=https://en.wikipedia.org/wiki/East_Asia>East Asia</a>, <a href=https://en.wikipedia.org/wiki/South_Asia>South Asia</a>, and <a href=https://en.wikipedia.org/wiki/Africa>Africa</a> is significantly lower than that for <a href=https://en.wikipedia.org/wiki/Western_world>Western region</a>. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that : 1) are concerned with <a href=https://en.wikipedia.org/wiki/Culture>culture-related scenarios</a>, e.g., <a href=https://en.wikipedia.org/wiki/Wedding>weddings</a>, <a href=https://en.wikipedia.org/wiki/Religion>religious activities</a>, and <a href=https://en.wikipedia.org/wiki/Festival>festivals</a> ; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--163 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.163" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.163/>Reference-Centric Models for Grounded Collaborative Dialogue</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/j/justin-chiu/>Justin Chiu</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--163><div class="card-body p-3 small">We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our dialogue agent accurately grounds referents from the partner&#8217;s utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades. Our agent substantially outperforms the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, obtaining a 20 % relative improvement in successful task completion in self-play evaluations and a 50 % relative improvement in success in human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.164/>CrossVQA : Scalably Generating Benchmarks for Systematically Testing VQA Generalization<span class=acl-fixed-case>C</span>ross<span class=acl-fixed-case>VQA</span>: Scalably Generating Benchmarks for Systematically Testing <span class=acl-fixed-case>VQA</span> Generalization</a></strong><br><a href=/people/a/arjun-akula/>Arjun Akula</a>
|
<a href=/people/s/soravit-changpinyo/>Soravit Changpinyo</a>
|
<a href=/people/b/boqing-gong/>Boqing Gong</a>
|
<a href=/people/p/piyush-sharma/>Piyush Sharma</a>
|
<a href=/people/s/song-chun-zhu/>Song-Chun Zhu</a>
|
<a href=/people/r/radu-soricut/>Radu Soricut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--164><div class="card-body p-3 small">One challenge in evaluating visual question answering (VQA) models in the cross-dataset adaptation setting is that the distribution shifts are multi-modal, making it difficult to identify if it is the shifts in visual or language features that play a key role. In this paper, we propose a semi-automatic framework for generating disentangled shifts by introducing a controllable visual question-answer generation (VQAG) module that is capable of generating highly-relevant and diverse question-answer pairs with the desired dataset style. We use it to create CrossVQA, a collection of test splits for assessing VQA generalization based on the VQA2, VizWiz, and Open Images datasets. We provide an analysis of our generated datasets and demonstrate its utility by using them to evaluate several state-of-the-art VQA systems. One important finding is that the visual shifts in cross-dataset VQA matter more than the <a href=https://en.wikipedia.org/wiki/Language_shift>language shifts</a>. More broadly, we present a scalable framework for systematically evaluating the machine with little human intervention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--168 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.168" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.168/>Neural Path Hunter : Reducing <a href=https://en.wikipedia.org/wiki/Hallucination>Hallucination</a> in Dialogue Systems via Path Grounding</a></strong><br><a href=/people/n/nouha-dziri/>Nouha Dziri</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/o/osmar-r-zaiane/>Osmar Zaïane</a>
|
<a href=/people/a/avishek-joey-bose/>Avishek Joey Bose</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--168><div class="card-body p-3 small">Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing <a href=https://en.wikipedia.org/wiki/Hallucination>hallucination</a> of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of <a href=https://en.wikipedia.org/wiki/Hallucination>hallucination</a> followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35 % based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.169/>Thinking Clearly, Talking Fast : Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems</a></strong><br><a href=/people/y/yicheng-zou/>Yicheng Zou</a>
|
<a href=/people/z/zhihua-liu/>Zhihua Liu</a>
|
<a href=/people/x/xingwu-hu/>Xingwu Hu</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--169><div class="card-body p-3 small">Human dialogue contains evolving concepts, and speakers naturally associate multiple concepts to compose a response. However, current dialogue models with the seq2seq framework lack the ability to effectively manage concept transitions and can hardly introduce multiple concepts to responses in a sequential decoding manner. To facilitate a controllable and coherent dialogue, in this work, we devise a concept-guided non-autoregressive model (CG-nAR) for open-domain dialogue generation. The proposed model comprises a multi-concept planning module that learns to identify multiple associated <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> from a concept graph and a customized Insertion Transformer that performs concept-guided non-autoregressive generation to complete a response. The experimental results on two public datasets show that CG-nAR can produce diverse and coherent responses, outperforming state-of-the-art baselines in both automatic and human evaluations with substantially faster inference speed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.170" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.170/>Perspective-taking and <a href=https://en.wikipedia.org/wiki/Pragmatics>Pragmatics</a> for Generating Empathetic Responses Focused on Emotion Causes</a></strong><br><a href=/people/h/hyunwoo-kim/>Hyunwoo Kim</a>
|
<a href=/people/b/byeongchang-kim/>Byeongchang Kim</a>
|
<a href=/people/g/gunhee-kim/>Gunhee Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--170><div class="card-body p-3 small">Empathy is a complex cognitive ability based on the reasoning of others&#8217; affective states. In order to better understand others and express stronger <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> in dialogues, we argue that two issues must be tackled at the same time : (i) identifying which word is the cause for the other&#8217;s emotion from his or her utterance and (ii) reflecting those specific words in the response generation. However, previous approaches for recognizing emotion cause words in <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> require sub-utterance level annotations, which can be demanding. Taking inspiration from <a href=https://en.wikipedia.org/wiki/Social_cognition>social cognition</a>, we leverage a generative estimator to infer emotion cause words from utterances with no word-level label. Also, we introduce a novel method based on <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatics</a> to make dialogue models focus on targeted words in the input during generation. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is applicable to any dialogue models with no additional training on the fly. We show our approach improves multiple best-performing dialogue agents on generating more focused empathetic responses in terms of both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--172 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.172/>CoLV : A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>LV</span>: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation</a></strong><br><a href=/people/h/haolan-zhan/>Haolan Zhan</a>
|
<a href=/people/l/lei-shen/>Lei Shen</a>
|
<a href=/people/h/hongshen-chen/>Hongshen Chen</a>
|
<a href=/people/h/hainan-zhang/>Hainan Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--172><div class="card-body p-3 small">Knowledge-grounded dialogue generation has achieved promising performance with the engagement of external knowledge sources. Typical approaches towards this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> usually perform relatively independent two sub-tasks, i.e., knowledge selection and knowledge-aware response generation. In this paper, in order to improve the diversity of both knowledge selection and knowledge-aware response generation, we propose a collaborative latent variable (CoLV) model to integrate these two aspects simultaneously in separate yet collaborative latent spaces, so as to capture the inherent correlation between knowledge selection and response generation. During generation, our proposed model firstly draws knowledge candidate from the latent space conditioned on the dialogue context, and then samples a response from another collaborative latent space conditioned on both the context and the selected knowledge. Experimental results on two widely-used knowledge-grounded dialogue datasets show that our model outperforms previous methods on both knowledge selection and response generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.176" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.176/>Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks</a></strong><br><a href=/people/q/qingbin-liu/>Qingbin Liu</a>
|
<a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/j/jiansong-chen/>Jiansong Chen</a>
|
<a href=/people/x/xunliang-cai/>Xunliang Cai</a>
|
<a href=/people/f/fan-yang/>Fan Yang</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--176><div class="card-body p-3 small">Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that <a href=https://en.wikipedia.org/wiki/KPN>KPN</a> effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25 % and 8.27 % of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--178 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.178/>Different Strokes for Different Folks : Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks</a></strong><br><a href=/people/y/yao-qiu/>Yao Qiu</a>
|
<a href=/people/j/jinchao-zhang/>Jinchao Zhang</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--178><div class="card-body p-3 small">Loading models pre-trained on the large-scale corpus in the general domain and fine-tuning them on specific downstream tasks is gradually becoming a paradigm in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. Previous investigations prove that introducing a further pre-training phase between pre-training and fine-tuning phases to adapt the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the domain-specific unlabeled data can bring positive effects. However, most of these further <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training</a> works just keep running the conventional <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training task</a>, e.g., masked language model, which can be regarded as the <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> to bridge the data distribution gap. After observing diverse downstream tasks, we suggest that different <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> may also need a further pre-training phase with appropriate training tasks to bridge the task formulation gap. To investigate this, we carry out a study for improving multiple task-oriented dialogue downstream tasks through designing various <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> at the further pre-training phase. The experiment shows that different downstream tasks prefer different further pre-training tasks, which have intrinsic correlation and most further pre-training tasks significantly improve certain target tasks rather than all. Our investigation indicates that it is of great importance and effectiveness to design appropriate further pre-training tasks modeling specific information that benefit downstream tasks. Besides, we present multiple constructive empirical conclusions for enhancing task-oriented dialogues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--179 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.179" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.179/>Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation</a></strong><br><a href=/people/l/leyang-cui/>Leyang Cui</a>
|
<a href=/people/y/yu-wu/>Yu Wu</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--179><div class="card-body p-3 small">Although pre-training models have achieved great success in dialogue generation, their performance drops dramatically when the input contains an entity that does not appear in pre-training and fine-tuning datasets (unseen entity). To address this issue, existing <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> leverage an <a href=https://en.wikipedia.org/wiki/Knowledge_base>external knowledge base</a> to generate appropriate responses. In real-world practical, the entity may not be included by the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> or suffer from the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of <a href=https://en.wikipedia.org/wiki/Knowledge_retrieval>knowledge retrieval</a>. To deal with this problem, instead of introducing <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> as the input, we force the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn a better semantic representation by predicting the information in the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, only based on the input context. Specifically, with the help of a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, we introduce two auxiliary training objectives : 1) Interpret Masked Word, which conjectures the meaning of the masked entity given the context ; 2) Hypernym Generation, which predicts the hypernym of the entity based on the context. Experiment results on two dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.181" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.181/>Unsupervised Conversation Disentanglement through <a href=https://en.wikipedia.org/wiki/Co-training>Co-Training</a></a></strong><br><a href=/people/h/hui-liu/>Hui Liu</a>
|
<a href=/people/z/zhan-shi/>Zhan Shi</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--181><div class="card-body p-3 small">Conversation disentanglement aims to separate intermingled messages into detached sessions, which is a fundamental task in understanding multi-party conversations. Existing work on conversation disentanglement relies heavily upon human-annotated datasets, which is expensive to obtain in practice. In this work, we explore training a conversation disentanglement model without referencing any human annotations. Our method is built upon the deep co-training algorithm, which consists of two <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> : a message-pair classifier and a session classifier. The former is responsible of retrieving local relations between two messages while the latter categorizes a message to a session by capturing context-aware information. Both the two <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a> are initialized respectively with pseudo data built from the <a href=https://en.wikipedia.org/wiki/Text_corpus>unannotated corpus</a>. During the deep co-training process, we use the session classifier as a reinforcement learning component to learn a session assigning policy by maximizing the local rewards given by the message-pair classifier. For the message-pair classifier, we enrich its training data by retrieving message pairs with high confidence from the disentangled sessions predicted by the session classifier. Experimental results on the large Movie Dialogue Dataset demonstrate that our proposed approach achieves competitive performance compared to previous supervised methods. Further experiments show that the predicted disentangled conversations can promote the performance on the downstream task of multi-party response selection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.184" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.184/>EARL : Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning<span class=acl-fixed-case>EARL</span>: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning</a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/y/yong-liu/>Yong Liu</a>
|
<a href=/people/w/wei-chen/>Wei Chen</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--184><div class="card-body p-3 small">Generating informative and appropriate responses is challenging but important for building human-like dialogue systems. Although various knowledge-grounded conversation models have been proposed, these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> have limitations in utilizing knowledge that infrequently occurs in the training data, not to mention integrating unseen knowledge into conversation generation. In this paper, we propose an Entity-Agnostic Representation Learning (EARL) method to introduce <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> to informative conversation generation. Unlike traditional approaches that parameterize the specific representation for each entity, EARL utilizes the context of conversations and the relational structure of knowledge graphs to learn the category representation for entities, which is generalized to incorporating unseen entities in <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> into conversation generation. Automatic and manual evaluations demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate more informative, coherent, and natural responses than baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.185" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.185/>DialogueCSE : Dialogue-based Contrastive Learning of Sentence Embeddings<span class=acl-fixed-case>D</span>ialogue<span class=acl-fixed-case>CSE</span>: Dialogue-based Contrastive Learning of Sentence Embeddings</a></strong><br><a href=/people/c/che-liu/>Che Liu</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/j/jinghua-liu/>Jinghua Liu</a>
|
<a href=/people/j/jian-sun/>Jian Sun</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--185><div class="card-body p-3 small">Learning sentence embeddings from <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a context-aware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets : the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in terms of MAP and Spearman&#8217;s correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is provided.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--187 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.187/>Not Just <a href=https://en.wikipedia.org/wiki/Classification>Classification</a> : Recognizing Implicit Discourse Relation on Joint Modeling of Classification and Generation</a></strong><br><a href=/people/f/feng-jiang/>Feng Jiang</a>
|
<a href=/people/y/yaxin-fan/>Yaxin Fan</a>
|
<a href=/people/x/xiaomin-chu/>Xiaomin Chu</a>
|
<a href=/people/p/peifeng-li/>Peifeng Li</a>
|
<a href=/people/q/qiaoming-zhu/>Qiaoming Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--187><div class="card-body p-3 small">Implicit discourse relation recognition (IDRR) is a critical task in <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a>. Previous studies only regard it as a classification task and lack an in-depth understanding of the semantics of different relations. Therefore, we first view IDRR as a <a href=https://en.wikipedia.org/wiki/Machine_learning>generation task</a> and further propose a method joint modeling of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>generation</a>. Specifically, we propose a joint model, CG-T5, to recognize the relation label and generate the target sentence containing the meaning of relations simultaneously. Furthermore, we design three target sentence forms, including the question form, for the generation model to incorporate prior knowledge. To address the issue that large discourse units are hardly embedded into the target sentence, we also propose a target sentence construction mechanism that automatically extracts core sentences from those large discourse units. Experimental results both on Chinese MCDTB and English PDTB datasets show that our model CG-T5 achieves the best performance against several state-of-the-art systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.189.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.189" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.189/>Multimodal Phased Transformer for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/j/junyan-cheng/>Junyan Cheng</a>
|
<a href=/people/i/iordanis-fostiropoulos/>Iordanis Fostiropoulos</a>
|
<a href=/people/b/barry-boehm/>Barry Boehm</a>
|
<a href=/people/m/mohammad-soleymani/>Mohammad Soleymani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--189><div class="card-body p-3 small">Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>quadratic complexity</a> of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and <a href=https://en.wikipedia.org/wiki/Memory_footprint>memory footprint</a>. SPT uses a <a href=https://en.wikipedia.org/wiki/Sampling_(signal_processing)>sampling function</a> to generate a sparse attention matrix and compress a long sequence to a shorter sequence of hidden states. SPT concurrently captures interactions between the hidden states of different modalities at every layer. To further improve the efficiency of our method, we use Layer-wise parameter sharing and Factorized Co-Attention that share parameters between Cross Attention Blocks, with minimal impact on task performance. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with three sentiment analysis datasets and achieve comparable or superior performance compared with the existing methods, with a 90 % reduction in the number of parameters. We conclude that (SPT) along with parameter sharing can capture multimodal interactions with reduced model size and improved sample efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--190 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.190.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.190/>Hierarchical Multi-label Text Classification with Horizontal and Vertical Category Correlations</a></strong><br><a href=/people/l/linli-xu/>Linli Xu</a>
|
<a href=/people/s/sijie-teng/>Sijie Teng</a>
|
<a href=/people/r/ruoyu-zhao/>Ruoyu Zhao</a>
|
<a href=/people/j/junliang-guo/>Junliang Guo</a>
|
<a href=/people/c/chi-xiao/>Chi Xiao</a>
|
<a href=/people/d/deqiang-jiang/>Deqiang Jiang</a>
|
<a href=/people/b/bo-ren/>Bo Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--190><div class="card-body p-3 small">Hierarchical multi-label text classification (HMTC) deals with the challenging task where an instance can be assigned to multiple hierarchically structured categories at the same time. The majority of prior studies either focus on reducing the HMTC task into a flat multi-label problem ignoring the vertical category correlations or exploiting the dependencies across different hierarchical levels without considering the horizontal correlations among categories at the same level, which inevitably leads to fundamental information loss. In this paper, we propose a novel HMTC framework that considers both vertical and horizontal category correlations. Specifically, we first design a loosely coupled graph convolutional neural network as the representation extractor to obtain <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> for words, documents, and, more importantly, level-wise representations for categories, which are not considered in previous works. Then, the learned category representations are adopted to capture the vertical dependencies among levels of category hierarchy and model the horizontal correlations. Finally, based on the document embeddings and category embeddings, we design a <a href=https://en.wikipedia.org/wiki/Hybrid_algorithm>hybrid algorithm</a> to predict the categories of the entire hierarchical structure. Extensive experiments conducted on real-world HMTC datasets validate the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> with significant improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--192 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.192" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.192/>FLiText : A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks<span class=acl-fixed-case>FL</span>i<span class=acl-fixed-case>T</span>ext: A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks</a></strong><br><a href=/people/c/chen-liu/>Chen Liu</a>
|
<a href=/people/z/zhang-mengchao/>Zhang Mengchao</a>
|
<a href=/people/f/fu-zhibing/>Fu Zhibing</a>
|
<a href=/people/p/panpan-hou/>Panpan Hou</a>
|
<a href=/people/y/yu-li/>Yu Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--192><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>, state-of-the-art (SOTA) semi-supervised learning (SSL) frameworks have shown great performance on deep pre-trained language models such as BERT, and are expected to significantly reduce the demand for manual labeling. However, our empirical studies indicate that these frameworks are not suitable for lightweight models such as TextCNN, LSTM and etc. In this work, we develop a new SSL framework called FLiText, which stands for Faster and Lighter semi-supervised Text classification. FLiText introduces an inspirer network together with the consistency regularization framework, which leverages a generalized regular constraint on the lightweight models for efficient <a href=https://en.wikipedia.org/wiki/Transport_Layer_Security>SSL</a>. As a result, FLiText obtains new SOTA performance for lightweight models across multiple SSL benchmarks on text classification. Compared with existing SOTA SSL methods on TextCNN, FLiText improves the accuracy of lightweight model TextCNN from 51.00 % to 90.49 % on <a href=https://en.wikipedia.org/wiki/IMDb>IMDb</a>, 39.8 % to 58.06 % on <a href=https://en.wikipedia.org/wiki/Yelp>Yelp-5</a>, and from 55.3 % to 65.08 % on Yahoo ! Answer. In addition, compared with the <a href=https://en.wikipedia.org/wiki/Supervised_learning>fully supervised method</a> on the full dataset, FLiText just uses less than 1 % of labeled data to improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> by 6.59 %, 3.94 %, and 3.22 % on the datasets of <a href=https://en.wikipedia.org/wiki/IMDb>IMDb</a>, <a href=https://en.wikipedia.org/wiki/Yelp>Yelp-5</a>, and Yahoo ! Answer respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.195.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.195" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.195/>Transductive Learning for Unsupervised Text Style Transfer</a></strong><br><a href=/people/f/fei-xiao/>Fei Xiao</a>
|
<a href=/people/l/liang-pang/>Liang Pang</a>
|
<a href=/people/y/yanyan-lan/>Yanyan Lan</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/h/huawei-shen/>Huawei Shen</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--195><div class="card-body p-3 small">Unsupervised style transfer models are mainly based on an inductive learning approach, which represents the style as embeddings, decoder parameters, or discriminator parameters and directly applies these general rules to the test cases. However, the lacking of <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> hinders the ability of these inductive learning methods on this task. As a result, it is likely to cause severe inconsistent style expressions, like &#8216;the salad is rude&#8217;. To tackle this problem, we propose a novel transductive learning approach in this paper, based on a retrieval-based context-aware style representation. Specifically, an attentional encoder-decoder with a retriever framework is utilized. It involves top-K relevant sentences in the target style in the transfer process. In this way, we can learn a context-aware style embedding to alleviate the above inconsistency problem. In this paper, both sparse (BM25) and dense retrieval functions (MIPS) are used, and two objective functions are designed to facilitate joint learning. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. The proposed transductive learning approach is general and effective to the task of unsupervised style transfer, and we will apply it to the other two typical methods in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.199/>ConRPG : <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>Paraphrase Generation</a> using Contexts as Regularizer<span class=acl-fixed-case>C</span>on<span class=acl-fixed-case>RPG</span>: Paraphrase Generation using Contexts as Regularizer</a></strong><br><a href=/people/y/yuxian-meng/>Yuxian Meng</a>
|
<a href=/people/x/xiang-ao/>Xiang Ao</a>
|
<a href=/people/q/qing-he/>Qing He</a>
|
<a href=/people/x/xiaofei-sun/>Xiaofei Sun</a>
|
<a href=/people/q/qinghong-han/>Qinghong Han</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/c/chun-fan/>Chun Fan</a>
|
<a href=/people/j/jiwei-li/>Jiwei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--199><div class="card-body p-3 small">A long-standing issue with <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> is the lack of reliable supervision signals. In this paper, we propose a new <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised paradigm</a> for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> based on the assumption that the probabilities of generating two sentences with the same meaning given the same context should be the same. Inspired by this fundamental idea, we propose a pipelined system which consists of paraphrase candidate generation based on contextual language models, candidate filtering using scoring functions, and paraphrase model training based on the selected candidates. The proposed paradigm offers merits over existing paraphrase generation methods : (1) using the context regularizer on meanings, the model is able to generate massive amounts of high-quality paraphrase pairs ; (2) the combination of the huge amount of paraphrase candidates and further diversity-promoting filtering yields paraphrases with more lexical and syntactic diversity ; and (3) using human-interpretable scoring functions to select paraphrase pairs from candidates, the proposed framework provides a channel for developers to intervene with the data generation process, leading to a more controllable model. Experimental results across different tasks and datasets demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> significantly outperforms existing paraphrase approaches in both supervised and unsupervised setups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--202 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.202/>Asking Questions Like Educational Experts : Automatically Generating Question-Answer Pairs on Real-World Examination Data<span class=acl-fixed-case>A</span>utomatically Generating Question-Answer Pairs on Real-World Examination Data</a></strong><br><a href=/people/f/fanyi-qu/>Fanyi Qu</a>
|
<a href=/people/x/xin-jia/>Xin Jia</a>
|
<a href=/people/y/yunfang-wu/>Yunfang Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--202><div class="card-body p-3 small">Generating high quality question-answer pairs is a hard but meaningful task. Although previous works have achieved great results on answer-aware question generation, it is difficult to apply them into practical application in the education field. This paper for the first time addresses the question-answer pair generation task on the real-world examination data, and proposes a new unified framework on RACE. To capture the important information of the input passage we first automatically generate (rather than extracting) keyphrases, thus this task is reduced to keyphrase-question-answer triplet joint generation. Accordingly, we propose a multi-agent communication model to generate and optimize the question and keyphrases iteratively, and then apply the generated question and keyphrases to guide the generation of answers. To establish a solid benchmark, we build our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the strong generative pre-training model. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> makes great breakthroughs in the question-answer pair generation task. Moreover, we make a comprehensive analysis on our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, suggesting new directions for this challenging task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.203" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.203/>Syntactically-Informed Unsupervised Paraphrasing with Non-Parallel Data</a></strong><br><a href=/people/e/erguang-yang/>Erguang Yang</a>
|
<a href=/people/m/mingtong-liu/>Mingtong Liu</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/y/yujie-zhang/>Yujie Zhang</a>
|
<a href=/people/y/yao-meng/>Yao Meng</a>
|
<a href=/people/c/changjian-hu/>Changjian Hu</a>
|
<a href=/people/j/jinan-xu/>Jinan Xu</a>
|
<a href=/people/y/yufeng-chen/>Yufeng Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--203><div class="card-body p-3 small">Previous works on syntactically controlled paraphrase generation heavily rely on large-scale parallel paraphrase data that is not easily available for many languages and domains. In this paper, we take this research direction to the extreme and investigate whether it is possible to learn syntactically controlled paraphrase generation with nonparallel data. We propose a syntactically-informed unsupervised paraphrasing model based on conditional variational auto-encoder (VAE) which can generate texts in a specified syntactic structure. Particularly, we design a two-stage learning method to effectively train the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> using non-parallel data. The conditional VAE is trained to reconstruct the input sentence according to the given input and its <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. Furthermore, to improve the syntactic controllability and semantic consistency of the pre-trained conditional VAE, we fine-tune it using syntax controlling and cycle reconstruction learning objectives, and employ Gumbel-Softmax to combine these new learning objectives. Experiment results demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained only on non-parallel data is capable of generating diverse paraphrases with specified <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. Additionally, we validate the effectiveness of our method for generating syntactically adversarial examples on the sentiment analysis task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.204" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.204/>Exploring Task Difficulty for Few-Shot Relation Extraction</a></strong><br><a href=/people/j/jiale-han/>Jiale Han</a>
|
<a href=/people/b/bo-cheng/>Bo Cheng</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--204><div class="card-body p-3 small">Few-shot relation extraction (FSRE) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive results achieved, existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> still perform suboptimally when handling hard FSRE tasks, where the relations are fine-grained and similar to each other. We argue this is largely because existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not distinguish <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>hard tasks</a> from easy ones in the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>learning process</a>. In this paper, we introduce a novel approach based on contrastive learning that learns better <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> by exploiting relation label information. We further design a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> that allows the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to adaptively learn how to focus on hard tasks. Experiments on two standard datasets demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.208/>A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling</a></strong><br><a href=/people/f/feiliang-ren/>Feiliang Ren</a>
|
<a href=/people/l/longhui-zhang/>Longhui Zhang</a>
|
<a href=/people/s/shujuan-yin/>Shujuan Yin</a>
|
<a href=/people/x/xiaofeng-zhao/>Xiaofeng Zhao</a>
|
<a href=/people/s/shilei-liu/>Shilei Liu</a>
|
<a href=/people/b/bochao-li/>Bochao Li</a>
|
<a href=/people/y/yaduo-liu/>Yaduo Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--208><div class="card-body p-3 small">Table filling based relational triple extraction methods are attracting growing research interests due to their promising performance and their abilities on extracting triples from complex sentences. However, this kind of methods are far from their full potential because most of them only focus on using local features but ignore the global associations of relations and of token pairs, which increases the possibility of overlooking some important information during triple extraction. To overcome this deficiency, we propose a global feature-oriented triple extraction model that makes full use of the mentioned two kinds of global associations. Specifically, we first generate a <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>table feature</a> for each relation. Then two kinds of global associations are mined from the generated <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>table features</a>. Next, the mined global associations are integrated into the table feature of each relation. This generate-mine-integrate process is performed multiple times so that the table feature of each relation is refined step by step. Finally, each relation&#8217;s table is filled based on its refined table feature, and all triples linked to this relation are extracted based on its filled table. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on three <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. Experimental results show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is effective and it achieves state-of-the-art results on all of these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. The source code of our work is available at : https://github.com/neukg/GRTE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--212 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.212/>MapRE : An Effective Semantic Mapping Approach for Low-resource Relation Extraction<span class=acl-fixed-case>M</span>ap<span class=acl-fixed-case>RE</span>: An Effective Semantic Mapping Approach for Low-resource Relation Extraction</a></strong><br><a href=/people/m/manqing-dong/>Manqing Dong</a>
|
<a href=/people/c/chunguang-pan/>Chunguang Pan</a>
|
<a href=/people/z/zhipeng-luo/>Zhipeng Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--212><div class="card-body p-3 small">Neural relation extraction models have shown promising results in recent years ; however, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance drops dramatically given only a few training samples. Recent works try leveraging the advance in few-shot learning to solve the low resource problem, where they train label-agnostic models to directly compare the semantic similarities among context sentences in the embedding space. However, the label-aware information, i.e., the relation label that contains the semantic knowledge of the relation itself, is often neglected for prediction. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> considering both label-agnostic and label-aware semantic mapping information for low resource relation extraction. We show that incorporating the above two types of mapping information in both pretraining and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> can significantly improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance on low-resource relation extraction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--222 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.222" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.222/>Weakly-supervised Text Classification Based on Keyword Graph</a></strong><br><a href=/people/l/lu-zhang/>Lu Zhang</a>
|
<a href=/people/j/jiandong-ding/>Jiandong Ding</a>
|
<a href=/people/y/yi-xu/>Yi Xu</a>
|
<a href=/people/y/yingyao-liu/>Yingyao Liu</a>
|
<a href=/people/s/shuigeng-zhou/>Shuigeng Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--222><div class="card-body p-3 small">Weakly-supervised text classification has received much attention in recent years for it can alleviate the heavy burden of annotating massive data. Among them, keyword-driven methods are the mainstream where user-provided keywords are exploited to generate pseudo-labels for unlabeled texts. However, existing methods treat <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> independently, thus ignore the correlation among them, which should be useful if properly exploited. In this paper, we propose a novel framework called ClassKG to explore keyword-keyword correlation on keyword graph by GNN. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is an <a href=https://en.wikipedia.org/wiki/Iterative_and_incremental_development>iterative process</a>. In each iteration, we first construct a keyword graph, so the task of assigning pseudo labels is transformed to annotating keyword subgraphs. To improve the annotation quality, we introduce a self-supervised task to pretrain a subgraph annotator, and then finetune it. With the pseudo labels generated by the subgraph annotator, we then train a <a href=https://en.wikipedia.org/wiki/Text_classification>text classifier</a> to classify the unlabeled texts. Finally, we re-extract <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> from the classified texts. Extensive experiments on both long-text and short-text datasets show that our method substantially outperforms the existing ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--223 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.223" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.223/>Efficient-FedRec : Efficient Federated Learning Framework for Privacy-Preserving News Recommendation<span class=acl-fixed-case>F</span>ed<span class=acl-fixed-case>R</span>ec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation</a></strong><br><a href=/people/j/jingwei-yi/>Jingwei Yi</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/r/ruixuan-liu/>Ruixuan Liu</a>
|
<a href=/people/g/guangzhong-sun/>Guangzhong Sun</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--223><div class="card-body p-3 small">News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users&#8217; historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple clients to collaboratively train models without sharing their private data. However, the computation and communication cost of directly learning many existing news recommendation models in a federated way are unacceptable for user clients. In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation. Instead of training and communicating the whole <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, we decompose the news recommendation model into a large news model maintained in the server and a light-weight user model shared on both server and clients, where news representations and user model are communicated between server and clients. More specifically, the clients request the <a href=https://en.wikipedia.org/wiki/User_model>user model</a> and <a href=https://en.wikipedia.org/wiki/News>news representations</a> from the server, and send their locally computed gradients to the server for <a href=https://en.wikipedia.org/wiki/News_aggregator>aggregation</a>. The server updates its global user model with the aggregated gradients, and further updates its news model to infer updated news representations. Since the local gradients may contain private information, we propose a secure aggregation method to aggregate gradients in a privacy-preserving way. Experiments on two real-world datasets show that our method can reduce the computation and communication cost on clients while keep promising model performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.224" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.224/>RocketQAv2 : A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking<span class=acl-fixed-case>R</span>ocket<span class=acl-fixed-case>QA</span>v2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking</a></strong><br><a href=/people/r/ruiyang-ren/>Ruiyang Ren</a>
|
<a href=/people/y/yingqi-qu/>Yingqi Qu</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/w/wayne-xin-zhao/>Wayne Xin Zhao</a>
|
<a href=/people/q/qiaoqiao-she/>QiaoQiao She</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--224><div class="card-body p-3 small">In various <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedures</a> contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other&#8217;s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.com/PaddlePaddle/RocketQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.227/>Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrieval</a></strong><br><a href=/people/x/xueguang-ma/>Xueguang Ma</a>
|
<a href=/people/m/minghan-li/>Minghan Li</a>
|
<a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--227><div class="card-body p-3 small">Recent work has shown that dense passage retrieval techniques achieve better ranking accuracy in open-domain question answering compared to sparse retrieval techniques such as <a href=https://en.wikipedia.org/wiki/BM25>BM25</a>, but at the cost of large space and memory requirements. In this paper, we analyze the <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundancy</a> present in encoded dense vectors and show that the default dimension of 768 is unnecessarily large. To improve space efficiency, we propose a simple unsupervised compression pipeline that consists of <a href=https://en.wikipedia.org/wiki/Principal_component_analysis>principal component analysis (PCA)</a>, product quantization, and hybrid search. We further investigate other supervised baselines and find surprisingly that unsupervised PCA outperforms them in some settings. We perform extensive experiments on five question answering datasets and demonstrate that our best pipeline achieves good accuracyspace trade-offs, for example, 48 compression with less than 3 % drop in top-100 retrieval accuracy on average or 96 compression with less than 4 % drop. Code and data are available at.<tex-math>48\\times</tex-math> compression with less than 3% drop in top-100 retrieval accuracy on average or <tex-math>96\\times</tex-math> compression with less than 4% drop. Code and data are available at <url>http://pyserini.io/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.228/>Relation Extraction with Word Graphs from N-grams</a></strong><br><a href=/people/h/han-qin/>Han Qin</a>
|
<a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/y/yan-song/>Yan Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--228><div class="card-body p-3 small">Most recent studies for relation extraction (RE) leverage the dependency tree of the input sentence to incorporate syntax-driven contextual information to improve model performance, with little attention paid to the limitation where high-quality dependency parsers in most cases unavailable, especially for in-domain scenarios. To address this limitation, in this paper, we propose attentive graph convolutional networks (A-GCN) to improve neural RE methods with an unsupervised manner to build the context graph, without relying on the existence of a dependency parser. Specifically, we construct the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> from n-grams extracted from a lexicon built from pointwise mutual information (PMI) and apply <a href=https://en.wikipedia.org/wiki/Attention>attention</a> over the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. Therefore, different word pairs from the contexts within and across <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> are weighted in the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.229" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.229/>A Bayesian Framework for Information-Theoretic Probing<span class=acl-fixed-case>B</span>ayesian Framework for Information-Theoretic Probing</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--229><div class="card-body p-3 small">Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that <a href=https://en.wikipedia.org/wiki/Probing>probing</a> should be seen as approximating a <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a>. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a>, however, assumes the true <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> of a pair of <a href=https://en.wikipedia.org/wiki/Random_variable>random variables</a> is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agentsallowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and <a href=https://en.wikipedia.org/wiki/Information>information</a> can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--230 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.230/>Masked Language Modeling and the Distributional Hypothesis : Order Word Matters Pre-training for Little</a></strong><br><a href=/people/k/koustuv-sinha/>Koustuv Sinha</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--230><div class="card-body p-3 small">A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation : MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> after fine-tuning on many downstream tasksincluding tasks specifically designed to be challenging for models that ignore <a href=https://en.wikipedia.org/wiki/Word_order>word order</a>. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pre-training, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.234" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.234/>Linguistic Dependencies and Statistical Dependence</a></strong><br><a href=/people/j/jacob-louis-hoover/>Jacob Louis Hoover</a>
|
<a href=/people/w/wenyu-du/>Wenyu Du</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/t/timothy-odonnell/>Timothy J. O’Donnell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--234><div class="card-body p-3 small">Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>, <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a>, and <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>statistical dependence</a> between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most 0.5. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.<tex-math>\\approx 0.5</tex-math>. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--236 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.236/>A Simple and Effective Positional Encoding for Transformers</a></strong><br><a href=/people/p/pu-chin-chen/>Pu-Chin Chen</a>
|
<a href=/people/h/henry-tsai/>Henry Tsai</a>
|
<a href=/people/s/srinadh-bhojanapalli/>Srinadh Bhojanapalli</a>
|
<a href=/people/h/hyung-won-chung/>Hyung Won Chung</a>
|
<a href=/people/y/yin-wen-chang/>Yin-Wen Chang</a>
|
<a href=/people/c/chun-sung-ferng/>Chun-Sung Ferng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--236><div class="card-body p-3 small">Transformer models are permutation equivariant. To supply the order and type information of the input tokens, position and segment embeddings are usually added to the input. Recent works proposed variations of positional encodings with relative position encodings achieving better performance. Our analysis shows that the gain actually comes from moving positional information to attention layer from the input. Motivated by this, we introduce Decoupled Positional Attention for Transformers (DIET), a simple yet effective mechanism to encode position and segment information into the Transformer models. The proposed method has faster training and inference time, while achieving competitive performance on GLUE, XTREME and WMT benchmarks. We further generalize our method to long-range transformers and show performance gain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.237" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.237/>Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models</a></strong><br><a href=/people/a/anlin-qu/>Anlin Qu</a>
|
<a href=/people/j/jianwei-niu/>Jianwei Niu</a>
|
<a href=/people/s/shasha-mo/>Shasha Mo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--237><div class="card-body p-3 small">Relative position embedding (RPE) is a successful method to explicitly and efficaciously encode position information into Transformer models. In this paper, we investigate the potential problems in Shaw-RPE and XL-RPE, which are the most representative and prevalent RPEs, and propose two novel RPEs called Low-level Fine-grained High-level Coarse-grained (LFHC) RPE and Gaussian Cumulative Distribution Function (GCDF) RPE. LFHC-RPE is an improvement of Shaw-RPE, which enhances the perception ability at medium and long relative positions. GCDF-RPE utilizes the excellent properties of the <a href=https://en.wikipedia.org/wiki/Gaussian_function>Gaussian function</a> to amend the prior encoding mechanism in XL-RPE. Experimental results on nine authoritative datasets demonstrate the effectiveness of our methods empirically. Furthermore, GCDF-RPE achieves the best overall performance among five different <a href=https://en.wikipedia.org/wiki/Randomized_algorithm>RPEs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.238" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.238/>Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup</a></strong><br><a href=/people/g/guang-liu/>Guang Liu</a>
|
<a href=/people/y/yuzhao-mao/>Yuzhao Mao</a>
|
<a href=/people/h/huang-hailong/>Huang Hailong</a>
|
<a href=/people/g/gao-weiguo/>Gao Weiguo</a>
|
<a href=/people/l/li-xuan/>Li Xuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--238><div class="card-body p-3 small">Mixup is a recent <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizer</a> for current deep classification networks. Through training a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> on <a href=https://en.wikipedia.org/wiki/Convex_combination>convex combinations</a> of pairs of examples and their labels, it imposes locally linear constraints on the model&#8217;s input space. However, such strict linear constraints often lead to <a href=https://en.wikipedia.org/wiki/Underfitting>under-fitting</a> which degrades the effects of <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a>. Noticeably, this issue is getting more serious when the resource is extremely limited. To address these issues, we propose the Adversarial Mixing Policy (AMP), organized in a min-max-rand formulation, to relax the Locally Linear Constraints in Mixup. Specifically, AMP adds a small <a href=https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)>adversarial perturbation</a> to the mixing coefficients rather than the examples. Thus, slight <a href=https://en.wikipedia.org/wiki/Nonlinear_system>non-linearity</a> is injected in-between the synthetic examples and synthetic labels. By training on these <a href=https://en.wikipedia.org/wiki/Data>data</a>, the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep networks</a> are further regularized, and thus achieve a lower predictive error rate. Experiments on five text classification benchmarks and five backbone models have empirically shown that our methods reduce the error rate over Mixup variants in a significant margin (up to 31.3 %), especially in low-resource conditions (up to 17.5 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--246 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.246/>Layer-wise Model Pruning based on <a href=https://en.wikipedia.org/wiki/Mutual_information>Mutual Information</a></a></strong><br><a href=/people/c/chun-fan/>Chun Fan</a>
|
<a href=/people/j/jiwei-li/>Jiwei Li</a>
|
<a href=/people/t/tianwei-zhang/>Tianwei Zhang</a>
|
<a href=/people/x/xiang-ao/>Xiang Ao</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/y/yuxian-meng/>Yuxian Meng</a>
|
<a href=/people/x/xiaofei-sun/>Xiaofei Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--246><div class="card-body p-3 small">Inspired by mutual information (MI) based feature selection in SVMs and <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>, in this paper, we propose MI-based layer-wise pruning : for each layer of a multi-layer neural network, neurons with higher values of MI with respect to preserved neurons in the upper layer are preserved. Starting from the top softmax layer, layer-wise pruning proceeds in a top-down fashion until reaching the bottom word embedding layer. The proposed pruning strategy offers merits over weight-based pruning techniques : (1) it avoids irregular memory access since representations and matrices can be squeezed into their smaller but dense counterparts, leading to greater speedup ; (2) in a manner of top-down pruning, the proposed method operates from a more global perspective based on training signals in the top layer, and prunes each layer by propagating the effect of global signals through layers, leading to better performances at the same sparsity level. Extensive experiments show that at the same sparsity level, the proposed strategy offers both greater speedup and higher performances than weight-based pruning methods (e.g., magnitude pruning, movement pruning).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--249 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.249" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.249/>Frustratingly Simple Pretraining Alternatives to Masked Language Modeling</a></strong><br><a href=/people/a/atsuki-yamaguchi/>Atsuki Yamaguchi</a>
|
<a href=/people/g/george-chrysostomou/>George Chrysostomou</a>
|
<a href=/people/k/katerina-margatina/>Katerina Margatina</a>
|
<a href=/people/n/nikolaos-aletras/>Nikolaos Aletras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--249><div class="card-body p-3 small">Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [ MASK ] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside <a href=https://en.wikipedia.org/wiki/Multilevel_marketing>MLM</a> other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of <a href=https://en.wikipedia.org/wiki/Machine_learning>MLM</a>. Empirical results on GLUE and SQUAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41 % of the BERT-BASE&#8217;s parameters, BERT-MEDIUM results in only a 1 % drop in GLUE scores with our best objective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.250.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--250 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.250 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.250" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.250/>HRKD : Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression<span class=acl-fixed-case>HRKD</span>: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression</a></strong><br><a href=/people/c/chenhe-dong/>Chenhe Dong</a>
|
<a href=/people/y/yaliang-li/>Yaliang Li</a>
|
<a href=/people/y/ying-shen/>Ying Shen</a>
|
<a href=/people/m/minghui-qiu/>Minghui Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--250><div class="card-body p-3 small">On many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural network methods</a>. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at.<url>https://github.com/cheneydon/hrkd</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--252 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.252/>Re-embedding Difficult Samples via Mutual Information Constrained Semantically Oversampling for Imbalanced Text Classification</a></strong><br><a href=/people/j/jiachen-tian/>Jiachen Tian</a>
|
<a href=/people/s/shizhan-chen/>Shizhan Chen</a>
|
<a href=/people/x/xiaowang-zhang/>Xiaowang Zhang</a>
|
<a href=/people/z/zhiyong-feng/>Zhiyong Feng</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/s/shaojuan-wu/>Shaojuan Wu</a>
|
<a href=/people/c/chunliu-dou/>Chunliu Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--252><div class="card-body p-3 small">Difficult samples of the minority class in imbalanced text classification are usually hard to be classified as they are embedded into an overlapping semantic region with the majority class. In this paper, we propose a Mutual Information constrained Semantically Oversampling framework (MISO) that can generate anchor instances to help the backbone network determine the re-embedding position of a non-overlapping representation for each difficult sample. MISO consists of (1) a semantic fusion module that learns entangled semantics among difficult and majority samples with an adaptive multi-head attention mechanism, (2) a mutual information loss that forces our model to learn new representations of entangled semantics in the non-overlapping region of the minority class, and (3) a coupled adversarial encoder-decoder that fine-tunes disentangled semantic representations to remain their correlations with the minority class, and then using these disentangled semantic representations to generate anchor instances for each difficult sample. Experiments on a variety of imbalanced text classification tasks demonstrate that anchor instances help <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> achieve significant improvements over strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--255 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.255/>MetaTS : Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision<span class=acl-fixed-case>M</span>eta<span class=acl-fixed-case>TS</span>: Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision</a></strong><br><a href=/people/z/zheng-li/>Zheng Li</a>
|
<a href=/people/d/danqing-zhang/>Danqing Zhang</a>
|
<a href=/people/t/tianyu-cao/>Tianyu Cao</a>
|
<a href=/people/y/ying-wei/>Ying Wei</a>
|
<a href=/people/y/yiwei-song/>Yiwei Song</a>
|
<a href=/people/b/bing-yin/>Bing Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--255><div class="card-body p-3 small">Sequence labeling aims to predict a fine-grained sequence of labels for the text. However, such <a href=https://en.wikipedia.org/wiki/Formulation>formulation</a> hinders the effectiveness of <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a> due to the lack of token-level annotated data. This is exacerbated when we meet a diverse range of languages. In this work, we explore multilingual sequence labeling with minimal supervision using a single unified model for <a href=https://en.wikipedia.org/wiki/Multilingualism>multiple languages</a>. Specifically, we propose a Meta Teacher-Student (MetaTS) Network, a novel meta learning method to alleviate data scarcity by leveraging large multilingual unlabeled data. Prior teacher-student frameworks of self-training rely on rigid teaching strategies, which may hardly produce high-quality pseudo-labels for consecutive and interdependent tokens. On the contrary, MetaTS allows the teacher to dynamically adapt its pseudo-annotation strategies by the student&#8217;s feedback on the generated pseudo-labeled data of each language and thus mitigate error propagation from noisy pseudo-labels. Extensive experiments on both public and real-world multilingual sequence labeling datasets empirically demonstrate the effectiveness of MetaTS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--256 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.256" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.256/>Neural Machine Translation with Heterogeneous Topic Knowledge Embeddings</a></strong><br><a href=/people/w/weixuan-wang/>Weixuan Wang</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--256><div class="card-body p-3 small">Neural Machine Translation (NMT) has shown a strong ability to utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>local context</a> to disambiguate the meaning of words. However, it remains a challenge for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> to leverage <a href=https://en.wikipedia.org/wiki/Context_(language_use)>broader context information</a> like topics. In this paper, we propose heterogeneous ways of embedding topic information at the sentence level into an NMT model to improve <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance. Specifically, the topic information can be incorporated as pre-encoder topic embedding, post-encoder topic embedding, and decoder topic embedding to increase the likelihood of selecting target words from the same topic of the source sentence. Experimental results show that NMT models with the proposed topic knowledge embedding outperform the baselines on the English-German and English-French translation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--259 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.259/>Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding</a></strong><br><a href=/people/y/yingmei-guo/>Yingmei Guo</a>
|
<a href=/people/l/linjun-shou/>Linjun Shou</a>
|
<a href=/people/j/jian-pei/>Jian Pei</a>
|
<a href=/people/m/ming-gong/>Ming Gong</a>
|
<a href=/people/m/mingxing-xu/>Mingxing Xu</a>
|
<a href=/people/z/zhiyong-wu/>Zhiyong Wu</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--259><div class="card-body p-3 small">Lack of training data presents a grand challenge to scaling out spoken language understanding (SLU) to low-resource languages. Although various data augmentation approaches have been proposed to synthesize training data in low-resource target languages, the augmented data sets are often noisy, and thus impede the performance of SLU models. In this paper we focus on mitigating <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in augmented data. We develop a denoising training approach. Multiple models are trained with data produced by various <a href=https://en.wikipedia.org/wiki/Augmented_reality>augmented methods</a>. Those <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> provide supervision signals to each other. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms the existing <a href=https://en.wikipedia.org/wiki/State_(computer_science)>state</a> of the art by 3.05 and 4.24 percentage points on two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>, respectively. The code will be made open sourced on github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.262/>Encouraging Lexical Translation Consistency for Document-Level Neural Machine Translation</a></strong><br><a href=/people/x/xinglin-lyu/>Xinglin Lyu</a>
|
<a href=/people/j/junhui-li/>Junhui Li</a>
|
<a href=/people/z/zhengxian-gong/>Zhengxian Gong</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--262><div class="card-body p-3 small">Recently a number of approaches have been proposed to improve <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance for document-level neural machine translation (NMT). However, few are focusing on the subject of lexical translation consistency. In this paper we apply one translation per discourse in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>, and aim to encourage lexical translation consistency for document-level NMT. This is done by first obtaining a word link for each source word in a document, which tells the positions where the source word appears. Then we encourage the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> of those words within a <a href=https://en.wikipedia.org/wiki/Hyperlink>link</a> to be consistent in two ways. On the one hand, when encoding sentences within a document we properly share <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> of those words. On the other hand, we propose an auxiliary loss function to better constrain that their translation should be consistent. Experimental results on ChineseEnglish and EnglishFrench translation tasks show that our approach not only achieves state-of-the-art performance in BLEU scores, but also greatly improves lexical consistency in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--267 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.267/>Self-Supervised Quality Estimation for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/y/yuanhang-zheng/>Yuanhang Zheng</a>
|
<a href=/people/z/zhixing-tan/>Zhixing Tan</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/m/mieradilijiang-maimaiti/>Mieradilijiang Maimaiti</a>
|
<a href=/people/h/huanbo-luan/>Huanbo Luan</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--267><div class="card-body p-3 small">Quality estimation (QE) of machine translation (MT) aims to evaluate the quality of machine-translated sentences without references and is important in practical applications of MT. Training QE models require massive parallel data with hand-crafted quality annotations, which are time-consuming and labor-intensive to obtain. To address the issue of the absence of annotated training data, previous studies attempt to develop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> on several QE tasks in different language pairs and domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.269.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--269 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.269 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.269.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.269" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.269/>STANKER : Stacking Network based on Level-grained Attention-masked BERT for Rumor Detection on Social Media<span class=acl-fixed-case>STANKER</span>: Stacking Network based on Level-grained Attention-masked <span class=acl-fixed-case>BERT</span> for Rumor Detection on Social Media</a></strong><br><a href=/people/d/dongning-rao/>Dongning Rao</a>
|
<a href=/people/x/xin-miao/>Xin Miao</a>
|
<a href=/people/z/zhihua-jiang/>Zhihua Jiang</a>
|
<a href=/people/r/ran-li/>Ran Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--269><div class="card-body p-3 small">Rumor detection on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> puts pre-trained language models (LMs), such as BERT, and auxiliary features, such as comments, into use. However, on the one hand, rumor detection datasets in Chinese companies with comments are rare ; on the other hand, intensive interaction of attention on Transformer-based models like BERT may hinder performance improvement. To alleviate these problems, we build a new Chinese microblog dataset named Weibo20 by collecting posts and associated comments from <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Sina Weibo</a> and propose a new ensemble named STANKER (Stacking neTwork bAsed-on atteNtion-masKed BERT). STANKER adopts two level-grained attention-masked BERT (LGAM-BERT) models as base encoders. Unlike the original BERT, our new LGAM-BERT model takes comments as important auxiliary features and masks co-attention between posts and comments on lower-layers. Experiments on Weibo20 and three existing social media datasets showed that STANKER outperformed all compared models, especially beating the old state-of-the-art on Weibo dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.276.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--276 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.276 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.276/>GMH : A General Multi-hop Reasoning Model for KG Completion<span class=acl-fixed-case>GMH</span>: A General Multi-hop Reasoning Model for <span class=acl-fixed-case>KG</span> Completion</a></strong><br><a href=/people/y/yao-zhang/>Yao Zhang</a>
|
<a href=/people/h/hongru-liang/>Hongru Liang</a>
|
<a href=/people/a/adam-jatowt/>Adam Jatowt</a>
|
<a href=/people/w/wenqiang-lei/>Wenqiang Lei</a>
|
<a href=/people/x/xin-wei/>Xin Wei</a>
|
<a href=/people/n/ning-jiang/>Ning Jiang</a>
|
<a href=/people/z/zhenglu-yang/>Zhenglu Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--276><div class="card-body p-3 small">Knowledge graphs are essential for numerous downstream natural language processing applications, but are typically incomplete with many facts missing. This results in research efforts on multi-hop reasoning task, which can be formulated as a search process and current <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> typically perform short distance reasoning. However, the long-distance reasoning is also vital with the ability to connect the superficially unrelated entities. To the best of our knowledge, there lacks a general framework that approaches multi-hop reasoning in mixed long-short distance reasoning scenarios. We argue that there are two key issues for a general multi-hop reasoning model : i) where to go, and ii) when to stop. Therefore, we propose a general model which resolves the issues with three modules : 1) the local-global knowledge module to estimate the possible paths, 2) the differentiated action dropout module to explore a diverse set of paths, and 3) the adaptive stopping search module to avoid over searching. The comprehensive results on three datasets demonstrate the superiority of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with significant improvements against baselines in both short and long distance reasoning scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--281 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.281/>Self-Supervised Curriculum Learning for Spelling Error Correction</a></strong><br><a href=/people/z/zifa-gan/>Zifa Gan</a>
|
<a href=/people/h/hongfei-xu/>Hongfei Xu</a>
|
<a href=/people/h/hongying-zan/>Hongying Zan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--281><div class="card-body p-3 small">Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance has been shown sensitive to the difficulty of training examples, and CL has been shown effective to address this. In SEC, the data from different language learners are naturally distributed at different difficulty levels (some errors made by beginners are obvious to correct while some made by fluent speakers are hard), and we expect that designing a curriculum correspondingly for model learning may also help its training and bring about better performance. In this paper, we study how to further improve the performance of the state-of-the-art SEC method with CL, and propose a Self-Supervised Curriculum Learning (SSCL) approach. Specifically, we directly use the cross-entropy loss as criteria for : 1) scoring the difficulty of training data, and 2) evaluating the competence of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. In our approach, CL improves the model training, which in return improves the CL measurement. In our experiments on the SIGHAN 2015 Chinese spelling check task, we show that SSCL is superior to previous norm-based and uncertainty-aware approaches, and establish a new state of the art (74.38 % F1).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.282.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--282 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.282 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.282/>Fix-Filter-Fix : Intuitively Connect Any Models for Effective Bug Fixing</a></strong><br><a href=/people/h/haiwen-hong/>Haiwen Hong</a>
|
<a href=/people/j/jingfeng-zhang/>Jingfeng Zhang</a>
|
<a href=/people/y/yin-zhang/>Yin Zhang</a>
|
<a href=/people/y/yao-wan/>Yao Wan</a>
|
<a href=/people/y/yulei-sui/>Yulei Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--282><div class="card-body p-3 small">Locating and fixing bugs is a time-consuming task. Most neural machine translation (NMT) based approaches for automatically bug fixing lack generality and do not make full use of the rich information in the source code. In NMT-based bug fixing, we find some predicted code identical to the input <a href=https://en.wikipedia.org/wiki/Software_bug>buggy code</a> (called unchanged fix) in NMT-based approaches due to high similarity between buggy and fixed code (e.g., the difference may only appear in one particular line). Obviously, unchanged fix is not the correct fix because it is the same as the <a href=https://en.wikipedia.org/wiki/Software_bug>buggy code</a> that needs to be fixed. Based on these, we propose an intuitive yet effective general framework (called Fix-Filter-Fix or F3) for <a href=https://en.wikipedia.org/wiki/Patch_(computing)>bug fixing</a>. F3 connects models with our filter mechanism to filter out the last <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s unchanged fix to the next. We propose an F3 theory that can quantitatively and accurately calculate the F3 lifting effect. To evaluate, we implement the Seq2Seq Transformer (ST) and the AST2Seq Transformer (AT) to form some basic F3 instances, called F3_ST+AT and F3_AT+ST. Comparing them with single model approaches and many model connection baselines across four datasets validates the effectiveness and generality of F3 and corroborates our findings and methodology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.283/>Neuro-Symbolic Reinforcement Learning with First-Order Logic</a></strong><br><a href=/people/d/daiki-kimura/>Daiki Kimura</a>
|
<a href=/people/m/masaki-ono/>Masaki Ono</a>
|
<a href=/people/s/subhajit-chaudhury/>Subhajit Chaudhury</a>
|
<a href=/people/r/ryosuke-kohita/>Ryosuke Kohita</a>
|
<a href=/people/a/akifumi-wachi/>Akifumi Wachi</a>
|
<a href=/people/d/don-joven-agravante/>Don Joven Agravante</a>
|
<a href=/people/m/michiaki-tatsubori/>Michiaki Tatsubori</a>
|
<a href=/people/a/asim-munawar/>Asim Munawar</a>
|
<a href=/people/a/alexander-gray/>Alexander Gray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--283><div class="card-body p-3 small">Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained <a href=https://en.wikipedia.org/wiki/Policy>policies</a> is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neuro-symbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text observation and external word meaning network (ConceptNet), then train a <a href=https://en.wikipedia.org/wiki/Policy>policy</a> in the <a href=https://en.wikipedia.org/wiki/Social_network>network</a> with directly interpretable logical operators. Our experimental results show RL training with the proposed method converges significantly faster than other state-of-the-art neuro-symbolic methods in a TextWorld benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.284.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--284 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.284 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.284" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.284/>Biomedical Concept Normalization by Leveraging Hypernyms</a></strong><br><a href=/people/c/cheng-yan/>Cheng Yan</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/y/yafei-shi/>Yafei Shi</a>
|
<a href=/people/s/shengping-liu/>Shengping Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--284><div class="card-body p-3 small">Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation of hypernym-hyponym entity pairs. The experimental results show that BCNH outperforms the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the NCBI dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--285 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.285" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.285/>Leveraging Capsule Routing to Associate Knowledge with Medical Literature Hierarchically</a></strong><br><a href=/people/x/xin-liu/>Xin Liu</a>
|
<a href=/people/q/qingcai-chen/>Qingcai Chen</a>
|
<a href=/people/j/junying-chen/>Junying Chen</a>
|
<a href=/people/w/wenxiu-zhou/>Wenxiu Zhou</a>
|
<a href=/people/t/tingyu-liu/>Tingyu Liu</a>
|
<a href=/people/x/xinlan-yang/>Xinlan Yang</a>
|
<a href=/people/w/weihua-peng/>Weihua Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--285><div class="card-body p-3 small">Integrating knowledge into text is a promising way to enrich text representation, especially in the <a href=https://en.wikipedia.org/wiki/Medicine>medical field</a>. However, undifferentiated knowledge not only confuses the text representation but also imports unexpected noises. In this paper, to alleviate this problem, we propose leveraging capsule routing to associate knowledge with <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> hierarchically (called HiCapsRKL). Firstly, HiCapsRKL extracts two empirically designed text fragments from <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> and encodes them into fragment representations respectively. Secondly, the capsule routing algorithm is applied to two fragment representations. Through the capsule computing and <a href=https://en.wikipedia.org/wiki/Dynamic_routing>dynamic routing</a>, each representation is processed into a new representation (denoted as caps-representation), and we integrate the caps-representations as information gain to associate knowledge with <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> hierarchically. Finally, HiCapsRKL are validated on relevance prediction and medical literature retrieval test sets. The experimental results and analyses show that HiCapsRKLcan more accurately associate knowledge with <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> than mainstream methods. In summary, HiCapsRKL can efficiently help selecting the most relevant knowledge to the <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a>, which may be an alternative attempt to improve knowledge-based text representation. Source code is released on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--287 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.287" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.287/>SpellBERT : A Lightweight Pretrained Model for Chinese Spelling Check<span class=acl-fixed-case>S</span>pell<span class=acl-fixed-case>BERT</span>: A Lightweight Pretrained Model for <span class=acl-fixed-case>C</span>hinese Spelling Check</a></strong><br><a href=/people/t/tuo-ji/>Tuo Ji</a>
|
<a href=/people/h/hang-yan/>Hang Yan</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--287><div class="card-body p-3 small">Chinese Spelling Check (CSC) is to detect and correct Chinese spelling errors. Many models utilize a predefined confusion set to learn a <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> between correct characters and its visually similar or phonetically similar misuses but the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> may be out-of-domain. To that end, we propose SpellBERT, a pretrained model with graph-based extra features and independent on confusion set. To explicitly capture the two erroneous patterns, we employ a graph neural network to introduce radical and pinyin information as visual and phonetic features. For better fusing these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with <a href=https://en.wikipedia.org/wiki/Character_encoding>character representations</a>, we devise masked language model alike <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training tasks</a>. With this feature-rich pre-training, SpellBERT with only half size of BERT can show competitive performance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--288 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.288" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.288/>Automated Generation of Accurate & Fluent Medical X-ray Reports<span class=acl-fixed-case>X</span>-ray Reports</a></strong><br><a href=/people/h/hoang-nguyen/>Hoang Nguyen</a>
|
<a href=/people/d/dong-nie/>Dong Nie</a>
|
<a href=/people/t/taivanbat-badamdorj/>Taivanbat Badamdorj</a>
|
<a href=/people/y/yujie-liu/>Yujie Liu</a>
|
<a href=/people/y/yingying-zhu/>Yingying Zhu</a>
|
<a href=/people/j/jason-truong/>Jason Truong</a>
|
<a href=/people/l/li-cheng/>Li Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--288><div class="card-body p-3 small">Our paper aims to automate the generation of medical reports from chest X-ray image inputs, a critical yet time-consuming task for radiologists. Existing medical report generation efforts emphasize producing human-readable reports, yet the generated text may not be well aligned to the clinical facts. Our generated medical reports, on the other hand, are fluent and, more importantly, clinically accurate. This is achieved by our fully differentiable and end-to-end paradigm that contains three complementary modules : taking the chest X-ray images and clinical history document of patients as inputs, our classification module produces an internal checklist of disease-related topics, referred to as enriched disease embedding ; the embedding representation is then passed to our transformer-based generator, to produce the medical report ; meanwhile, our generator also creates a weighted embedding representation, which is fed to our interpreter to ensure consistency with respect to disease-related topics. Empirical evaluations demonstrate very promising results achieved by our approach on commonly-used metrics concerning <a href=https://en.wikipedia.org/wiki/Fluency>language fluency</a> and clinical accuracy. Moreover, noticeable performance gains are consistently observed when additional input information is available, such as the clinical document and extra scans from different views.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--295 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.295/>Enhancing Multiple-choice Machine Reading Comprehension by Punishing Illogical Interpretations</a></strong><br><a href=/people/y/yiming-ju/>Yiming Ju</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/z/zhixing-tian/>Zhixing Tian</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/x/xiaohuan-cao/>Xiaohuan Cao</a>
|
<a href=/people/w/wenting-zhao/>Wenting Zhao</a>
|
<a href=/people/j/jinlong-li/>Jinlong Li</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--295><div class="card-body p-3 small">Machine Reading Comprehension (MRC), which requires a machine to answer questions given the relevant documents, is an important way to test machines&#8217; ability to understand <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>. Multiple-choice MRC is one of the most studied tasks in <a href=https://en.wikipedia.org/wiki/Medical_Subject_Headings>MRC</a> due to the convenience of evaluation and the flexibility of answer format. Post-hoc interpretation aims to explain a trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and reveal how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> arrives at the prediction. One of the most important interpretation forms is to attribute model decisions to input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Based on post-hoc interpretation methods, we assess attributions of paragraphs in multiple-choice MRC and improve the model by punishing the illogical attributions. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can improve model performance without any external information and model structure change. Furthermore, we also analyze how and why such a self-training method works.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.296.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--296 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.296 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.296" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.296/>Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models</a></strong><br><a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/r/rumei-li/>Rumei Li</a>
|
<a href=/people/s/sirui-wang/>Sirui Wang</a>
|
<a href=/people/h/hongzhi-zhang/>Hongzhi Zhang</a>
|
<a href=/people/z/zan-daoguang/>Zan Daoguang</a>
|
<a href=/people/f/fuzheng-zhang/>Fuzheng Zhang</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--296><div class="card-body p-3 small">The key challenge of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the textual information carried by the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a>. Meanwhile, pre-trained language models learn massive open-world knowledge from the large corpus, but it is in the <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language form</a> and not structured. To bridge the gap between the natural language and the structured KB, we propose three relation learning tasks for BERT-based KBQA, including relation extraction, relation matching, and relation reasoning. By relation-augmented training, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> learns to align the natural language expressions to the relations in the <a href=https://en.wikipedia.org/wiki/Knowledge_base>KB</a> as well as reason over the missing connections in the <a href=https://en.wikipedia.org/wiki/Knowledge_base>KB</a>. Experiments on WebQSP show that our method consistently outperforms other baselines, especially when the KB is incomplete.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--300 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.300 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.300" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.300/>FinQA : A Dataset of <a href=https://en.wikipedia.org/wiki/Numerical_analysis>Numerical Reasoning</a> over Financial Data<span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>QA</span>: A Dataset of Numerical Reasoning over Financial Data</a></strong><br><a href=/people/z/zhiyu-chen/>Zhiyu Chen</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/c/charese-smiley/>Charese Smiley</a>
|
<a href=/people/s/sameena-shah/>Sameena Shah</a>
|
<a href=/people/i/iana-borova/>Iana Borova</a>
|
<a href=/people/d/dylan-langdon/>Dylan Langdon</a>
|
<a href=/people/r/reema-moussa/>Reema Moussa</a>
|
<a href=/people/m/matt-beane/>Matt Beane</a>
|
<a href=/people/t/ting-hao-huang/>Ting-Hao Huang</a>
|
<a href=/people/b/bryan-r-routledge/>Bryan Routledge</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--300><div class="card-body p-3 small">The sheer volume of <a href=https://en.wikipedia.org/wiki/Financial_statement>financial statements</a> makes it difficult for humans to access and analyze a business&#8217;s financials. Robust <a href=https://en.wikipedia.org/wiki/Numerical_analysis>numerical reasoning</a> likewise faces unique challenges in this <a href=https://en.wikipedia.org/wiki/Domain_of_a_function>domain</a>. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the <a href=https://en.wikipedia.org/wiki/Mathematical_finance>finance domain</a> includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, FinQA, with Question-Answering pairs over Financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> and conduct comprehensive experiments in our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset the first of its kind should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available at https://github.com/czyssrs/FinQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--301 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.301/>FiD-Ex : Improving Sequence-to-Sequence Models for Extractive Rationale Generation<span class=acl-fixed-case>F</span>i<span class=acl-fixed-case>D</span>-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation</a></strong><br><a href=/people/k/kushal-lakhotia/>Kushal Lakhotia</a>
|
<a href=/people/b/bhargavi-paranjape/>Bhargavi Paranjape</a>
|
<a href=/people/a/asish-ghoshal/>Asish Ghoshal</a>
|
<a href=/people/s/scott-yih/>Scott Yih</a>
|
<a href=/people/y/yashar-mehdad/>Yashar Mehdad</a>
|
<a href=/people/s/srini-iyer/>Srini Iyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--301><div class="card-body p-3 small">Natural language (NL) explanations of model predictions are gaining popularity as a means to understand and verify decisions made by large black-box pre-trained models, for tasks such as Question Answering (QA) and Fact Verification. Recently, pre-trained sequence to sequence (seq2seq) models have proven to be very effective in jointly making predictions, as well as generating NL explanations. However, these models have many shortcomings ; they can fabricate explanations even for incorrect predictions, they are difficult to adapt to long input documents, and their training requires a large amount of labeled data. In this paper, we develop FiD-Ex, which addresses these shortcomings for seq2seq models by : 1) introducing sentence markers to eliminate explanation fabrication by encouraging extractive generation, 2) using the fusion-in-decoder architecture to handle long input contexts, and 3) intermediate fine-tuning on re-structured open domain QA datasets to improve few-shot performance. FiD-Ex significantly improves over prior work in terms of explanation metrics and task accuracy on five tasks from the ERASER explainability benchmark in both fully supervised and few-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--302 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.302/>RockNER : A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models<span class=acl-fixed-case>R</span>ock<span class=acl-fixed-case>NER</span>: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models</a></strong><br><a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/w/wenyang-gao/>Wenyang Gao</a>
|
<a href=/people/j/jun-yan/>Jun Yan</a>
|
<a href=/people/r/ryan-moreno/>Ryan Moreno</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--302><div class="card-body p-3 small">To audit the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of named entity recognition (NER) models, we propose RockNER, a simple yet effective method to create natural adversarial examples. Specifically, at the entity level, we replace target entities with other entities of the same semantic class in <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> ; at the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context level</a>, we use pre-trained <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> (e.g., BERT) to generate word substitutions. Together, the two levels of at- tack produce natural adversarial examples that result in a shifted distribution from the training data on which our target models have been trained. We apply the proposed method to the OntoNotes dataset and create a new benchmark named OntoRock for evaluating the robustness of existing NER models via a systematic evaluation protocol. Our experiments and analysis reveal that even the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has a significant performance drop, and these models seem to memorize in-domain entity patterns instead of reasoning from the context. Our work also studies the effects of a few simple data augmentation methods to improve the <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> of NER models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--305 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.305" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.305/>COUGH : A Challenge Dataset and Models for COVID-19 FAQ Retrieval<span class=acl-fixed-case>COUGH</span>: A Challenge Dataset and Models for <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>FAQ</span> Retrieval</a></strong><br><a href=/people/x/xinliang-frederick-zhang/>Xinliang Frederick Zhang</a>
|
<a href=/people/h/heming-sun/>Heming Sun</a>
|
<a href=/people/x/xiang-yue/>Xiang Yue</a>
|
<a href=/people/s/simon-lin/>Simon Lin</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--305><div class="card-body p-3 small">We present a large, challenging <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, COUGH, for COVID-19 FAQ retrieval. Similar to a standard <a href=https://en.wikipedia.org/wiki/FAQ>FAQ dataset</a>, COUGH consists of three parts : FAQ Bank, Query Bank and Relevance Set. The FAQ Bank contains ~16 K <a href=https://en.wikipedia.org/wiki/FAQ>FAQ items</a> scraped from 55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query Bank and Relevance Set, where the former contains 1,236 human-paraphrased queries while the latter contains ~32 human-annotated FAQ items for each query. We analyze COUGH by testing different FAQ retrieval models built on top of <a href=https://en.wikipedia.org/wiki/BM25>BM25</a> and BERT, among which the best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> achieves 48.8 under P@5, indicating a great challenge presented by COUGH and encouraging future research for further improvement. Our COUGH dataset is available at https://github.com/sunlab-osu/covid-faq.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--307 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.307.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.307/>WinoLogic : A Zero-Shot Logic-based Diagnostic Dataset for Winograd Schema Challenge<span class=acl-fixed-case>W</span>ino<span class=acl-fixed-case>L</span>ogic: <span class=acl-fixed-case>A</span> Zero-Shot Logic-based Diagnostic Dataset for <span class=acl-fixed-case>W</span>inograd <span class=acl-fixed-case>S</span>chema <span class=acl-fixed-case>C</span>hallenge</a></strong><br><a href=/people/w/weinan-he/>Weinan He</a>
|
<a href=/people/c/canming-huang/>Canming Huang</a>
|
<a href=/people/y/yongmei-liu/>Yongmei Liu</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--307><div class="card-body p-3 small">The recent success of neural language models (NLMs) on the <a href=https://en.wikipedia.org/wiki/Winograd_Schema_Challenge>Winograd Schema Challenge</a> has called for further investigation of the commonsense reasoning ability of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Previous diagnostic datasets rely on <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> which fails to provide coherent commonsense crucial for solving WSC problems. To better evaluate NLMs, we propose a logic-based framework that focuses on high-quality <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. Specifically, we identify and collect formal knowledge formulas verified by <a href=https://en.wikipedia.org/wiki/Automated_theorem_proving>theorem provers</a> and translate such <a href=https://en.wikipedia.org/wiki/Well-formed_formula>formulas</a> into <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language sentences</a>. Based on these true knowledge sentences, adversarial false ones are generated. We propose a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> named WinoLogic with these sentences. Given a problem in WinoLogic, NLMs need to decide whether the plausible knowledge sentences could correctly solve the corresponding WSC problems in a zero-shot setting. We also ask human annotators to validate WinoLogic to ensure it is human-agreeable. Experiments show that NLMs still struggle to comprehend commonsense knowledge as humans do, indicating that their reasoning ability could have been overestimated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--309 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.309/>Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast</a></strong><br><a href=/people/l/liang-wang/>Liang Wang</a>
|
<a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/j/jingming-liu/>Jingming Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--309><div class="card-body p-3 small">In this paper, we propose to align sentence representations from different languages into a unified embedding space, where semantic similarities (both cross-lingual and monolingual) can be computed with a simple <a href=https://en.wikipedia.org/wiki/Dot_product>dot product</a>. Pre-trained <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are fine-tuned with the translation ranking task. Existing work (Feng et al., 2020) uses sentences within the same batch as negatives, which can suffer from the issue of easy negatives. We adapt <a href=https://en.wikipedia.org/wiki/Molybdenum_disulfide>MoCo</a> (He et al., 2020) to further improve the quality of <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a>. As the experimental results show, the sentence representations produced by our model achieve the new state-of-the-art on several tasks, including Tatoeba en-zh similarity search (Artetxe andSchwenk, 2019b), BUCC en-zh bitext mining, and semantic textual similarity on 7 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--315 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.315/>Virtual Data Augmentation : A Robust and General Framework for Fine-tuning Pre-trained Models</a></strong><br><a href=/people/k/kun-zhou/>Kun Zhou</a>
|
<a href=/people/w/wayne-xin-zhao/>Wayne Xin Zhao</a>
|
<a href=/people/s/sirui-wang/>Sirui Wang</a>
|
<a href=/people/f/fuzheng-zhang/>Fuzheng Zhang</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--315><div class="card-body p-3 small">Recent works have shown that powerful pre-trained language models (PLM) can be fooled by small perturbations or intentional attacks. To solve this issue, various data augmentation techniques are proposed to improve the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of PLMs. However, it is still challenging to augment semantically relevant examples with sufficient diversity. In this work, we present Virtual Data Augmentation (VDA), a general framework for robustly fine-tuning PLMs. Based on the original token embeddings, we construct a multinomial mixture for augmenting virtual data embeddings, where a masked language model guarantees the semantic relevance and the <a href=https://en.wikipedia.org/wiki/Gaussian_noise>Gaussian noise</a> provides the augmentation diversity. Furthermore, a regularized training strategy is proposed to balance the two aspects. Extensive experiments on six datasets show that our approach is able to improve the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of PLMs and alleviate the performance degradation under adversarial attacks. Our codes and data are publicly available at blue.<url>https://github.com/RUCAIBox/VDA</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--317 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.317.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.317" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.317/>To be Closer : Learning to Link up Aspects with Opinions</a></strong><br><a href=/people/y/yuxiang-zhou/>Yuxiang Zhou</a>
|
<a href=/people/l/lejian-liao/>Lejian Liao</a>
|
<a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/z/zhanming-jie/>Zhanming Jie</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--317><div class="card-body p-3 small">Dependency parse trees are helpful for discovering the opinion words in aspect-based sentiment analysis (ABSA) (CITATION). However, the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a> obtained from off-the-shelf dependency parsers are static, and could be sub-optimal in ABSA. This is because the syntactic trees are not designed for capturing the interactions between opinion words and aspect words. In this work, we aim to shorten the distance between <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> and corresponding opinion words by learning an aspect-centric tree structure. The aspect and opinion words are expected to be closer along such <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a> compared to the standard dependency parse tree. The learning process allows the <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a> to adaptively correlate the aspect and opinion words, enabling us to better identify the polarity in the ABSA task. We conduct experiments on five aspect-based sentiment datasets, and the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms recent strong baselines. Furthermore, our thorough analysis demonstrates the average distance between aspect and opinion words are shortened by at least 19 % on the standard SemEval Restaurant14 (CITATION) dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--319 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.319/>Argument Pair Extraction with Mutual Guidance and Inter-sentence Relation Graph</a></strong><br><a href=/people/j/jianzhu-bao/>Jianzhu Bao</a>
|
<a href=/people/b/bin-liang/>Bin Liang</a>
|
<a href=/people/j/jingyi-sun/>Jingyi Sun</a>
|
<a href=/people/y/yice-zhang/>Yice Zhang</a>
|
<a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--319><div class="card-body p-3 small">Argument pair extraction (APE) aims to extract interactive argument pairs from two passages of a discussion. Previous work studied this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> in the context of peer review and rebuttal, and decomposed it into a sequence labeling task and a sentence relation classification task. However, despite the promising performance, such an approach obtains the argument pairs implicitly by the two decomposed tasks, lacking explicitly modeling of the argument-level interactions between argument pairs. In this paper, we tackle the APE task by a mutual guidance framework, which could utilize the information of an argument in one passage to guide the identification of arguments that can form pairs with it in another passage. In this manner, two passages can mutually guide each other in the process of APE. Furthermore, we propose an inter-sentence relation graph to effectively model the inter-relations between two sentences and thus facilitates the extraction of argument pairs. Our proposed method can better represent the holistic argument-level semantics and thus explicitly capture the complex correlations between argument pairs. Experimental results show that our approach significantly outperforms the current <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--321 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.321/>Improving Federated Learning for Aspect-based Sentiment Analysis via Topic Memories</a></strong><br><a href=/people/h/han-qin/>Han Qin</a>
|
<a href=/people/g/guimin-chen/>Guimin Chen</a>
|
<a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/y/yan-song/>Yan Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--321><div class="card-body p-3 small">Aspect-based sentiment analysis (ABSA) predicts the sentiment polarity towards a particular aspect term in a sentence, which is an important task in real-world applications. To perform ABSA, the trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is required to have a good understanding of the contextual information, especially the particular patterns that suggest the sentiment polarity. However, these patterns typically vary in different sentences, especially when the sentences come from different sources (domains), which makes ABSA still very challenging. Although combining labeled data across different sources (domains) is a promising solution to address the challenge, in practical applications, these labeled data are usually stored at different locations and might be inaccessible to each other due to privacy or legal concerns (e.g., the data are owned by different companies). To address this issue and make the best use of all labeled data, we propose a novel ABSA model with federated learning (FL) adopted to overcome the data isolation limitations and incorporate topic memory (TM) proposed to take the cases of data from diverse sources (domains) into consideration. Particularly, TM aims to identify different isolated data sources due to data inaccessibility by providing useful <a href=https://en.wikipedia.org/wiki/Categorical_variable>categorical information</a> for localized predictions. Experimental results on a simulated environment for <a href=https://en.wikipedia.org/wiki/FL_(programming_language)>FL</a> with three nodes demonstrate the effectiveness of our approach, where TM-FL outperforms different baselines including some well-designed <a href=https://en.wikipedia.org/wiki/FL_(programming_language)>FL frameworks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--323 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.323" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.323/>CTAL : Pre-training Cross-modal Transformer for Audio-and-Language Representations<span class=acl-fixed-case>CTAL</span>: Pre-training Cross-modal Transformer for Audio-and-Language Representations</a></strong><br><a href=/people/h/hang-li/>Hang Li</a>
|
<a href=/people/w/wenbiao-ding/>Wenbiao Ding</a>
|
<a href=/people/y/yu-kang/>Yu Kang</a>
|
<a href=/people/t/tianqiao-liu/>Tianqiao Liu</a>
|
<a href=/people/z/zhongqin-wu/>Zhongqin Wu</a>
|
<a href=/people/z/zitao-liu/>Zitao Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--323><div class="card-body p-3 small">Existing audio-language task-specific predictive approaches focus on building complicated late-fusion mechanisms. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are facing challenges of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> with limited labels and low model generalization abilities. In this paper, we present a Cross-modal Transformer for Audio-and-Language, i.e., CTAL, which aims to learn the intra-modality and inter-modality connections between audio and language through two proxy tasks on a large amount of audio-and-language pairs : masked language modeling and masked cross-modal acoustic modeling. After fine-tuning our pre-trained model on multiple downstream audio-and-language tasks, we observe significant improvements across various tasks, such as, <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, and <a href=https://en.wikipedia.org/wiki/Speaker_verification>speaker verification</a>. On this basis, we further propose a specially-designed fusion mechanism that can be used in fine-tuning phase, which allows our pre-trained model to achieve better performance. Lastly, we demonstrate detailed ablation studies to prove that both our novel cross-modality fusion component and audio-language pre-training methods significantly contribute to the promising results. The code and pre-trained models are available at https://github.com/tal-ai/CTAL_EMNLP2021.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--325 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.325/>Mutual-Learning Improves End-to-End Speech Translation</a></strong><br><a href=/people/j/jiawei-zhao/>Jiawei Zhao</a>
|
<a href=/people/w/wei-luo/>Wei Luo</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/a/andrew-gilman/>Andrew Gilman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--325><div class="card-body p-3 small">A currently popular research area in end-to-end speech translation is the use of knowledge distillation from a <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT) task</a> to improve the <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation (ST) task</a>. However, such scenario obviously only allows one way transfer, which is limited by the performance of the teacher model. Therefore, We hypothesis that the knowledge distillation-based approaches are sub-optimal. In this paper, we propose an alternativea trainable mutual-learning scenario, where the MT and the ST models are collaboratively trained and are considered as peers, rather than teacher / student. This allows us to improve the performance of end-to-end ST more effectively than with a teacher-student paradigm. As a side benefit, performance of the MT model also improves. Experimental results show that in our mutual-learning scenario, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can effectively utilise the auxiliary information from peer models and achieve compelling results on Must-C dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--328 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.328/>Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments<span class=acl-fixed-case>LAW</span>) Supervision for Vision-and-Language Navigation in Continuous Environments</a></strong><br><a href=/people/s/sonia-raychaudhuri/>Sonia Raychaudhuri</a>
|
<a href=/people/s/saim-wani/>Saim Wani</a>
|
<a href=/people/s/shivansh-patel/>Shivansh Patel</a>
|
<a href=/people/u/unnat-jain/>Unnat Jain</a>
|
<a href=/people/a/angel-chang/>Angel Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--328><div class="card-body p-3 small">In the Vision-and-Language Navigation (VLN) task an embodied agent navigates a 3D environment, following natural language instructions. A challenge in this task is how to handle &#8216;off the path&#8217; scenarios where an agent veers from a reference path. Prior work supervises the agent with actions based on the shortest path from the agent&#8217;s location to the goal, but such goal-oriented supervision is often not in alignment with the instruction. Furthermore, the evaluation metrics employed by prior work do not measure how much of a language instruction the agent is able to follow. In this work, we propose a simple and effective language-aligned supervision scheme, and a new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> that measures the number of sub-instructions the agent has completed during <a href=https://en.wikipedia.org/wiki/Navigation>navigation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--329 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.329/>How to leverage the multimodal EHR data for better medical prediction?<span class=acl-fixed-case>EHR</span> data for better medical prediction?</a></strong><br><a href=/people/b/bo-yang/>Bo Yang</a>
|
<a href=/people/l/lijun-wu/>Lijun Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--329><div class="card-body p-3 small">Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> to improve the quality of service and reduce costs. However, the complexity of electronic health records (EHR) data is a challenge for the application of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. Specifically, the data produced in the hospital admissions are monitored by the <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHR system</a>, which includes structured data like <a href=https://en.wikipedia.org/wiki/Thermoregulation>daily body temperature</a> and <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a> like free text and laboratory measurements. Although there are some preprocessing frameworks proposed for specific <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHR data</a>, the clinical notes that contain significant clinical value are beyond the realm of their consideration. Besides, whether these different <a href=https://en.wikipedia.org/wiki/Data>data</a> from various views are all beneficial to the medical tasks and how to best utilize these <a href=https://en.wikipedia.org/wiki/Data>data</a> remain unclear. Therefore, in this paper, we first extract the accompanying clinical notes from <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHR</a> and propose a method to integrate these <a href=https://en.wikipedia.org/wiki/Data>data</a>, we also comprehensively study the different models and the <a href=https://en.wikipedia.org/wiki/Data>data leverage methods</a> for better medical task prediction performance. The results on two prediction tasks show that our fused model with different data outperforms the state-of-the-art method without clinical notes, which illustrates the importance of our fusion method and the clinical note features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--331 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.331/>Frame Semantic-Enhanced Sentence Modeling for Sentence-level Extractive Text Summarization</a></strong><br><a href=/people/y/yong-guan/>Yong Guan</a>
|
<a href=/people/s/shaoru-guo/>Shaoru Guo</a>
|
<a href=/people/r/ru-li/>Ru Li</a>
|
<a href=/people/x/xiaoli-li/>Xiaoli Li</a>
|
<a href=/people/h/hongye-tan/>Hongye Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--331><div class="card-body p-3 small">Sentence-level extractive text summarization aims to select important sentences from a given document. However, it is very challenging to model the importance of sentences. In this paper, we propose a novel Frame Semantic-Enhanced Sentence Modeling for Extractive Summarization, which leverages <a href=https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)>Frame semantics</a> to model sentences from both intra-sentence level and inter-sentence level, facilitating the text summarization task. In particular, intra-sentence level semantics leverage Frames and Frame Elements to model internal semantic structure within a sentence, while inter-sentence level semantics leverage Frame-to-Frame relations to model relationships among sentences. Extensive experiments on two benchmark corpus CNN / DM and NYT demonstrate that our model outperforms six state-of-the-art methods significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--332 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.332" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.332/>CAST : Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees<span class=acl-fixed-case>CAST</span>: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees</a></strong><br><a href=/people/e/ensheng-shi/>Ensheng Shi</a>
|
<a href=/people/y/yanlin-wang/>Yanlin Wang</a>
|
<a href=/people/l/lun-du/>Lun Du</a>
|
<a href=/people/h/hongyu-zhang/>Hongyu Zhang</a>
|
<a href=/people/s/shi-han/>Shi Han</a>
|
<a href=/people/d/dongmei-zhang/>Dongmei Zhang</a>
|
<a href=/people/h/hongbin-sun/>Hongbin Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--332><div class="card-body p-3 small">Code summarization aims to generate concise natural language descriptions of source code, which can help improve program comprehension and maintenance. Recent studies show that syntactic and structural information extracted from abstract syntax trees (ASTs) is conducive to summary generation. However, existing approaches fail to fully capture the rich information in ASTs because of the large size / depth of ASTs. In this paper, we propose a novel model CAST that hierarchically splits and reconstructs ASTs. First, we hierarchically split a large <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>AST</a> into a set of <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>subtrees</a> and utilize a <a href=https://en.wikipedia.org/wiki/Recursive_neural_network>recursive neural network</a> to encode the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>subtrees</a>. Then, we aggregate the embeddings of subtrees by reconstructing the split ASTs to get the representation of the complete AST. Finally, AST representation, together with source code embedding obtained by a vanilla code token encoder, is used for <a href=https://en.wikipedia.org/wiki/Automatic_programming>code summarization</a>. Extensive experiments, including the ablation study and the human evaluation, on benchmarks have demonstrated the power of CAST. To facilitate reproducibility, our code and data are available at https://github.com/DeepSoftwareAnalytics/CAST.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--335 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.335" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.335/>Transformer-based Lexically Constrained Headline Generation</a></strong><br><a href=/people/k/kosuke-yamada/>Kosuke Yamada</a>
|
<a href=/people/y/yuta-hitomi/>Yuta Hitomi</a>
|
<a href=/people/h/hideaki-tamori/>Hideaki Tamori</a>
|
<a href=/people/r/ryohei-sasano/>Ryohei Sasano</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/k/koichi-takeda/>Koichi Takeda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--335><div class="card-body p-3 small">This paper explores a variant of automatic headline generation methods, where a generated headline is required to include a given phrase such as a company or a product name. Previous methods using Transformer-based models generate a <a href=https://en.wikipedia.org/wiki/Headline>headline</a> including a given phrase by providing the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> with additional information corresponding to the given phrase. However, these methods can not always include the phrase in the generated <a href=https://en.wikipedia.org/wiki/Headline>headline</a>. Inspired by previous RNN-based methods generating token sequences in backward and forward directions from the given phrase, we propose a simple Transformer-based method that guarantees to include the given phrase in the high-quality generated headline. We also consider a new headline generation strategy that takes advantage of the controllable generation order of Transformer. Our experiments with the Japanese News Corpus demonstrate that our methods, which are guaranteed to include the phrase in the generated <a href=https://en.wikipedia.org/wiki/Headline>headline</a>, achieve ROUGE scores comparable to previous Transformer-based methods. We also show that our generation strategy performs better than previous <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.337.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--337 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.337 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.337/>Gradient-Based Adversarial Factual Consistency Evaluation for Abstractive Summarization</a></strong><br><a href=/people/z/zhiyuan-zeng/>Zhiyuan Zeng</a>
|
<a href=/people/j/jiaze-chen/>Jiaze Chen</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--337><div class="card-body p-3 small">Neural abstractive summarization systems have gained significant progress in recent years. However, abstractive summarization often produce inconsisitent statements or false facts. How to automatically generate highly abstract yet factually correct summaries? In this paper, we proposed an efficient weak-supervised adversarial data augmentation approach to form the factual consistency dataset. Based on the artificial dataset, we train an evaluation model that can not only make accurate and robust factual consistency discrimination but is also capable of making interpretable factual errors tracing by backpropagated gradient distribution on token embeddings. Experiments and analysis conduct on public annotated summarization and factual consistency datasets demonstrate our approach effective and reasonable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.339.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--339 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.339 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.339.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.339/>A Unified Encoding of Structures in Transition Systems</a></strong><br><a href=/people/t/tao-ji/>Tao Ji</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/x/xiaoling-wang/>Xiaoling Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--339><div class="card-body p-3 small">Transition systems usually contain various <a href=https://en.wikipedia.org/wiki/Dynamical_system>dynamic structures</a> (e.g., <a href=https://en.wikipedia.org/wiki/Stack_(abstract_data_type)>stacks</a>, buffers). An ideal transition-based model should encode these <a href=https://en.wikipedia.org/wiki/Mathematical_structure>structures</a> completely and efficiently. Previous works relying on <a href=https://en.wikipedia.org/wiki/Template_processor>templates</a> or neural network structures either only encode partial structure information or suffer from computation efficiency. In this paper, we propose a novel attention-based encoder unifying representation of all structures in a <a href=https://en.wikipedia.org/wiki/Transition_system>transition system</a>. Specifically, we separate two views of items on structures, namely structure-invariant view and structure-dependent view. With the help of parallel-friendly attention network, we are able to encoding <a href=https://en.wikipedia.org/wiki/Transition_state>transition states</a> with O(1) additional complexity (with respect to basic feature extractors). Experiments on the PTB and UD show that our proposed method significantly improves the test speed and achieves the best transition-based model, and is comparable to state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--342 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.342" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.342/>Topic Transferable Table Question Answering</a></strong><br><a href=/people/s/saneem-chemmengath/>Saneem Chemmengath</a>
|
<a href=/people/v/vishwajeet-kumar/>Vishwajeet Kumar</a>
|
<a href=/people/s/samarth-bharadwaj/>Samarth Bharadwaj</a>
|
<a href=/people/j/jaydeep-sen/>Jaydeep Sen</a>
|
<a href=/people/m/mustafa-canim/>Mustafa Canim</a>
|
<a href=/people/s/soumen-chakrabarti/>Soumen Chakrabarti</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a>
|
<a href=/people/k/karthik-sankaranarayanan/>Karthik Sankaranarayanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--342><div class="card-body p-3 small">Weakly-supervised table question-answering (TableQA) models have achieved state-of-art performance by using pre-trained BERT transformer to jointly encoding a question and a table to produce structured query for the question. However, in practical settings TableQA systems are deployed over table corpora having topic and word distributions quite distinct from BERT&#8217;s pretraining corpus. In this work we simulate the practical topic shift scenario by designing novel challenge benchmarks WikiSQL-TS and WikiTable-TS, consisting of train-dev-test splits in five distinct topic groups, based on the popular WikiSQL and WikiTable-Questions datasets. We empirically show that, despite pre-training on large open-domain text, performance of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> degrades significantly when they are evaluated on unseen topics. In response, we propose T3QA (Topic Transferable Table Question Answering) a pragmatic adaptation framework for TableQA comprising of : (1) topic-specific vocabulary injection into BERT, (2) a novel text-to-text transformer generator (such as T5, GPT2) based natural language question generation pipeline focused on generating topic-specific training data, and (3) a logical form re-ranker. We show that T3QA provides a reasonably good baseline for our topic shift benchmarks. We believe our topic split benchmarks will lead to robust TableQA solutions that are better suited for practical deployment</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--343 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.343/>WebSRC : A Dataset for Web-Based Structural Reading Comprehension<span class=acl-fixed-case>W</span>eb<span class=acl-fixed-case>SRC</span>: A Dataset for Web-Based Structural Reading Comprehension</a></strong><br><a href=/people/x/xingyu-chen/>Xingyu Chen</a>
|
<a href=/people/z/zihan-zhao/>Zihan Zhao</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/j/jiabao-ji/>JiaBao Ji</a>
|
<a href=/people/d/danyang-zhang/>Danyang Zhang</a>
|
<a href=/people/a/ao-luo/>Ao Luo</a>
|
<a href=/people/y/yuxuan-xiong/>Yuxuan Xiong</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--343><div class="card-body p-3 small">Web search is an essential way for humans to obtain information, but it&#8217;s still a great challenge for machines to understand the contents of web pages. In this paper, we introduce the task of web-based structural reading comprehension. Given a <a href=https://en.wikipedia.org/wiki/Web_page>web page</a> and a question about it, the task is to find an answer from the web page. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> requires a system not only to understand the <a href=https://en.wikipedia.org/wiki/Semantics>semantics of texts</a> but also the <a href=https://en.wikipedia.org/wiki/Web_design>structure of the web page</a>. Moreover, we proposed WebSRC, a novel Web-based Structural Reading Comprehension dataset. WebSRC consists of 400 K question-answer pairs, which are collected from 6.4 K web pages with corresponding HTML source code, <a href=https://en.wikipedia.org/wiki/Screenshot>screenshots</a>, and <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>. Each question in WebSRC requires a certain structural understanding of a web page to answer, and the answer is either a text span on the web page or yes / no. We evaluate various strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to show the difficulty of our <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We also investigate the usefulness of structural information and <a href=https://en.wikipedia.org/wiki/Visual_system>visual features</a>. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and baselines have been publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--344 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.344" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.344/>Cryptonite : A Cryptic Crossword Benchmark for Extreme Ambiguity in Language</a></strong><br><a href=/people/a/avia-efrat/>Avia Efrat</a>
|
<a href=/people/u/uri-shaham/>Uri Shaham</a>
|
<a href=/people/d/dan-kilman/>Dan Kilman</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--344><div class="card-body p-3 small">Current NLP datasets targeting <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> can be solved by a native speaker with relative ease. We present <a href=https://en.wikipedia.org/wiki/Cryptonite>Cryptonite</a>, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in <a href=https://en.wikipedia.org/wiki/Cryptonite>Cryptonite</a> is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a challenge even for experienced solvers, though top-tier experts can solve them with almost 100 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Cryptonite is a challenging task for current <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> ; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, on par with the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of a rule-based clue solver (8.6 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.346.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--346 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.346 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.346/>Improving Query Graph Generation for Complex Question Answering over Knowledge Base</a></strong><br><a href=/people/k/kechen-qin/>Kechen Qin</a>
|
<a href=/people/c/cheng-li/>Cheng Li</a>
|
<a href=/people/v/virgil-pavlu/>Virgil Pavlu</a>
|
<a href=/people/j/javed-aslam/>Javed Aslam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--346><div class="card-body p-3 small">Most of the existing Knowledge-based Question Answering (KBQA) methods first learn to map the given question to a query graph, and then convert the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> to an executable query to find the answer. The query graph is typically expanded progressively from the topic entity based on a sequence prediction model. In this paper, we propose a new solution to query graph generation that works in the opposite manner : we start with the entire <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> and gradually shrink it to the desired query graph. This approach improves both the <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a> and the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of query graph generation, especially for complex multi-hop questions. Experimental results show that our method achieves state-of-the-art performance on ComplexWebQuestion (CWQ) dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--349 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.349" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.349/>Generic resources are what you need : Style transfer tasks without task-specific parallel training data</a></strong><br><a href=/people/h/huiyuan-lai/>Huiyuan Lai</a>
|
<a href=/people/a/antonio-toral/>Antonio Toral</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--349><div class="card-body p-3 small">Style transfer aims to rewrite a source text in a different target style while preserving its content. We propose a novel approach to this task that leverages generic resources, and without using any task-specific parallel (sourcetarget) data outperforms existing unsupervised approaches on the two most popular style transfer tasks : formality transfer and polarity swap. In practice, we adopt a multi-step procedure which builds on a generic pre-trained sequence-to-sequence model (BART). First, we strengthen the model&#8217;s ability to rewrite by further pre-training BART on both an existing collection of generic paraphrases, as well as on synthetic pairs created using a general-purpose lexical resource. Second, through an iterative back-translation approach, we train two models, each in a transfer direction, so that they can provide each other with synthetically generated pairs, dynamically in the training process. Lastly, we let our best resulting <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generate static synthetic pairs to be used in a supervised training regime. Besides methodology and state-of-the-art results, a core contribution of this work is a reflection on the nature of the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> we address, and how their differences are highlighted by their response to our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.350.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--350 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.350 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.350/>Revisiting Pivot-Based Paraphrase Generation : Language Is Not the Only Optional Pivot</a></strong><br><a href=/people/y/yitao-cai/>Yitao Cai</a>
|
<a href=/people/y/yue-cao/>Yue Cao</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--350><div class="card-body p-3 small">Paraphrases refer to texts that convey the same meaning with different expression forms. Pivot-based methods, also known as the <a href=https://en.wikipedia.org/wiki/Round-trip_translation>round-trip translation</a>, have shown promising results in generating high-quality paraphrases. However, existing pivot-based methods all rely on language as the pivot, where large-scale, high-quality parallel bilingual texts are required. In this paper, we explore the feasibility of using semantic and syntactic representations as the pivot for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>. Concretely, we transform a sentence into a variety of different semantic or syntactic representations (including AMR, UD, and latent semantic representation), and then decode the sentence back from the semantic representations. We further explore a pretraining-based approach to compress the pipeline process into an end-to-end framework. We conduct experiments comparing different approaches with different kinds of pivots. Experimental results show that taking AMR as pivot can obtain <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> with better quality than taking <a href=https://en.wikipedia.org/wiki/Language>language</a> as the pivot. The end-to-end framework can reduce <a href=https://en.wikipedia.org/wiki/Semantic_shift>semantic shift</a> when language is used as the pivot. Besides, several unsupervised pivot-based methods can generate <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> with similar quality as the supervised sequence-to-sequence model, which indicates that parallel data of paraphrases may not be necessary for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--356 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.356/>DuRecDial 2.0 : A Bilingual Parallel Corpus for Conversational Recommendation<span class=acl-fixed-case>D</span>u<span class=acl-fixed-case>R</span>ec<span class=acl-fixed-case>D</span>ial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation</a></strong><br><a href=/people/z/zeming-liu/>Zeming Liu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/z/zheng-yu-niu/>Zheng-Yu Niu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--356><div class="card-body p-3 small">In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between DuRecDial 2.0 and existing conversational recommendation datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in DuRecDial 2.0 is annotated in two languages, both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, while other datasets are built with the setting of a single language. We collect 8.2k dialogs aligned across English and Chinese languages (16.5k dialogs and 255k utterances in total) that are annotated by crowdsourced workers with strict quality control procedure. We then build monolingual, multilingual, and cross-lingual conversational recommendation baselines on DuRecDial 2.0. Experiment results show that the use of additional English data can bring performance improvement for Chinese conversational recommendation, indicating the benefits of DuRecDial 2.0. Finally, this dataset provides a challenging testbed for future studies of monolingual, multilingual, and cross-lingual conversational recommendation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--357 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.357" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.357/>End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs</a></strong><br><a href=/people/d/dinesh-raghu/>Dinesh Raghu</a>
|
<a href=/people/s/shantanu-agarwal/>Shantanu Agarwal</a>
|
<a href=/people/s/sachindra-joshi/>Sachindra Joshi</a>
|
<a href=/people/m/mausam/>Mausam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--357><div class="card-body p-3 small">We propose a novel <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> within end-to-end learning of task oriented dialogs (TOD), in which the dialog system mimics a troubleshooting agent who helps a user by diagnosing their problem (e.g., car not starting). Such dialogs are grounded in domain-specific flowcharts, which the agent is supposed to follow during the conversation. Our task exposes novel technical challenges for neural TOD, such as grounding an utterance to the <a href=https://en.wikipedia.org/wiki/Flowchart>flowchart</a> without explicit annotation, referring to additional manual pages when user asks a clarification question, and ability to follow unseen flowcharts at test time. We release a dataset (FLODIAL) consisting of 2,738 dialogs grounded on 12 different troubleshooting flowcharts. We also design a neural model, FLONET, which uses a retrieval-augmented generation architecture to train the dialog agent. Our experiments find that FLONET can do zero-shot transfer to unseen flowcharts, and sets a strong baseline for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.358.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--358 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.358 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.358" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.358/>Dimensional Emotion Detection from Categorical Emotion</a></strong><br><a href=/people/s/sungjoon-park/>Sungjoon Park</a>
|
<a href=/people/j/jiseon-kim/>Jiseon Kim</a>
|
<a href=/people/s/seonghyeon-ye/>Seonghyeon Ye</a>
|
<a href=/people/j/jaeyeol-jeon/>Jaeyeol Jeon</a>
|
<a href=/people/h/hee-young-park/>Hee Young Park</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--358><div class="card-body p-3 small">We present a model to predict fine-grained emotions along the continuous dimensions of <a href=https://en.wikipedia.org/wiki/Valence_(psychology)>valence</a>, <a href=https://en.wikipedia.org/wiki/Arousal>arousal</a>, and dominance (VAD) with a corpus with categorical emotion annotations. Our model is trained by minimizing the EMD (Earth Mover&#8217;s Distance) loss between the predicted <a href=https://en.wikipedia.org/wiki/Emotion_classification>VAD score distribution</a> and the categorical emotion distributions sorted along <a href=https://en.wikipedia.org/wiki/Emotion_classification>VAD</a>, and it can simultaneously classify the emotion categories and predict the <a href=https://en.wikipedia.org/wiki/Emotion_classification>VAD scores</a> for a given sentence. We use pre-trained RoBERTa-Large and fine-tune on three different corpora with categorical labels and evaluate on EmoBank corpus with VAD scores. We show that our approach reaches comparable performance to that of the state-of-the-art classifiers in categorical emotion classification and shows significant positive correlations with the ground truth VAD scores. Also, further training with supervision of VAD labels leads to improved performance especially when dataset is small. We also present examples of predictions of appropriate emotion words that are not part of the original annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--360 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.360" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.360/>Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection</a></strong><br><a href=/people/x/xincheng-ju/>Xincheng Ju</a>
|
<a href=/people/d/dong-zhang/>Dong Zhang</a>
|
<a href=/people/r/rong-xiao/>Rong Xiao</a>
|
<a href=/people/j/junhui-li/>Junhui Li</a>
|
<a href=/people/s/shoushan-li/>Shoushan Li</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--360><div class="card-body p-3 small">Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both <a href=https://en.wikipedia.org/wiki/Aspect_(linguistics)>aspect terms</a> and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multi-modal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores the better applications. Therefore, in this paper, we are the first to jointly perform multi-modal ATE (MATE) and multi-modal ASC (MASC), and we propose a multi-modal joint learning approach with auxiliary cross-modal relation detection for multi-modal aspect-level sentiment analysis (MALSA). Specifically, we first build an auxiliary text-image relation detection module to control the proper exploitation of visual information. Second, we adopt the hierarchical framework to bridge the multi-modal connection between <a href=https://en.wikipedia.org/wiki/MATE_(software)>MATE</a> and MASC, as well as separately visual guiding for each sub module. Finally, we can obtain all aspect-level sentiment polarities dependent on the jointly extracted specific aspects. Extensive experiments show the effectiveness of our approach against the joint textual approaches, pipeline and collapsed multi-modal approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.361.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--361 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.361 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.361" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.361/>Solving Aspect Category Sentiment Analysis as a Text Generation Task</a></strong><br><a href=/people/j/jian-liu/>Jian Liu</a>
|
<a href=/people/z/zhiyang-teng/>Zhiyang Teng</a>
|
<a href=/people/l/leyang-cui/>Leyang Cui</a>
|
<a href=/people/h/hanmeng-liu/>Hanmeng Liu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--361><div class="card-body p-3 small">Aspect category sentiment analysis has attracted increasing research attention. The dominant methods make use of pre-trained language models by learning effective aspect category-specific representations, and adding specific output layers to its pre-trained representation. We consider a more direct way of making use of pre-trained language models, by casting the ACSA tasks into natural language generation tasks, using natural language sentences to represent the output. Our method allows more direct use of pre-trained knowledge in seq2seq language models by directly following the task setting during <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training</a>. Experiments on several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> show that our method gives the best reported results, having large advantages in few-shot and zero-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--364 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.364/>CHoRaL : Collecting Humor Reaction Labels from Millions of Social Media Users<span class=acl-fixed-case>CH</span>o<span class=acl-fixed-case>R</span>a<span class=acl-fixed-case>L</span>: Collecting Humor Reaction Labels from Millions of Social Media Users</a></strong><br><a href=/people/z/zixiaofan-yang/>Zixiaofan Yang</a>
|
<a href=/people/s/shayan-hooshmand/>Shayan Hooshmand</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--364><div class="card-body p-3 small">Humor detection has gained attention in recent years due to the desire to understand <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> with <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a>. However, substantial individual and cultural differences in humor perception make it very difficult to collect a large-scale humor dataset with reliable humor labels. We propose CHoRaL, a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> to generate perceived humor labels on Facebook posts, using the naturally available user reactions to these posts with no manual annotation needed. CHoRaL provides both binary labels and continuous scores of <a href=https://en.wikipedia.org/wiki/Humour>humor</a> and non-humor. We present the largest <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to date with labeled humor on 785 K posts related to COVID-19. Additionally, we analyze the expression of COVID-related humor in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> by extracting lexico-semantic and affective features from the posts, and build humor detection models with performance similar to humans. CHoRaL enables the development of large-scale humor detection models on any topic and opens a new path to the study of <a href=https://en.wikipedia.org/wiki/Humour>humor</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--366 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.366" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.366/>CodRED : A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild<span class=acl-fixed-case>C</span>od<span class=acl-fixed-case>RED</span>: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild</a></strong><br><a href=/people/y/yuan-yao/>Yuan Yao</a>
|
<a href=/people/j/jiaju-du/>Jiaju Du</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--366><div class="card-body p-3 small">Existing relation extraction (RE) methods typically focus on extracting relational facts between entity pairs within single sentences or documents. However, a large quantity of relational facts in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> can only be inferred across documents in practice. In this work, we present the problem of cross-document RE, making an initial step towards <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition</a> in the wild. To facilitate the research, we construct the first human-annotated cross-document RE dataset CodRED. Compared to existing RE datasets, CodRED presents two key challenges : Given two entities, (1) it requires finding the relevant documents that can provide clues for identifying their relations ; (2) it requires reasoning over multiple documents to extract the relational facts. We conduct comprehensive experiments to show that CodRED is challenging to existing RE methods including strong BERT-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.368.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--368 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.368 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.368.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.368" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.368/>We Need to Talk About train-dev-test Splits</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--368><div class="card-body p-3 small">Standard train-dev-test splits used to benchmark multiple <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> against each other are ubiquitously used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. In this setup, the train data is used for training the model, the development set for evaluating different versions of the proposed model(s) during development, and the test set to confirm the answers to the main research question(s). However, the introduction of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has led to a different use of these standard splits ; the development set is now often used for <a href=https://en.wikipedia.org/wiki/Model_selection>model selection</a> during the training procedure. Because of this, comparing multiple versions of the same model during development leads to overestimation on the development data. As an effect, people have started to compare an increasing amount of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the test data, leading to faster overfitting and expiration of our test sets. We propose to use a tune-set when developing neural network methods, which can be used for model picking so that comparing the different versions of a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can safely be done on the development data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.369.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--369 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.369 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.369" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.369/>PhoMT : A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation<span class=acl-fixed-case>P</span>ho<span class=acl-fixed-case>MT</span>: A High-Quality and Large-Scale Benchmark Dataset for <span class=acl-fixed-case>V</span>ietnamese-<span class=acl-fixed-case>E</span>nglish Machine Translation</a></strong><br><a href=/people/l/long-doan/>Long Doan</a>
|
<a href=/people/l/linh-the-nguyen/>Linh The Nguyen</a>
|
<a href=/people/n/nguyen-luong-tran/>Nguyen Luong Tran</a>
|
<a href=/people/t/thai-hoang/>Thai Hoang</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--369><div class="card-body p-3 small">We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02 M sentence pairs, which is 2.9 M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong neural baselines and well-known automatic translation engines on our dataset and find that in both automatic and human evaluations : the best performance is obtained by fine-tuning the pre-trained sequence-to-sequence denoising auto-encoder mBART. To our best knowledge, this is the first large-scale Vietnamese-English machine translation study. We hope our publicly available dataset and study can serve as a starting point for future research and applications on Vietnamese-English machine translation. We release our dataset at : https://github.com/VinAIResearch/PhoMT</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.374.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--374 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.374 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.374" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.374/>Mind the Style of Text ! Adversarial and Backdoor Attacks Based on Text Style Transfer</a></strong><br><a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/y/yangyi-chen/>Yangyi Chen</a>
|
<a href=/people/x/xurui-zhang/>Xurui Zhang</a>
|
<a href=/people/m/mukai-li/>Mukai Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--374><div class="card-body p-3 small">Adversarial attacks and <a href=https://en.wikipedia.org/wiki/Backdoor_(computing)>backdoor attacks</a> are two common security threats that hang over <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its meaning. We design an adversarial attack method and a <a href=https://en.wikipedia.org/wiki/Backdoor_(computing)>backdoor attack method</a>, and conduct extensive experiments to evaluate them. Experimental results show that popular NLP models are vulnerable to both adversarial and backdoor attacks based on text style transferthe attack success rates can exceed 90 % without much effort. It reflects the limited ability of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP models</a> to handle the feature of text style that has not been widely realized. In addition, the style transfer-based adversarial and backdoor attack methods show superiority to <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> in many aspects. All the code and data of this paper can be obtained at https://github.com/thunlp/StyleAttack.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--376 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.376/>Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes</a></strong><br><a href=/people/t/tomasz-limisiewicz/>Tomasz Limisiewicz</a>
|
<a href=/people/d/david-marecek/>David Mareček</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--376><div class="card-body p-3 small">State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages. The novel Orthogonal Structural Probe (Limisiewicz and Mareek, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual annotated datasets. We evaluate syntactic (UD) and lexical (WordNet) structural information encoded inmBERT&#8217;s contextual representations for nine diverse languages. We observe that for languages closely related to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, no transformation is needed. The evaluated information is encoded in a shared cross-lingual embedding space. For other languages, it is beneficial to apply <a href=https://en.wikipedia.org/wiki/Orthogonal_transformation>orthogonal transformation</a> learned separately for each language. We successfully apply our findings to zero-shot and few-shot cross-lingual parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--381 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.381" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.381/>Extracting Fine-Grained Knowledge Graphs of Scientific Claims : Dataset and Transformer-Based Results</a></strong><br><a href=/people/i/ian-magnusson/>Ian Magnusson</a>
|
<a href=/people/s/scott-friedman/>Scott Friedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--381><div class="card-body p-3 small">Recent transformer-based approaches demonstrate promising results on relational scientific information extraction. Existing datasets focus on high-level description of how research is carried out. Instead we focus on the subtleties of how experimental associations are presented by building SciClaim, a dataset of scientific claims drawn from Social and Behavior Science (SBS), <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a>, and CORD-19 papers. Our novel graph annotation schema incorporates not only coarse-grained entity spans as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> and relations as edges between them, but also fine-grained attributes that modify entities and their relations, for a total of 12,738 labels in the corpus. By including more label types and more than twice the label density of previous datasets, SciClaim captures causal, comparative, predictive, statistical, and proportional associations over experimental variables along with their qualifications, subtypes, and evidence. We extend work in transformer-based joint entity and relation extraction to effectively infer our schema, showing the promise of fine-grained knowledge graphs in scientific claims and beyond.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--385 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.385" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.385/>AVocaDo : Strategy for Adapting Vocabulary to Downstream Domain<span class=acl-fixed-case>AV</span>oca<span class=acl-fixed-case>D</span>o: Strategy for Adapting Vocabulary to Downstream Domain</a></strong><br><a href=/people/j/jimin-hong/>Jimin Hong</a>
|
<a href=/people/t/taehee-kim/>TaeHee Kim</a>
|
<a href=/people/h/hyesu-lim/>Hyesu Lim</a>
|
<a href=/people/j/jaegul-choo/>Jaegul Choo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--385><div class="card-body p-3 small">During the fine-tuning phase of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, the pretrained vocabulary remains unchanged, while <a href=https://en.wikipedia.org/wiki/Parameter>model parameters</a> are updated. The <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> generated based on the pretrained data is suboptimal for <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream data</a> when domain discrepancy exists. We propose to consider the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> as an optimizable parameter, allowing us to update the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> by expanding it with domain specific vocabulary based on a tokenization statistic. Furthermore, we preserve the embeddings of the added words from <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> to downstream data by utilizing knowledge learned from a pretrained language model with a regularization term. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieved consistent performance improvements on diverse domains (i.e., <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedical</a>, <a href=https://en.wikipedia.org/wiki/Computer_science>computer science</a>, <a href=https://en.wikipedia.org/wiki/News>news</a>, and reviews).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.388.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--388 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.388 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.388" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.388/>Can <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> be Biomedical Knowledge Bases?</a></strong><br><a href=/people/m/mujeen-sung/>Mujeen Sung</a>
|
<a href=/people/j/jinhyuk-lee/>Jinhyuk Lee</a>
|
<a href=/people/s/sean-yi/>Sean Yi</a>
|
<a href=/people/m/minji-jeon/>Minji Jeon</a>
|
<a href=/people/s/sungdong-kim/>Sungdong Kim</a>
|
<a href=/people/j/jaewoo-kang/>Jaewoo Kang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--388><div class="card-body p-3 small">Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>LMs</a> contain and how we can extract that knowledge, treating <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>LMs</a> as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49 K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51 % Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.391.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--391 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.391 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.391" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.391/>Learning grounded word meaning representations on similarity graphs</a></strong><br><a href=/people/m/mariella-dimiccoli/>Mariella Dimiccoli</a>
|
<a href=/people/h/herwig-wendt/>Herwig Wendt</a>
|
<a href=/people/p/pau-batlle-franch/>Pau Batlle Franch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--391><div class="card-body p-3 small">This paper introduces a novel approach to learn visually grounded meaning representations of words as low-dimensional node embeddings on an underlying graph hierarchy. The lower level of the <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchy</a> models <a href=https://en.wikipedia.org/wiki/Modality_(semiotics)>modality-specific word representations</a>, conditioned to another <a href=https://en.wikipedia.org/wiki/Modality_(semiotics)>modality</a>, through dedicated but communicating graphs, while the higher level puts these representations together on a single <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> to learn a representation jointly from both modalities. The topology of each <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> models similarity relations among words, and is estimated jointly with the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph embedding</a>. The assumption underlying this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is that words sharing similar meaning correspond to <a href=https://en.wikipedia.org/wiki/Community>communities</a> in an underlying <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> in a <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>low-dimensional space</a>. We named this model Hierarchical Multi-Modal Similarity Graph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE to simulate human similarity judgments and concept categorization, outperforming the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.394.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--394 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.394 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.394" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.394/>On the Relation between Syntactic Divergence and Zero-Shot Performance</a></strong><br><a href=/people/o/ofir-arviv/>Ofir Arviv</a>
|
<a href=/people/d/dmitry-nikolaev/>Dmitry Nikolaev</a>
|
<a href=/people/t/taelin-karidi/>Taelin Karidi</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--394><div class="card-body p-3 small">We explore the link between the extent to which syntactic relations are preserved in <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and the ease of correctly constructing a <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a> in a zero-shot setting. While previous work suggests such a relation, it tends to focus on the macro level and not on the level of individual edgesa gap we aim to address. As a test case, we take the transfer of Universal Dependencies (UD) parsing from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to a diverse set of languages and conduct two sets of experiments. In one, we analyze zero-shot performance based on the extent to which English source edges are preserved in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In another, we apply three linguistically motivated transformations to UD, creating more cross-lingually stable versions of it, and assess their zero-shot parsability. In order to compare <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance across different schemes, we perform extrinsic evaluation on the downstream task of cross-lingual relation extraction (RE) using a subset of a standard English RE benchmark translated to <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>. In both sets of experiments, our results suggest a strong relation between cross-lingual stability and zero-shot parsing performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.395.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--395 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.395 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.395" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.395/>Improved Latent Tree Induction with Distant Supervision via Span Constraints</a></strong><br><a href=/people/z/zhiyang-xu/>Zhiyang Xu</a>
|
<a href=/people/a/andrew-drozdov/>Andrew Drozdov</a>
|
<a href=/people/j/jay-yoon-lee/>Jay Yoon Lee</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/s/subendhu-rongali/>Subendhu Rongali</a>
|
<a href=/people/d/dylan-finkbeiner/>Dylan Finkbeiner</a>
|
<a href=/people/s/shilpa-suresh/>Shilpa Suresh</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--395><div class="card-body p-3 small">For over thirty years, researchers have developed and analyzed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for latent tree induction as an approach for unsupervised syntactic parsing. Nonetheless, modern <a href=https://en.wikipedia.org/wiki/System>systems</a> still do not perform well enough compared to their supervised counterparts to have any practical use as structural annotation of text. In this work, we present a technique that uses distant supervision in the form of <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>span constraints</a> (i.e. phrase bracketing) to improve performance in unsupervised constituency parsing. Using a relatively small number of span constraints we can substantially improve the output from DIORA, an already competitive unsupervised parsing system. Compared with full parse tree annotation, span constraints can be acquired with minimal effort, such as with a lexicon derived from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, to find exact text matches. Our experiments show span constraints based on entities improves constituency parsing on English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to any domain where span constraints are easily attainable, and as a case study we demonstrate its effectiveness by parsing biomedical text from the CRAFT dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.397.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--397 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.397 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.397" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.397/>Just Say No : Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts</a></strong><br><a href=/people/a/ashutosh-baheti/>Ashutosh Baheti</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/m/mark-riedl/>Mark Riedl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--397><div class="card-body p-3 small">Dialogue models trained on <a href=https://en.wikipedia.org/wiki/Conversation>human conversations</a> inadvertently learn to generate <a href=https://en.wikipedia.org/wiki/Toxicity>toxic responses</a>. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> and <a href=https://en.wikipedia.org/wiki/List_of_human_positions>stance</a>. Our analysis reveals that 42 % of human responses agree with toxic comments, whereas only 13 % agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 <a href=https://en.wikipedia.org/wiki/F-number>F1</a> for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19 % reduction in agreement with offensive comments and produces 29 % fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.398/>Multi-Modal Open-Domain Dialogue</a></strong><br><a href=/people/k/kurt-shuster/>Kurt Shuster</a>
|
<a href=/people/e/eric-michael-smith/>Eric Michael Smith</a>
|
<a href=/people/d/da-ju/>Da Ju</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--398><div class="card-body p-3 small">Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020 ; Roller et al., 2020). However, if we want to build <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of getting humans to engage in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based conversation. We additionally investigate and incorporate safety components in our final <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, and show that such efforts do not diminish model performance with respect to human preference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--402 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.402/>RAST : Domain-Robust Dialogue Rewriting as Sequence Tagging<span class=acl-fixed-case>RAST</span>: Domain-Robust Dialogue Rewriting as Sequence Tagging</a></strong><br><a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/l/liwei-wang/>Liwei Wang</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--402><div class="card-body p-3 small">The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s outputs may lack fluency. To alleviate this issue, we inject the loss signal from <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> over the current state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> when transferring to another <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--404 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.404" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.404/>Dialogue State Tracking with a <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a> using Schema-Driven Prompting</a></strong><br><a href=/people/c/chia-hsuan-lee/>Chia-Hsuan Lee</a>
|
<a href=/people/h/hao-cheng/>Hao Cheng</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--404><div class="card-body p-3 small">Task-oriented conversational systems often use dialogue state tracking to represent the user&#8217;s intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have been obtained using more general architectures based on pretrained language models. Here, we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide task-aware history encoding that is used for both categorical and non-categorical slots. We further improve performance by augmenting the prompting with schema descriptions, a naturally occurring source of in-domain knowledge. Our purely <a href=https://en.wikipedia.org/wiki/Generative_model>generative system</a> achieves state-of-the-art performance on MultiWOZ 2.2 and achieves competitive performance on two other <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> : MultiWOZ 2.1 and M2M. The data and code will be available at https://github.com/chiahsuan156/DST-as-Prompting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--409 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.409" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.409/>Pre-train or Annotate? Domain Adaptation with a Constrained Budget</a></strong><br><a href=/people/f/fan-bai/>Fan Bai</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--409><div class="card-body p-3 small">Recent work has demonstrated that pre-training in-domain language models can boost performance when adapting to a new domain. However, the costs associated with pre-training raise an important question : given a fixed budget, what steps should an NLP practitioner take to maximize performance? In this paper, we study domain adaptation under budget constraints, and approach it as a customer choice problem between data annotation and pre-training. Specifically, we measure the annotation cost of three procedural text datasets and the pre-training cost of three in-domain language models. Then we evaluate the utility of different combinations of pre-training and <a href=https://en.wikipedia.org/wiki/Annotation>data annotation</a> under varying <a href=https://en.wikipedia.org/wiki/Budget_constraint>budget constraints</a> to assess which combination strategy works best. We find that, for small budgets, spending all funds on <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> leads to the best performance ; once the budget becomes large enough, a combination of data annotation and in-domain pre-training works more optimally. We therefore suggest that task-specific data annotation should be part of an economical strategy when adapting an NLP model to a new domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--416 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.416" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.416/>Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning</a></strong><br><a href=/people/l/li-zhou/>Li Zhou</a>
|
<a href=/people/k/kevin-small/>Kevin Small</a>
|
<a href=/people/y/yong-zhang/>Yong Zhang</a>
|
<a href=/people/s/sandeep-atluri/>Sandeep Atluri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--416><div class="card-body p-3 small">Motivated by suggested question generation in conversational news recommendation systems, we propose a model for generating question-answer pairs (QA pairs) with self-contained, summary-centric questions and length-constrained, article-summarizing answers. We begin by collecting a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> with questions as titles and pairing them with summaries of varying length. This dataset is used to learn a QA pair generation model producing summaries as answers that balance brevity with sufficiency jointly with their corresponding questions. We then reinforce the QA pair generation process with a differentiable reward function to mitigate <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a>, a common problem in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. Both automatic metrics and human evaluation demonstrate these QA pairs successfully capture the central gists of the articles and achieve high answer accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--417 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.417/>Unsupervised Paraphrasing with Pretrained Language Models</a></strong><br><a href=/people/t/tong-niu/>Tong Niu</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/y/yingbo-zhou/>Yingbo Zhou</a>
|
<a href=/people/n/nitish-shirish-keskar/>Nitish Shirish Keskar</a>
|
<a href=/people/h/huan-wang/>Huan Wang</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--417><div class="card-body p-3 small">Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a>, which require a large amount of labeled data that is costly to collect. To address this drawback, we adopt a transfer learning approach and propose a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training pipeline</a> that enables pre-trained language models to generate high-quality paraphrases in an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setting</a>. Our recipe consists of task-adaptation, self-supervision, and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a <a href=https://en.wikipedia.org/wiki/Surface_form>surface form</a> dissimilar from the input, whenever the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> emits a token contained in the source sequence, DB prevents the model from outputting the subsequent source token for the next generation step. We show with automatic and human evaluations that our approach achieves state-of-the-art performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and is robust to domain shift between the two datasets of distinct distributions. We also demonstrate that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> transfers to <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a> in other languages without any additional <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--418 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.418/>Profanity-Avoiding Training Framework for Seq2seq Models with Certified Robustness</a></strong><br><a href=/people/h/hengtong-zhang/>Hengtong Zhang</a>
|
<a href=/people/t/tianhang-zheng/>Tianhang Zheng</a>
|
<a href=/people/y/yaliang-li/>Yaliang Li</a>
|
<a href=/people/j/jing-gao/>Jing Gao</a>
|
<a href=/people/l/lu-su/>Lu Su</a>
|
<a href=/people/b/bo-li-vanderbilt/>Bo Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--418><div class="card-body p-3 small">Seq2seq models have demonstrated their incredible effectiveness in a large variety of applications. However, recent research has shown that inappropriate language in training samples and well-designed testing cases can induce seq2seq models to output <a href=https://en.wikipedia.org/wiki/Profanity>profanity</a>. These outputs may potentially hurt the usability of seq2seq models and make the end-users feel offended. To address this problem, we propose a training framework with certified robustness to eliminate the causes that trigger the generation of profanity. The proposed training framework leverages merely a short list of <a href=https://en.wikipedia.org/wiki/Profanity>profanity examples</a> to prevent seq2seq models from generating a broader spectrum of <a href=https://en.wikipedia.org/wiki/Profanity>profanity</a>. The framework is composed of a pattern-eliminating training component to suppress the impact of language patterns with <a href=https://en.wikipedia.org/wiki/Profanity>profanity</a> in the training set, and a trigger-resisting training component to provide certified robustness for seq2seq models against intentionally injected profanity-triggering expressions in test samples. In the experiments, we consider two representative NLP tasks that seq2seq can be applied to, i.e., style transfer and dialogue generation. Extensive experimental results show that the proposed training framework can successfully prevent the NLP models from generating <a href=https://en.wikipedia.org/wiki/Profanity>profanity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--419 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.419" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.419/>Journalistic Guidelines Aware News Image Captioning</a></strong><br><a href=/people/x/xuewen-yang/>Xuewen Yang</a>
|
<a href=/people/s/svebor-karaman/>Svebor Karaman</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/a/alejandro-jaimes/>Alejandro Jaimes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--419><div class="card-body p-3 small">The task of news article image captioning aims to generate descriptive and informative captions for news article images. Unlike conventional image captions that simply describe the content of the image in general terms, news image captions follow journalistic guidelines and rely heavily on named entities to describe the image content, often drawing context from the whole article they are associated with. In this work, we propose a new approach to this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, motivated by caption guidelines that journalists follow. Our approach, Journalistic Guidelines Aware News Image Captioning (JoGANIC), leverages the structure of captions to improve the generation quality and guide our representation design. Experimental results, including detailed ablation studies, on two large-scale publicly available datasets show that JoGANIC substantially outperforms state-of-the-art methods both on caption generation and named entity related metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.420/>AESOP : Paraphrase Generation with Adaptive Syntactic Control<span class=acl-fixed-case>AESOP</span>: Paraphrase Generation with Adaptive Syntactic Control</a></strong><br><a href=/people/j/jiao-sun/>Jiao Sun</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--420><div class="card-body p-3 small">We propose to control <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> through carefully chosen target syntactic structures to generate more proper and higher quality <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>. Our model, AESOP, leverages a pretrained language model and adds deliberately chosen syntactical control via a retrieval-based selection module to generate fluent paraphrases. Experiments show that AESOP achieves state-of-the-art performances on semantic preservation and syntactic conformation on two benchmark datasets with ground-truth syntactic control from human-annotated exemplars. Moreover, with the retrieval-based target syntax selection module, AESOP generates paraphrases with even better qualities than the current best model using human-annotated target syntactic parses according to human evaluation. We further demonstrate the effectiveness of AESOP to improve classification models&#8217; <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to syntactic perturbation by <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> on two GLUE tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--421 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.421/>Refocusing on Relevance : Personalization in NLG<span class=acl-fixed-case>NLG</span></a></strong><br><a href=/people/s/shiran-dudy/>Shiran Dudy</a>
|
<a href=/people/s/steven-bedrick/>Steven Bedrick</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--421><div class="card-body p-3 small">Many NLG tasks such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, dialogue response, or <a href=https://en.wikipedia.org/wiki/Question_answering>open domain question answering</a>, focus primarily on a source text in order to generate a target response. This standard approach falls short, however, when a user&#8217;s intent or context of work is not easily recoverable based solely on that source text a scenario that we argue is more of the rule than the exception. In this work, we argue that NLG systems in general should place a much higher level of emphasis on making use of additional context, and suggest that <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> (as used in Information Retrieval) be thought of as a crucial tool for designing user-oriented text-generating tasks. We further discuss possible harms and hazards around such <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a>, and argue that value-sensitive design represents a crucial path forward through these challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--427 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.427" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.427/>Learning Prototype Representations Across Few-Shot Tasks for Event Detection</a></strong><br><a href=/people/v/viet-lai/>Viet Lai</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--427><div class="card-body p-3 small">We address the <a href=https://en.wikipedia.org/wiki/Sampling_bias>sampling bias</a> and outlier issues in few-shot learning for event detection, a subtask of <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. We propose to model the relations between training tasks in episodic few-shot learning by introducing cross-task prototypes. We further propose to enforce prediction consistency among <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> across tasks to make the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> more robust to <a href=https://en.wikipedia.org/wiki/Outlier>outliers</a>. Our extensive experiment shows a consistent improvement on three few-shot learning datasets. The findings suggest that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is more robust when labeled data of novel event types is limited. The source code is available at http://github.com/laiviet/fsl-proact.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--428 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.428" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.428/>Lifelong Event Detection with Knowledge Transfer</a></strong><br><a href=/people/p/pengfei-yu/>Pengfei Yu</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/p/prem-natarajan/>Prem Natarajan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--428><div class="card-body p-3 small">Traditional supervised Information Extraction (IE) methods can extract structured knowledge elements from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a>, but it is limited to a pre-defined target ontology. In reality, the ontology of interest may change over time, adding emergent new types or more fine-grained subtypes. We propose a new lifelong learning framework to address this challenge. We focus on lifelong event detection as an exemplar case and propose a new problem formulation that is also generalizable to other IE tasks. In event detection and more general IE tasks, rich correlations or <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a> exist among hierarchical knowledge element types. In our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, knowledge is being transferred between learned <a href=https://en.wikipedia.org/wiki/Event_(computing)>old event types</a> and <a href=https://en.wikipedia.org/wiki/Event_(computing)>new event types</a>. Specifically, we update old knowledge with new event types&#8217; mentions using a self-training loss. In addition, we aggregate old event types&#8217; representations based on their similarities with new event types to initialize the new event types&#8217; representations. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> outperforms competitive <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> with a 5.1 % absolute gain in the F1 score. Moreover, our proposed framework can boost the F1 score for over 30 % absolute gain on some new long-tail rare event types with few training instances. Our knowledge transfer module improves performance on both learned event types and new event types under the lifelong learning setting, showing that it helps consolidate old knowledge and improve novel <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--432 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.432/>Adversarial Attack against Cross-lingual Knowledge Graph Alignment</a></strong><br><a href=/people/z/zeru-zhang/>Zeru Zhang</a>
|
<a href=/people/z/zijie-zhang/>Zijie Zhang</a>
|
<a href=/people/y/yang-zhou/>Yang Zhou</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/x/xiaoying-han/>Xiaoying Han</a>
|
<a href=/people/d/dejing-dou/>Dejing Dou</a>
|
<a href=/people/t/tianshi-che/>Tianshi Che</a>
|
<a href=/people/d/da-yan/>Da Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--432><div class="card-body p-3 small">Recent literatures have shown that knowledge graph (KG) learning models are highly vulnerable to adversarial attacks. However, there is still a paucity of vulnerability analyses of cross-lingual entity alignment under adversarial attacks. This paper proposes an adversarial attack model with two novel attack techniques to perturb the KG structure and degrade the quality of deep cross-lingual entity alignment. First, an entity density maximization method is employed to hide the attacked entities in dense regions in two KGs, such that the derived perturbations are unnoticeable. Second, an attack signal amplification method is developed to reduce the gradient vanishing issues in the process of adversarial attacks for further improving the attack effectiveness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.435.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--435 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.435 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.435/>Incorporating medical knowledge in BERT for clinical relation extraction<span class=acl-fixed-case>BERT</span> for clinical relation extraction</a></strong><br><a href=/people/a/arpita-roy/>Arpita Roy</a>
|
<a href=/people/s/shimei-pan/>Shimei Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--435><div class="card-body p-3 small">In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a>. Trained with massive general-domain text, these pre-trained language models capture rich syntactic, semantic and discourse information in the text. However, due to the differences between general and specific domain text (e.g., <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> versus clinic notes), these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> may not be ideal for <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific tasks</a> (e.g., extracting clinical relations). Furthermore, it may require additional <a href=https://en.wikipedia.org/wiki/Medicine>medical knowledge</a> to understand <a href=https://en.wikipedia.org/wiki/Medical_literature>clinical text</a> properly. To solve these issues, in this research, we conduct a comprehensive examination of different techniques to add <a href=https://en.wikipedia.org/wiki/Medicine>medical knowledge</a> into a pre-trained BERT model for clinical relation extraction. Our best model outperforms the state-of-the-art systems on the benchmark i2b2 / VA 2010 clinical relation extraction dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--436 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.436" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.436/>ECONET : Effective Continual Pretraining of <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Event Temporal Reasoning<span class=acl-fixed-case>ECONET</span>: Effective Continual Pretraining of Language Models for Event Temporal Reasoning</a></strong><br><a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--436><div class="card-body p-3 small">While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This * * E**ffective * * CON**tinual pre-training framework for * * E**vent * * T**emporal reasoning (ECONET) improves the PTLMs&#8217; fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--441 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.441" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.441/>Corpus-based Open-Domain Event Type Induction</a></strong><br><a href=/people/j/jiaming-shen/>Jiaming Shen</a>
|
<a href=/people/y/yunyi-zhang/>Yunyi Zhang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--441><div class="card-body p-3 small">Traditional event extraction methods require predefined <a href=https://en.wikipedia.org/wiki/Event_(computing)>event types</a> and their corresponding annotations to learn event extractors. These prerequisites are often hard to be satisfied in real-world applications. This work presents a corpus-based open-domain event type induction method that automatically discovers a set of event types from a given corpus. As events of the same type could be expressed in multiple ways, we propose to represent each event type as a cluster of predicate sense, object head pairs. Specifically, our method (1) selects salient predicates and object heads, (2) disambiguates predicate senses using only a verb sense dictionary, and (3) obtains event types by jointly embedding and clustering predicate sense, object head pairs in a latent spherical space. Our experiments, on three datasets from different domains, show our method can discover salient and high-quality event types, according to both automatic and human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--442 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.442/>PDALN : Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition<span class=acl-fixed-case>PDALN</span>: Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition</a></strong><br><a href=/people/t/tao-zhang/>Tao Zhang</a>
|
<a href=/people/c/congying-xia/>Congying Xia</a>
|
<a href=/people/p/philip-s-yu/>Philip S. Yu</a>
|
<a href=/people/z/zhiwei-liu/>Zhiwei Liu</a>
|
<a href=/people/s/shu-zhao/>Shu Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--442><div class="card-body p-3 small">Cross-domain Named Entity Recognition (NER) transfers the NER knowledge from high-resource domains to the low-resource target domain. Due to limited labeled resources and domain shift, cross-domain NER is a challenging task. To address these challenges, we propose a progressive domain adaptation Knowledge Distillation (KD) approach PDALN. It achieves superior domain adaptability by employing three components : (1) Adaptive data augmentation techniques, which alleviate cross-domain gap and label sparsity simultaneously ; (2) Multi-level Domain invariant features, derived from a multi-grained MMD (Maximum Mean Discrepancy) approach, to enable knowledge transfer across domains ; (3) Advanced KD schema, which progressively enables powerful pre-trained language models to perform domain adaptation. Extensive experiments on four benchmarks show that PDALN can effectively adapt high-resource domains to low-resource target domains, even if they are diverse in terms and writing styles. Comparison with other <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> indicates the state-of-the-art performance of PDALN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.453.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--453 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.453 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.453/>Average Approximates First Principal Component? An Empirical Analysis on Representations from Neural Language Models</a></strong><br><a href=/people/z/zihan-wang/>Zihan Wang</a>
|
<a href=/people/c/chengyu-dong/>Chengyu Dong</a>
|
<a href=/people/j/jingbo-shang/>Jingbo Shang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--453><div class="card-body p-3 small">Contextualized representations based on neural language models have furthered the state of the art in various NLP tasks. Despite its great success, the nature of such <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> remains a mystery. In this paper, we present an empirical property of these representationsaverage approximates <a href=https://en.wikipedia.org/wiki/Principal_component_analysis>first principal component</a>. Specifically, experiments show that the average of these representations shares almost the same direction as the first principal component of the matrix whose columns are these representations. We believe this explains why the average representation is always a simple yet strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Our further examinations show that this property also holds in more challenging scenarios, for example, when the representations are from a model right after its random initialization. Therefore, we conjecture that this property is intrinsic to the distribution of representations and not necessarily related to the input structure. We realize that these representations empirically follow a <a href=https://en.wikipedia.org/wiki/Normal_distribution>normal distribution</a> for each dimension, and by assuming this is true, we demonstrate that the empirical property can be in fact derived mathematically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--460 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.460" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.460/>Continual Few-Shot Learning for Text Classification</a></strong><br><a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--460><div class="card-body p-3 small">Natural Language Processing (NLP) is increasingly relying on general end-to-end systems that need to handle many different linguistic phenomena and nuances. For example, a Natural Language Inference (NLI) system has to recognize <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>, handle numbers, perform <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>, etc. Our solutions to complex problems are still far from perfect, so it is important to create systems that can learn to correct mistakes quickly, incrementally, and with little training data. In this work, we propose a continual few-shot learning (CFL) task, in which a system is challenged with a difficult phenomenon and asked to learn to correct mistakes with only a few (10 to 15) training examples. To this end, we first create benchmarks based on previously annotated data : two NLI (ANLI and SNLI) and one sentiment analysis (IMDB) datasets. Next, we present various <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> from diverse <a href=https://en.wikipedia.org/wiki/Paradigm_(disambiguation)>paradigms</a> (e.g., memory-aware synapses and Prototypical networks) and compare them on few-shot learning and continual few-shot learning setups. Our contributions are in creating a benchmark suite and evaluation protocol for continual few-shot learning on the text classification tasks, and making several interesting observations on the behavior of similarity-based methods. We hope that our work serves as a useful starting point for future work on this important topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.461.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--461 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.461 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.461" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.461/>Efficient Nearest Neighbor Language Models</a></strong><br><a href=/people/j/junxian-he/>Junxian He</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--461><div class="card-body p-3 small">Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>inference overhead</a> and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--464 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.464" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.464/>Gradient-based Adversarial Attacks against Text Transformers</a></strong><br><a href=/people/c/chuan-guo/>Chuan Guo</a>
|
<a href=/people/a/alexandre-sablayrolles/>Alexandre Sablayrolles</a>
|
<a href=/people/h/herve-jegou/>Hervé Jégou</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--464><div class="card-body p-3 small">We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with matching imperceptibility as per automated and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.465.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--465 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.465 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.465" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.465/>Do Transformer Modifications Transfer Across Implementations and Applications?</a></strong><br><a href=/people/s/sharan-narang/>Sharan Narang</a>
|
<a href=/people/h/hyung-won-chung/>Hyung Won Chung</a>
|
<a href=/people/y/yi-tay/>Yi Tay</a>
|
<a href=/people/l/liam-fedus/>Liam Fedus</a>
|
<a href=/people/t/thibault-fevry/>Thibault Fevry</a>
|
<a href=/people/m/michael-matena/>Michael Matena</a>
|
<a href=/people/k/karishma-malkan/>Karishma Malkan</a>
|
<a href=/people/n/noah-fiedel/>Noah Fiedel</a>
|
<a href=/people/n/noam-shazeer/>Noam Shazeer</a>
|
<a href=/people/z/zhenzhong-lan/>Zhenzhong Lan</a>
|
<a href=/people/y/yanqi-zhou/>Yanqi Zhou</a>
|
<a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/n/nan-ding/>Nan Ding</a>
|
<a href=/people/j/jake-marcus/>Jake Marcus</a>
|
<a href=/people/a/adam-roberts/>Adam Roberts</a>
|
<a href=/people/c/colin-raffel/>Colin Raffel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--465><div class="card-body p-3 small">The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Surprisingly, we find that most <a href=https://en.wikipedia.org/wiki/Mod_(video_gaming)>modifications</a> do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same <a href=https://en.wikipedia.org/wiki/Codebase>codebase</a> that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.466.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--466 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.466 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.466/>Paired Examples as Indirect Supervision in Latent Decision Models</a></strong><br><a href=/people/n/nitish-gupta/>Nitish Gupta</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--466><div class="card-body p-3 small">Compositional, structured models are appealing because they explicitly decompose problems and provide interpretable intermediate outputs that give confidence that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is not simply latching onto data artifacts. Learning these models is challenging, however, because end-task supervision only provides a weak indirect signal on what values the latent decisions should take. This often results in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> failing to learn to perform the intermediate tasks correctly. In this work, we introduce a way to leverage paired examples that provide stronger cues for learning latent decisions. When two related training examples share internal substructure, we add an additional training objective to encourage consistency between their latent decisions. Such an objective does not require external supervision for the values of the latent output, or even the end task, yet provides an additional training signal to that provided by individual training examples themselves. We apply our method to improve compositional question answering using neural module networks on the DROP dataset. We explore three ways to acquire paired questions in DROP : (a) discovering naturally occurring paired examples within the dataset, (b) constructing paired examples using templates, and (c) generating paired examples using a question generation model. We empirically demonstrate that our proposed approach improves both in- and out-of-distribution generalization and leads to correct latent decision predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.467.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--467 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.467 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.467" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.467/>Pairwise Supervised Contrastive Learning of Sentence Representations</a></strong><br><a href=/people/d/dejiao-zhang/>Dejiao Zhang</a>
|
<a href=/people/s/shang-wen-li/>Shang-Wen Li</a>
|
<a href=/people/w/wei-xiao/>Wei Xiao</a>
|
<a href=/people/h/henghui-zhu/>Henghui Zhu</a>
|
<a href=/people/r/ramesh-nallapati/>Ramesh Nallapati</a>
|
<a href=/people/a/andrew-o-arnold/>Andrew O. Arnold</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--467><div class="card-body p-3 small">Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with <a href=https://en.wikipedia.org/wiki/Triplet_loss>triplet loss</a> or siamese loss. Nevertheless, they share a common weakness : sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by the fact that the vanilla siamese or triplet losses only learn from individual sentence pairs or triplets, which often suffer from bad local optima. In this paper, we propose PairSupCon, an instance discrimination based approach aiming to bridge semantic entailment and contradiction understanding with high-level categorical concept encoding. We evaluate PairSupCon on various <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a> that involve understanding sentence semantics at different <a href=https://en.wikipedia.org/wiki/Granularity>granularities</a>. We outperform the previous state-of-the-art method with 10%13 % averaged improvement on eight clustering tasks, and 5%6 % averaged improvement on seven semantic textual similarity (STS) tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.474.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--474 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.474 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.474/>Classification-based Quality Estimation : Small and Efficient Models for Real-world Applications</a></strong><br><a href=/people/s/shuo-sun/>Shuo Sun</a>
|
<a href=/people/a/ahmed-el-kishky/>Ahmed El-Kishky</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/j/james-cross/>James Cross</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--474><div class="card-body p-3 small">Sentence-level Quality estimation (QE) of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> is traditionally formulated as a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression task</a>, and the performance of QE models is typically measured by <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> with human labels. Recent QE models have achieved previously-unseen levels of correlation with human judgments, but they rely on large multilingual contextualized language models that are computationally expensive and make them infeasible for real-world applications. In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression task</a>. However, we argue that the level of expressiveness of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a continuous range is unnecessary given the downstream applications of <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a>, and show that reframing <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a> as a classification problem and evaluating <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE models</a> using classification metrics would better reflect their actual performance in real-world applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--477 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.477" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.477/>Rule-based Morphological Inflection Improves Neural Terminology Translation</a></strong><br><a href=/people/w/weijia-xu/>Weijia Xu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--477><div class="card-body p-3 small">Current approaches to incorporating terminology constraints in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> typically assume that the constraint terms are provided in their correct <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological forms</a>. This limits their application to real-world scenarios where <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint terms</a> are provided as lemmas. In this paper, we introduce a modular framework for incorporating <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>lemma constraints</a> in neural MT (NMT) in which linguistic knowledge and diverse types of NMT models can be flexibly applied. It is based on a novel cross-lingual inflection module that inflects the target lemma constraints based on the source context. We explore linguistically motivated rule-based and data-driven neural-based inflection modules and design English-German health and English-Lithuanian news test suites to evaluate them in domain adaptation and low-resource MT settings. Results show that our rule-based inflection module helps NMT models incorporate <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>lemma constraints</a> more accurately than a neural module and outperforms the existing end-to-end approach with lower training costs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.479.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--479 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.479 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.479.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.479/>Good-Enough Example Extrapolation</a></strong><br><a href=/people/j/jason-wei/>Jason Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--479><div class="card-body p-3 small">This paper asks whether extrapolating the hidden space distribution of text examples from one class onto another is a valid <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. To operationalize this question, I propose a simple data augmentation protocol called good-enough example extrapolation (GE3). GE3 is lightweight and has no hyperparameters. Applied to three text classification datasets for various data imbalance scenarios, GE3 improves performance more than <a href=https://en.wikipedia.org/wiki/Upsampling>upsampling</a> and other hidden-space data augmentation methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.489.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--489 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.489 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.489/>A Scalable Framework for Learning From Implicit User Feedback to Improve <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a> in Large-Scale Conversational AI Systems<span class=acl-fixed-case>AI</span> Systems</a></strong><br><a href=/people/s/sunghyun-park/>Sunghyun Park</a>
|
<a href=/people/h/han-li/>Han Li</a>
|
<a href=/people/a/ameen-patel/>Ameen Patel</a>
|
<a href=/people/s/sidharth-mudgal/>Sidharth Mudgal</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a>
|
<a href=/people/y/young-bum-kim/>Young-Bum Kim</a>
|
<a href=/people/s/spyros-matsoukas/>Spyros Matsoukas</a>
|
<a href=/people/r/ruhi-sarikaya/>Ruhi Sarikaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--489><div class="card-body p-3 small">Natural Language Understanding (NLU) is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> and improving NLU for a large-scale production system across 10 domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.495.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--495 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.495 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.495" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.495/>Single-dataset Experts for Multi-dataset Question Answering</a></strong><br><a href=/people/d/dan-friedman/>Dan Friedman</a>
|
<a href=/people/b/ben-dodge/>Ben Dodge</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--495><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> have been created for training reading comprehension models, and a natural question is whether we can combine them to build <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that (1) perform better on all of the training datasets and (2) generalize and transfer better to new datasets. Prior work has addressed this goal by training one network simultaneously on multiple datasets, which works well on average but is prone to over- or under-fitting different sub- distributions and might transfer worse compared to source models with more overlap with the target dataset. Our approach is to model multi-dataset question answering with an ensemble of single-dataset experts, by training a collection of lightweight, dataset-specific adapter modules (Houlsby et al., 2019) that share an underlying Transformer model. We find that these Multi-Adapter Dataset Experts (MADE) outperform all our baselines in terms of in-distribution accuracy, and simple methods based on parameter-averaging lead to better zero-shot generalization and few-shot transfer performance, offering a strong and versatile starting point for building new reading comprehension systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.496.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--496 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.496 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.496" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.496/>Simple Entity-Centric Questions Challenge Dense Retrievers</a></strong><br><a href=/people/c/christopher-sciavolino/>Christopher Sciavolino</a>
|
<a href=/people/z/zexuan-zhong/>Zexuan Zhong</a>
|
<a href=/people/j/jinhyuk-lee/>Jinhyuk Lee</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--496><div class="card-body p-3 small">Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed <a href=https://en.wikipedia.org/wiki/Sparse_matrix>sparse models</a> using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval</a>. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> (e.g., Where was Arve Furset born?), and observe that dense retrievers drastically under-perform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.497.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--497 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.497 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.497" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.497/>Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization</a></strong><br><a href=/people/a/ansong-ni/>Ansong Ni</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--497><div class="card-body p-3 small">Question Answering (QA) tasks requiring information from multiple documents often rely on a <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval model</a> to identify relevant information for <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. The retrieval model is typically trained to maximize the likelihood of the labeled supporting evidence. However, when retrieving from large text corpora such as <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, the correct answer can often be obtained from multiple evidence candidates. Moreover, not all such candidates are labeled as positive during <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, rendering the training signal weak and noisy. This problem is exacerbated when the questions are unanswerable or when the answers are Boolean, since the model can not rely on lexical overlap to make a connection between the answer and supporting evidence. We develop a new parameterization of set-valued retrieval that handles unanswerable queries, and we show that marginalizing over this set during training allows a model to mitigate false negatives in supporting evidence annotations. We test our method on two multi-document QA datasets, <a href=https://en.wikipedia.org/wiki/IIRC>IIRC</a> and HotpotQA. On IIRC, we show that joint modeling with marginalization improves <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance by 5.5 F1 points and achieves a new state-of-the-art performance of 50.5 F1. We also show that retrieval marginalization results in 4.1 QA F1 improvement over a non-marginalized baseline on HotpotQA in the fullwiki setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--500 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.500 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.500" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.500/>BiSECT : Learning to Split and Rephrase Sentences with Bitexts<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>SECT</span>: Learning to Split and Rephrase Sentences with Bitexts</a></strong><br><a href=/people/j/joongwon-kim/>Joongwon Kim</a>
|
<a href=/people/m/mounica-maddela/>Mounica Maddela</a>
|
<a href=/people/r/reno-kriz/>Reno Kriz</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--500><div class="card-body p-3 small">An important task in NLP applications such as <a href=https://en.wikipedia.org/wiki/Sentence_simplification>sentence simplification</a> is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for this &#8216;split and rephrase&#8217; task. Our BiSECT training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to convert both sides of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> into the same language. BiSECT contains higher quality training examples than the previous Split and Rephrase corpora, with sentence splits that require more significant modifications. We categorize examples in our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and use these categories in a novel model that allows us to target specific regions of the input sentence to be split and edited. Moreover, we show that models trained on BiSECT can perform a wider variety of split operations and improve upon previous state-of-the-art approaches in automatic and human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--502 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.502/>Universal Sentence Representation Learning with Conditional Masked Language Model</a></strong><br><a href=/people/z/ziyi-yang/>Ziyi Yang</a>
|
<a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/d/daniel-cer/>Daniel Cer</a>
|
<a href=/people/j/jax-law/>Jax Law</a>
|
<a href=/people/e/eric-darve/>Eric Darve</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--502><div class="card-body p-3 small">This paper presents a novel training method, Conditional Masked Language Modeling (CMLM), to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval, even outperforming models learned using supervised signals. As a fully <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning method</a>, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval (BR) and natural language inference (NLI) tasks outperforms the previous state-of-the-art multilingual models by a large margin, e.g. 10 % improvement upon baseline models on cross-lingual semantic search. We explore the same language bias of the learned representations, and propose a simple, post-training and model agnostic approach to remove the language identifying information from the representation while still retaining sentence semantics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--504 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.504" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.504/>Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models</a></strong><br><a href=/people/t/tuhin-chakrabarty/>Tuhin Chakrabarty</a>
|
<a href=/people/a/aadit-trivedi/>Aadit Trivedi</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--504><div class="card-body p-3 small">Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the implicit premise in an <a href=https://en.wikipedia.org/wiki/Enthymeme>enthymeme</a>, which requires not only an understanding of the stated conclusion and premise but also additional inferences that could depend on <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. The largest available <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for <a href=https://en.wikipedia.org/wiki/Enthymemes>enthymemes</a> (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset : <a href=https://en.wikipedia.org/wiki/Abductive_reasoning>Abductive reasoning</a> in narrative text (Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets.<i>implicit premise in an enthymeme</i>, which requires not only an understanding of the stated conclusion and premise but also additional inferences that could depend on commonsense knowledge. The largest available dataset for enthymemes (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during fine-tuning improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--505 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.505" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.505/>Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks</a></strong><br><a href=/people/y/yichen-jiang/>Yichen Jiang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--505><div class="card-body p-3 small">Systematic compositionality is an essential mechanism in <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>, allowing the recombination of known parts to create novel <a href=https://en.wikipedia.org/wiki/Idiom>expressions</a>. However, existing neural models have been shown to lack this basic ability in learning <a href=https://en.wikipedia.org/wiki/Computer_algebra>symbolic structures</a>. Motivated by the failure of a Transformer model on the SCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing a command into actions, we propose two auxiliary sequence prediction tasks as additional training supervision. These automatically-generated sequences are more representative of the underlying compositional symbolic structures of the input data. During <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> jointly predicts the next action and the next tokens in the auxiliary sequences at each step. Experiments on the SCAN dataset show that our method encourages the Transformer to understand compositional structures of the command, improving its accuracy on multiple challenging splits from 10 % to 100 %. With only 418 (5 %) training instances, our <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> still achieves 97.8 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the MCD1 split. Therefore, we argue that <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a> can be induced in <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformers</a> given minimal but proper guidance. We also show that a better result is achieved using less contextualized vectors as the attention&#8217;s query, providing insights into architecture choices in achieving systematic compositionality. Finally, we show positive generalization results on the grounded-SCAN task (Ruis et al., 2020).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--508 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.508" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.508/>Think about it ! Improving defeasible reasoning by first modeling the question scenario.</a></strong><br><a href=/people/a/aman-madaan/>Aman Madaan</a>
|
<a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/y/yiming-yang/>Yiming Yang</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--508><div class="card-body p-3 small">Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on <a href=https://en.wikipedia.org/wiki/Defeasible_reasoning>defeasible reasoning</a> suggests that a person forms a mental model of the problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the question scenario before answering a defeasible query. Our approach is, given a question, to have a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> first create a <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph of relevant influences</a>, and then leverage that <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph</a> as an additional input when answering the question. Our system, CURIOUS, achieves a new state-of-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by guiding a <a href=https://en.wikipedia.org/wiki/System>system</a> to think about a question and explicitly model the scenario, rather than answering reflexively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--511 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.511" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.511/>Improving Stance Detection with Multi-Dataset Learning and Knowledge Distillation</a></strong><br><a href=/people/y/yingjie-li/>Yingjie Li</a>
|
<a href=/people/c/chenye-zhao/>Chenye Zhao</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--511><div class="card-body p-3 small">Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as legalization of abortion. Despite significant progress on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, one of the remaining challenges is the scarcity of <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>. Besides, most previous works focused on a hard-label training in which meaningful similarities among categories are discarded during training. To address these challenges, first, we evaluate a multi-target and a multi-dataset training settings by training one <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on each dataset and datasets of different domains, respectively. We show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can learn more universal representations with respect to targets in these settings. Second, we investigate the knowledge distillation in stance detection and observe that transferring knowledge from a teacher model to a student model can be beneficial in our proposed training settings. Moreover, we propose an Adaptive Knowledge Distillation (AKD) method that applies instance-specific temperature scaling to the teacher and student predictions. Results show that the multi-dataset model performs best on all datasets and it can be further improved by the proposed AKD, outperforming the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by a large margin. We publicly release our code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--513 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.513/>Improving Pre-trained Vision-and-Language Embeddings for Phrase Grounding</a></strong><br><a href=/people/z/zi-yi-dou/>Zi-Yi Dou</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--513><div class="card-body p-3 small">Phrase grounding aims to map textual phrases to their associated image regions, which can be a prerequisite for multimodal reasoning and can benefit tasks requiring identifying objects based on language. With pre-trained vision-and-language models achieving impressive performance across tasks, it remains unclear if we can directly utilize their learned embeddings for phrase grounding without <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. To this end, we propose a method to extract matched phrase-region pairs from pre-trained vision-and-language embeddings and propose four fine-tuning objectives to improve the model phrase grounding ability using image-caption data without any supervised grounding signals. Experiments on two representative datasets demonstrate the effectiveness of our objectives, outperforming baseline models in both weakly-supervised and supervised phrase grounding settings. In addition, we evaluate the aligned embeddings on several other downstream tasks and show that we can achieve better phrase grounding without sacrificing <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation generality</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--515 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.515/>Hitting your MARQ : Multimodal ARgument Quality Assessment in Long Debate Video<span class=acl-fixed-case>MARQ</span>: Multimodal <span class=acl-fixed-case>AR</span>gument Quality Assessment in Long Debate Video</a></strong><br><a href=/people/m/md-kamrul-hasan/>Md Kamrul Hasan</a>
|
<a href=/people/j/james-spann/>James Spann</a>
|
<a href=/people/m/masum-hasan/>Masum Hasan</a>
|
<a href=/people/m/md-saiful-islam/>Md Saiful Islam</a>
|
<a href=/people/k/kurtis-haut/>Kurtis Haut</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/e/ehsan-hoque/>Ehsan Hoque</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--515><div class="card-body p-3 small">The combination of <a href=https://en.wikipedia.org/wiki/Gesture>gestures</a>, <a href=https://en.wikipedia.org/wiki/Intonation_(linguistics)>intonations</a>, and <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual content</a> plays a key role in <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argument delivery</a>. However, the current literature mostly considers textual content while assessing the quality of an argument, and it is limited to <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> containing short sequences (18-48 words). In this paper, we study argument quality assessment in a multimodal context, and experiment on DBATES, a publicly available dataset of long debate videos. First, we propose a set of interpretable debate centric features such as clarity, content variation, body movement cues, and pauses, inspired by theories of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation quality</a>. Second, we design the Multimodal ARgument Quality assessor (MARQ) a hierarchical neural network model that summarizes the multimodal signals on long sequences and enriches the multimodal embedding with debate centric features. Our proposed MARQ model achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 81.91 % on the argument quality prediction task and outperforms established <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline models</a> with an <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error rate reduction</a> of 22.7 %. Through ablation studies, we demonstrate the importance of multimodal cues in modeling argument quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--516 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.516/>Mind the Context : The Impact of <a href=https://en.wikipedia.org/wiki/Contextualization>Contextualization</a> in Neural Module Networks for Grounding Visual Referring Expressions</a></strong><br><a href=/people/a/arjun-akula/>Arjun Akula</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/k/keze-wang/>Keze Wang</a>
|
<a href=/people/s/song-chun-zhu/>Song-Chun Zhu</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--516><div class="card-body p-3 small">Neural module networks (NMN) are a popular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> as they lack the ability to share weights and exploit associations between similar textual contexts (e.g. dark cube on the left vs. black cube on the left). In this work, we address these limitations and evaluate the impact of contextual clues in improving the performance of NMN models. First, we address the problem of fixed textual inputs by parameterizing the module arguments. This substantially reduce the number of <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> in NMN by up to 75 % without any loss in performance. Next we propose a method to contextualize our parameterized model to enhance the module&#8217;s capacity in exploiting the visiolinguistic associations. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art NMN model on CLEVR-Ref+ dataset with +8.1 % improvement in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the single-referent test set and +4.3 % on the full test set. Additionally, we demonstrate that contextualization provides +11.2 % and +1.7 % improvements in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over prior NMN models on CLOSURE and NLVR2. We further evaluate the impact of our <a href=https://en.wikipedia.org/wiki/Contextualization>contextualization</a> by constructing a <a href=https://en.wikipedia.org/wiki/Contrast_(vision)>contrast set</a> for CLEVR-Ref+, which we call CC-Ref+. We significantly outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a> by as much as +10.4 % absolute <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on CC-Ref+, illustrating the generalization skills of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--524 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.524" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.524/>Knowledge Base Completion Meets Transfer Learning</a></strong><br><a href=/people/v/vid-kocijan/>Vid Kocijan</a>
|
<a href=/people/t/thomas-lukasiewicz/>Thomas Lukasiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--524><div class="card-body p-3 small">The aim of knowledge base completion is to predict unseen facts from existing facts in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. In this work, we introduce the first approach for <a href=https://en.wikipedia.org/wiki/Transfer_of_knowledge>transfer of knowledge</a> from one collection of facts to another without the need for entity or relation matching. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> works for both canonicalized knowledge bases and uncanonicalized or open knowledge bases, i.e., <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> where more than one copy of a real-world entity or relation may exist. Such <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> are a natural output of automated information extraction tools that extract <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>. Our main contribution is a method that can make use of a large-scale pretraining on facts, collected from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>, to improve predictions on <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> from a specific domain. The introduced method is the most impactful on small datasets such as ReVerb20 K, where we obtained a 6 % absolute increase of mean reciprocal rank and 65 % relative decrease of mean rank over the previously best method, despite not relying on large pre-trained models like BERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--526 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.526/>Towards Zero-Shot Knowledge Distillation for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/a/ahmad-rashid/>Ahmad Rashid</a>
|
<a href=/people/v/vasileios-lioutas/>Vasileios Lioutas</a>
|
<a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--526><div class="card-body p-3 small">Knowledge distillation (KD) is a common <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer algorithm</a> used for <a href=https://en.wikipedia.org/wiki/Data_compression>model compression</a> across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher&#8217;s training data for <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such <a href=https://en.wikipedia.org/wiki/Data_(computing)>data</a>. We present, to the best of our knowledge, the first work on Zero-shot Knowledge Distillation for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, where the student learns from the much larger teacher without any task specific data. Our solution combines out-of-domain data and <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a> to learn the teacher&#8217;s output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75 % and 92 % of the teacher&#8217;s classification score (accuracy or F1) while compressing the model 30 times.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--529 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.529" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.529/>QuestEval : <a href=https://en.wikipedia.org/wiki/Summarization>Summarization</a> Asks for Fact-based Evaluation<span class=acl-fixed-case>Q</span>uest<span class=acl-fixed-case>E</span>val: Summarization Asks for Fact-based Evaluation</a></strong><br><a href=/people/t/thomas-scialom/>Thomas Scialom</a>
|
<a href=/people/p/paul-alexis-dray/>Paul-Alexis Dray</a>
|
<a href=/people/s/sylvain-lamprier/>Sylvain Lamprier</a>
|
<a href=/people/b/benjamin-piwowarski/>Benjamin Piwowarski</a>
|
<a href=/people/j/jacopo-staiano/>Jacopo Staiano</a>
|
<a href=/people/a/alex-wang/>Alex Wang</a>
|
<a href=/people/p/patrick-gallinari/>Patrick Gallinari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--529><div class="card-body p-3 small">Summarization evaluation remains an open research problem : current <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a>. In this paper, we extend previous approaches and propose a unified framework, named QuestEval. In contrast to established metrics such as ROUGE or BERTScore, QuestEval does not require any <a href=https://en.wikipedia.org/wiki/Ground_truth>ground-truth reference</a>. Nonetheless, QuestEval substantially improves the correlation with human judgments over four evaluation dimensions (consistency, coherence, fluency, and relevance), as shown in extensive experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--531 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.531" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.531/>Finding a Balanced Degree of Automation for Summary Evaluation</a></strong><br><a href=/people/s/shiyue-zhang/>Shiyue Zhang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--531><div class="card-body p-3 small">Human evaluation for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization tasks</a> is reliable but brings in issues of reproducibility and high costs. Automatic metrics are cheap and reproducible but sometimes poorly correlated with <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a>. In this work, we propose flexible semiautomatic to automatic summary evaluation metrics, following the Pyramid human evaluation method. Semi-automatic Lite2Pyramid retains the reusable human-labeled Summary Content Units (SCUs) for reference(s) but replaces the manual work of judging SCUs&#8217; presence in system summaries with a natural language inference (NLI) model. Fully automatic Lite3Pyramid further substitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via a semantic role labeling (SRL) model. Finally, we propose <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>in-between metrics</a>, Lite2.xPyramid, where we use a simple <a href=https://en.wikipedia.org/wiki/Regressor>regressor</a> to predict how well the STUs can simulate SCUs and retain SCUs that are more difficult to simulate, which provides a smooth transition and balance between <a href=https://en.wikipedia.org/wiki/Automation>automation</a> and manual evaluation. Comparing to 15 existing metrics, we evaluate human-metric correlations on 3 existing meta-evaluation datasets and our newly collected PyrXSum (with 100/10 XSum examples / systems). It shows that Lite2Pyramid consistently has the best summary-level correlations ; Lite3Pyramid works better than or comparable to other automatic metrics ; Lite2.xPyramid trades off small correlation drops for larger manual effort reduction, which can reduce costs for future data collection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.535.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--535 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.535 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.535/>Controlling Machine Translation for Multiple Attributes with Additive Interventions</a></strong><br><a href=/people/a/andrea-schioppa/>Andrea Schioppa</a>
|
<a href=/people/d/david-vilar/>David Vilar</a>
|
<a href=/people/a/artem-sokolov/>Artem Sokolov</a>
|
<a href=/people/k/katja-filippova/>Katja Filippova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--535><div class="card-body p-3 small">Fine-grained control of machine translation (MT) outputs along multiple attributes is critical for many modern MT applications and is a requirement for gaining users&#8217; trust. A standard approach for exerting control in MT is to prepend the input with a special tag to signal the desired output attribute. Despite its simplicity, attribute tagging has several drawbacks : continuous values must be binned into discrete categories, which is unnatural for certain applications ; interference between multiple tags is poorly understood. We address these problems by introducing vector-valued interventions which allow for fine-grained control over multiple attributes simultaneously via a weighted linear combination of the corresponding vectors. For some attributes, our approach even allows for fine-tuning a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained without <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> to support such interventions. In experiments with three attributes (length, politeness and monotonicity) and two language pairs (English to German and Japanese) our models achieve better <a href=https://en.wikipedia.org/wiki/Scientific_control>control</a> over a wider range of tasks compared to <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging</a>, and translation quality does not degrade when no control is requested. Finally, we demonstrate how to enable <a href=https://en.wikipedia.org/wiki/Control_theory>control</a> in an already trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> after a relatively cheap fine-tuning stage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--536 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.536/>A Generative Framework for Simultaneous Machine Translation</a></strong><br><a href=/people/y/yishu-miao/>Yishu Miao</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--536><div class="card-body p-3 small">We propose a generative framework for simultaneous machine translation. Conventional approaches use a fixed number of source words to translate or learn dynamic policies for the number of source words by <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. Here we formulate <a href=https://en.wikipedia.org/wiki/Simultaneous_translation>simultaneous translation</a> as a structural sequence-to-sequence learning problem. A <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> is introduced to model read or translate actions at every time step, which is then integrated out to consider all the possible translation policies. A re-parameterised Poisson prior is used to regularise the policies which allows the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to explicitly balance translation quality and <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a>. The experiments demonstrate the effectiveness and robustness of the generative framework, which achieves the best BLEU scores given different average translation latencies on benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.538.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--538 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.538 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.538" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.538/>Boosting Cross-Lingual Transfer via <a href=https://en.wikipedia.org/wiki/Self-learning>Self-Learning</a> with Uncertainty Estimation</a></strong><br><a href=/people/l/liyan-xu/>Liyan Xu</a>
|
<a href=/people/x/xuchao-zhang/>Xuchao Zhang</a>
|
<a href=/people/x/xujiang-zhao/>Xujiang Zhao</a>
|
<a href=/people/h/haifeng-chen/>Haifeng Chen</a>
|
<a href=/people/f/feng-chen/>Feng Chen</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--538><div class="card-body p-3 small">Recent multilingual pre-trained language models have achieved remarkable zero-shot performance, where the model is only finetuned on one source language and directly evaluated on target languages. In this work, we propose a self-learning framework that further utilizes unlabeled data of target languages, combined with uncertainty estimation in the process to select high-quality silver labels. Three different uncertainties are adapted and analyzed specifically for the cross lingual transfer : Language Heteroscedastic / Homoscedastic Uncertainty (LEU / LOU), Evidential Uncertainty (EVI). We evaluate our framework with uncertainties on two cross-lingual tasks including Named Entity Recognition (NER) and Natural Language Inference (NLI) covering 40 languages in total, which outperforms the baselines significantly by 10 F1 for NER on average and 2.5 accuracy for NLI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--540 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.540" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.540/>Interactive Machine Comprehension with Dynamic Knowledge Graphs</a></strong><br><a href=/people/x/xingdi-yuan/>Xingdi Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--540><div class="card-body p-3 small">Interactive machine reading comprehension (iMRC) is machine comprehension tasks where knowledge sources are partially observable. An agent must interact with an environment sequentially to gather necessary knowledge in order to answer a question. We hypothesize that <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph representations</a> are good inductive biases, which can serve as an agent&#8217;s memory mechanism in iMRC tasks. We explore four different categories of <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> that can capture <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text information</a> at various levels. We describe methods that dynamically build and update these <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> during information gathering, as well as neural models to encode <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph representations</a> in RL agents. Extensive experiments on iSQuAD suggest that graph representations can result in significant performance improvements for RL agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.542.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--542 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.542 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.542" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.542/>Visual News : Benchmark and Challenges in News Image Captioning</a></strong><br><a href=/people/f/fuxiao-liu/>Fuxiao Liu</a>
|
<a href=/people/y/yinghan-wang/>Yinghan Wang</a>
|
<a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--542><div class="card-body p-3 small">We propose Visual News Captioner, an entity-aware model for the task of news image captioning. We also introduce Visual News, a large-scale benchmark consisting of more than one million news images along with associated news articles, image captions, author information, and other <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>. Unlike the standard image captioning task, news images depict situations where people, locations, and <a href=https://en.wikipedia.org/wiki/News>events</a> are of paramount importance. Our proposed method can effectively combine visual and textual features to generate captions with richer information such as <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a> and <a href=https://en.wikipedia.org/wiki/Non-physical_entity>entities</a>. More specifically, built upon the Transformer architecture, our model is further equipped with novel multi-modal feature fusion techniques and attention mechanisms, which are designed to generate named entities more accurately. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> utilizes much fewer parameters while achieving slightly better prediction results than competing methods. Our larger and more diverse Visual News dataset further highlights the remaining challenges in captioning news images.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--543 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.543" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.543/>Integrating Visuospatial, Linguistic, and Commonsense Structure into Story Visualization</a></strong><br><a href=/people/a/adyasha-maharana/>Adyasha Maharana</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--543><div class="card-body p-3 small">While much research has been done in text-to-image synthesis, little work has been done to explore the usage of linguistic structure of the input text. Such information is even more important for story visualization since its inputs have an explicit <a href=https://en.wikipedia.org/wiki/Narrative_structure>narrative structure</a> that needs to be translated into an image sequence (or visual story). Prior work in this domain has shown that there is ample room for improvement in the generated image sequence in terms of visual quality, <a href=https://en.wikipedia.org/wiki/Consistency>consistency</a> and <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a>. In this paper, we first explore the use of constituency parse trees using a Transformer-based recurrent architecture for encoding structured input. Second, we augment the structured input with commonsense information and study the impact of this external knowledge on the generation of visual story. Third, we also incorporate visual structure via bounding boxes and dense captioning to provide feedback about the characters / objects in generated images within a dual learning setup. We show that off-the-shelf dense-captioning models trained on Visual Genome can improve the spatial structure of images from a different target domain without needing fine-tuning. We train the model end-to-end using intra-story contrastive loss (between words and image sub-regions) and show significant improvements in visual quality. Finally, we provide an analysis of the linguistic and visuo-spatial information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--547 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.547.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.547" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.547/>Tribrid : Stance Classification with Neural Inconsistency Detection</a></strong><br><a href=/people/s/song-yang/>Song Yang</a>
|
<a href=/people/j/jacopo-urbani/>Jacopo Urbani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--547><div class="card-body p-3 small">We study the problem of performing automatic stance classification on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> with neural architectures such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>. Although these architectures deliver impressive results, their level is not yet comparable to the one of humans and they might produce errors that have a significant impact on the downstream task (e.g., fact-checking). To improve the performance, we present a new neural architecture where the input also includes automatically generated negated perspectives over a given claim. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is jointly learned to make simultaneously multiple predictions, which can be used either to improve the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> of the original perspective or to filter out doubtful predictions. In the first case, we propose a weakly supervised method for combining the predictions into a final one. In the second case, we show that using the <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence scores</a> to remove doubtful predictions allows our method to achieve human-like performance over the retained information, which is still a sizable part of the original input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--549 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.549" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.549/>Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks</a></strong><br><a href=/people/g/gael-guibon/>Gaël Guibon</a>
|
<a href=/people/m/matthieu-labeau/>Matthieu Labeau</a>
|
<a href=/people/h/helene-flamein/>Hélène Flamein</a>
|
<a href=/people/l/luce-lefeuvre/>Luce Lefeuvre</a>
|
<a href=/people/c/chloe-clavel/>Chloé Clavel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--549><div class="card-body p-3 small">Several recent studies on dyadic human-human interactions have been done on conversations without specific business objectives. However, many companies might benefit from studies dedicated to more precise environments such as after sales services or customer satisfaction surveys. In this work, we place ourselves in the scope of a live chat customer service in which we want to detect <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> and their evolution in the conversation flow. This context leads to multiple challenges that range from exploiting restricted, small and mostly unlabeled datasets to finding and adapting methods for such <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>. We tackle these challenges by using Few-Shot Learning while making the hypothesis it can serve conversational emotion classification for different languages and sparse labels. We contribute by proposing a variation of Prototypical Networks for sequence labeling in conversation that we name ProtoSeq. We test this method on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with different languages : daily conversations in English and customer service chat conversations in <a href=https://en.wikipedia.org/wiki/French_language>French</a>. When applied to emotion classification in conversations, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> proved to be competitive even when compared to other ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.553.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--553 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.553 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.553/>When is Wall a Pared and when a Muro? : Extracting Rules Governing Lexical Selection</a></strong><br><a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/k/kayo-yin/>Kayo Yin</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--553><div class="card-body p-3 small">Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun wall has different lexical manifestations in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> pared refers to an indoor wall while muro refers to an outside wall. However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way. In this work, we present a method for automatically identifying fine-grained lexical distinctions, and extracting rules explaining these distinctions in a human- and machine-readable format. We confirm the quality of these extracted rules in a language learning setup for two languages, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>, where we use the rules to teach non-native speakers when to translate a given ambiguous word into its different possible translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.556.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--556 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.556 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.556" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.556/>Continuous Entailment Patterns for Lexical Inference in Context</a></strong><br><a href=/people/m/martin-schmitt/>Martin Schmitt</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--556><div class="card-body p-3 small">Combining a pretrained language model (PLM) with textual patterns has been shown to help in both zero- and few-shot settings. For zero-shot performance, it makes sense to design <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> that closely resemble the text seen during self-supervised pretraining because the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> has never seen anything else. Supervised training allows for more flexibility. If we allow for tokens outside the PLM&#8217;s vocabulary, <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> can be adapted more flexibly to a PLM&#8217;s idiosyncrasies. Contrasting patterns where a token can be any continuous vector from those where a discrete choice between vocabulary elements has to be made, we call our method CONtinous pAtterNs (CONAN). We evaluate CONAN on two established benchmarks for lexical inference in context (LIiC) a.k.a. predicate entailment, a challenging natural language understanding task with relatively <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>small training data</a>. In a direct comparison with discrete patterns, <a href=https://en.wikipedia.org/wiki/CONAN>CONAN</a> consistently leads to improved performance, setting a new state of the art. Our experiments give valuable insights on the kind of <a href=https://en.wikipedia.org/wiki/Pattern>pattern</a> that enhances a PLM&#8217;s performance on LIiC and raise important questions regarding our understanding of PLMs using text patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.557.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--557 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.557 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.557.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.557/>Numeracy enhances the Literacy of Language Models</a></strong><br><a href=/people/a/avijit-thawani/>Avijit Thawani</a>
|
<a href=/people/j/jay-pujara/>Jay Pujara</a>
|
<a href=/people/f/filip-ilievski/>Filip Ilievski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--557><div class="card-body p-3 small">Specialized number representations in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction. But humans also use <a href=https://en.wikipedia.org/wiki/Numeracy>numeracy</a> to make better sense of <a href=https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences>world concepts</a>, e.g., you can seat 5 people in your &#8216;room&#8217; but not 500. Does a better grasp of numbers improve a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s understanding of other concepts and words? This paper studies the effect of using six different <a href=https://en.wikipedia.org/wiki/Encoder>number encoders</a> on the task of masked word prediction (MWP), as a proxy for evaluating <a href=https://en.wikipedia.org/wiki/Literacy>literacy</a>. To support this investigation, we develop Wiki-Convert, a 900,000 sentence dataset annotated with numbers and units, to avoid conflating nominal and ordinal number occurrences. We find a significant improvement in MWP for sentences containing numbers, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a BERT baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers. We release all code at https://git.io/JuZXn.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.558.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--558 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.558 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.558/>Students Who Study Together Learn Better : On the Importance of Collective Knowledge Distillation for Domain Transfer in Fact Verification</a></strong><br><a href=/people/m/mitch-paul-mithun/>Mitch Paul Mithun</a>
|
<a href=/people/s/sandeep-suntwal/>Sandeep Suntwal</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--558><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> produce state-of-the- art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. Previous works have proposed delexicalization as a form of knowledge distillation to reduce the dependency on such lexical artifacts. However, a critical unsolved issue that remains is how much delexicalization to apply : a little helps reduce <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>, but too much discards useful information. We propose <a href=https://en.wikipedia.org/wiki/Group_learning>Group Learning</a>, a knowledge and model distillation approach for fact verification in which multiple student models have access to different delexicalized views of the data, but are encouraged to learn from each other through pair-wise consistency losses. In several cross-domain experiments between the FEVER and FNC fact verification datasets, we show that our approach learns the best delexicalization strategy for the given training dataset, and outperforms state-of-the-art <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> that rely on the original data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.560.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--560 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.560 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.560/>Joint Passage Ranking for Diverse Multi-Answer Retrieval</a></strong><br><a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/k/kenton-lee/>Kenton Lee</a>
|
<a href=/people/m/ming-wei-chang/>Ming-Wei Chang</a>
|
<a href=/people/k/kristina-toutanova/>Kristina Toutanova</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--560><div class="card-body p-3 small">We study multi-answer retrieval, an under-explored problem that requires retrieving passages to cover multiple distinct answers for a given question. This task requires joint modeling of retrieved passages, as models should not repeatedly retrieve passages containing the same answer at the cost of missing a different valid answer. Prior work focusing on single-answer retrieval is limited as it can not reason about the set of passages jointly. In this paper, we introduce JPR, a joint passage retrieval model focusing on <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a>. To model the <a href=https://en.wikipedia.org/wiki/Joint_probability>joint probability</a> of the retrieved passages, JPR makes use of an autoregressive reranker that selects a sequence of passages, equipped with novel <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and decoding algorithms</a>. Compared to prior approaches, JPR achieves significantly better answer coverage on three multi-answer datasets. When combined with downstream question answering, the improved retrieval enables larger answer generation models since they need to consider fewer passages, establishing a new state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.561.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--561 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.561 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.561/>Generative Context Pair Selection for Multi-hop Question Answering</a></strong><br><a href=/people/d/dheeru-dua/>Dheeru Dua</a>
|
<a href=/people/c/cicero-dos-santos/>Cicero Nogueira dos Santos</a>
|
<a href=/people/p/patrick-ng/>Patrick Ng</a>
|
<a href=/people/b/ben-athiwaratkun/>Ben Athiwaratkun</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--561><div class="card-body p-3 small">Compositional reasoning tasks such as multi-hop question answering require <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to learn how to make latent decisions using only weak supervision from the final answer. Crowdsourced datasets gathered for these tasks, however, often contain only a slice of the underlying task distribution, which can induce unanticipated biases such as shallow word overlap between the question and context. Recent works have shown that discriminative training results in models that exploit these underlying biases to achieve a better held-out performance, without learning the right way to reason. We propose a generative context selection model for multi-hop QA that reasons about how the given question could have been generated given a context pair and not just independent contexts. We show that on HotpotQA, while being comparable to the state-of-the-art answering performance, our proposed generative passage selection model has a better performance (4.9 % higher than baseline) on adversarial held-out set which tests robustness of <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s multi-hop reasoning capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.563.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--563 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.563 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.563/>Have You Seen That Number? Investigating Extrapolation in Question Answering Models</a></strong><br><a href=/people/j/jeonghwan-kim/>Jeonghwan Kim</a>
|
<a href=/people/g/giwon-hong/>Giwon Hong</a>
|
<a href=/people/k/kyung-min-kim/>Kyung-min Kim</a>
|
<a href=/people/j/junmo-kang/>Junmo Kang</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--563><div class="card-body p-3 small">Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> fail to extrapolate to unseen numbers. Presenting <a href=https://en.wikipedia.org/wiki/Number>numbers</a> as digit-by-digit input to the model, we also propose the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat <a href=https://en.wikipedia.org/wiki/Number>numbers</a> differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.<i>E-digit</i> number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.568.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--568 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.568 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.568" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.568/>I Wish I Would Have Loved This One, But I Did n’t A Multilingual Dataset for Counterfactual Detection in Product Review<span class=acl-fixed-case>I</span> Wish <span class=acl-fixed-case>I</span> Would Have Loved This One, But <span class=acl-fixed-case>I</span> Didn’t – A Multilingual Dataset for Counterfactual Detection in Product Review</a></strong><br><a href=/people/j/james-oneill/>James O’Neill</a>
|
<a href=/people/p/polina-rozenshtein/>Polina Rozenshtein</a>
|
<a href=/people/r/ryuichi-kiryo/>Ryuichi Kiryo</a>
|
<a href=/people/m/motoko-kubota/>Motoko Kubota</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--568><div class="card-body p-3 small">Counterfactual statements describe events that did not or can not take place. We consider the problem of counterfactual detection (CFD) in product reviews. For this purpose, we annotate a multilingual CFD dataset from Amazon product reviews covering <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactual statements</a> written in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and Japanese languages. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is unique as it contains <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a> in multiple languages, covers a new application area of <a href=https://en.wikipedia.org/wiki/Review_site>e-commerce reviews</a>, and provides high quality professional annotations. We train <a href=https://en.wikipedia.org/wiki/Computational_fluid_dynamics>CFD models</a> using different text representation methods and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. We find that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are robust against the <a href=https://en.wikipedia.org/wiki/Selection_bias>selectional biases</a> introduced due to cue phrase-based sentence selection. Moreover, our CFD dataset is compatible with prior datasets and can be merged to learn accurate CFD models. Applying <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> on English counterfactual examples to create multilingual data performs poorly, demonstrating the language-specificity of this problem, which has been ignored so far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.570.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--570 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.570 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.570" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.570/>Evaluating the Morphosyntactic Well-formedness of Generated Texts</a></strong><br><a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/d/david-r-mortensen/>David R. Mortensen</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--570><div class="card-body p-3 small">Text generation systems are ubiquitous in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>. However, evaluation of these <a href=https://en.wikipedia.org/wiki/System>systems</a> remains a challenge, especially in multilingual settings. In this paper, we propose L&#8217;AMBRE a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to train robust parsers. We show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> on the task of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> through a diachronic study of systems translating into morphologically-rich languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.574.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--574 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.574 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.574" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.574/>ValNorm Quantifies <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> to Reveal Consistent Valence Biases Across Languages and Over Centuries<span class=acl-fixed-case>V</span>al<span class=acl-fixed-case>N</span>orm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries</a></strong><br><a href=/people/a/autumn-toney/>Autumn Toney</a>
|
<a href=/people/a/aylin-caliskan/>Aylin Caliskan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--574><div class="card-body p-3 small">Word embeddings learn <a href=https://en.wikipedia.org/wiki/Implicit_stereotype>implicit biases</a> from linguistic regularities captured by word co-occurrence statistics. By extending methods that quantify human-like biases in word embeddings, we introduce ValNorm, a novel intrinsic evaluation task and method to quantify the valence dimension of affect in human-rated word sets from <a href=https://en.wikipedia.org/wiki/Social_psychology>social psychology</a>. We apply ValNorm on static word embeddings from seven languages (Chinese, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, German, Polish, Portuguese, Spanish, and Turkish) and from historical English text spanning 200 years. ValNorm achieves consistently high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in quantifying the valence of non-discriminatory, non-social group word sets. Specifically, ValNorm achieves a <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> of r=0.88 for human judgment scores of valence for 399 words collected to establish pleasantness norms in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In contrast, we measure <a href=https://en.wikipedia.org/wiki/Gender_role>gender stereotypes</a> using the same set of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and find that <a href=https://en.wikipedia.org/wiki/Bias>social biases</a> vary across languages. Our results indicate that valence associations of non-discriminatory, non-social group words represent widely-shared associations, in seven languages and over 200 years.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.576.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--576 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.576 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.576" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.576/>Robust Open-Vocabulary Translation from Visual Text Representations</a></strong><br><a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/d/david-etter/>David Etter</a>
|
<a href=/people/m/matt-post/>Matt Post</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--576><div class="card-body p-3 small">Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an &#8216;open vocabulary.&#8217; This approach relies on consistent and correct underlying unicode sequences, and makes <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> susceptible to degradation from common types of <a href=https://en.wikipedia.org/wiki/Noise>noise</a> and <a href=https://en.wikipedia.org/wiki/Genetic_variation>variation</a>. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows. We show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using visual text representations approach or match performance of traditional text models on small and larger datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of <a href=https://en.wikipedia.org/wiki/Noise_(electronics)>noise</a>, achieving e.g., 25.9 <a href=https://en.wikipedia.org/wiki/Bitwise_operation>BLEU</a> on a character permuted GermanEnglish task where subword models degrade to 1.9.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.578.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--578 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.578 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.578" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.578/>Improving Multilingual Translation by Representation and Gradient Regularization</a></strong><br><a href=/people/y/yilin-yang/>Yilin Yang</a>
|
<a href=/people/a/akiko-eriguchi/>Akiko Eriguchi</a>
|
<a href=/people/a/alexandre-muzio/>Alexandre Muzio</a>
|
<a href=/people/p/prasad-tadepalli/>Prasad Tadepalli</a>
|
<a href=/people/s/stefan-lee/>Stefan Lee</a>
|
<a href=/people/h/hany-hassan-awadalla/>Hany Hassan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--578><div class="card-body p-3 small">Multilingual Neural Machine Translation (NMT) enables one <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to serve all <a href=https://en.wikipedia.org/wiki/Translation>translation directions</a>, including ones that are unseen during training, i.e. zero-shot translation. Despite being theoretically attractive, current models often produce low quality translations commonly failing to even produce outputs in the right target language. In this work, we observe that off-target translation is dominant even in strong multilingual systems, trained on massive multilingual corpora. To address this issue, we propose a joint approach to regularize NMT models at both representation-level and gradient-level. At the representation level, we leverage an auxiliary target language prediction task to regularize decoder outputs to retain information about the target language. At the gradient level, we leverage a small amount of direct data (in thousands of sentence pairs) to regularize model gradients. Our results demonstrate that our approach is highly effective in both reducing off-target translation occurrences and improving zero-shot translation performance by +5.59 and +10.38 BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> also works well when the small amount of direct data is not available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.579.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--579 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.579 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.579" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.579/>Learning Kernel-Smoothed Machine Translation with Retrieved Examples</a></strong><br><a href=/people/q/qingnan-jiang/>Qingnan Jiang</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/j/jun-cao/>Jun Cao</a>
|
<a href=/people/s/shanbo-cheng/>Shanbo Cheng</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--579><div class="card-body p-3 small">How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, updating the deployed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, <a href=https://en.wikipedia.org/wiki/Nonparametric_statistics>non-parametric methods</a> are prone to overfit the retrieved examples. In this work, we propose to learn Kernel-Smoothed Translation with Example Retrieval (KSTER), an effective approach to adapt neural machine translation models online. Experiments on domain adaptation and multi-domain machine translation datasets show that even without expensive retraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over the best existing online adaptation methods. The code and trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are released at https://github.com/jiangqn/KSTER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.580.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--580 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.580 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.580/>Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training</a></strong><br><a href=/people/m/minghao-wu/>Minghao Wu</a>
|
<a href=/people/y/yitong-li/>Yitong Li</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--580><div class="card-body p-3 small">Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the high resource ones. However, automatic balancing methods usually depend on the intra- and inter-dataset characteristics, which is usually agnostic or requires human priors. In this work, we propose an approach, MultiUAT, that dynamically adjusts the training data usage based on the model&#8217;s uncertainty on a small set of trusted clean data for multi-corpus machine translation. We experiments with two classes of uncertainty measures on multilingual (16 languages with 4 settings) and multi-domain settings (4 for in-domain and 2 for out-of-domain on English-German translation) and demonstrate our approach MultiUAT substantially outperforms its baselines, including both static and dynamic strategies. We analyze the cross-domain transfer and show the deficiency of static and similarity based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.583.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--583 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.583 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.583/>Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering</a></strong><br><a href=/people/s/siddhant-garg/>Siddhant Garg</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--583><div class="card-body p-3 small">In this paper we propose a novel approach towards improving the efficiency of Question Answering (QA) systems by filtering out questions that will not be answered by them. This is based on an interesting new finding : the answer confidence scores of state-of-the-art QA systems can be approximated well by models solely using the input question text. This enables preemptive filtering of questions that are not answered by the <a href=https://en.wikipedia.org/wiki/System>system</a> due to their answer confidence scores being lower than the <a href=https://en.wikipedia.org/wiki/System>system threshold</a>. Specifically, we learn Transformer-based question models by distilling Transformer-based answering models. Our experiments on three popular QA datasets and one industrial QA benchmark demonstrate the ability of our question models to approximate the Precision / Recall curves of the target QA system well. These question models, when used as filters, can effectively trade off lower computation cost of QA systems for lower <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a>, e.g., reducing computation by ~60 %, while only losing ~3-4 % of <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.585.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--585 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.585 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.585/>Explaining Answers with Entailment Trees</a></strong><br><a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/z/zhengnan-xie/>Zhengnan Xie</a>
|
<a href=/people/h/hannah-smith/>Hannah Smith</a>
|
<a href=/people/l/leighanna-pipatanangkura/>Leighanna Pipatanangkura</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--585><div class="card-body p-3 small">Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a rationale). If this could be done, new opportunities for understanding and debugging the <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks : generate a valid <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment tree</a> given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. We show that a strong <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> can partially solve these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, in particular when the relevant sentences are included in the input (e.g., 35 % of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.593.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--593 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.593 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.593/>Effective Sequence-to-Sequence Dialogue State Tracking</a></strong><br><a href=/people/j/jeffrey-zhao/>Jeffrey Zhao</a>
|
<a href=/people/m/mahdis-mahdieh/>Mahdis Mahdieh</a>
|
<a href=/people/y/ye-zhang/>Ye Zhang</a>
|
<a href=/people/y/yuan-cao/>Yuan Cao</a>
|
<a href=/people/y/yonghui-wu/>Yonghui Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--593><div class="card-body p-3 small">Sequence-to-sequence models have been applied to a wide variety of NLP tasks, but how to properly use them for dialogue state tracking has not been systematically investigated. In this paper, we study this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> from the perspectives of pre-training objectives as well as the formats of context representations. We demonstrate that the choice of pre-training objective makes a significant difference to the state tracking quality. In particular, we find that masked span prediction is more effective than auto-regressive language modeling. We also explore using Pegasus, a span prediction-based pre-training objective for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, for the state tracking model. We found that pre-training for the seemingly distant summarization task works surprisingly well for dialogue state tracking. In addition, we found that while recurrent state context representation works also reasonably well, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> may have a hard time recovering from earlier mistakes. We conducted experiments on the MultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.598.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--598 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.598 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.598/>RICA : Evaluating Robust Inference Capabilities Based on Commonsense Axioms<span class=acl-fixed-case>RICA</span>: Evaluating Robust Inference Capabilities Based on Commonsense Axioms</a></strong><br><a href=/people/p/pei-zhou/>Pei Zhou</a>
|
<a href=/people/r/rahul-khanna/>Rahul Khanna</a>
|
<a href=/people/s/seyeon-lee/>Seyeon Lee</a>
|
<a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/d/daniel-ho/>Daniel Ho</a>
|
<a href=/people/j/jay-pujara/>Jay Pujara</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--598><div class="card-body p-3 small">Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA : Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--600 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.600 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.600" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.600/>MATE : Multi-view Attention for Table Transformer Efficiency<span class=acl-fixed-case>MATE</span>: Multi-view Attention for Table Transformer Efficiency</a></strong><br><a href=/people/j/julian-eisenschlos/>Julian Eisenschlos</a>
|
<a href=/people/m/maharshi-gor/>Maharshi Gor</a>
|
<a href=/people/t/thomas-mueller/>Thomas Müller</a>
|
<a href=/people/w/william-cohen/>William Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--600><div class="card-body p-3 small">This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20 % of relational tables on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose <a href=https://en.wikipedia.org/wiki/MATE_(software)>MATE</a>, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> for <a href=https://en.wikipedia.org/wiki/Table_(information)>tabular data</a>, and sets a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for three table reasoning datasets. For HybridQA (Chen et al., 2020), a dataset that involves large documents containing tables, we improve the best prior result by 19 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--602 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.602" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.602/>When Attention Meets Fast Recurrence : Training <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> with Reduced Compute</a></strong><br><a href=/people/t/tao-lei/>Tao Lei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--602><div class="card-body p-3 small">Large language models have become increasingly difficult to train because of the growing <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time and cost</a>. In this work, we present SRU++, a highly-efficient <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that combines fast recurrence and <a href=https://en.wikipedia.org/wiki/Attention>attention</a> for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an <a href=https://en.wikipedia.org/wiki/Supercomputer>8-GPU machine</a>. We further demonstrate that SRU++ requires minimal <a href=https://en.wikipedia.org/wiki/Attention>attention</a> for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--603 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.603/>Universal-KD : Attention-based Output-Grounded Intermediate Layer Knowledge Distillation<span class=acl-fixed-case>KD</span>: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation</a></strong><br><a href=/people/y/yimeng-wu/>Yimeng Wu</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a>
|
<a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/m/md-akmal-haidar/>Md Akmal Haidar</a>
|
<a href=/people/a/ali-ghodsi/>Ali Ghodsi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--603><div class="card-body p-3 small">Intermediate layer matching is shown as an effective approach for improving knowledge distillation (KD). However, this technique applies <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching</a> in the hidden spaces of two different <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>networks</a> (i.e. student and teacher), which lacks clear interpretability. Moreover, intermediate layer KD can not easily deal with other problems such as layer mapping search and architecture mismatch (i.e. it requires the teacher and student to be of the same model type). To tackle the aforementioned problems all together, we propose Universal-KD to match intermediate layers of the teacher and the student in the output space (by adding pseudo classifiers on intermediate layers) via the attention-based layer projection. By doing this, our unified approach has three merits : (i) it can be flexibly combined with current intermediate layer distillation techniques to improve their results (ii) the pseudo classifiers of the teacher can be deployed instead of extra expensive teacher assistant networks to address the capacity gap problem in KD which is a common issue when the gap between the size of the teacher and student networks becomes too large ; (iii) it can be used in cross-architecture intermediate layer KD. We did comprehensive experiments in distilling BERT-base into BERT-4, RoBERTa-large into DistilRoBERTa and BERT-base into CNN and LSTM-based models. Results on the GLUE tasks show that our approach is able to outperform other KD techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--605 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.605" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.605/>Word-Level Coreference Resolution</a></strong><br><a href=/people/v/vladimir-dobrovolskii/>Vladimir Dobrovolskii</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--605><div class="card-body p-3 small">Recent coreference resolution models rely heavily on span representations to find <a href=https://en.wikipedia.org/wiki/Coreference>coreference links</a> between word spans. As the number of spans is O(n^2) in the length of text and the number of potential links is O(n^4), various pruning techniques are necessary to make this approach computationally feasible. We propose instead to consider <a href=https://en.wikipedia.org/wiki/Coreference>coreference links</a> between individual words rather than word spans and then reconstruct the word spans. This reduces the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> of the coreference model to O(n^2) and allows it to consider all potential mentions without pruning any of them out. We also demonstrate that, with these changes, SpanBERT for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> will be significantly outperformed by RoBERTa. While being highly efficient, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs competitively with recent coreference resolution systems on the OntoNotes benchmark.<tex-math>O(n^2)</tex-math> in the length of text and the number of potential links is <tex-math>O(n^4)</tex-math>, various pruning techniques are necessary to make this approach computationally feasible. We propose instead to consider coreference links between individual words rather than word spans and then reconstruct the word spans. This reduces the complexity of the coreference model to <tex-math>O(n^2)</tex-math> and allows it to consider all potential mentions without pruning any of them out. We also demonstrate that, with these changes, SpanBERT for coreference resolution will be significantly outperformed by RoBERTa. While being highly efficient, our model performs competitively with recent coreference resolution systems on the OntoNotes benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--611 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.611" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.611/>LM-Critic : Language Models for Unsupervised Grammatical Error Correction<span class=acl-fixed-case>LM</span>-Critic: Language Models for Unsupervised Grammatical Error Correction</a></strong><br><a href=/people/m/michihiro-yasunaga/>Michihiro Yasunaga</a>
|
<a href=/people/j/jure-leskovec/>Jure Leskovec</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--611><div class="card-body p-3 small">Grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs for training, but obtaining such annotation can be prohibitively expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program without any labeled examples, but this relies on a perfect critic (e.g., a compiler) that returns whether an example is valid or not, which does not exist for the GEC task. In this work, we show how to leverage a pretrained language model (LM) in defining an LM-Critic, which judges a sentence to be grammatical if the LM assigns it a higher probability than its local perturbations. We apply this LM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap realistic ungrammatical / grammatical pairs for training a corrector. We evaluate our approach on GEC datasets on multiple domains (CoNLL-2014, BEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing methods in both the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setting</a> (+7.7 F0.5) and the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised setting</a> (+0.5 F0.5).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--615 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.615/>Come hither or go away? Recognising pre-electoral coalition signals in the news</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/a/anna-adendorf/>Anna Adendorf</a>
|
<a href=/people/o/oke-bahnsen/>Oke Bahnsen</a>
|
<a href=/people/l/lukas-stoetzer/>Lukas Stoetzer</a>
|
<a href=/people/h/heiner-stuckenschmidt/>Heiner Stuckenschmidt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--615><div class="card-body p-3 small">In this paper, we introduce the task of political coalition signal prediction from text, that is, the task of recognizing from the <a href=https://en.wikipedia.org/wiki/News_media>news coverage</a> leading up to an election the (un)willingness of political parties to form a <a href=https://en.wikipedia.org/wiki/Coalition_government>government coalition</a>. We decompose our problem into two related, but distinct tasks : (i) predicting whether a reported statement from a politician or a journalist refers to a potential <a href=https://en.wikipedia.org/wiki/Coalition>coalition</a> and (ii) predicting the polarity of the signal namely, whether the speaker is in favour of or against the coalition. For this, we explore the benefits of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> and investigate which setup and task formulation is best suited for each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-task</a>. We evaluate our approach, based on hand-coded newspaper articles, covering elections in three countries (Ireland, <a href=https://en.wikipedia.org/wiki/Germany>Germany</a>, Austria) and two languages (English, German). Our results show that the multi-task learning approach can further improve results over a strong monolingual transfer learning baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--616 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.616" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.616/># HowYouTagTweets : Learning User Hashtagging Preferences via Personalized Topic Attention<span class=acl-fixed-case>H</span>ow<span class=acl-fixed-case>Y</span>ou<span class=acl-fixed-case>T</span>ag<span class=acl-fixed-case>T</span>weets: Learning User Hashtagging Preferences via Personalized Topic Attention</a></strong><br><a href=/people/y/yuji-zhang/>Yuji Zhang</a>
|
<a href=/people/y/yubo-zhang/>Yubo Zhang</a>
|
<a href=/people/c/chunpu-xu/>Chunpu Xu</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/z/ziyan-jiang/>Ziyan Jiang</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--616><div class="card-body p-3 small">Millions of <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> are created on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> every day to cross-refer messages concerning similar topics. To help people find the topics they want to discuss, this paper characterizes a user&#8217;s hashtagging preferences via predicting how likely they will post with a <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a>. It is hypothesized that one&#8217;s interests in a <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> are related with what they said before (user history) and the existing posts present the <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> (hashtag contexts). These factors are married in the deep semantic space built with a pre-trained BERT and a neural topic model via <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a>. In this way, user interests learned from the past can be customized to match future <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a>, which is beyond the capability of existing <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> assuming unchanged hashtag semantics. Furthermore, we propose a novel personalized topic attention to capture salient contents to personalize hashtag contexts. Experiments on a large-scale Twitter dataset show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms the state-of-the-art recommendation approach without exploiting latent topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.618.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--618 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.618" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.618/>Proxy Indicators for the Quality of Open-domain Dialogues</a></strong><br><a href=/people/r/rostislav-nedelchev/>Rostislav Nedelchev</a>
|
<a href=/people/j/jens-lehmann/>Jens Lehmann</a>
|
<a href=/people/r/ricardo-usbeck/>Ricardo Usbeck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--618><div class="card-body p-3 small">The automatic evaluation of open-domain dialogues remains a largely unsolved challenge. Despite the abundance of work done in the field, human judges have to evaluate dialogues&#8217; quality. As a consequence, performing such <a href=https://en.wikipedia.org/wiki/Evaluation>evaluations</a> at scale is usually expensive. This work investigates using a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep-learning model</a> trained on the General Language Understanding Evaluation (GLUE) benchmark to serve as a quality indication of open-domain dialogues. The aim is to use the various GLUE tasks as different perspectives on judging the quality of conversation, thus reducing the need for additional training data or responses that serve as quality references. Due to this nature, the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can infer various quality metrics and can derive a component-based overall score. We achieve statistically significant <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation coefficients</a> of up to 0.7.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.619.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--619 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.619/>Q^2 : Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a><span class=tex-math>Q<sup>2</sup></span>: <span class=acl-fixed-case>E</span>valuating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering</a></strong><br><a href=/people/o/or-honovich/>Or Honovich</a>
|
<a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/e/ella-neeman/>Ella Neeman</a>
|
<a href=/people/i/idan-szpektor/>Idan Szpektor</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--619><div class="card-body p-3 small">Neural knowledge-grounded generative models for <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. Our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, denoted Q^2, compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough <a href=https://en.wikipedia.org/wiki/Meta-analysis>meta-evaluation</a> of Q^2 against other <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> using this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and two others, where it consistently shows higher correlation with human judgements.<tex-math>Q^2</tex-math>, compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of <tex-math>Q^2</tex-math> against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--622 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.622" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.622/>Zero-Shot Dialogue State Tracking via Cross-Task Transfer</a></strong><br><a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/z/zhenpeng-zhou/>Zhenpeng Zhou</a>
|
<a href=/people/p/paul-a-crook/>Paul Crook</a>
|
<a href=/people/z/zhiguang-wang/>Zhiguang Wang</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/e/eunjoon-cho/>Eunjoon Cho</a>
|
<a href=/people/r/rajen-subba/>Rajen Subba</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--622><div class="card-body p-3 small">Zero-shot transfer learning for dialogue state tracking (DST) enables us to handle a variety of task-oriented dialogue domains without the expense of collecting in-domain data. In this work, we propose to transfer the cross-task knowledge from general question answering (QA) corpora for the zero-shot DST task. Specifically, we propose TransferQA, a transferable generative QA model that seamlessly combines extractive QA and multi-choice QA via a text-to-text transformer framework, and tracks both categorical slots and non-categorical slots in DST. In addition, we introduce two effective ways to construct unanswerable questions, namely, negative question sampling and context truncation, which enable our model to handle none value slots in the zero-shot DST setting. The extensive experiments show that our approaches substantially improve the existing zero-shot and few-shot results on MultiWoz. Moreover, compared to the fully trained <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on the Schema-Guided Dialogue dataset, our approach shows better generalization ability in unseen domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.623.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--623 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.623/>Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance</a></strong><br><a href=/people/c/carel-van-niekerk/>Carel van Niekerk</a>
|
<a href=/people/a/andrey-malinin/>Andrey Malinin</a>
|
<a href=/people/c/christian-geishauser/>Christian Geishauser</a>
|
<a href=/people/m/michael-heck/>Michael Heck</a>
|
<a href=/people/h/hsien-chin-lin/>Hsien-chin Lin</a>
|
<a href=/people/n/nurul-lubis/>Nurul Lubis</a>
|
<a href=/people/s/shutong-feng/>Shutong Feng</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--623><div class="card-body p-3 small">The ability to identify and resolve <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> is crucial for the <a href=https://en.wikipedia.org/wiki/Robustness_(evolution)>robustness</a> of a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a>. Indeed, this has been confirmed empirically on <a href=https://en.wikipedia.org/wiki/System>systems</a> that utilise <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian approaches</a> to dialogue belief tracking. However, such <a href=https://en.wikipedia.org/wiki/System>systems</a> consider only <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence estimates</a> and have difficulty scaling to more complex settings. Neural dialogue systems, on the other hand, rarely take <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainties</a> into account. They are therefore overconfident in their decisions and less robust. Moreover, the performance of the tracking task is often evaluated in isolation, without consideration of its effect on the downstream policy optimisation. We propose the use of different uncertainty measures in neural belief tracking. The effects of these measures on the downstream task of policy optimisation are evaluated by adding selected measures of <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> to the <a href=https://en.wikipedia.org/wiki/Feature_space>feature space</a> of the policy and training policies through interaction with a user simulator. Both human and simulated user results show that incorporating these measures leads to improvements both of the performance and of the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of the downstream dialogue policy. This highlights the importance of developing neural dialogue belief trackers that take <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> into account.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--624 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.624/>Dynamic Forecasting of Conversation Derailment</a></strong><br><a href=/people/y/yova-kementchedjhieva/>Yova Kementchedjhieva</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--624><div class="card-body p-3 small">Online conversations can sometimes take a turn for the worse, either due to systematic cultural differences, accidental misunderstandings, or mere malice. Automatically forecasting derailment in public online conversations provides an opportunity to take early action to moderate it. Previous work in this space is limited, and we extend <a href=https://en.wikipedia.org/wiki/It_(pronoun)>it</a> in several ways. We apply a pretrained language encoder to the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, which outperforms earlier approaches. We further experiment with shifting the training paradigm for the task from a static to a dynamic one to increase the forecast horizon. This approach shows mixed results : in a high-quality data setting, a longer average forecast horizon can be achieved at the cost of a small drop in F1 ; in a low-quality data setting, however, dynamic training propagates the noise and is highly detrimental to performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.628.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--628 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.628 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.628" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.628/>CAPE : Context-Aware Private Embeddings for Private Language Learning<span class=acl-fixed-case>CAPE</span>: Context-Aware Private Embeddings for Private Language Learning</a></strong><br><a href=/people/r/richard-plant/>Richard Plant</a>
|
<a href=/people/d/dimitra-gkatzia/>Dimitra Gkatzia</a>
|
<a href=/people/v/valerio-giuffrida/>Valerio Giuffrida</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--628><div class="card-body p-3 small">Neural language models have contributed to state-of-the-art results in a number of downstream applications including <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, intent classification and others. However, obtaining text representations or embeddings using these models risks encoding <a href=https://en.wikipedia.org/wiki/Personal_data>personally identifiable information</a> learned from language and context cues that may lead to privacy leaks. To ameliorate this issue, we propose Context-Aware Private Embeddings (CAPE), a novel approach which combines <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> and <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a> to preserve <a href=https://en.wikipedia.org/wiki/Privacy>privacy</a> during training of embeddings. Specifically, CAPE firstly applies calibrated noise through <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> to maintain the privacy of text representations by preserving the encoded semantic links while obscuring sensitive information. Next, CAPE employs an adversarial training regime that obscures identified private variables. Experimental results demonstrate that our proposed approach is more effective in reducing private information leakage than either single intervention, with approximately a 3 % reduction in attacker performance compared to the best-performing current method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--629 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.629" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.629/>Text Detoxification using Large Pre-trained Neural Models</a></strong><br><a href=/people/d/david-dale/>David Dale</a>
|
<a href=/people/a/anton-voronov/>Anton Voronov</a>
|
<a href=/people/d/daryna-dementieva/>Daryna Dementieva</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/o/olga-kozlova/>Olga Kozlova</a>
|
<a href=/people/n/nikita-semenov/>Nikita Semenov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--629><div class="card-body p-3 small">We present two novel <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> for eliminating toxicity in text. Our first method combines two recent ideas : (1) guidance of the generation process with small style-conditional language models and (2) use of paraphrasing models to perform style transfer. We use a well-performing <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphraser</a> guided by style-trained language models to keep the text content and remove <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a>. Our second method uses BERT to replace toxic words with their non-offensive synonyms. We make the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> more flexible by enabling BERT to replace mask tokens with a variable number of words. Finally, we present the first large-scale comparative study of style transfer models on the task of toxicity removal. We compare our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> with a number of <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for style transfer. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are evaluated in a reference-free way using a combination of unsupervised style transfer metrics. Both <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> we suggest yield new SOTA results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.630.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--630 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.630 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.630" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.630/>Document-Level Text Simplification : Dataset, Criteria and Baseline</a></strong><br><a href=/people/r/renliang-sun/>Renliang Sun</a>
|
<a href=/people/h/hanqi-jin/>Hanqi Jin</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--630><div class="card-body p-3 small">Text simplification is a valuable technique. However, current research is limited to <a href=https://en.wikipedia.org/wiki/Sentence_simplification>sentence simplification</a>. In this paper, we define and investigate a new task of document-level text simplification, which aims to simplify a document consisting of multiple sentences. Based on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia dumps</a>, we first construct a large-scale dataset named D-Wikipedia and perform analysis and human evaluation on it to show that the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is reliable. Then, we propose a new automatic evaluation metric called D-SARI that is more suitable for the document-level simplification task. Finally, we select several representative models as baseline models for this task and perform automatic evaluation and human evaluation. We analyze the results and point out the shortcomings of the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.632.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--632 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.632 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.632/>Paraphrasing Compound Nominalizations</a></strong><br><a href=/people/j/john-s-y-lee/>John Lee</a>
|
<a href=/people/h/ho-hung-lim/>Ho Hung Lim</a>
|
<a href=/people/c/carol-webster/>Carol Webster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--632><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Nominalization>nominalization</a> uses a <a href=https://en.wikipedia.org/wiki/Deverbal_noun>deverbal noun</a> to describe an event associated with its underlying verb. Commonly found in academic and formal texts, nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Our goal is to interpret <a href=https://en.wikipedia.org/wiki/Nominalization>nominalizations</a> by generating clausal paraphrases. We address compound nominalizations with both nominal and adjectival modifiers, as well as prepositional phrases. In evaluations on a number of <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a>, we obtained the strongest performance by using a pre-trained contextualized language model to re-rank paraphrase candidates identified by a textual entailment model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.635.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--635 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.635 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.635" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.635/>TDEER : An Efficient Translating Decoding Schema for Joint Extraction of Entities and Relations<span class=acl-fixed-case>TDEER</span>: An Efficient Translating Decoding Schema for Joint Extraction of Entities and Relations</a></strong><br><a href=/people/x/xianming-li/>Xianming Li</a>
|
<a href=/people/x/xiaotian-luo/>Xiaotian Luo</a>
|
<a href=/people/c/chenghao-dong/>Chenghao Dong</a>
|
<a href=/people/d/daichuan-yang/>Daichuan Yang</a>
|
<a href=/people/b/beidi-luan/>Beidi Luan</a>
|
<a href=/people/z/zhen-he/>Zhen He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--635><div class="card-body p-3 small">Joint extraction of entities and relations from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured texts</a> to form factual triples is a fundamental task of constructing a Knowledge Base (KB). A common method is to decode triples by predicting entity pairs to obtain the corresponding <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a>. However, it is still challenging to handle this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> efficiently, especially for the overlapping triple problem. To address such a problem, this paper proposes a novel efficient entities and relations extraction model called TDEER, which stands for Translating Decoding Schema for Joint Extraction of Entities and Relations. Unlike the common approaches, the proposed translating decoding schema regards the relation as a translating operation from subject to objects, i.e., TDEER decodes triples as subject + relation objects. TDEER can naturally handle the overlapping triple problem, because the translating decoding schema can recognize all possible triples, including overlapping and non-overlapping triples. To enhance <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a>, we introduce negative samples to alleviate <a href=https://en.wikipedia.org/wiki/Errors-in-variables_models>error accumulation</a> at different stages. Extensive experiments on public datasets demonstrate that TDEER produces competitive results compared with the state-of-the-art (SOTA) baselines. Furthermore, the computation complexity analysis indicates that TDEER is more efficient than powerful <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Especially, the proposed TDEER is 2 times faster than the recent SOTA models. The code is available at.<b>TDEER</b>, which stands for <b>T</b>ranslating <b>D</b>ecoding Schema for Joint <b>E</b>xtraction of <b>E</b>ntities and <b>R</b>elations. Unlike the common approaches, the proposed translating decoding schema regards the relation as a translating operation from subject to objects, i.e., TDEER decodes triples as <tex-math>subject + relation \\rightarrow objects</tex-math>. TDEER can naturally handle the overlapping triple problem, because the translating decoding schema can recognize all possible triples, including overlapping and non-overlapping triples. To enhance model robustness, we introduce negative samples to alleviate error accumulation at different stages. Extensive experiments on public datasets demonstrate that TDEER produces competitive results compared with the state-of-the-art (SOTA) baselines. Furthermore, the computation complexity analysis indicates that TDEER is more efficient than powerful baselines. Especially, the proposed TDEER is 2 times faster than the recent SOTA models. The code is available at <url>https://github.com/4AI/TDEER</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.636.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--636 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.636 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.636" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.636/>Extracting Event Temporal Relations via <a href=https://en.wikipedia.org/wiki/Hyperbolic_geometry>Hyperbolic Geometry</a></a></strong><br><a href=/people/x/xingwei-tan/>Xingwei Tan</a>
|
<a href=/people/g/gabriele-pergola/>Gabriele Pergola</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--636><div class="card-body p-3 small">Detecting events and their evolution through time is a crucial task in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. Recent neural approaches to event temporal relation extraction typically map events to embeddings in the <a href=https://en.wikipedia.org/wiki/Euclidean_space>Euclidean space</a> and train a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to detect temporal relations between event pairs. However, embeddings in the <a href=https://en.wikipedia.org/wiki/Euclidean_space>Euclidean space</a> can not capture richer <a href=https://en.wikipedia.org/wiki/Asymmetric_relation>asymmetric relations</a> such as event temporal relations. We thus propose to embed <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> into hyperbolic spaces, which are intrinsically oriented at modeling <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structures</a>. We introduce two approaches to encode <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> and their temporal relations in hyperbolic spaces. One approach leverages hyperbolic embeddings to directly infer <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event relations</a> through simple <a href=https://en.wikipedia.org/wiki/Operation_(mathematics)>geometrical operations</a>. In the second one, we devise an <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end architecture</a> composed of hyperbolic neural units tailored for the temporal relation extraction task. Thorough experimental assessments on widely used datasets have shown the benefits of revisiting the tasks on a different <a href=https://en.wikipedia.org/wiki/Space>geometrical space</a>, resulting in state-of-the-art performance on several standard <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. Finally, the ablation study and several qualitative analyses highlighted the rich event semantics implicitly encoded into hyperbolic spaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.637.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--637 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.637 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.637" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.637/>Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention</a></strong><br><a href=/people/j/jiawei-chen/>Jiawei Chen</a>
|
<a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--637><div class="card-body p-3 small">Event detection has long been troubled by the trigger curse : overfitting the trigger will harm the generalization ability while underfitting it will hurt the <a href=https://en.wikipedia.org/wiki/Detection>detection</a> performance. This <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is even more severe in few-shot scenario. In this paper, we identify and solve the trigger curse problem in few-shot event detection (FSED) from a causal view. By formulating FSED with a structural causal model (SCM), we found that the trigger is a confounder of the context and the result, which makes previous FSED methods much easier to overfit triggers. To resolve this problem, we propose to intervene on the context via backdoor adjustment during <a href=https://en.wikipedia.org/wiki/Training>training</a>. Experiments show that our method significantly improves the FSED on both ACE05 and MAVEN datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.639.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--639 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.639 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.639/>Time-dependent Entity Embedding is not All You Need : A Re-evaluation of Temporal Knowledge Graph Completion Models under a Unified Framework</a></strong><br><a href=/people/z/zhen-han/>Zhen Han</a>
|
<a href=/people/g/gengyuan-zhang/>Gengyuan Zhang</a>
|
<a href=/people/y/yunpu-ma/>Yunpu Ma</a>
|
<a href=/people/v/volker-tresp/>Volker Tresp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--639><div class="card-body p-3 small">Various temporal knowledge graph (KG) completion models have been proposed in the recent literature. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> usually contain two parts, a temporal embedding layer and a <a href=https://en.wikipedia.org/wiki/Score_(statistics)>score function</a> derived from existing static KG modeling approaches. Since the approaches differ along several dimensions, including different <a href=https://en.wikipedia.org/wiki/Score_(statistics)>score functions</a> and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training strategies</a>, the individual contributions of different temporal embedding techniques to model performance are not always clear. In this work, we systematically study six temporal embedding approaches and empirically quantify their performance across a wide range of configurations with about 3000 experiments and 13159 GPU hours. We classify the temporal embeddings into two classes : (1) timestamp embeddings and (2) time-dependent entity embeddings. Despite the common belief that the latter is more expressive, an extensive experimental study shows that timestamp embeddings can achieve on-par or even better performance with significantly fewer parameters. Moreover, we find that when trained appropriately, the relative performance differences between various temporal embeddings often shrink and sometimes even reverse when compared to prior results. For example, TTransE (CITATION), one of the first temporal KG models, can outperform more recent architectures on ICEWS datasets. To foster further research, we provide the first unified open-source framework for temporal KG completion models with full composability, where temporal embeddings, score functions, loss functions, regularizers, and the explicit modeling of reciprocal relations can be combined arbitrarily.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.643.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--643 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.643 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.643" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.643/>Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution</a></strong><br><a href=/people/y/yi-huang/>Yi Huang</a>
|
<a href=/people/b/buse-giledereli/>Buse Giledereli</a>
|
<a href=/people/a/abdullatif-koksal/>Abdullatif Köksal</a>
|
<a href=/people/a/arzucan-ozgur/>Arzucan Özgür</a>
|
<a href=/people/e/elif-ozkirimli/>Elif Ozkirimli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--643><div class="card-body p-3 small">Multi-label text classification is a challenging task because <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Resampling and re-weighting are common approaches used for addressing the <a href=https://en.wikipedia.org/wiki/Social_class>class imbalance problem</a>, however, they are not effective when there is label dependency besides <a href=https://en.wikipedia.org/wiki/Social_class>class imbalance</a> because they result in oversampling of common labels. Here, we introduce the application of balancing loss functions for multi-label text classification. We perform experiments on a general domain dataset with 90 labels (Reuters-21578) and a domain-specific dataset from <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a> with 18211 labels. We find that a distribution-balanced loss function, which inherently addresses both the class imbalance and label linkage problems, outperforms commonly used <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a>. Distribution balancing methods have been successfully used in the image recognition field. Here, we show their effectiveness in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Source code is available at https://github.com/blessu/BalancedLossNLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.644.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--644 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.644 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.644" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.644/>Bayesian Topic Regression for Causal Inference<span class=acl-fixed-case>B</span>ayesian Topic Regression for Causal Inference</a></strong><br><a href=/people/m/maximilian-ahrens/>Maximilian Ahrens</a>
|
<a href=/people/j/julian-ashwin/>Julian Ashwin</a>
|
<a href=/people/j/jan-peter-calliess/>Jan-Peter Calliess</a>
|
<a href=/people/v/vu-nguyen/>Vu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--644><div class="card-body p-3 small">Causal inference using observational text data is becoming increasingly popular in many research areas. This paper presents the Bayesian Topic Regression (BTR) model that uses both text and numerical information to model an outcome variable. It allows estimation of both discrete and continuous treatment effects. Furthermore, it allows for the inclusion of additional numerical confounding factors next to text data. To this end, we combine a supervised Bayesian topic model with a Bayesian regression framework and perform supervised representation learning for the text features jointly with the regression parameter training, respecting the <a href=https://en.wikipedia.org/wiki/Frisch&#8211;Waugh&#8211;Lovell_theorem>Frisch-Waugh-Lovell theorem</a>. Our paper makes two main contributions. First, we provide a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression framework</a> that allows <a href=https://en.wikipedia.org/wiki/Causal_inference>causal inference</a> in settings when both text and numerical confounders are of relevance. We show with synthetic and semi-synthetic datasets that our joint approach recovers ground truth with lower bias than any benchmark model, when text and numerical features are correlated. Second, experiments on two real-world datasets demonstrate that a joint and supervised learning strategy also yields superior prediction results compared to strategies that estimate regression weights for text and non-text features separately, being even competitive with more complex deep neural networks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.646.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--646 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.646 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.646/>What’s in Your Head? Emergent Behaviour in Multi-Task Transformer Models<span class=acl-fixed-case>W</span>hat’s in Your Head? <span class=acl-fixed-case>E</span>mergent Behaviour in Multi-Task Transformer Models</a></strong><br><a href=/people/m/mor-geva/>Mor Geva</a>
|
<a href=/people/u/uri-katz/>Uri Katz</a>
|
<a href=/people/a/aviv-ben-arie/>Aviv Ben-Arie</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--646><div class="card-body p-3 small">The primary paradigm for multi-task training in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this work, we examine the behaviour of non-target heads, that is, the output of heads when given input that belongs to a different task than the one they were trained for. We find that non-target heads exhibit <a href=https://en.wikipedia.org/wiki/Emergence>emergent behaviour</a>, which may either explain the target task, or generalize beyond their original task. For example, in a numerical reasoning task, a span extraction head extracts from the input the arguments to a computation that results in a number generated by a target generative head. In addition, a summarization head that is trained with a target question answering head, outputs query-based summaries when given a question and a context from which the answer is to be extracted. This <a href=https://en.wikipedia.org/wiki/Emergence>emergent behaviour</a> suggests that multi-task training leads to non-trivial extrapolation of skills, which can be harnessed for interpretability and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.647.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--647 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.647 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.647/>Do n’t Search for a Search Method Simple <a href=https://en.wikipedia.org/wiki/Heuristic>Heuristics</a> Suffice for Adversarial Text Attacks</a></strong><br><a href=/people/n/nathaniel-berger/>Nathaniel Berger</a>
|
<a href=/people/s/stefan-riezler/>Stefan Riezler</a>
|
<a href=/people/s/sebastian-ebert/>Sebastian Ebert</a>
|
<a href=/people/a/artem-sokolov/>Artem Sokolov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--647><div class="card-body p-3 small">Recently more attention has been given to adversarial attacks on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. A central research topic has been the investigation of <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithms</a> and search constraints, accompanied by <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark algorithms</a> and <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. We implement an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> inspired by zeroth order optimization-based attacks and compare with the benchmark results in the TextAttack framework. Surprisingly, we find that optimization-based methods do not yield any improvement in a constrained setup and slightly benefit from approximate gradient information only in unconstrained setups where search spaces are larger. In contrast, simple <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristics</a> exploiting <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbors</a> without querying the target function yield substantial success rates in constrained setups, and nearly full success rate in unconstrained setups, at an order of magnitude fewer queries. We conclude from these results that current TextAttack benchmark tasks are too easy and constraints are too strict, preventing meaningful research on black-box adversarial text attacks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.648.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--648 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.648 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.648" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.648/>Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods</a></strong><br><a href=/people/p/peru-bhardwaj/>Peru Bhardwaj</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a>
|
<a href=/people/l/luca-costabello/>Luca Costabello</a>
|
<a href=/people/d/declan-osullivan/>Declan O’Sullivan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--648><div class="card-body p-3 small">Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the <a href=https://en.wikipedia.org/wiki/Vulnerability_(computing)>security vulnerabilities</a> that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training time to cause model failure at test time. To select adversarial deletions, we propose to use the model-agnostic instance attribution methods from Interpretable Machine Learning, which identify the training instances that are most influential to a neural model&#8217;s predictions on test instances. We use these influential triples as adversarial deletions. We further propose a <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic method</a> to replace one of the two entities in each influential triple to generate adversarial additions. Our experiments show that the proposed strategies outperform the state-of-art data poisoning attacks on KGE models and improve the MRR degradation due to the attacks by up to 62 % over the baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.649.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--649 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.649 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.649/>Locke’s Holiday : Belief Bias in Machine Reading</a></strong><br><a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--649><div class="card-body p-3 small">I highlight a simple <a href=https://en.wikipedia.org/wiki/Failure_mode>failure mode</a> of state-of-the-art machine reading systems : when contexts do not align with commonly shared beliefs. For example, machine reading systems fail to answer What did Elizabeth want? correctly in the context of &#8216;My kingdom for a cough drop, cried Queen Elizabeth.&#8217; Biased by co-occurrence statistics in the training data of pretrained language models, systems predict my kingdom, rather than a cough drop. I argue such biases are analogous to human belief biases and present a carefully designed challenge dataset for English machine reading, called Auto-Locke, to quantify such effects. Evaluations of <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading systems</a> on Auto-Locke show the pervasiveness of <a href=https://en.wikipedia.org/wiki/Belief_bias>belief bias</a> in <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a>.<i>What did Elizabeth want?</i> correctly in the context of &#8216;My kingdom for a cough drop, cried Queen Elizabeth.&#8217; Biased by co-occurrence statistics in the training data of pretrained language models, systems predict <i>my kingdom</i>, rather than <i>a cough drop</i>. I argue such biases are analogous to human belief biases and present a carefully designed challenge dataset for English machine reading, called Auto-Locke, to quantify such effects. Evaluations of machine reading systems on Auto-Locke show the pervasiveness of belief bias in machine reading.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.650.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--650 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.650 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.650/>Sequence Length is a Domain : Length-based Overfitting in Transformer Models</a></strong><br><a href=/people/d/dusan-varis/>Dusan Varis</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--650><div class="card-body p-3 small">Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> during training. In practice, this is usually countered either by applying <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization methods</a> (e.g. dropout, L2-regularization) or by providing huge amounts of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017). We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing tasks and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.658.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--658 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.658 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.658/>Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs</a></strong><br><a href=/people/z/zhen-han/>Zhen Han</a>
|
<a href=/people/z/zifeng-ding/>Zifeng Ding</a>
|
<a href=/people/y/yunpu-ma/>Yunpu Ma</a>
|
<a href=/people/y/yujia-gu/>Yujia Gu</a>
|
<a href=/people/v/volker-tresp/>Volker Tresp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--658><div class="card-body p-3 small">There has been an increasing interest in inferring future links on temporal knowledge graphs (KG). While links on temporal KGs vary continuously over time, the existing approaches model the temporal KGs in discrete state spaces. To this end, we propose a novel continuum model by extending the idea of neural ordinary differential equations (ODEs) to multi-relational graph convolutional networks. The proposed model preserves the continuous nature of dynamic multi-relational graph data and encodes both temporal and structural information into continuous-time dynamic embeddings. In addition, a novel graph transition layer is applied to capture the transitions on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>dynamic graph</a>, i.e., <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edge formation</a> and dissolution. We perform extensive experiments on five benchmark datasets for temporal KG reasoning, showing our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s superior performance on the future link forecasting task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.661.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--661 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.661 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.661" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.661/>A Strong Baseline for Query Efficient Attacks in a Black Box Setting</a></strong><br><a href=/people/r/rishabh-maheshwary/>Rishabh Maheshwary</a>
|
<a href=/people/s/saket-maheshwary/>Saket Maheshwary</a>
|
<a href=/people/v/vikram-pudi/>Vikram Pudi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--661><div class="card-body p-3 small">Existing black box search methods have achieved high success rate in generating adversarial attacks against NLP models. However, such search methods are inefficient as they do not consider the amount of queries required to generate adversarial attacks. Also, prior attacks do not maintain a consistent search space while comparing different <a href=https://en.wikipedia.org/wiki/Search_algorithm>search methods</a>. In this paper, we propose a query efficient attack strategy to generate plausible adversarial examples on text classification and entailment tasks. Our attack jointly leverages <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> and locality sensitive hashing (LSH) to reduce the query count. We demonstrate the efficacy of our approach by comparing our attack with four <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> across three different <a href=https://en.wikipedia.org/wiki/Feasible_region>search spaces</a>. Further, we benchmark our results across the same <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> used in prior attacks. In comparison to <a href=https://en.wikipedia.org/wiki/Cyberattack>attacks</a> proposed, on an average, we are able to reduce the query count by 75 % across all <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and target models. We also demonstrate that our <a href=https://en.wikipedia.org/wiki/Cyberattack>attack</a> achieves a higher success rate when compared to prior attacks in a limited query setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.667.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--667 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.667 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.667/>Language Modeling, Lexical Translation, Reordering : The Training Process of NMT through the Lens of Classical SMT<span class=acl-fixed-case>NMT</span> through the Lens of Classical <span class=acl-fixed-case>SMT</span></a></strong><br><a href=/people/e/elena-voita/>Elena Voita</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--667><div class="card-body p-3 small">Differently from the traditional <a href=https://en.wikipedia.org/wiki/Machine_translation>statistical MT</a> that decomposes the translation task into distinct separately learned components, <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> uses a single <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to model the entire translation process. Despite <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in traditional <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>SMT</a>. In this work, we look at the competences related to three core SMT components and find that during training, NMT first focuses on learning target-side language modeling, then improves translation quality approaching word-by-word translation, and finally learns more complicated reordering patterns. We show that this behavior holds for several models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla non-autoregressive neural machine translation by guiding teacher model selection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.670.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--670 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.670 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.670.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.670" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.670/>Wino-X : Multilingual Winograd Schemas for Commonsense Reasoning and Coreference Resolution<span class=acl-fixed-case>X</span>: Multilingual <span class=acl-fixed-case>W</span>inograd Schemas for Commonsense Reasoning and Coreference Resolution</a></strong><br><a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--670><div class="card-body p-3 small">Winograd schemas are a well-established tool for evaluating coreference resolution (CoR) and commonsense reasoning (CSR) capabilities of computational models. So far, schemas remained largely confined to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, limiting their utility in multilingual settings. This work presents Wino-X, a parallel dataset of German, French, and Russian schemas, aligned with their English counterparts. We use this resource to investigate whether neural machine translation (NMT) models can perform CoR that requires commonsense knowledge and whether multilingual language models (MLLMs) are capable of CSR across multiple languages. Our findings show Wino-X to be exceptionally challenging for NMT systems that are prone to undesirable biases and unable to detect disambiguating information. We quantify biases using established <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a> and define ways to address both of these issues. We furthermore present evidence of active cross-lingual knowledge transfer in MLLMs, whereby fine-tuning models on English schemas yields CSR improvements in other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.671.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--671 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.671 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.671" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.671/>One Source, Two Targets : Challenges and Rewards of Dual Decoding<span class=acl-fixed-case>C</span>hallenges and Rewards of Dual Decoding</a></strong><br><a href=/people/j/jitao-xu/>Jitao Xu</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--671><div class="card-body p-3 small">Machine translation is generally understood as generating one target text from an input source document. In this paper, we consider a stronger requirement : to jointly generate two texts so that each output side effectively depends on the other. As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text. We present an analysis of possible implementations of dual decoding, and experiment with four <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>. Viewing the problem from multiple angles allows us to better highlight the challenges of dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.678.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--678 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.678 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.678/>Cross-Policy Compliance Detection via Question Answering</a></strong><br><a href=/people/m/marzieh-saeidi/>Marzieh Saeidi</a>
|
<a href=/people/m/majid-yazdani/>Majid Yazdani</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--678><div class="card-body p-3 small">Policy compliance detection is the task of ensuring that a scenario conforms to a <a href=https://en.wikipedia.org/wiki/Policy>policy</a> (e.g. a claim is valid according to government rules or a post in an online platform conforms to community guidelines). This task has been previously instantiated as a form of <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a>, which results in poor <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> due to the <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of the <a href=https://en.wikipedia.org/wiki/Policy>policies</a>. In this paper we propose to address <a href=https://en.wikipedia.org/wiki/Policy>policy compliance detection</a> via decomposing it into <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, where questions check whether the conditions stated in the <a href=https://en.wikipedia.org/wiki/Policy>policy</a> apply to the scenario, and an <a href=https://en.wikipedia.org/wiki/Expression_tree>expression tree</a> combines the answers to obtain the label. Despite the initial upfront annotation cost, we demonstrate that this approach results in better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, especially in the cross-policy setup where the <a href=https://en.wikipedia.org/wiki/Policy>policies</a> during testing are unseen in training. In addition, it allows us to use existing <a href=https://en.wikipedia.org/wiki/Question_answering>question answering models</a> pre-trained on existing <a href=https://en.wikipedia.org/wiki/Data_set>large datasets</a>. Finally, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> explicitly identifies the information missing from a scenario in case policy compliance can not be determined. We conduct our experiments using a recent dataset consisting of government policies, which we augment with expert annotations and find that the cost of annotating question answering decomposition is largely offset by improved inter-annotator agreement and speed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.680.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--680 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.680 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.680/>Unsupervised Multi-View Post-OCR Error Correction With Language Models<span class=acl-fixed-case>OCR</span> Error Correction With Language Models</a></strong><br><a href=/people/h/harsh-gupta/>Harsh Gupta</a>
|
<a href=/people/l/luciano-del-corro/>Luciano Del Corro</a>
|
<a href=/people/s/samuel-broscheit/>Samuel Broscheit</a>
|
<a href=/people/j/johannes-hoffart/>Johannes Hoffart</a>
|
<a href=/people/e/eliot-brenner/>Eliot Brenner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--680><div class="card-body p-3 small">We investigate post-OCR correction in a setting where we have access to different <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR views</a> of the same document. The goal of this study is to understand if a pretrained language model (LM) can be used in an unsupervised way to reconcile the different OCR views such that their combination contains fewer errors than each individual view. This approach is motivated by scenarios in which unconstrained text generation for <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error correction</a> is too risky. We evaluated different pretrained LMs on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and found significant gains in realistic scenarios with up to 15 % <a href=https://en.wikipedia.org/wiki/Receiver_operating_characteristic>WER</a> improvement over the best OCR view. We also show the importance of <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for post-OCR correction on out-of-domain documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.682.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--682 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.682 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.682/>BERT-Beta : A Proactive Probabilistic Approach to Text Moderation<span class=acl-fixed-case>BERT</span>-Beta: A Proactive Probabilistic Approach to Text Moderation</a></strong><br><a href=/people/f/fei-tan/>Fei Tan</a>
|
<a href=/people/y/yifan-hu/>Yifan Hu</a>
|
<a href=/people/k/kevin-yen/>Kevin Yen</a>
|
<a href=/people/c/changwei-hu/>Changwei Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--682><div class="card-body p-3 small">Text moderation for <a href=https://en.wikipedia.org/wiki/User-generated_content>user generated content</a>, which helps to promote healthy interaction among users, has been widely studied and many machine learning models have been proposed. In this work, we explore an alternative perspective by augmenting reactive reviews with proactive forecasting. Specifically, we propose a new concept text toxicity propensity to characterize the extent to which a text tends to attract toxic comments. Beta regression is then introduced to do the probabilistic modeling, which is demonstrated to function well in comprehensive experiments. We also propose an explanation method to communicate the model decision clearly. Both propensity scoring and <a href=https://en.wikipedia.org/wiki/Interpretation_(logic)>interpretation</a> benefit text moderation in a novel manner. Finally, the proposed scaling mechanism for the <a href=https://en.wikipedia.org/wiki/Linear_model>linear model</a> offers useful insights beyond this work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.685.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--685 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.685 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.685" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.685/>CodeT5 : Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation<span class=acl-fixed-case>C</span>ode<span class=acl-fixed-case>T</span>5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation</a></strong><br><a href=/people/y/yue-wang/>Yue Wang</a>
|
<a href=/people/w/weishi-wang/>Weishi Wang</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/s/steven-c-h-hoi/>Steven C.H. Hoi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--685><div class="card-body p-3 small">Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Besides, we propose a novel identifier-aware pre-training task that enables the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.686.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--686 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.686 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.686.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.686" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.686/>Detect and Classify Joint Span Detection and Classification for Health Outcomes</a></strong><br><a href=/people/m/micheal-abaho/>Micheal Abaho</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a>
|
<a href=/people/p/paula-williamson/>Paula Williamson</a>
|
<a href=/people/s/susanna-dodd/>Susanna Dodd</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--686><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Outcome_(probability)>health outcome</a> is a measurement or an observation used to capture and assess the effect of a treatment. Automatic detection of health outcomes from text would undoubtedly speed up access to evidence necessary in healthcare decision making. Prior work on outcome detection has modelled this task as either (a) a sequence labelling task, where the goal is to detect which text spans describe health outcomes, or (b) a classification task, where the goal is to classify a text into a predefined set of categories depending on an outcome that is mentioned somewhere in that text. However, this decoupling of span detection and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> is problematic from a modelling perspective and ignores global structural correspondences between sentence-level and word-level information present in a given text. To address this, we propose a method that uses both word-level and sentence-level information to simultaneously perform outcome span detection and outcome type classification. In addition to injecting contextual information to hidden vectors, we use label attention to appropriately weight both word and sentence level information. Experimental results on several benchmark datasets for health outcome detection show that our proposed method consistently outperforms decoupled methods, reporting competitive results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.687.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--687 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.687 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.687/>Multi-Class Grammatical Error Detection for Correction : A Tale of Two Systems<span class=acl-fixed-case>M</span>ulti-Class Grammatical Error Detection for Correction: <span class=acl-fixed-case>A</span> Tale of Two Systems</a></strong><br><a href=/people/z/zheng-yuan/>Zheng Yuan</a>
|
<a href=/people/s/shiva-taslimipoor/>Shiva Taslimipoor</a>
|
<a href=/people/c/christopher-davis/>Christopher Davis</a>
|
<a href=/people/c/christopher-bryant/>Christopher Bryant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--687><div class="card-body p-3 small">In this paper, we show how a multi-class grammatical error detection (GED) system can be used to improve grammatical error correction (GEC) for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Specifically, we first develop a new state-of-the-art binary detection system based on pre-trained ELECTRA, and then extend it to multi-class detection using different error type tagsets derived from the ERRANT framework. Output from this detection system is used as auxiliary input to fine-tune a novel encoder-decoder GEC model, and we subsequently re-rank the N-best GEC output to find the hypothesis that most agrees with the GED output. Results show that fine-tuning the GEC system using 4-class GED produces the best model, but re-ranking using 55-class GED leads to the best performance overall. This suggests that different multi-class GED systems benefit GEC in different ways. Ultimately, our system outperforms all other previous work that combines <a href=https://en.wikipedia.org/wiki/General_Educational_Development>GED</a> and GEC, and achieves a new single-model NMT-based state of the art on the BEA-test benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.688.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--688 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.688 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.688" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.688/>Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models</a></strong><br><a href=/people/t/tassilo-klein/>Tassilo Klein</a>
|
<a href=/people/m/moin-nabi/>Moin Nabi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--688><div class="card-body p-3 small">Can we get existing <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and refine them for zero-shot commonsense reasoning? This paper presents an initial study exploring the feasibility of zero-shot commonsense reasoning for the <a href=https://en.wikipedia.org/wiki/Winograd_Schema_Challenge>Winograd Schema Challenge</a> by formulating the task as self-supervised refinement of a pre-trained language model. In contrast to previous studies that rely on fine-tuning annotated datasets, we seek to boost conceptualization via loss landscape refinement. To this end, we propose a novel self-supervised learning approach that refines the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> utilizing a set of linguistic perturbations of similar concept relationships. Empirical analysis of our conceptually simple framework demonstrates the viability of zero-shot commonsense reasoning on multiple benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.689.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--689 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.689 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.689" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.689/>To Share or not to Share : Predicting Sets of Sources for Model Transfer Learning<span class=acl-fixed-case>P</span>redicting Sets of Sources for Model Transfer Learning</a></strong><br><a href=/people/l/lukas-lange/>Lukas Lange</a>
|
<a href=/people/j/jannik-strotgen/>Jannik Strötgen</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--689><div class="card-body p-3 small">In low-resource settings, model transfer can help to overcome a lack of labeled data for many tasks and domains. However, predicting useful transfer sources is a challenging problem, as even the most similar sources might lead to unexpected negative transfer results. Thus, ranking methods based on task and text similarity as suggested in prior work may not be sufficient to identify promising sources. To tackle this problem, we propose a new approach to automatically determine which and how many sources should be exploited. For this, we study the effects of model transfer on <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> across various domains and tasks and show that our methods based on model similarity and support vector machines are able to predict promising sources, resulting in performance increases of up to 24 F1 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.690.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--690 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.690 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.690/>Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting : Phenotype Annotation Use Case</a></strong><br><a href=/people/j/jingqing-zhang/>Jingqing Zhang</a>
|
<a href=/people/l/luis-bolanos-trujillo/>Luis Bolanos Trujillo</a>
|
<a href=/people/t/tong-li/>Tong Li</a>
|
<a href=/people/a/ashwani-tanwar/>Ashwani Tanwar</a>
|
<a href=/people/g/guilherme-freire/>Guilherme Freire</a>
|
<a href=/people/x/xian-yang/>Xian Yang</a>
|
<a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/v/vibhor-gupta/>Vibhor Gupta</a>
|
<a href=/people/y/yike-guo/>Yike Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--690><div class="card-body p-3 small">Contextualised word embeddings is a powerful tool to detect contextual synonyms. However, most of the current state-of-the-art (SOTA) deep learning concept extraction methods remain supervised and underexploit the potential of the context. In this paper, we propose a self-supervised pre-training approach which is able to detect contextual synonyms of concepts being training on the data created by shallow matching. We apply our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> in the sparse multi-class setting (over 15,000 concepts) to extract phenotype information from <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic health records</a>. We further investigate data augmentation techniques to address the problem of the class sparsity. Our approach achieves a new SOTA for the unsupervised phenotype concept annotation on clinical text on F1 and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a> outperforming the previous SOTA with a gain of up to 4.5 and 4.0 absolute points, respectively. After <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> with as little as 20 % of the labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic evaluation on three ICU benchmarks also shows the benefit of using the <a href=https://en.wikipedia.org/wiki/Phenotype>phenotypes</a> annotated by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.691.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--691 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.691 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.691/>ClauseRec : A Clause Recommendation Framework for AI-aided Contract Authoring<span class=acl-fixed-case>C</span>lause<span class=acl-fixed-case>R</span>ec: A Clause Recommendation Framework for <span class=acl-fixed-case>AI</span>-aided Contract Authoring</a></strong><br><a href=/people/v/vinay-aggarwal/>Vinay Aggarwal</a>
|
<a href=/people/a/aparna-garimella/>Aparna Garimella</a>
|
<a href=/people/b/balaji-vasan-srinivasan/>Balaji Vasan Srinivasan</a>
|
<a href=/people/a/anandhavelu-n/>Anandhavelu N</a>
|
<a href=/people/r/rajiv-jain/>Rajiv Jain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--691><div class="card-body p-3 small">Contracts are a common type of <a href=https://en.wikipedia.org/wiki/Legal_document>legal document</a> that frequent in several day-to-day business workflows. However, there has been very limited NLP research in processing such <a href=https://en.wikipedia.org/wiki/Document>documents</a>, and even lesser in generating them. These <a href=https://en.wikipedia.org/wiki/Contract>contracts</a> are made up of clauses, and the unique nature of these <a href=https://en.wikipedia.org/wiki/Clause>clauses</a> calls for specific methods to understand and generate such <a href=https://en.wikipedia.org/wiki/Document>documents</a>. In this paper, we introduce the task of clause recommendation, as a first step to aid and accelerate the authoring of contract documents. We propose a two-staged pipeline to first predict if a specific clause type is relevant to be added in a <a href=https://en.wikipedia.org/wiki/Contract>contract</a>, and then recommend the top clauses for the given type based on the contract context. We pre-train BERT on an existing library of clauses with two additional tasks and use it for our <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> and <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation</a>. We experiment with classification methods and similarity-based heuristics for clause relevance prediction, and generation-based methods for clause recommendation, and evaluate the results from various methods on several clause types. We provide analyses on the results, and further outline the limitations and future directions of this line of research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.692.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--692 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.692 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.692" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.692/>Finnish Dialect Identification : The Effect of Audio and Text<span class=acl-fixed-case>F</span>innish Dialect Identification: The Effect of Audio and Text</a></strong><br><a href=/people/m/mika-hamalainen/>Mika Hämäläinen</a>
|
<a href=/people/k/khalid-alnajjar/>Khalid Alnajjar</a>
|
<a href=/people/n/niko-partanen/>Niko Partanen</a>
|
<a href=/people/j/jack-rueter/>Jack Rueter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--692><div class="card-body p-3 small">Finnish is a language with multiple dialects that not only differ from each other in terms of <a href=https://en.wikipedia.org/wiki/Accent_(sociolinguistics)>accent</a> (pronunciation) but also in terms of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological forms</a> and <a href=https://en.wikipedia.org/wiki/Lexicon>lexical choice</a>. We present the first approach to automatically detect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> is received by combining both of the modalities, as text only reaches to an overall <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 57 %, where as text and audio reach to 85 %. Our code, models and data have been released openly on Github and <a href=https://en.wikipedia.org/wiki/Zenodo>Zenodo</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.694.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--694 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.694 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.694/>Expanding End-to-End Question Answering on Differentiable Knowledge Graphs with Intersection</a></strong><br><a href=/people/p/priyanka-sen/>Priyanka Sen</a>
|
<a href=/people/a/armin-oliya/>Armin Oliya</a>
|
<a href=/people/a/amir-saffari/>Amir Saffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--694><div class="card-body p-3 small">End-to-end question answering using a differentiable knowledge graph is a promising technique that requires only <a href=https://en.wikipedia.org/wiki/Supervised_learning>weak supervision</a>, produces interpretable results, and is fully differentiable. Previous implementations of this technique (Cohen et al, 2020) have focused on single-entity questions using a relation following operation. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> that explicitly handles multiple-entity questions by implementing a new intersection operation, which identifies the shared elements between two sets of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a>. We find that introducing intersection improves performance over a baseline model on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, WebQuestionsSP (69.6 % to 73.3 % Hits@1) and ComplexWebQuestions (39.8 % to 48.7 % Hits@1), and in particular, improves performance on questions with multiple entities by over 14 % on WebQuestionsSP and by 19 % on ComplexWebQuestions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.696.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--696 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.696 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.696/>Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation</a></strong><br><a href=/people/m/max-bartolo/>Max Bartolo</a>
|
<a href=/people/t/tristan-thrush/>Tristan Thrush</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--696><div class="card-body p-3 small">Despite recent progress, state-of-the-art question answering models remain vulnerable to a variety of <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial attacks</a>. While dynamic adversarial data collection, in which a human annotator tries to write examples that fool a model-in-the-loop, can improve model robustness, this process is expensive which limits the scale of the collected data. In this work, we are the first to use synthetic adversarial data generation to make <a href=https://en.wikipedia.org/wiki/Question_answering>question answering models</a> more robust to <a href=https://en.wikipedia.org/wiki/Adversarial_system>human adversaries</a>. We develop a data generation pipeline that selects source passages, identifies candidate answers, generates questions, then finally filters or re-labels them to improve <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a>. Using this approach, we amplify a smaller human-written adversarial dataset to a much larger set of synthetic question-answer pairs. By incorporating our synthetic data, we improve the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve model generalisation on nine of the twelve MRQA datasets. We further conduct a novel human-in-the-loop evaluation and show that our models are considerably more robust to new human-written adversarial examples : crowdworkers can fool our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> only 8.8 % of the time on average, compared to 17.6 % for a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained without synthetic data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.697.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--697 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.697 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.697/>BeliefBank : Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief<span class=acl-fixed-case>B</span>elief<span class=acl-fixed-case>B</span>ank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief</a></strong><br><a href=/people/n/nora-kassner/>Nora Kassner</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--697><div class="card-body p-3 small">Although pretrained language models (PTLMs) contain significant amounts of <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>, they can still produce inconsistent answers to questions when probed, even after specialized training. As a result, it can be hard to identify what the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> actually believes about the world, making it susceptible to <a href=https://en.wikipedia.org/wiki/Consistency>inconsistent behavior</a> and simple errors. Our goal is to reduce these problems. Our approach is to embed a PTLM in a broader system that also includes an evolving, symbolic memory of beliefs a BeliefBank that records but then may modify the raw PTLM answers. We describe two <a href=https://en.wikipedia.org/wiki/Mechanism_(sociology)>mechanisms</a> to improve belief consistency in the overall system. First, a reasoning component a weighted MaxSAT solver revises beliefs that significantly clash with others. Second, a feedback component issues future queries to the PTLM using known beliefs as context. We show that, in a controlled experimental setting, these two mechanisms result in more consistent beliefs in the overall system, improving both the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and consistency of its answers over time. This is significant as it is a first step towards PTLM-based architectures with a systematic notion of belief, enabling them to construct a more coherent picture of the world, and improve over time without model retraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.698.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--698 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.698 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.698" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.698/>MLEC-QA : A Chinese Multi-Choice Biomedical Question Answering Dataset<span class=acl-fixed-case>MLEC-QA</span>: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>M</span>ulti-<span class=acl-fixed-case>C</span>hoice <span class=acl-fixed-case>B</span>iomedical <span class=acl-fixed-case>Q</span>uestion <span class=acl-fixed-case>A</span>nswering <span class=acl-fixed-case>D</span>ataset</a></strong><br><a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/s/shangping-zhong/>Shangping Zhong</a>
|
<a href=/people/k/kaizhi-chen/>Kaizhi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--698><div class="card-body p-3 small">Question Answering (QA) has been successfully applied in scenarios of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer interaction</a> such as <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> and <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a>. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields : <a href=https://en.wikipedia.org/wiki/Clinic>Clinic</a>, <a href=https://en.wikipedia.org/wiki/Oral_medicine>Stomatology</a>, <a href=https://en.wikipedia.org/wiki/Public_health>Public Health</a>, <a href=https://en.wikipedia.org/wiki/Traditional_Chinese_medicine>Traditional Chinese Medicine</a>, and <a href=https://en.wikipedia.org/wiki/Traditional_Chinese_medicine>Traditional Chinese Medicine</a> Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40 % to 55 % on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.699.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--699 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.699 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.699" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.699/>IndoNLG : Benchmark and Resources for Evaluating Indonesian Natural Language Generation<span class=acl-fixed-case>I</span>ndo<span class=acl-fixed-case>NLG</span>: Benchmark and Resources for Evaluating <span class=acl-fixed-case>I</span>ndonesian Natural Language Generation</a></strong><br><a href=/people/s/samuel-cahyawijaya/>Samuel Cahyawijaya</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/b/bryan-wilie/>Bryan Wilie</a>
|
<a href=/people/k/karissa-vincentio/>Karissa Vincentio</a>
|
<a href=/people/x/xiaohong-li/>Xiaohong Li</a>
|
<a href=/people/a/adhiguna-kuncoro/>Adhiguna Kuncoro</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/z/zhi-yuan-lim/>Zhi Yuan Lim</a>
|
<a href=/people/s/syafri-bahar/>Syafri Bahar</a>
|
<a href=/people/m/masayu-khodra/>Masayu Khodra</a>
|
<a href=/people/a/ayu-purwarianti/>Ayu Purwarianti</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--699><div class="card-body p-3 small">Natural language generation (NLG) benchmarks provide an important avenue to measure progress and develop better NLG systems. Unfortunately, the lack of publicly available NLG benchmarks for low-resource languages poses a challenging barrier for building NLG systems that work well for languages with limited amounts of data. Here we introduce IndoNLG, the first benchmark to measure natural language generation (NLG) progress in three low-resourceyet widely spokenlanguages of Indonesia : <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian</a>, <a href=https://en.wikipedia.org/wiki/Javanese_language>Javanese</a>, and <a href=https://en.wikipedia.org/wiki/Sundanese_language>Sundanese</a>. Altogether, these <a href=https://en.wikipedia.org/wiki/Language>languages</a> are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks : <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, <a href=https://en.wikipedia.org/wiki/Chit-chat>chit-chat</a>, and three different pairs of machine translation (MT) tasks. We collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models : IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasksdespite using only one-fifth the parameters of a larger multilingual model, mBART-large (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, localized languages to achieve more efficient <a href=https://en.wikipedia.org/wiki/Learning>learning</a> and faster <a href=https://en.wikipedia.org/wiki/Inference>inference</a> at very low-resource languages like <a href=https://en.wikipedia.org/wiki/Javanese_language>Javanese</a> and <a href=https://en.wikipedia.org/wiki/Sundanese_language>Sundanese</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--701 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.701" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.701/>Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors<span class=acl-fixed-case>BERT</span>-Based Evaluation Metrics by Disentangling along Linguistic Factors</a></strong><br><a href=/people/m/marvin-kaster/>Marvin Kaster</a>
|
<a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--701><div class="card-body p-3 small">Evaluation metrics are a key ingredient for progress of <a href=https://en.wikipedia.org/wiki/Text_generator>text generation systems</a>. In recent years, several BERT-based evaluation metrics have been proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much better with human assessment of text generation quality than <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> or ROUGE, invented two decades ago. However, little is known what these metrics, which are based on black-box language model representations, actually capture (it is typically assumed they model semantic similarity). In this work, we use a simple regression based global explainability technique to disentangle metric scores along linguistic factors, including <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>, and lexical overlap. We show that the different <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> capture all aspects to some degree, but that they are all substantially sensitive to <a href=https://en.wikipedia.org/wiki/Lexical_overlap>lexical overlap</a>, just like <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and ROUGE. This exposes limitations of these novelly proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>, which we also highlight in an adversarial test scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--702 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.702" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.702/>Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization<span class=acl-fixed-case>SQL</span> Generalization</a></strong><br><a href=/people/y/yujian-gan/>Yujian Gan</a>
|
<a href=/people/x/xinyun-chen/>Xinyun Chen</a>
|
<a href=/people/m/matthew-purver/>Matthew Purver</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--702><div class="card-body p-3 small">Recently, there has been significant progress in studying <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> for translating text descriptions into SQL queries under the zero-shot cross-domain setting. Despite achieving good performance on some public benchmarks, we observe that existing text-to-SQL models do not generalize when facing <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> that does not frequently appear in the training data, which may render the worse prediction performance for unseen domains. In this work, we investigate the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of text-to-SQL models when the questions require rarely observed domain knowledge. In particular, we define five types of <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> and introduce Spider-DK (DK is the abbreviation of domain knowledge), a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-DK are selected from Spider, and we modify some samples by adding <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> that reflects real-world question paraphrases. We demonstrate that the prediction accuracy dramatically drops on samples that require such <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>, even if the <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> appears in the training set, and the model provides the correct predictions for related training samples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--704 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.704/>NeuTral Rewriter : A Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives<span class=acl-fixed-case>N</span>eu<span class=acl-fixed-case>T</span>ral <span class=acl-fixed-case>R</span>ewriter: <span class=acl-fixed-case>A</span> Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives</a></strong><br><a href=/people/e/eva-vanmassenhove/>Eva Vanmassenhove</a>
|
<a href=/people/c/chris-emmery/>Chris Emmery</a>
|
<a href=/people/d/dimitar-shterionov/>Dimitar Shterionov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--704><div class="card-body p-3 small">Recent years have seen an increasing need for <a href=https://en.wikipedia.org/wiki/Gender-neutral_language>gender-neutral and inclusive language</a>. Within the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, there are various mono- and bilingual use cases where <a href=https://en.wikipedia.org/wiki/Gender_inclusive_language>gender inclusive language</a> is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this work, we present a rule-based and a neural approach to gender-neutral rewriting for <a href=https://en.wikipedia.org/wiki/English_language>English</a> along with manually curated synthetic data (WinoBias+) and natural data (OpenSubtitles and Reddit) benchmarks. A detailed manual and automatic evaluation highlights how our NeuTral Rewriter, trained on <a href=https://en.wikipedia.org/wiki/Data>data</a> generated by the rule-based approach, obtains <a href=https://en.wikipedia.org/wiki/Word_error_rate>word error rates (WER)</a> below 0.18 % on synthetic, in-domain and out-domain test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--705 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.705" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.705/>Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset</a></strong><br><a href=/people/t/tianqing-fang/>Tianqing Fang</a>
|
<a href=/people/w/weiqi-wang/>Weiqi Wang</a>
|
<a href=/people/s/sehyun-choi/>Sehyun Choi</a>
|
<a href=/people/s/shibo-hao/>Shibo Hao</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a>
|
<a href=/people/b/bin-he/>Bin He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--705><div class="card-body p-3 small">Reasoning over commonsense knowledge bases (CSKB) whose elements are in the form of free-text is an important yet hard task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. While CSKB completion only fills the missing links within the domain of the CSKB, CSKB population is alternatively proposed with the goal of reasoning unseen assertions from external resources. In this task, CSKBs are grounded to a large-scale eventuality (activity, state, and event) graph to discriminate whether novel triples from the eventuality graph are plausible or not. However, existing evaluations on the population task are either not accurate (automatic evaluation with randomly sampled negative examples) or of small scale (human annotation). In this paper, we benchmark the CSKB population task with a new large-scale dataset by first aligning four popular CSKBs, and then presenting a high-quality human-annotated evaluation set to probe neural models&#8217; commonsense reasoning ability. We also propose a novel inductive commonsense reasoning model that reasons over <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>. Experimental results show that generalizing <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> on unseen assertions is inherently a hard task. Models achieving high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> during training perform poorly on the evaluation set, with a large gap between human performance. We will make the data publicly available for future contributions. Codes and data are available at https://github.com/HKUST-KnowComp/CSKB-Population.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--706 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.706.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.706/>Enhancing the Context Representation in Similarity-based Word Sense Disambiguation</a></strong><br><a href=/people/m/ming-wang/>Ming Wang</a>
|
<a href=/people/j/jianzhang-zhang/>Jianzhang Zhang</a>
|
<a href=/people/y/yinglin-wang/>Yinglin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--706><div class="card-body p-3 small">In previous similarity-based WSD systems, studies have allocated much effort on learning comprehensive sense embeddings using contextual representations and knowledge sources. However, the context embedding of an ambiguous word is learned using only the sentence where the word appears, neglecting its <a href=https://en.wikipedia.org/wiki/Context_(language_use)>global context</a>. In this paper, we investigate the contribution of both word-level and sense-level global context of an ambiguous word for <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>disambiguation</a>. Experiments have shown that the Context-Oriented Embedding (COE) can enhance a similarity-based system&#8217;s performance on WSD by relatively large margins, achieving state-of-the-art on all-words WSD benchmarks in knowledge-based category.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--710 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.710" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.710/>Cross-Domain Label-Adaptive Stance Detection</a></strong><br><a href=/people/m/momchil-hardalov/>Momchil Hardalov</a>
|
<a href=/people/a/arnav-arora/>Arnav Arora</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--710><div class="card-body p-3 small">Stance detection concerns the classification of a writer&#8217;s viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a>, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth analysis of 16 stance detection datasets, and we explore the possibility for cross-domain learning from them. Moreover, we propose an end-to-end unsupervised framework for out-of-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the cross-domain results, and we highlight the important factors influencing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--711 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.711" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.711/>Text AutoAugment : Learning Compositional Augmentation Policy for Text Classification<span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>A</span>ugment: Learning Compositional Augmentation Policy for Text Classification</a></strong><br><a href=/people/s/shuhuai-ren/>Shuhuai Ren</a>
|
<a href=/people/j/jinchao-zhang/>Jinchao Zhang</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--711><div class="card-body p-3 small">Data augmentation aims to enrich training samples for alleviating the <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting issue</a> in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previous <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a>, which decreases the diversity of the augmented data and thus restricts the performance gain. To overcome the above limitations, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> named Text AutoAugment (TAA) to establish a compositional and learnable paradigm for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. We regard a combination of various operations as an augmentation policy and utilize an efficient Bayesian Optimization algorithm to automatically search for the best <a href=https://en.wikipedia.org/wiki/Policy>policy</a>, which substantially improves the generalization capability of models. Experiments on six benchmark datasets show that TAA boosts <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> in low-resource and class-imbalanced regimes by an average of 8.8 % and 9.7 %, respectively, outperforming strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.712.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--712 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.712 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.712" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.712/>Distilling Relation Embeddings from Pretrained Language Models</a></strong><br><a href=/people/a/asahi-ushio/>Asahi Ushio</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--712><div class="card-body p-3 small">Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>relation embeddings</a>, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine-grained way than is possible with <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically generated) prompt, and we fine-tune the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, even without any task-specific fine-tuning. Source code to reproduce our experimental results and the model checkpoints are available in the following repository : https://github.com/asahi417/relbert</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--713 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.713" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.713/>Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning</a></strong><br><a href=/people/p/prasetya-utama/>Prasetya Utama</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/v/victor-sanh/>Victor Sanh</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--713><div class="card-body p-3 small">Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>inference heuristics</a> based on <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical overlap</a>, e.g., models incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is significantly less present in the zero-shot evaluation of the prompt-based model, indicating how <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a> can be destructive to useful knowledge learned during the pretraining. We then show that adding a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> that preserves pretraining weights is effective in mitigating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.717.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--717 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.717 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.717" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.717/>NB-MLM : Efficient Domain Adaptation of Masked Language Models for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a><span class=acl-fixed-case>NB</span>-<span class=acl-fixed-case>MLM</span>: Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis</a></strong><br><a href=/people/n/nikolay-arefyev/>Nikolay Arefyev</a>
|
<a href=/people/d/dmitrii-kharchev/>Dmitrii Kharchev</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--717><div class="card-body p-3 small">While Masked Language Models (MLM) are pre-trained on massive datasets, the additional training with the MLM objective on domain or task-specific data before fine-tuning for the final task is known to improve the final performance. This is usually referred to as the domain or task adaptation step. However, unlike the initial pre-training, this step is performed for each domain or task individually and is still rather slow, requiring several GPU days compared to several GPU hours required for the final task fine-tuning. We argue that the standard MLM objective leads to inefficiency when it is used for the adaptation step because it mostly learns to predict the most frequent words, which are not necessarily related to a final task. We propose a technique for more efficient <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> that focuses on predicting words with large weights of the <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Naive Bayes classifier</a> trained for the task at hand, which are likely more relevant than the most frequent words. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> provides faster adaptation and better final performance for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> compared to the standard approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.720.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--720 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.720 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.720/>Unimodal and Crossmodal Refinement Network for Multimodal Sequence Fusion</a></strong><br><a href=/people/x/xiaobao-guo/>Xiaobao Guo</a>
|
<a href=/people/a/adams-kong/>Adams Kong</a>
|
<a href=/people/h/huan-zhou/>Huan Zhou</a>
|
<a href=/people/x/xianfeng-wang/>Xianfeng Wang</a>
|
<a href=/people/m/min-wang/>Min Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--720><div class="card-body p-3 small">Effective <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal representation</a> and complementary crossmodal representation fusion are both important in multimodal representation learning. Prior works often modulate one modal feature to another straightforwardly and thus, underutilizing both unimodal and crossmodal representation refinements, which incurs a bottleneck of performance improvement. In this paper, Unimodal and Crossmodal Refinement Network (UCRN) is proposed to enhance both unimodal and crossmodal representations. Specifically, to improve <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal representations</a>, a unimodal refinement module is designed to refine modality-specific learning via iteratively updating the distribution with transformer-based attention layers. Self-quality improvement layers are followed to generate the desired weighted representations progressively. Subsequently, those unimodal representations are projected into a common latent space, regularized by a multimodal Jensen-Shannon divergence loss for better crossmodal refinement. Lastly, a crossmodal refinement module is employed to integrate all information. By hierarchical explorations on unimodal, bimodal, and trimodal interactions, UCRN is highly robust against missing modality and noisy data. Experimental results on MOSI and MOSEI datasets illustrated that the proposed UCRN outperforms recent state-of-the-art techniques and its robustness is highly preferred in real multimodal sequence fusion scenarios. Codes will be shared publicly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--728 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.728/>Towards Label-Agnostic Emotion Embeddings</a></strong><br><a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/l/luise-modersohn/>Luise Modersohn</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--728><div class="card-body p-3 small">Research in emotion analysis is scattered across different label formats (e.g., polarity types, basic emotion categories, and affective dimensions), <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic levels</a> (word vs. sentence vs. discourse), and, of course, (few well-resourced but much more under-resourced) <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural languages and text genres</a> (e.g., product reviews, <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, news). The resulting heterogeneity makes data and software developed under these conflicting constraints hard to compare and challenging to integrate. To resolve this unsatisfactory state of affairs we here propose a training scheme that learns a shared latent representation of emotion independent from different label formats, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural languages</a>, and even disparate model architectures. Experiments on a wide range of datasets indicate that this approach yields the desired <a href=https://en.wikipedia.org/wiki/Interoperability>interoperability</a> without penalizing prediction quality. Code and data are archived under DOI 10.5281 / zenodo.5466068.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.729.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--729 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.729 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.729" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.729/>Collaborative Learning of Bidirectional Decoders for Unsupervised Text Style Transfer</a></strong><br><a href=/people/y/yun-ma/>Yun Ma</a>
|
<a href=/people/y/yangbin-chen/>Yangbin Chen</a>
|
<a href=/people/x/xudong-mao/>Xudong Mao</a>
|
<a href=/people/q/qing-li/>Qing Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--729><div class="card-body p-3 small">Unsupervised text style transfer aims to alter the underlying style of the text to a desired value while keeping its style-independent semantics, without the support of parallel training corpora. Existing methods struggle to achieve both high style conversion rate and low content loss, exhibiting the over-transfer and under-transfer problems. We attribute these problems to the conflicting driving forces of the style conversion goal and content preservation goal. In this paper, we propose a collaborative learning framework for unsupervised text style transfer using a pair of bidirectional decoders, one decoding from left to right while the other decoding from right to left. In our collaborative learning mechanism, each <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> is regularized by knowledge from its peer which has a different <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition process</a>. The difference is guaranteed by their opposite decoding directions and a distinguishability constraint. As a result, mutual knowledge distillation drives both decoders to a better optimum and alleviates the over-transfer and under-transfer problems. Experimental results on two benchmark datasets show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves strong empirical results on both style compatibility and content preservation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.730.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--730 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.730 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.730" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.730/>Exploring Non-Autoregressive Text Style Transfer</a></strong><br><a href=/people/y/yun-ma/>Yun Ma</a>
|
<a href=/people/q/qing-li/>Qing Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--730><div class="card-body p-3 small">In this paper, we explore Non-AutoRegressive (NAR) decoding for unsupervised text style transfer. We first propose a base NAR model by directly adapting the common training scheme from its AutoRegressive (AR) counterpart. Despite the faster inference speed over the <a href=https://en.wikipedia.org/wiki/Autoregressive_model>AR model</a>, this NAR model sacrifices its transfer performance due to the lack of <a href=https://en.wikipedia.org/wiki/Conditional_dependence>conditional dependence</a> between output tokens. To this end, we investigate three <a href=https://en.wikipedia.org/wiki/Methodology>techniques</a>, i.e., knowledge distillation, contrastive learning, and iterative decoding, for performance enhancement. Experimental results on two benchmark datasets suggest that, although the base NAR model is generally inferior to AR decoding, their performance gap can be clearly narrowed when empowering NAR decoding with knowledge distillation, contrastive learning, and iterative decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.733.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--733 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.733 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.733/>Progressively Guide to Attend : An Iterative Alignment Framework for Temporal Sentence Grounding</a></strong><br><a href=/people/d/daizong-liu/>Daizong Liu</a>
|
<a href=/people/x/xiaoye-qu/>Xiaoye Qu</a>
|
<a href=/people/p/pan-zhou/>Pan Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--733><div class="card-body p-3 small">A key solution to temporal sentence grounding (TSG) exists in how to learn effective alignment between vision and language features extracted from an untrimmed video and a sentence description. Existing methods mainly leverage vanilla soft attention to perform the alignment in a single-step process. However, such single-step attention is insufficient in practice, since complicated relations between inter- and intra-modality are usually obtained through multi-step reasoning. In this paper, we propose an Iterative Alignment Network (IA-Net) for TSG task, which iteratively interacts inter- and intra-modal features within multiple steps for more accurate grounding. Specifically, during the iterative reasoning process, we pad multi-modal features with learnable parameters to alleviate the nowhere-to-attend problem of non-matched frame-word pairs, and enhance the basic co-attention mechanism in a parallel manner. To further calibrate the misaligned attention caused by each reasoning step, we also devise a calibration module following each attention module to refine the alignment knowledge. With such iterative alignment scheme, our IA-Net can robustly capture the fine-grained relations between vision and language domains step-by-step for progressively reasoning the temporal boundaries. Extensive experiments conducted on three challenging benchmarks demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better than the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-arts</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.741.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--741 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.741 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.741" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.741/>ARMAN : Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization<span class=acl-fixed-case>ARMAN</span>: <span class=acl-fixed-case>P</span>re-training with <span class=acl-fixed-case>S</span>emantically <span class=acl-fixed-case>S</span>electing and <span class=acl-fixed-case>R</span>eordering of <span class=acl-fixed-case>S</span>entences for <span class=acl-fixed-case>P</span>ersian <span class=acl-fixed-case>A</span>bstractive <span class=acl-fixed-case>S</span>ummarization</a></strong><br><a href=/people/a/alireza-salemi/>Alireza Salemi</a>
|
<a href=/people/e/emad-kebriaei/>Emad Kebriaei</a>
|
<a href=/people/g/ghazal-neisi-minaei/>Ghazal Neisi Minaei</a>
|
<a href=/people/a/azadeh-shakery/>Azadeh Shakery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--741><div class="card-body p-3 small">Abstractive text summarization is one of the areas influenced by the emergence of pre-trained language models. Current pre-training works in abstractive summarization give more points to the summaries with more words in common with the main text and pay less attention to the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between generated sentences and the original document. We propose ARMAN, a Transformer-based encoder-decoder model pre-trained with three novel objectives to address this issue. In <a href=https://en.wikipedia.org/wiki/ARMAN>ARMAN</a>, salient sentences from a document are selected according to a modified semantic score to be masked and form a pseudo summary. To summarize more accurately and similar to human writing patterns, we applied modified sentence reordering. We evaluated our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on six downstream Persian summarization tasks. Experimental results show that our proposed model achieves state-of-the-art performance on all six summarization tasks measured by ROUGE and BERTScore. Our models also outperform prior works in <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a>, question paraphrasing, and <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple choice question answering</a>. Finally, we established a <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a> and show that using the semantic score significantly improves summarization results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.745.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--745 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.745 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.745" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.745/>Revisiting Tri-training of Dependency Parsers</a></strong><br><a href=/people/j/joachim-wagner/>Joachim Wagner</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--745><div class="card-body p-3 small">We compare two orthogonal semi-supervised learning techniques, namely tri-training and pretrained word embeddings, in the task of dependency parsing. We explore language-specific FastText and ELMo embeddings and multilingual BERT embeddings. We focus on a low resource scenario as <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a> can be expected to have the most impact here. Based on treebank size and available ELMo models, we select <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>, Uyghur (a zero-shot language for mBERT) and <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>. Furthermore, we include <a href=https://en.wikipedia.org/wiki/English_language>English</a> in a simulated low-resource setting. We find that pretrained word embeddings make more effective use of unlabelled data than tri-training but that the two approaches can be successfully combined.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.746.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--746 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.746 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.746" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.746/>Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion : Re-explore Zero-Shot Learning for Slot Filling</a></strong><br><a href=/people/l/liwen-wang/>Liwen Wang</a>
|
<a href=/people/x/xuefeng-li/>Xuefeng Li</a>
|
<a href=/people/j/jiachi-liu/>Jiachi Liu</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--746><div class="card-body p-3 small">Zero-shot cross-domain slot filling alleviates the <a href=https://en.wikipedia.org/wiki/Data_dependence>data dependence</a> in the case of data scarcity in the target domain, which has aroused extensive research. However, as most of the existing methods do not achieve effective <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> to the target domain, they just fit the distribution of the seen slot and show poor performance on unseen slot in the target domain. To solve this, we propose a novel approach based on prototypical contrastive learning with a dynamic label confusion strategy for zero-shot slot filling. The prototypical contrastive learning aims to reconstruct the semantic constraints of labels, and we introduce the label confusion strategy to establish the label dependence between the source domains and the target domain on-the-fly. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant improvement on the unseen slots, while also set new state-of-the-arts on slot filling task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.747.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--747 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.747 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.747/>Neuralizing Regular Expressions for Slot Filling</a></strong><br><a href=/people/c/chengyue-jiang/>Chengyue Jiang</a>
|
<a href=/people/z/zijian-jin/>Zijian Jin</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--747><div class="card-body p-3 small">Neural models and <a href=https://en.wikipedia.org/wiki/Mathematical_logic>symbolic rules</a> such as <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> have their respective merits and weaknesses. In this paper, we study the integration of the two approaches for the slot filling task by converting <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> into <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Specifically, we first convert <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> into a special form of finite-state transducers, then unfold its approximate inference algorithm as a bidirectional recurrent neural model that performs slot filling via <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. Experimental results show that our model has superior zero-shot and few-shot performance and stays competitive when there are sufficient training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.755.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--755 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.755 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.755/>Case-based Reasoning for Natural Language Queries over Knowledge Bases</a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a>
|
<a href=/people/d/dung-thai/>Dung Thai</a>
|
<a href=/people/a/ameya-godbole/>Ameya Godbole</a>
|
<a href=/people/e/ethan-perez/>Ethan Perez</a>
|
<a href=/people/j/jay-yoon-lee/>Jay Yoon Lee</a>
|
<a href=/people/l/lizhen-tan/>Lizhen Tan</a>
|
<a href=/people/l/lazaros-polymenakos/>Lazaros Polymenakos</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--755><div class="card-body p-3 small">It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions a <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a <a href=https://en.wikipedia.org/wiki/Parametric_model>parametric model</a> that can generate a <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the CWQ dataset, CBR-KBQA outperforms the current state of the art by 11 % on <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Furthermore, we show that CBR-KBQA is capable of using new cases without any further training : by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.<i>without</i> any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.756.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--756 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.756 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.756" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.756/>Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation</a></strong><br><a href=/people/c/chen-zhao/>Chen Zhao</a>
|
<a href=/people/c/chenyan-xiong/>Chenyan Xiong</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--756><div class="card-body p-3 small">Open-domain question answering answers a question based on evidence retrieved from a <a href=https://en.wikipedia.org/wiki/Text_corpus>large corpus</a>. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> that rely on them can not transfer to the more common setting, where only questionanswer pairs are available. This paper investigates whether <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> can learn to find evidence from a large corpus, with only distant supervision from answer labels for model training, thereby generating no additional annotation cost. We introduce a novel approach (DistDR) that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn the most likely evidence. Without using any evidence labels, DistDR is on par with fully-supervised state-of-the-art methods on both multi-hop and single-hop QA benchmarks. Our analysis confirms that DistDR finds more accurate evidence over iterations, which leads to model improvements. The code is available at https://github.com/henryzhao5852/DistDR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.760.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--760 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.760 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.760/>Set Generation Networks for End-to-End Knowledge Base Population</a></strong><br><a href=/people/d/dianbo-sui/>Dianbo Sui</a>
|
<a href=/people/c/chenhao-wang/>Chenhao Wang</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--760><div class="card-body p-3 small">The task of <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base population (KBP)</a> aims to discover facts about entities from texts and expand a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> with these facts. Previous studies shape end-to-end KBP as a machine translation task, which is required to convert unordered fact into a sequence according to a pre-specified order. However, the facts stated in a sentence are unordered in essence. In this paper, we formulate end-to-end KBP as a direct set generation problem, avoiding considering the order of multiple facts. To solve the set generation problem, we propose networks featured by <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a> with non-autoregressive parallel decoding. Unlike previous approaches that use an autoregressive decoder to generate facts one by one, the proposed networks can directly output the final set of facts in one shot. Furthermore, to train the <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a>, we also design a <a href=https://en.wikipedia.org/wiki/Loss_function>set-based loss</a> that forces unique predictions via <a href=https://en.wikipedia.org/wiki/Bipartite_matching>bipartite matching</a>. Compared with cross-entropy loss that highly penalizes small shifts in fact order, the proposed bipartite matching loss is invariant to any permutation of predictions. Benefiting from getting rid of the burden of predicting the order of multiple facts, our proposed networks achieve state-of-the-art (SoTA) performance on two benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.762.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--762 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.762 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.762" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.762/>Progressive Adversarial Learning for Bootstrapping : A Case Study on Entity Set Expansion</a></strong><br><a href=/people/l/lingyong-yan/>Lingyong Yan</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--762><div class="card-body p-3 small">Bootstrapping has become the mainstream method for entity set expansion. Conventional bootstrapping methods mostly define the expansion boundary using seed-based distance metrics, which heavily depend on the quality of selected seeds and are hard to be adjusted due to the extremely sparse supervision. In this paper, we propose BootstrapGAN, a new learning method for <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping</a> which jointly models the <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping process</a> and the boundary learning process in a GAN framework. Specifically, the expansion boundaries of different bootstrapping iterations are learned via different discriminator networks ; the bootstrapping network is the generator to generate new positive entities, and the discriminator networks identify the expansion boundaries by trying to distinguish the generated entities from known positive entities. By iteratively performing the above <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a>, the generator and the discriminators can reinforce each other and be progressively refined along the whole bootstrapping process. Experiments show that BootstrapGAN achieves the new state-of-the-art entity set expansion performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.765.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--765 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.765 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.765" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.765/>A Relation-Oriented Clustering Method for Open Relation Extraction</a></strong><br><a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/y/yaqian-zhou/>Yaqian Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--765><div class="card-body p-3 small">The clustering-based unsupervised relation discovery method has gradually become one of the important methods of open relation extraction (OpenRE). However, high-dimensional vectors can encode complex linguistic information which leads to the problem that the derived clusters can not explicitly align with the relational semantic classes. In this work, we propose a relation-oriented clustering model and use it to identify the novel relations in the unlabeled data. Specifically, to enable the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to learn to cluster <a href=https://en.wikipedia.org/wiki/Relational_model>relational data</a>, our method leverages the readily available labeled data of pre-defined relations to learn a relation-oriented representation. We minimize distance between the instance with same relation by gathering the instances towards their corresponding relation centroids to form a cluster structure, so that the learned representation is cluster-friendly. To reduce the clustering bias on predefined classes, we optimize the model by minimizing a joint objective on both labeled and unlabeled data. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> reduces the <a href=https://en.wikipedia.org/wiki/Error_rate>error rate</a> by 29.2 % and 15.7 %, on two datasets respectively, compared with current SOTA methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.768.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--768 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.768 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.768/>Meta Distant Transfer Learning for Pre-trained Language Models</a></strong><br><a href=/people/c/chengyu-wang/>Chengyu Wang</a>
|
<a href=/people/h/haojie-pan/>Haojie Pan</a>
|
<a href=/people/m/minghui-qiu/>Minghui Qiu</a>
|
<a href=/people/j/jun-huang/>Jun Huang</a>
|
<a href=/people/f/fei-yang/>Fei Yang</a>
|
<a href=/people/y/yin-zhang/>Yin Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--768><div class="card-body p-3 small">With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from <a href=https://en.wikipedia.org/wiki/Negative_transfer>negative transfer</a>. Inspired by <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a>, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation learning to mine implicit relations among multiple tasks and classes. Based on the results, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> trains a PLM-based meta-learner to capture the transferable knowledge across tasks. The weighted maximum entropy regularizers are proposed to make meta-learner more task-agnostic and unbiased. Finally, the meta-learner can be fine-tuned to fit each <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> with better parameter initialization. We evaluate Meta-DTL using both BERT and ALBERT on seven public datasets. Experiment results confirm the superiority of Meta-DTL as it consistently outperforms strong baselines. We find that Meta-DTL is highly effective when very few data is available for the target task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.769.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--769 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.769 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.769/>UniKER : A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>KER</span>: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference</a></strong><br><a href=/people/k/kewei-cheng/>Kewei Cheng</a>
|
<a href=/people/z/ziqing-yang/>Ziqing Yang</a>
|
<a href=/people/m/ming-zhang/>Ming Zhang</a>
|
<a href=/people/y/yizhou-sun/>Yizhou Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--769><div class="card-body p-3 small">Knowledge graph inference has been studied extensively due to its wide applications. It has been addressed by two lines of research, i.e., the more traditional <a href=https://en.wikipedia.org/wiki/Logical_consequence>logical rule reasoning</a> and the more recent knowledge graph embedding (KGE). Several attempts have been made to combine KGE and <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logical rules</a> for better knowledge graph inference. Unfortunately, they either simply treat logical rules as additional constraints into KGE loss or use <a href=https://en.wikipedia.org/wiki/Probabilistic_model>probabilistic model</a> to approximate the exact <a href=https://en.wikipedia.org/wiki/Logical_inference>logical inference</a> (i.e., MAX-SAT). Even worse, both approaches need to sample ground rules to tackle the scalability issue, as the total number of ground rules is intractable in practice, making them less effective in handling logical rules. In this paper, we propose a novel framework UniKER to address these challenges by restricting <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logical rules</a> to be definite Horn rules, which can fully exploit the knowledge in <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logical rules</a> and enable the mutual enhancement of logical rule-based reasoning and KGE in an extremely efficient way. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms in terms of both <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a> and effectiveness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.771.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--771 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.771 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.771/>Jointly Learning to Repair Code and Generate Commit Message</a></strong><br><a href=/people/j/jiaqi-bai/>Jiaqi Bai</a>
|
<a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/a/ambrosio-blanco/>Ambrosio Blanco</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--771><div class="card-body p-3 small">We propose a novel <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> of jointly repairing program codes and generating commit messages. Code repair and commit message generation are two essential and related tasks for <a href=https://en.wikipedia.org/wiki/Software_development>software development</a>. However, existing work usually performs the two <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> independently. We construct a multilingual triple dataset including <a href=https://en.wikipedia.org/wiki/Software_bug>buggy code</a>, fixed code, and commit messages for this novel task. We first introduce a cascaded method with two models, one is to generate the fixed code first, and the other generates the commit message based on the fixed and original codes. We enhance the cascaded method with different training approaches, including the teacher-student method, the multi-task method, and the back-translation method. To deal with the error propagation problem of the cascaded method, we also propose a joint model that can both repair the <a href=https://en.wikipedia.org/wiki/Source_code>program code</a> and generate the commit message in a unified framework. Massive experiments on our constructed buggy-fixed-commit dataset reflect the challenge of this task and that the enhanced cascaded model and the proposed joint model significantly outperform baselines in both quality of code and commit messages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.773.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--773 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.773 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.773/>On Pursuit of Designing Multi-modal Transformer for Video Grounding</a></strong><br><a href=/people/m/meng-cao/>Meng Cao</a>
|
<a href=/people/l/long-chen/>Long Chen</a>
|
<a href=/people/m/mike-zheng-shou/>Mike Zheng Shou</a>
|
<a href=/people/c/can-zhang/>Can Zhang</a>
|
<a href=/people/y/yuexian-zou/>Yuexian Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--773><div class="card-body p-3 small">Video grounding aims to localize the temporal segment corresponding to a <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence query</a> from an <a href=https://en.wikipedia.org/wiki/Video_editing>untrimmed video</a>. Almost all existing video grounding methods fall into two frameworks : 1) <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>Top-down model</a> : It predefines a set of segment candidates and then conducts segment classification and <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a>. 2) Bottom-up model : It directly predicts frame-wise probabilities of the referential segment boundaries. However, all these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> are not end-to-end, i.e., they always rely on some time-consuming post-processing steps to refine predictions. To this end, we reformulate video grounding as a set prediction task and propose a novel end-to-end multi-modal Transformer model, dubbed as GTR. Specifically, GTR has two <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> for video and language encoding, and a cross-modal decoder for grounding prediction. To facilitate the end-to-end training, we use a Cubic Embedding layer to transform the raw videos into a set of visual tokens. To better fuse these two modalities in the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>, we design a new Multi-head Cross-Modal Attention. The whole <a href=https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units>GTR</a> is optimized via a Many-to-One matching loss. Furthermore, we conduct comprehensive studies to investigate different model design choices. Extensive results on three <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> have validated the superiority of GTR. All three typical GTR variants achieve record-breaking performance on all datasets and <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>, with several times faster <a href=https://en.wikipedia.org/wiki/Time_complexity>inference speed</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.775.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--775 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.775 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.775" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.775/>Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers</a></strong><br><a href=/people/s/stella-frank/>Stella Frank</a>
|
<a href=/people/e/emanuele-bugliarello/>Emanuele Bugliarello</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--775><div class="card-body p-3 small">Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are not symmetrically cross-modal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.778.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--778 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.778 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.778" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.778/>QA-Align : Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions<span class=acl-fixed-case>QA</span>-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions</a></strong><br><a href=/people/d/daniela-brook-weiss/>Daniela Brook Weiss</a>
|
<a href=/people/p/paul-roit/>Paul Roit</a>
|
<a href=/people/a/ayal-klein/>Ayal Klein</a>
|
<a href=/people/o/ori-ernst/>Ori Ernst</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--778><div class="card-body p-3 small">Multi-text applications, such as <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a>, are typically required to model redundancies across related texts. Current methods confronting <a href=https://en.wikipedia.org/wiki/Consolidation_(business)>consolidation</a> struggle to fuse overlapping information. In order to explicitly represent content overlap, we propose to align predicate-argument relations across texts, providing a potential scaffold for information consolidation. We go beyond clustering coreferring mentions, and instead model overlap with respect to redundancy at a propositional level, rather than merely detecting shared referents. Our <a href=https://en.wikipedia.org/wiki/Setting_(narrative)>setting</a> exploits QA-SRL, utilizing question-answer pairs to capture predicate-argument relations, facilitating laymen annotation of cross-text alignments. We employ crowd-workers for constructing a dataset of QA-based alignments, and present a baseline QA alignment model trained over our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Analyses show that our new task is semantically challenging, capturing content overlap beyond <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical similarity</a> and complements cross-document coreference with proposition-level links, offering potential use for downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.780.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--780 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.780 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.780" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.780/>Exploiting Twitter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings<span class=acl-fixed-case>T</span>witter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings</a></strong><br><a href=/people/m/marco-di-giovanni/>Marco Di Giovanni</a>
|
<a href=/people/m/marco-brambilla/>Marco Brambilla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--780><div class="card-body p-3 small">Semantic sentence embeddings are usually supervisedly built minimizing distances between pairs of embeddings of sentences labelled as semantically similar by annotators. Since big labelled datasets are rare, in particular for non-English languages, and expensive, recent studies focus on <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approaches</a> that require not-paired input sentences. We instead propose a language-independent approach to build large datasets of pairs of informal texts weakly similar, without manual human effort, exploiting Twitter&#8217;s intrinsic powerful signals of relatedness : replies and quotes of tweets. We use the collected pairs to train a Transformer model with triplet-like structures, and we test the generated embeddings on Twitter NLP similarity tasks (PIT and TURL) and STSb. We also introduce four new sentence ranking evaluation benchmarks of informal texts, carefully extracted from the initial collections of tweets, proving not only that our best model learns classical Semantic Textual Similarity, but also excels on tasks where pairs of sentences are not exact paraphrases. Ablation studies reveal how increasing the corpus size influences positively the results, even at 2 M samples, suggesting that bigger collections of Tweets still do not contain redundant information about semantic similarities. Code available at https://github.com/marco-digio/Twitter4SSE</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.781.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--781 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.781 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.781/>Guilt by Association : Emotion Intensities in Lexical Representations</a></strong><br><a href=/people/s/shahab-raji/>Shahab Raji</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--781><div class="card-body p-3 small">What do linguistic models reveal about the <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.782.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--782 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.782 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.782/>Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender</a></strong><br><a href=/people/s/sky-ch-wang/>Sky CH-Wang</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--782><div class="card-body p-3 small">Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these <a href=https://en.wikipedia.org/wiki/Choice>choices</a> in aggregate allows us to examine large-scale attitude shifts within a population. Here, we develop computational methods to study <a href=https://en.wikipedia.org/wiki/Word_choice>word choice</a> within a sociolinguistic lexical variablealternate words used to express the same conceptin order to test for change in the United States towards <a href=https://en.wikipedia.org/wiki/Human_sexuality>sexuality</a> and <a href=https://en.wikipedia.org/wiki/Gender>gender</a>. We examine two variables : i) referents to significant others, such as the word partner and ii) referents to an indefinite person, both of which could optionally be marked with gender. The linguistic choices in each variable allow us to study increased rates of acceptances of gay marriage and <a href=https://en.wikipedia.org/wiki/Gender_equality>gender equality</a>, respectively. In longitudinal analyses across <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> over 87 M messages, we demonstrate that attitudes are changing but that these changes are driven by specific demographics within the United States. Further, in a quasi-causal analysis, we show that passages of Marriage Equality Acts in different states are drivers of <a href=https://en.wikipedia.org/wiki/Linguistic_change>linguistic change</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.785.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--785 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.785 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.785/>Assessing the Reliability of Word Embedding Gender Bias Measures</a></strong><br><a href=/people/y/yupei-du/>Yupei Du</a>
|
<a href=/people/q/qixiang-fang/>Qixiang Fang</a>
|
<a href=/people/d/dong-nguyen/>Dong Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--785><div class="card-body p-3 small">Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these <a href=https://en.wikipedia.org/wiki/Measurement>measures</a> can suffer from <a href=https://en.wikipedia.org/wiki/Observational_error>measurement error</a>. One indication of measurement quality is <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a>, concerning the extent to which a <a href=https://en.wikipedia.org/wiki/Measurement>measure</a> produces consistent results. In this paper, we assess three types of <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of <a href=https://en.wikipedia.org/wiki/Random_seed>random seeds</a>, scoring rules and <a href=https://en.wikipedia.org/wiki/Word_formation>words</a>. Furthermore, we analyse the effects of various factors on these <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a>&#8217; <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability scores</a>. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.788.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--788 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.788 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.788" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.788/>SWEAT : Scoring Polarization of Topics across Different Corpora<span class=acl-fixed-case>SWEAT</span>: Scoring Polarization of Topics across Different Corpora</a></strong><br><a href=/people/f/federico-bianchi/>Federico Bianchi</a>
|
<a href=/people/m/marco-marelli/>Marco Marelli</a>
|
<a href=/people/p/paolo-nicoli/>Paolo Nicoli</a>
|
<a href=/people/m/matteo-palmonari/>Matteo Palmonari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--788><div class="card-body p-3 small">Understanding differences of viewpoints across corpora is a fundamental task for <a href=https://en.wikipedia.org/wiki/Computational_social_sciences>computational social sciences</a>. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two distributional representations. To this end, <a href=https://en.wikipedia.org/wiki/SWEAT>SWEAT</a> uses two additional wordsets, deemed to have opposite valence, to represent two different <a href=https://en.wikipedia.org/wiki/Zeros_and_poles>poles</a>. We validate our approach and illustrate a case study to show the usefulness of the introduced <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.791.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--791 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.791 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.791" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.791/>PAUSE : Positive and Annealed Unlabeled Sentence Embedding<span class=acl-fixed-case>PAUSE</span>: Positive and Annealed Unlabeled Sentence Embedding</a></strong><br><a href=/people/l/lele-cao/>Lele Cao</a>
|
<a href=/people/e/emil-larsson/>Emil Larsson</a>
|
<a href=/people/v/vilhelm-von-ehrenheim/>Vilhelm von Ehrenheim</a>
|
<a href=/people/d/dhiana-deva-cavalcanti-rocha/>Dhiana Deva Cavalcanti Rocha</a>
|
<a href=/people/a/anna-martin/>Anna Martin</a>
|
<a href=/people/s/sonja-horn/>Sonja Horn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--791><div class="card-body p-3 small">Sentence embedding refers to a set of effective and versatile techniques for converting raw text into numerical vector representations that can be used in a wide range of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) applications</a>. The majority of these techniques are either <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised</a> or unsupervised. Compared to the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a>, the supervised ones make less assumptions about <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization objectives</a> and usually achieve better results. However, the training requires a large amount of labeled sentence pairs, which is not available in many industrial scenarios. To that end, we propose a generic and end-to-end approach PAUSE (Positive and Annealed Unlabeled Sentence Embedding), capable of learning high-quality sentence embeddings from a partially labeled dataset. We experimentally show that PAUSE achieves, and sometimes surpasses, state-of-the-art results using only a small fraction of labeled sentence pairs on various benchmark tasks. When applied to a real industrial use case where labeled samples are scarce, PAUSE encourages us to extend our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> without the burden of extensive manual annotation work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.793.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--793 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.793 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.793" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.793/>An Information-Theoretic Characterization of Morphological Fusion</a></strong><br><a href=/people/n/neil-rathi/>Neil Rathi</a>
|
<a href=/people/m/michael-hahn/>Michael Hahn</a>
|
<a href=/people/r/richard-futrell/>Richard Futrell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--793><div class="card-body p-3 small">Linguistic typology generally divides <a href=https://en.wikipedia.org/wiki/Synthetic_language>synthetic languages</a> into groups based on their <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological fusion</a>. However, this <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a> has long been thought to be best considered a matter of degree. We present an information-theoretic measure, called <a href=https://en.wikipedia.org/wiki/Information_fusion>informational fusion</a>, to quantify the degree of fusion of a given set of morphological features in a surface form, which naturally provides such a graded scale. Informational fusion is able to encapsulate not only concatenative, but also nonconcatenative morphological systems (e.g. Arabic), abstracting away from any notions of morpheme segmentation. We then show, on a sample of twenty-one languages, that our measure recapitulates the usual linguistic classifications for concatenative systems, and provides new measures for nonconcatenative ones. We also evaluate the long-standing hypotheses that more frequent forms are more fusional, and that paradigm size anticorrelates with degree of fusion. We do not find evidence for the idea that <a href=https://en.wikipedia.org/wiki/Language>languages</a> have characteristic levels of fusion ; rather, the degree of fusion varies across part-of-speech within languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.794.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--794 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.794 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.794/>The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning</a></strong><br><a href=/people/y/yuchen-lian/>Yuchen Lian</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/t/tessa-verhoef/>Tessa Verhoef</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--794><div class="card-body p-3 small">Natural languages display a trade-off among different strategies to convey <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>, such as <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> or <a href=https://en.wikipedia.org/wiki/Inflection>inflection</a>. This trade-off, however, has not appeared in recent simulations of iterated language learning with neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in light of three factors that play an important role in comparable experiments from the Language Evolution field : (i) speaker bias towards efficient messaging, (ii) non systematic input languages, and (iii) learning bottleneck. Our simulations show that neural agents mainly strive to maintain the utterance type distribution observed during <a href=https://en.wikipedia.org/wiki/Learning>learning</a>, instead of developing a more efficient or systematic language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--800 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.800 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.800" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.800/>UNKs Everywhere : Adapting Multilingual Language Models to New Scripts<span class=acl-fixed-case>UNK</span>s Everywhere: <span class=acl-fixed-case>A</span>dapting Multilingual Language Models to New Scripts</a></strong><br><a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--800><div class="card-body p-3 small">Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model&#8217;s embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT&#8217;s and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--805 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.805" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.805/>Discretized Integrated Gradients for Explaining Language Models</a></strong><br><a href=/people/s/soumya-sanyal/>Soumya Sanyal</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--805><div class="card-body p-3 small">As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a> computed at the interpolated points and consequently, the quality of the generated explanations. Here we propose Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>embedding space</a>, yielding more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encourage reproducible research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--814 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.814/>XLEnt : Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment<span class=acl-fixed-case>XLE</span>nt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment</a></strong><br><a href=/people/a/ahmed-el-kishky/>Ahmed El-Kishky</a>
|
<a href=/people/a/adithya-renduchintala/>Adithya Renduchintala</a>
|
<a href=/people/j/james-cross/>James Cross</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--814><div class="card-body p-3 small">Cross-lingual named-entity lexica are an important resource to multilingual NLP tasks such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and cross-lingual wikification. While <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> contain a large number of entities in high-resource languages such as <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>, corresponding entities for lower-resource languages are often missing. To address this, we propose Lexical-Semantic-Phonetic Align (LSP-Align), a technique to automatically mine cross-lingual entity lexica from mined web data. We demonstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.816.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--816 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.816 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.816/>Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction<span class=acl-fixed-case>R</span>elation <span class=acl-fixed-case>E</span>xtraction</a></strong><br><a href=/people/b/bruno-taille/>Bruno Taillé</a>
|
<a href=/people/v/vincent-guigue/>Vincent Guigue</a>
|
<a href=/people/g/geoffrey-scoutheeten/>Geoffrey Scoutheeten</a>
|
<a href=/people/p/patrick-gallinari/>Patrick Gallinari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--816><div class="card-body p-3 small">State-of-the-art NLP models can adopt shallow heuristics that limit their generalization capability (McCoy et al., 2019). Such <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> include lexical overlap with the training set in Named-Entity Recognition (Taille et al., 2020) and Event or Type heuristics in Relation Extraction (Rosenman et al., 2020). In the more realistic end-to-end RE setting, we can expect yet another <a href=https://en.wikipedia.org/wiki/Heuristic>heuristic</a> : the mere retention of training relation triples. In this paper we propose two experiments confirming that retention of known facts is a key factor of performance on standard <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>. Furthermore, one experiment suggests that a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline model</a> able to use intermediate type representations is less prone to over-rely on retention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.817.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--817 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.817 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.817" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.817/>Automatic Text Evaluation through the Lens of Wasserstein Barycenters<span class=acl-fixed-case>W</span>asserstein Barycenters</a></strong><br><a href=/people/p/pierre-colombo/>Pierre Colombo</a>
|
<a href=/people/g/guillaume-staerman/>Guillaume Staerman</a>
|
<a href=/people/c/chloe-clavel/>Chloé Clavel</a>
|
<a href=/people/p/pablo-piantanida/>Pablo Piantanida</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--817><div class="card-body p-3 small">A new metric BaryScore to evaluate text generation based on deep contextualized embeddings (e.g., BERT, Roberta, ELMo) is introduced. This <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> is motivated by a new <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> relying on optimal transport tools, i.e., <a href=https://en.wikipedia.org/wiki/Wasserstein_distance>Wasserstein distance</a> and <a href=https://en.wikipedia.org/wiki/Barycenter>barycenter</a>. By modelling the layer output of deep contextualized embeddings as a <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> rather than by a vector embedding ; this framework provides a natural way to aggregate the different outputs through the Wasserstein space topology. In addition, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> provides theoretical grounds to our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> and offers an alternative to available solutions (e.g., MoverScore and BertScore). Numerical evaluation is performed on four different tasks : <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, data2text generation and image captioning. Our results show that BaryScore outperforms other BERT based metrics and exhibits more consistent behaviour in particular for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>.<i>e.g.</i>, BERT, Roberta, ELMo) is introduced. This metric is motivated by a new framework relying on optimal transport tools, <i>i.e.</i>, Wasserstein distance and barycenter. By modelling the layer output of deep contextualized embeddings as a probability distribution rather than by a vector embedding; this framework provides a natural way to aggregate the different outputs through the Wasserstein space topology. In addition, it provides theoretical grounds to our metric and offers an alternative to available solutions (<i>e.g.</i>, MoverScore and BertScore). Numerical evaluation is performed on four different tasks: machine translation, summarization, data2text generation and image captioning. Our results show that BaryScore outperforms other BERT based metrics and exhibits more consistent behaviour in particular for text summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.820.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--820 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.820 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.820/>Robustness Evaluation of Entity Disambiguation Using Prior Probes : the Case of Entity Overshadowing</a></strong><br><a href=/people/v/vera-provatorova/>Vera Provatorova</a>
|
<a href=/people/s/samarth-bhargav/>Samarth Bhargav</a>
|
<a href=/people/s/svitlana-vakulenko/>Svitlana Vakulenko</a>
|
<a href=/people/e/evangelos-kanoulas/>Evangelos Kanoulas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--820><div class="card-body p-3 small">Entity disambiguation (ED) is the last step of entity linking (EL), when candidate entities are reranked according to the context they appear in. All datasets for training and evaluating models for EL consist of convenience samples, such as news articles and <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, that propagate the prior probability bias of the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity distribution</a> towards more frequently occurring entities. It was shown that the performance of the EL systems on such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> is overestimated since it is possible to obtain higher accuracy scores by merely learning the prior. To provide a more adequate evaluation benchmark, we introduce the ShadowLink dataset, which includes 16 K short text snippets annotated with entity mentions. We evaluate and report the performance of popular EL systems on the ShadowLink benchmark. The results show a considerable difference in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> between more and less common entities for all of the EL systems under evaluation, demonstrating the effect of prior probability bias and entity overshadowing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.821.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--821 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.821 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.821" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.821/>IndoNLI : A Natural Language Inference Dataset for Indonesian<span class=acl-fixed-case>I</span>ndo<span class=acl-fixed-case>NLI</span>: A Natural Language Inference Dataset for <span class=acl-fixed-case>I</span>ndonesian</a></strong><br><a href=/people/r/rahmad-mahendra/>Rahmad Mahendra</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/s/samuel-louvan/>Samuel Louvan</a>
|
<a href=/people/f/fahrurrozi-rahman/>Fahrurrozi Rahman</a>
|
<a href=/people/c/clara-vania/>Clara Vania</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--821><div class="card-body p-3 small">We present IndoNLI, the first human-elicited NLI dataset for <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian</a>. We adapt the data collection protocol for MNLI and collect ~18 K sentence pairs annotated by crowd workers and experts. The expert-annotated data is used exclusively as a test set. It is designed to provide a challenging test-bed for Indonesian NLI by explicitly incorporating various linguistic phenomena such as numerical reasoning, structural changes, idioms, or temporal and spatial reasoning. Experiment results show that XLM-R outperforms other pre-trained models in our data. The best performance on the expert-annotated data is still far below <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human performance</a> (13.4 % accuracy gap), suggesting that this <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> is especially challenging. Furthermore, our analysis shows that our expert-annotated data is more diverse and contains fewer annotation artifacts than the crowd-annotated data. We hope this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can help accelerate progress in Indonesian NLP research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.824.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--824 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.824 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.824" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.824/>Efficient Sampling of Dependency Structure</a></strong><br><a href=/people/r/ran-zmigrod/>Ran Zmigrod</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--824><div class="card-body p-3 small">Probabilistic distributions over spanning trees in <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graphs</a> are a fundamental model of dependency structure in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, syntactic dependency trees. In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, dependency trees often have an additional root constraint : only one edge may emanate from the root. However, no <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling algorithm</a> has been presented in the literature to account for this additional <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a>. In this paper, we adapt two spanning tree sampling algorithms to faithfully sample dependency trees from a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> subject to the root constraint. Wilson (1996 (&#8217;s sampling algorithm has a <a href=https://en.wikipedia.org/wiki/Time_complexity>running time</a> of O(H) where H is the mean hitting time of the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. Colbourn (1996)&#8217;s sampling algorithm has a <a href=https://en.wikipedia.org/wiki/Time_complexity>running time</a> of O(N3), which is often greater than the mean hitting time of a <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graph</a>. Additionally, we build upon Colbourn&#8217;s algorithm and present a novel extension that can sample K trees without replacement in O(K N3 + K2 N) time. To the best of our knowledge, no <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> has been given for sampling spanning trees without replacement from a <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graph</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.825.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--825 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.825 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.825" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.825/>Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering</a></strong><br><a href=/people/d/daniel-fernandez-gonzalez/>Daniel Fernández-González</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--825><div class="card-body p-3 small">Discontinuous constituent parsers have always lagged behind continuous approaches in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Speed>speed</a>, as the presence of constituents with discontinuous yield introduces extra <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> to the task. However, a discontinuous tree can be converted into a continuous variant by reordering tokens. Based on that, we propose to reduce discontinuous parsing to a continuous problem, which can then be directly solved by any off-the-shelf continuous parser. To that end, we develop a Pointer Network capable of accurately generating the continuous token arrangement for a given input sentence and define a <a href=https://en.wikipedia.org/wiki/Bijection>bijective function</a> to recover the original order. Experiments on the main benchmarks with two continuous parsers prove that our approach is on par in accuracy with purely discontinuous state-of-the-art algorithms, but considerably faster.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.826.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--826 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.826 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.826" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.826/>A New Representation for Span-based CCG Parsing<span class=acl-fixed-case>CCG</span> Parsing</a></strong><br><a href=/people/y/yoshihide-kato/>Yoshihide Kato</a>
|
<a href=/people/s/shigeki-matsubara/>Shigeki Matsubara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--826><div class="card-body p-3 small">This paper proposes a new <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a> for CCG derivations. CCG derivations are represented as trees whose nodes are labeled with categories strictly restricted by CCG rule schemata. This characteristic is not suitable for span-based parsing models because they predict node labels independently. In other words, span-based models may generate invalid CCG derivations that violate the rule schemata. Our proposed representation decomposes CCG derivations into several independent pieces and prevents the span-based parsing models from violating the schemata. Our experimental result shows that an off-the-shelf span-based parser with our representation is comparable with previous CCG parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.828.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--828 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.828 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.828" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.828/>PermuteFormer : Efficient Relative Position Encoding for Long Sequences<span class=acl-fixed-case>P</span>ermute<span class=acl-fixed-case>F</span>ormer: Efficient Relative Position Encoding for Long Sequences</a></strong><br><a href=/people/p/peng-chen/>Peng Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--828><div class="card-body p-3 small">A recent variation of <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a>, <a href=https://en.wikipedia.org/wiki/Performer>Performer</a>, scales <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a> to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to <a href=https://en.wikipedia.org/wiki/Performer_(disambiguation)>Performer</a>. Based on the analysis, we propose PermuteFormer, a Performer-based model with relative position encoding that scales linearly on long sequences. PermuteFormer applies position-dependent transformation on queries and keys to encode positional information into the attention module. This <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformation</a> is carefully crafted so that the final output of self-attention is not affected by absolute positions of tokens. PermuteFormer introduces negligible <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>computational overhead</a> by design that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> runs as fast as Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long sequences, as well as WikiText-103, a language modeling dataset. The experiments show that PermuteFormer uniformly improves the performance of Performer with almost no <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>computational overhead</a> and outperforms vanilla Transformer on most of the tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.829.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--829 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.829 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.829" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.829/>Block Pruning For Faster Transformers</a></strong><br><a href=/people/f/francois-lagunas/>François Lagunas</a>
|
<a href=/people/e/ella-charlaix/>Ella Charlaix</a>
|
<a href=/people/v/victor-sanh/>Victor Sanh</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--829><div class="card-body p-3 small">Pre-training has improved <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>model accuracy</a> for both classification and generation tasks at the cost of introducing much larger and slower <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine-tuning. We find that this approach learns to prune out full components of the underlying <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, such as attention heads. Experiments consider classification and generation tasks, yielding among other results a pruned model that is a 2.4x faster, 74 % smaller BERT on SQuAD v1, with a 1 % drop on F1, competitive both with distilled models in speed and pruned models in size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.831.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--831 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.831 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.831" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.831/>How to Train BERT with an Academic Budget<span class=acl-fixed-case>BERT</span> with an Academic Budget</a></strong><br><a href=/people/p/peter-izsak/>Peter Izsak</a>
|
<a href=/people/m/moshe-berchansky/>Moshe Berchansky</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--831><div class="card-body p-3 small">While large language models a la BERT are used ubiquitously in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, pretraining them is considered a luxury that only a few well-funded industry labs can afford. How can one train such <a href=https://en.wikipedia.org/wiki/Physical_model>models</a> with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a single low-end deep learning server. We demonstrate that through a combination of software optimizations, design choices, and hyperparameter tuning, it is possible to produce models that are competitive with BERT-base on GLUE tasks at a fraction of the original pretraining cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.834.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--834 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.834 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.834" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.834/>Pushing on Text Readability Assessment : A Transformer Meets Handcrafted Linguistic Features</a></strong><br><a href=/people/b/bruce-w-lee/>Bruce W. Lee</a>
|
<a href=/people/y/yoo-sung-jang/>Yoo Sung Jang</a>
|
<a href=/people/j/jason-lee/>Jason Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--834><div class="card-body p-3 small">We report two essential improvements in readability assessment : 1. three novel features in advanced semantics and 2. the timely evidence that traditional ML models (e.g. Random Forest, using handcrafted features) can combine with <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a> (e.g. RoBERTa) to augment <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> performance. First, we explore suitable <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a> and traditional ML models. Then, we extract 255 handcrafted linguistic features using self-developed extraction software. Finally, we assemble those to create several hybrid models, achieving state-of-the-art (SOTA) accuracy on popular <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in readability assessment. The use of handcrafted features help model performance on smaller datasets. Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification accuracy of 99 %, a 20.3 % increase from the previous SOTA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.837.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--837 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.837 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.837" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.837/>MTAdam : Automatic Balancing of Multiple Training Loss Terms<span class=acl-fixed-case>MTA</span>dam: Automatic Balancing of Multiple Training Loss Terms</a></strong><br><a href=/people/i/itzik-malkiel/>Itzik Malkiel</a>
|
<a href=/people/l/lior-wolf/>Lior Wolf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--837><div class="card-body p-3 small">When training neural models, it is common to combine multiple loss terms. The balancing of these <a href=https://en.wikipedia.org/wiki/Term_(logic)>terms</a> requires considerable human effort and is computationally demanding. Moreover, the optimal trade-off between the <a href=https://en.wikipedia.org/wiki/Loss_function>loss terms</a> can change as training progresses, e.g., for <a href=https://en.wikipedia.org/wiki/Loss_function>adversarial terms</a>. In this work, we generalize the Adam optimization algorithm to handle multiple loss terms. The guiding principle is that for every layer, the gradient magnitude of the terms should be balanced. To this end, the Multi-Term Adam (MTAdam) computes the <a href=https://en.wikipedia.org/wiki/Derivative>derivative</a> of each loss term separately, infers the first and second moments per parameter and loss term, and calculates a first moment for the magnitude per layer of the gradients arising from each loss. This magnitude is used to continuously balance the <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a> across all layers, in a manner that both varies from one layer to the next and dynamically changes over time. Our results show that training with the new method leads to fast recovery from suboptimal initial loss weighting and to training outcomes that match or improve conventional training with the prescribed hyperparameters of each method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.839.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--839 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.839 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.839" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.839/>Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning</a></strong><br><a href=/people/x/xinghua-zhang/>Xinghua Zhang</a>
|
<a href=/people/b/bowen-yu/>Bowen Yu</a>
|
<a href=/people/t/tingwen-liu/>Tingwen Liu</a>
|
<a href=/people/z/zhenyu-zhang/>Zhenyu Zhang</a>
|
<a href=/people/j/jiawei-sheng/>Jiawei Sheng</a>
|
<a href=/people/x/xue-mengge/>Xue Mengge</a>
|
<a href=/people/h/hongbo-xu/>Hongbo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--839><div class="card-body p-3 small">Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotations, while most prior denoising works are only concerned with one kind of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and fail to fully explore useful information in the training set. To address this issue, we propose a robust learning paradigm named Self-Collaborative Denoising Learning (SCDL), which jointly trains two teacher-student networks in a mutually-beneficial manner to iteratively perform noisy label refinery. Each network is designed to exploit reliable labels via self denoising, and two networks communicate with each other to explore unreliable annotations by collaborative denoising. Extensive experimental results on five real-world datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.842.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--842 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.842 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.842.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.842" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.842/>VeeAlign : Multifaceted Context Representation Using Dual Attention for Ontology Alignment<span class=acl-fixed-case>V</span>ee<span class=acl-fixed-case>A</span>lign: Multifaceted Context Representation Using Dual Attention for Ontology Alignment</a></strong><br><a href=/people/v/vivek-iyer/>Vivek Iyer</a>
|
<a href=/people/a/arvind-agarwal/>Arvind Agarwal</a>
|
<a href=/people/h/harshit-kumar/>Harshit Kumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--842><div class="card-body p-3 small">Ontology Alignment is an important research problem applied to various fields such as <a href=https://en.wikipedia.org/wiki/Data_integration>data integration</a>, <a href=https://en.wikipedia.org/wiki/Data_transmission>data transfer</a>, <a href=https://en.wikipedia.org/wiki/Data_preparation>data preparation</a>, etc. State-of-the-art (SOTA) Ontology Alignment systems typically use naive domain-dependent approaches with handcrafted rules or domain-specific architectures, making them unscalable and inefficient. In this work, we propose VeeAlign, a Deep Learning based model that uses a novel dual-attention mechanism to compute the contextualized representation of a concept which, in turn, is used to discover alignments. By doing this, not only is our approach able to exploit both syntactic and semantic information encoded in <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontologies</a>, it is also, by design, flexible and scalable to different domains with minimal effort. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on four different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from different domains and languages, and establish its superiority through these results as well as detailed ablation studies. The code and datasets used are available at https://github.com/Remorax/VeeAlign.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.844.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--844 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.844 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.844" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.844/>GeneSis : A Generative Approach to Substitutes in Context<span class=acl-fixed-case>G</span>ene<span class=acl-fixed-case>S</span>is: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>G</span>enerative <span class=acl-fixed-case>A</span>pproach to <span class=acl-fixed-case>S</span>ubstitutes in <span class=acl-fixed-case>C</span>ontext</a></strong><br><a href=/people/c/caterina-lacerra/>Caterina Lacerra</a>
|
<a href=/people/r/rocco-tripodi/>Rocco Tripodi</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--844><div class="card-body p-3 small">The lexical substitution task aims at generating a list of suitable replacements for a target word in context, ideally keeping the meaning of the modified text unchanged. While its usage has increased in recent years, the paucity of annotated data prevents the finetuning of neural models on the task, hindering the full fruition of recently introduced powerful architectures such as <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. Furthermore, <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> is usually evaluated in a framework that is strictly bound to a limited vocabulary, making it impossible to credit appropriate, but out-of-vocabulary, substitutes. To assess these issues, we proposed GeneSis (Generating Substitutes in contexts), the first generative approach to <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a>. Thanks to a seq2seq model, we generate substitutes for a word according to the context it appears in, attaining state-of-the-art results on different <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>. Moreover, our approach allows silver data to be produced for further improving the performances of lexical substitution systems. Along with an extensive analysis of GeneSis results, we also present a human evaluation of the generated substitutes in order to assess their quality. We release the fine-tuned models, the generated datasets, and the code to reproduce the experiments at https://github.com/SapienzaNLP/genesis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.847.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--847 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.847 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.847/>Detecting Contact-Induced Semantic Shifts : What Can Embedding-Based Methods Do in Practice?<span class=acl-fixed-case>W</span>hat Can Embedding-Based Methods Do in Practice?</a></strong><br><a href=/people/f/filip-miletic/>Filip Miletic</a>
|
<a href=/people/a/anne-przewozny-desriaux/>Anne Przewozny-Desriaux</a>
|
<a href=/people/l/ludovic-tanguy/>Ludovic Tanguy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--847><div class="card-body p-3 small">This study investigates the applicability of semantic change detection methods in descriptively oriented linguistic research. It specifically focuses on contact-induced semantic shifts in <a href=https://en.wikipedia.org/wiki/Quebec_English>Quebec English</a>. We contrast synchronic data from different regions in order to identify the meanings that are specific to <a href=https://en.wikipedia.org/wiki/Quebec>Quebec</a> and potentially related to <a href=https://en.wikipedia.org/wiki/Language_contact>language contact</a>. Type-level embeddings are used to detect new semantic shifts, and token-level embeddings to isolate regionally specific occurrences. We introduce a new 80-item test set and conduct both quantitative and qualitative evaluations. We demonstrate that diachronic word embedding methods can be applied to contact-induced semantic shifts observed in <a href=https://en.wikipedia.org/wiki/Synchrony_and_diachrony>synchrony</a>, obtaining results comparable to the state of the art on similar tasks in <a href=https://en.wikipedia.org/wiki/Diachrony>diachrony</a>. However, we show that encouraging evaluation results do not translate to practical value in detecting new semantic shifts. Finally, our application of token-level embeddings accelerates manual data exploration and provides an efficient way of scaling up sociolinguistic analyses.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>