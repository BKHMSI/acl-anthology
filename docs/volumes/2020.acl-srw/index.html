<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2020.acl-srw.pdf>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></h2><p class=lead><a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>,
<a href=/people/j/jiangming-liu/>Jiangming Liu</a>,
<a href=/people/y/yizhong-wang/>Yizhong Wang</a>,
<a href=/people/r/rotem-dror/>Rotem Dror</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.acl-srw</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venue:</dt><dd><a href=/venues/acl/>ACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.acl-srw>https://aclanthology.org/2020.acl-srw</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.acl-srw.pdf>https://aclanthology.org/2020.acl-srw.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.acl-srw.pdf title="Open PDF of 'Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+58th+Annual+Meeting+of+the+Association+for+Computational+Linguistics%3A+Student+Research+Workshop" title="Search for 'Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.acl-srw.0/>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></strong><br><a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/j/jiangming-liu/>Jiangming Liu</a>
|
<a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/r/rotem-dror/>Rotem Dror</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928637 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-srw.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.1/>Adaptive Transformers for Learning Multimodal Representations</a></strong><br><a href=/people/p/prajjwal-bhargava/>Prajjwal Bhargava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--1><div class="card-body p-3 small">The usage of <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a> has grown from learning about <a href=https://en.wikipedia.org/wiki/Semantics>language semantics</a> to forming meaningful visiolinguistic representations. These <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> are often over-parametrized, requiring large amounts of computation. In this work, we extend adaptive approaches to learn more about <a href=https://en.wikipedia.org/wiki/Interpretability>model interpretability</a> and computational efficiency. Specifically, we study <a href=https://en.wikipedia.org/wiki/Attention_span>attention spans</a>, sparse, and structured dropout methods to help understand how their <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> extends for vision and language tasks. We further show that these approaches can help us learn more about how the <a href=https://en.wikipedia.org/wiki/Neural_network>network</a> perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928640 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.3/>Unsupervised Paraphasia Classification in <a href=https://en.wikipedia.org/wiki/Aphasia>Aphasic Speech</a></a></strong><br><a href=/people/s/sharan-pai/>Sharan Pai</a>
|
<a href=/people/n/nikhil-sachdeva/>Nikhil Sachdeva</a>
|
<a href=/people/p/prince-sachdeva/>Prince Sachdeva</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--3><div class="card-body p-3 small">Aphasia is a <a href=https://en.wikipedia.org/wiki/Speech-language_pathology>speech and language disorder</a> which results from <a href=https://en.wikipedia.org/wiki/Brain_damage>brain damage</a>, often characterized by word retrieval deficit (anomia) resulting in naming errors (paraphasia). Automatic paraphasia detection has many benefits for both treatment and diagnosis of Aphasia and its type. But supervised learning methods ca nt be properly utilized as there is a lack of aphasic speech data. In this paper, we describe our novel <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> which can be implemented without the need for labeled paraphasia data. Our evaluations show that our method outperforms previous work based on <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> and transfer learning approaches for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We demonstrate the utility of our method as an essential first step in developing augmentative and alternative communication (AAC) devices for patients suffering from <a href=https://en.wikipedia.org/wiki/Aphasia>aphasia</a> in any language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928636 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.5/>Grammatical Error Correction Using Pseudo Learner Corpus Considering Learner’s Error Tendency</a></strong><br><a href=/people/y/yujin-takahashi/>Yujin Takahashi</a>
|
<a href=/people/s/satoru-katsumata/>Satoru Katsumata</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--5><div class="card-body p-3 small">Recently, several studies have focused on improving the performance of grammatical error correction (GEC) tasks using pseudo data. However, a large amount of pseudo data are required to train an accurate GEC model. To address the limitations of language and computational resources, we assume that introducing pseudo errors into sentences similar to those written by the language learners is more efficient, rather than incorporating random pseudo errors into monolingual data. In this regard, we study the effect of pseudo data on GEC task performance using two approaches. First, we extract sentences that are similar to the learners&#8217; sentences from monolingual data. Second, we generate realistic pseudo errors by considering error types that learners often make. Based on our comparative results, we observe that <a href=https://en.wikipedia.org/wiki/F-number>F0.5 scores</a> for the Russian GEC task are significantly improved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928639 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.12/>Media Bias, the <a href=https://en.wikipedia.org/wiki/Social_science>Social Sciences</a>, and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> : Automating Frame Analyses to Identify Bias by Word Choice and Labeling<span class=acl-fixed-case>NLP</span>: Automating Frame Analyses to Identify Bias by Word Choice and Labeling</a></strong><br><a href=/people/f/felix-hamborg/>Felix Hamborg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--12><div class="card-body p-3 small">Media bias can strongly impact the public perception of topics reported in the news. A difficult to detect, yet powerful form of slanted news coverage is called bias by word choice and labeling (WCL). WCL bias can occur, for example, when journalists refer to the same semantic concept by using different terms that frame the concept differently and consequently may lead to different assessments by readers, such as the terms <a href=https://en.wikipedia.org/wiki/Resistance_movement>freedom fighters</a> and <a href=https://en.wikipedia.org/wiki/Terrorism>terrorists</a>, or <a href=https://en.wikipedia.org/wiki/Gun_politics_in_the_United_States>gun rights</a> and <a href=https://en.wikipedia.org/wiki/Gun_control>gun control</a>. In this research project, I aim to devise methods that identify instances of WCL bias and estimate the frames they induce, e.g., not only is terrorists of negative polarity but also ascribes to <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a> and <a href=https://en.wikipedia.org/wiki/Fear>fear</a>. To achieve this, I plan to research methods using <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> while employing models and using analysis concepts from the <a href=https://en.wikipedia.org/wiki/Social_science>social sciences</a>, where researchers have studied <a href=https://en.wikipedia.org/wiki/Media_bias>media bias</a> for decades. The first results indicate the effectiveness of this <a href=https://en.wikipedia.org/wiki/Interdisciplinarity>interdisciplinary research approach</a>. My vision is to devise a system that helps news readers to become aware of the differences in media coverage caused by bias.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928649 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.13/>SCAR : Sentence Compression using Autoencoders for Reconstruction<span class=acl-fixed-case>SCAR</span>: Sentence Compression using Autoencoders for Reconstruction</a></strong><br><a href=/people/c/chanakya-malireddy/>Chanakya Malireddy</a>
|
<a href=/people/t/tirth-maniar/>Tirth Maniar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--13><div class="card-body p-3 small">Sentence compression is the task of shortening a sentence while retaining its meaning. Most methods proposed for this task rely on labeled or paired corpora (containing pairs of verbose and compressed sentences), which is often expensive to collect. To overcome this limitation, we present a novel unsupervised deep learning framework (SCAR) for deletion-based sentence compression. SCAR is primarily composed of two encoder-decoder pairs : a <a href=https://en.wikipedia.org/wiki/Dynamic_range_compression>compressor</a> and a <a href=https://en.wikipedia.org/wiki/Reconstructor>reconstructor</a>. The <a href=https://en.wikipedia.org/wiki/Dynamic_range_compression>compressor</a> masks the input, and the <a href=https://en.wikipedia.org/wiki/Reconstructor>reconstructor</a> tries to regenerate it. The <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is entirely trained on unlabeled data and does not require additional inputs such as explicit syntactic information or optimal compression length. SCAR&#8217;s merit lies in the novel Linkage Loss function, which correlates the compressor and its effect on reconstruction, guiding it to drop inferable tokens. SCAR achieves higher ROUGE scores on benchmark datasets than the existing state-of-the-art methods and baselines. We also conduct a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> to demonstrate the application of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> as a text highlighting system. Using our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to underscore salient information facilitates speed-reading and reduces the time required to skim a document.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928652 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.15/>Multi-Task Neural Model for Agglutinative Language Translation</a></strong><br><a href=/people/y/yirong-pan/>Yirong Pan</a>
|
<a href=/people/x/xiao-li/>Xiao Li</a>
|
<a href=/people/y/yating-yang/>Yating Yang</a>
|
<a href=/people/r/rui-dong/>Rui Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--15><div class="card-body p-3 small">Neural machine translation (NMT) has achieved impressive performance recently by using large-scale parallel corpora. However, it struggles in the low-resource and morphologically-rich scenarios of agglutinative language translation task. Inspired by the finding that monolingual data can greatly improve the NMT performance, we propose a multi-task neural model that jointly learns to perform <a href=https://en.wikipedia.org/wiki/Bi-directional_translation>bi-directional translation</a> and <a href=https://en.wikipedia.org/wiki/Stemming>agglutinative language stemming</a>. Our approach employs the shared encoder and decoder to train a single model without changing the standard NMT architecture but instead adding a token before each source-side sentence to specify the desired target outputs of the two different tasks. Experimental results on Turkish-English and Uyghur-Chinese show that our proposed approach can significantly improve the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance on agglutinative languages by using a small amount of monolingual data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928655 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.18/>Why is penguin more similar to <a href=https://en.wikipedia.org/wiki/Polar_bear>polar bear</a> than to <a href=https://en.wikipedia.org/wiki/Sea_gull>sea gull</a>? Analyzing conceptual knowledge in distributional models</a></strong><br><a href=/people/p/pia-sommerauer/>Pia Sommerauer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--18><div class="card-body p-3 small">What do powerful <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> of word mean- ing created from <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional data</a> (e.g. Word2vec (Mikolov et al., 2013) BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018)) represent? What causes words to be similar in the <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a>? What type of information is lacking? This thesis proposal presents a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for investigating the information encoded in distributional semantic models. Several <a href=https://en.wikipedia.org/wiki/Mathematical_analysis>analysis methods</a> have been suggested, but they have been shown to be limited and are not well understood. This approach pairs observations made on actual corpora with insights obtained from data manipulation experiments. The expected outcome is a better understanding of (1) the semantic information we can infer purely based on linguistic co-occurrence patterns and (2) the potential of distributional semantic models to pick up linguistic evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.acl-srw.22.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.acl-srw.22.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928675 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.22/>Efficient <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> for Low-Resource Languages via Exploiting Related Languages</a></strong><br><a href=/people/v/vikrant-goyal/>Vikrant Goyal</a>
|
<a href=/people/s/sourav-kumar/>Sourav Kumar</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Misra Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--22><div class="card-body p-3 small">A large percentage of the world&#8217;s population speaks a language of the Indian subcontinent, comprising languages from both Indo-Aryan (e.g. Hindi, Punjabi, <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a>, etc.) and Dravidian (e.g. Tamil, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, etc.) families. A universal characteristic of <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian languages</a> is their <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>complex morphology</a>, which, when combined with the general lack of sufficient quantities of high-quality parallel data, can make developing <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT) systems</a> for these languages difficult. Neural Machine Translation (NMT) is a rapidly advancing MT paradigm and has shown promising results for many language pairs, especially in large training data scenarios. Since the condition of large parallel corpora is not met for Indian-English language pairs, we present our efforts towards building efficient NMT systems between <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian languages</a> (specifically Indo-Aryan languages) and English via efficiently exploiting parallel data from the related languages. We propose a technique called Unified Transliteration and Subword Segmentation to leverage language similarity while exploiting parallel data from related language pairs. We also propose a Multilingual Transfer Learning technique to leverage parallel data from multiple related languages to assist <a href=https://en.wikipedia.org/wiki/Translation>translation</a> for low resource language pair of interest. Our experiments demonstrate an overall average improvement of 5 BLEU points over the standard Transformer-based NMT baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928664 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.24/>Crossing the Line : Where do Demographic Variables Fit into Humor Detection?</a></strong><br><a href=/people/j/j-a-meaney/>J. A. Meaney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--24><div class="card-body p-3 small">Recent <a href=https://en.wikipedia.org/wiki/Humour>humor classification shared tasks</a> have struggled with two issues : either the data comprises a highly constrained genre of humor which does not broadly represent <a href=https://en.wikipedia.org/wiki/Humour>humor</a>, or the data is so indiscriminate that the inter-annotator agreement on its <a href=https://en.wikipedia.org/wiki/Humour>humor content</a> is drastically low. These tasks typically average over all annotators&#8217; judgments, in spite of the fact that <a href=https://en.wikipedia.org/wiki/Humour>humor</a> is a highly subjective phenomenon. We argue that <a href=https://en.wikipedia.org/wiki/Demography>demographic factors</a> influence whether a text is perceived as humorous or not. We propose the addition of <a href=https://en.wikipedia.org/wiki/Demography>demographic information</a> about the humor annotators in order to bin ratings more sensibly. We also suggest the addition of an &#8216;offensive&#8217; label to distinguish between different generations, in terms of <a href=https://en.wikipedia.org/wiki/Humour>humor</a>. This would allow for more nuanced shared tasks and could lead to better performance on downstream <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>, such as <a href=https://en.wikipedia.org/wiki/Content-control_software>content moderation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928654 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.27/>uBLEU : Uncertainty-Aware Automatic Evaluation Method for Open-Domain Dialogue Systems<span class=acl-fixed-case>BLEU</span>: Uncertainty-Aware Automatic Evaluation Method for Open-Domain Dialogue Systems</a></strong><br><a href=/people/y/yuma-tsuta/>Yuma Tsuta</a>
|
<a href=/people/n/naoki-yoshinaga/>Naoki Yoshinaga</a>
|
<a href=/people/m/masashi-toyoda/>Masashi Toyoda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--27><div class="card-body p-3 small">Because open-domain dialogues allow diverse responses, basic reference-based metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> do not work well unless we prepare a massive reference set of high-quality responses for input utterances. To reduce this burden, a human-aided, uncertainty-aware metric, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, has been proposed ; it embeds human judgment on the quality of reference outputs into the computation of multiple-reference BLEU. In this study, we instead propose a fully automatic, uncertainty-aware evaluation method for open-domain dialogue systems, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. This method first collects diverse reference responses from massive dialogue data and then annotates their quality judgments by using a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> trained on automatically collected training data. Experimental results on massive Twitter data confirmed that <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> is comparable to <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> in terms of its correlation with human judgment and that the state of the art automatic evaluation method, RUBER, is improved by integrating <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928673 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-srw.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.30/>Embeddings of Label Components for Sequence Labeling : A Case Study of Fine-grained Named Entity Recognition</a></strong><br><a href=/people/t/takuma-kato/>Takuma Kato</a>
|
<a href=/people/k/kaori-abe/>Kaori Abe</a>
|
<a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/s/shumpei-miyawaki/>Shumpei Miyawaki</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--30><div class="card-body p-3 small">In general, the labels used in <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> consist of different types of elements. For example, IOB-format entity labels, such as B-Person and I-Person, can be decomposed into span (B and I) and type information (Person). However, while most sequence labeling models do not consider such label components, the shared components across labels, such as <a href=https://en.wikipedia.org/wiki/Person_(disambiguation)>Person</a>, can be beneficial for label prediction. In this work, we propose to integrate label component information as <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> into <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Through experiments on English and Japanese fine-grained named entity recognition, we demonstrate that the proposed method improves performance, especially for instances with low-frequency labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.acl-srw.38.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928632 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.38/>Checkpoint Reranking : An Approach to Select Better Hypothesis for Neural Machine Translation Systems</a></strong><br><a href=/people/v/vinay-pandramish/>Vinay Pandramish</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Misra Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--38><div class="card-body p-3 small">In this paper, we propose a method of <a href=https://en.wikipedia.org/wiki/Ranking>re-ranking</a> the outputs of Neural Machine Translation (NMT) systems. After the decoding process, we select a few last iteration outputs in the training process as the N-best list. After training a Neural Machine Translation (NMT) baseline system, it has been observed that these iteration outputs have an oracle score higher than <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> up to 1.01 BLEU points compared to the last iteration of the trained system. We come up with a ranking mechanism by solely focusing on the decoder&#8217;s ability to generate distinct tokens and without the usage of any <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> or data. With this <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, we achieved a translation improvement up to +0.16 BLEU points over baseline. We also evaluate our approach by applying the coverage penalty to the training process. In cases of moderate coverage penalty, the oracle scores are higher than the final iteration up to +0.99 BLEU points, and our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> gives an improvement up to +0.17 BLEU points. With excessive penalty, there is a decrease in translation quality compared to the baseline system. Still, an increase in <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle scores</a> up to +1.30 is observed with the re-ranking algorithm giving an improvement up to +0.15 BLEU points is found in case of excessive penalty. The proposed re-ranking method is a generic one and can be extended to other language pairs as well.<tex-math>N</tex-math>-best list. After training a Neural Machine Translation (NMT) baseline system, it has been observed that these iteration outputs have an oracle score higher than baseline up to 1.01 BLEU points compared to the last iteration of the trained system.We come up with a ranking mechanism by solely focusing on the decoder&#8217;s ability to generate distinct tokens and without the usage of any language model or data. With this method, we achieved a translation improvement up to +0.16 BLEU points over baseline.We also evaluate our approach by applying the coverage penalty to the training process.In cases of moderate coverage penalty, the oracle scores are higher than the final iteration up to +0.99 BLEU points, and our algorithm gives an improvement up to +0.17 BLEU points.With excessive penalty, there is a decrease in translation quality compared to the baseline system. Still, an increase in oracle scores up to +1.30 is observed with the re-ranking algorithm giving an improvement up to +0.15 BLEU points is found in case of excessive penalty.The proposed re-ranking method is a generic one and can be extended to other language pairs as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928661 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-srw.39" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.39/>Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup</a></strong><br><a href=/people/j/jishnu-ray-chowdhury/>Jishnu Ray Chowdhury</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a>
|
<a href=/people/d/doina-caragea/>Doina Caragea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--39><div class="card-body p-3 small">Distinguishing informative and actionable messages from a <a href=https://en.wikipedia.org/wiki/Social_media>social media platform</a> like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> is critical for facilitating <a href=https://en.wikipedia.org/wiki/Emergency_management>disaster management</a>. For this purpose, we compile a multilingual dataset of over 130 K samples for multi-label classification of disaster-related tweets. We present a masking-based loss function for partially labelled samples and demonstrate the effectiveness of Manifold Mixup in the text domain. Our main <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is based on Multilingual BERT, which we further improve with Manifold Mixup. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generalizes to unseen disasters in the test set. Furthermore, we analyze the capability of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for zero-shot generalization to new languages. Our code, dataset, and other resources are available on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928672 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.41/>Exploring the Role of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>Context</a> to Distinguish Rhetorical and Information-Seeking Questions</a></strong><br><a href=/people/y/yuan-zhuang/>Yuan Zhuang</a>
|
<a href=/people/e/ellen-riloff/>Ellen Riloff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--41><div class="card-body p-3 small">Social media posts often contain questions, but many of the questions are rhetorical and do not seek information. Our work studies the problem of distinguishing rhetorical and information-seeking questions on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Most work has focused on <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> of the question itself, but we hypothesize that the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>prior context</a> plays a role too. This paper introduces a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> containing questions in tweets paired with their prior tweets to provide context. We create classification models to assess the difficulty of distinguishing rhetorical and information-seeking questions, and experiment with different properties of the prior context. Our results show that the prior tweet and topic features can improve performance on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>