<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</h2><p class=lead><a href=/people/s/steffen-eger/>Steffen Eger</a>,
<a href=/people/y/yang-gao/>Yang Gao</a>,
<a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>,
<a href=/people/w/wei-zhao/>Wei Zhao</a>,
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.eval4nlp-1</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/eval4nlp/>Eval4NLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.eval4nlp-1>https://aclanthology.org/2020.eval4nlp-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+First+Workshop+on+Evaluation+and+Comparison+of+NLP+Systems" title="Search for 'Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.0/>Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.eval4nlp-1.3.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939718 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.3/>Item Response Theory for Efficient Human Evaluation of Chatbots</a></strong><br><a href=/people/j/joao-sedoc/>Jo√£o Sedoc</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--3><div class="card-body p-3 small">Conversational agent quality is currently assessed using human evaluation, and often requires an exorbitant number of comparisons to achieve <a href=https://en.wikipedia.org/wiki/Statistical_significance>statistical significance</a>. In this paper, we introduce Item Response Theory (IRT) for chatbot evaluation, using a paired comparison in which annotators judge which system responds better to the next turn of a conversation. IRT is widely used in educational testing for simultaneously assessing the ability of test takers and the quality of test questions. It is similarly well suited for chatbot evaluation since <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> allows the assessment of both <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and the prompts used to evaluate them. We use IRT to efficiently assess <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>, and show that different examples from the evaluation set are better suited for comparing high-quality (nearer to human performance) than low-quality systems. Finally, we use IRT to reduce the number of evaluation examples assessed by human annotators while retaining discriminative power.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939719 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.eval4nlp-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.4/>ViLBERTScore : Evaluating Image Caption Using Vision-and-Language BERT<span class=acl-fixed-case>V</span>i<span class=acl-fixed-case>LBERTS</span>core: Evaluating Image Caption Using Vision-and-Language <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/h/hwanhee-lee/>Hwanhee Lee</a>
|
<a href=/people/s/seunghyun-yoon/>Seunghyun Yoon</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/d/doo-soon-kim/>Doo Soon Kim</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--4><div class="card-body p-3 small">In this paper, we propose an evaluation metric for image captioning systems using both image and text information. Unlike the previous methods that rely on textual representations in evaluating the caption, our approach uses visiolinguistic representations. The proposed method generates image-conditioned embeddings for each token using ViLBERT from both generated and reference texts. Then, these contextual embeddings from each of the two sentence-pair are compared to compute the <a href=https://en.wikipedia.org/wiki/Similarity_score>similarity score</a>. Experimental results on three benchmark datasets show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> correlates significantly better with human judgments than all existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939709 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.5/>BLEU Neighbors : A Reference-less Approach to Automatic Evaluation<span class=acl-fixed-case>BLEU</span> Neighbors: A Reference-less Approach to Automatic Evaluation</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a>
|
<a href=/people/d/dorsa-sadigh/>Dorsa Sadigh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--5><div class="card-body p-3 small">Evaluation is a bottleneck in the development of natural language generation (NLG) models. Automatic metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> rely on references, but for tasks such as open-ended generation, there are no references to draw upon. Although <a href=https://en.wikipedia.org/wiki/Language>language diversity</a> can be estimated using statistical measures such as <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a>, measuring language quality requires <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a>. However, because human evaluation at scale is slow and expensive, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is used sparingly ; <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can not be used to rapidly iterate on NLG models, in the way <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> is used for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. To this end, we propose BLEU Neighbors, a nearest neighbors model for estimating language quality by using the BLEU score as a <a href=https://en.wikipedia.org/wiki/Positive-definite_kernel>kernel function</a>. On existing datasets for chitchat dialogue and open-ended sentence generation, we find that on average the quality estimation from a BLEU Neighbors model has a lower <a href=https://en.wikipedia.org/wiki/Mean_squared_error>mean squared error</a> and higher <a href=https://en.wikipedia.org/wiki/Spearman_correlation>Spearman correlation</a> with the ground truth than individual human annotators. Despite its simplicity, BLEU Neighbors even outperforms state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on automatically grading essays, including <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that have access to a gold-standard reference essay.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939707 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.8/>Artemis : A Novel Annotation Methodology for Indicative Single Document Summarization</a></strong><br><a href=/people/r/rahul-jha/>Rahul Jha</a>
|
<a href=/people/k/keping-bi/>Keping Bi</a>
|
<a href=/people/y/yang-li/>Yang Li</a>
|
<a href=/people/m/mahdi-pakdaman/>Mahdi Pakdaman</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/i/ivan-zhiboedov/>Ivan Zhiboedov</a>
|
<a href=/people/k/kieran-mcdonald/>Kieran McDonald</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--8><div class="card-body p-3 small">We describe Artemis (Annotation methodology for Rich, Tractable, Extractive, Multi-domain, Indicative Summarization), a novel hierarchical annotation process that produces indicative summaries for documents from multiple domains. Current summarization evaluation datasets are single-domain and focused on a few domains for which naturally occurring summaries can be easily found, such as news and scientific articles. These are not sufficient for training and evaluation of summarization models for use in <a href=https://en.wikipedia.org/wiki/Document_management_system>document management</a> and <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval systems</a>, which need to deal with documents from multiple domains. Compared to other annotation methods such as Relative Utility and Pyramid, Artemis is more tractable because judges do n&#8217;t need to look at all the sentences in a document when making an importance judgment for one of the sentences, while providing similarly rich sentence importance annotations. We describe the annotation process in detail and compare <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> with other similar evaluation systems. We also present analysis and experimental results over a sample set of 532 annotated documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939710 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.9/>Probabilistic Extension of Precision, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a>, and <a href=https://en.wikipedia.org/wiki/F-number>F1 Score</a> for More Thorough Evaluation of Classification Models</a></strong><br><a href=/people/r/reda-yacouby/>Reda Yacouby</a>
|
<a href=/people/d/dustin-axman/>Dustin Axman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--9><div class="card-body p-3 small">In pursuit of the perfect supervised NLP classifier, razor thin margins and low-resource test sets can make modeling decisions difficult. Popular metrics such as <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>Accuracy</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>Precision</a>, and Recall are often insufficient as they fail to give a complete picture of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s behavior. We present a probabilistic extension of Precision, Recall, and F1 score, which we refer to as confidence-Precision (cPrecision), confidence-Recall (cRecall), and confidence-F1 (cF1) respectively. The proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> address some of the challenges faced when evaluating large-scale NLP systems, specifically when the model&#8217;s confidence score assignments have an impact on the system&#8217;s behavior. We describe four key benefits of our proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> as compared to their threshold-based counterparts. Two of these benefits, which we refer to as robustness to missing values and sensitivity to model confidence score assignments are self-evident from the metrics&#8217; definitions ; the remaining benefits, generalization, and functional consistency are demonstrated empirically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939720 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.14/>On Aligning OpenIE Extractions with Knowledge Bases : A Case Study<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>IE</span> Extractions with Knowledge Bases: A Case Study</a></strong><br><a href=/people/k/kiril-gashteovski/>Kiril Gashteovski</a>
|
<a href=/people/r/rainer-gemulla/>Rainer Gemulla</a>
|
<a href=/people/b/bhushan-kotnis/>Bhushan Kotnis</a>
|
<a href=/people/s/sven-hertling/>Sven Hertling</a>
|
<a href=/people/c/christian-meilicke/>Christian Meilicke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--14><div class="card-body p-3 small">Open information extraction (OIE) is the task of extracting relations and their corresponding arguments from a natural language text in un- supervised manner. Outputs of such systems are used for downstream tasks such as <a href=https://en.wikipedia.org/wiki/Question_answering>ques- tion answering</a> and automatic knowledge base (KB) construction. Many of these downstream tasks rely on aligning OIE triples with refer- ence KBs. Such alignments are usually eval- uated w.r.t. a specific downstream task and, to date, no direct manual evaluation of such alignments has been performed. In this paper, we directly evaluate how OIE triples from the OPIEC corpus are related to the DBpedia KB w.r.t. information content. First, we investigate OPIEC triples and <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia facts</a> having the same arguments by comparing the information on the OIE surface relation with the KB rela- tion. Second, we evaluate the expressibility of general OPIEC triples in <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a>. We in- vestigate whetherand, if so, howa given OIE triple can be mapped to a single KB fact. We found that such mappings are not always possible because the information in the OIE triples tends to be more specific. Our evalua- tion suggests, however, that significant part of OIE triples can be expressed by means of KB formulas instead of individual facts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939708 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.eval4nlp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.15/>ClusterDataSplit : Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation<span class=acl-fixed-case>C</span>luster<span class=acl-fixed-case>D</span>ata<span class=acl-fixed-case>S</span>plit: Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation</a></strong><br><a href=/people/h/hanna-wecker/>Hanna Wecker</a>
|
<a href=/people/a/annemarie-friedrich/>Annemarie Friedrich</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--15><div class="card-body p-3 small">This paper adds to the ongoing discussion in the natural language processing community on how to choose a good development set. Motivated by the real-life necessity of applying <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> to different <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>data distributions</a>, we propose a clustering-based data splitting algorithm. It creates development (or test) sets which are lexically different from the training data while ensuring similar label distributions. Hence, we are able to create challenging cross-validation evaluation setups while abstracting away from performance differences resulting from label distribution shifts between training and test data. In addition, we present a <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python-based tool</a> for analyzing and visualizing data split characteristics and model performance. We illustrate the workings and results of our approach using a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and a patent classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939713 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.16/>Best Practices for Crowd-based Evaluation of German Summarization : Comparing Crowd, Expert and Automatic Evaluation<span class=acl-fixed-case>G</span>erman Summarization: Comparing Crowd, Expert and Automatic Evaluation</a></strong><br><a href=/people/n/neslihan-iskender/>Neslihan Iskender</a>
|
<a href=/people/t/tim-polzehl/>Tim Polzehl</a>
|
<a href=/people/s/sebastian-moller/>Sebastian M√∂ller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--16><div class="card-body p-3 small">One of the main challenges in the development of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization tools</a> is <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization quality evaluation</a>. On the one hand, the human assessment of summarization quality conducted by <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic experts</a> is slow, expensive, and still not a standardized procedure. On the other hand, the automatic assessment metrics are reported not to correlate high enough with <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human quality ratings</a>. As a solution, we propose <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> as a fast, scalable, and cost-effective alternative to expert evaluations to assess the intrinsic and extrinsic quality of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> by comparing crowd ratings with expert ratings and automatic metrics such as ROUGE, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, or BertScore on a German summarization data set. Our results provide a basis for best practices for crowd-based summarization evaluation regarding major influential factors such as the best annotation aggregation method, the influence of readability and reading effort on summarization evaluation, and the optimal number of crowd workers to achieve comparable results to experts, especially when determining factors such as overall quality, grammaticality, referential clarity, focus, structure & coherence, summary usefulness, and summary informativeness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939712 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.17/>Evaluating Word Embeddings on Low-Resource Languages</a></strong><br><a href=/people/n/nathan-stringham/>Nathan Stringham</a>
|
<a href=/people/m/mike-izbicki/>Mike Izbicki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--17><div class="card-body p-3 small">The analogy task introduced by Mikolov et al. (2013) has become the standard metric for tuning the hyperparameters of word embedding models. In this paper, however, we argue that the analogy task is unsuitable for low-resource languages for two reasons : (1) it requires that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> be trained on large amounts of text, and (2) analogies may not be well-defined in some low-resource settings. We solve these problems by introducing the OddOneOut and Topk tasks, which are specifically designed for <a href=https://en.wikipedia.org/wiki/Model_selection>model selection</a> in the low-resource setting. We use these metrics to successfully tune hyperparameters for a low-resource emoji embedding task and word embeddings on 16 extinct languages. The largest of these languages (Ancient Hebrew) has a 41 million token dataset, and the smallest (Old Gujarati) has only a 1813 token dataset.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>