<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Computational Linguistics, Volume 47, Issue 3 - November 2021 - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Computational Linguistics, Volume 47, Issue 3 - November 2021</h2><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.cl-3</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Cambridge, MA</dd><dt>Venue:</dt><dd><a href=/venues/cl/>CL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>MIT Press</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.cl-3>https://aclanthology.org/2021.cl-3</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Computational+Linguistics%2C+Volume+47%2C+Issue+3+-+November+2021" title="Search for 'Computational Linguistics, Volume 47, Issue 3 - November 2021' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-3.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-3--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-3.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-3.16/>The Taxonomy of Writing Systems : How to Measure How Logographic a System Is</a></strong><br><a href=/people/r/richard-sproat/>Richard Sproat</a>
|
<a href=/people/a/alexander-gutkin/>Alexander Gutkin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-3--16><div class="card-body p-3 small">Taxonomies of writing systems since Gelb (1952) have classified systems based on what the written symbols represent : if they represent words or morphemes, they are logographic ; if syllables, syllabic ; if segments, alphabetic ; and so forth. Sproat (2000) and Rogers (2005) broke with tradition by splitting the logographic and phonographic aspects into two dimensions, with <a href=https://en.wikipedia.org/wiki/Logography>logography</a> being graded rather than a categorical distinction. A system could be syllabic, and highly logographic ; or alphabetic, and mostly non-logographic. This accords better with how <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a> actually work, but neither author proposed a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for measuring <a href=https://en.wikipedia.org/wiki/Logography>logography</a>. In this article we propose a novel measure of the degree of <a href=https://en.wikipedia.org/wiki/Logography>logography</a> that uses an attention-based sequence-to-sequence model trained to predict the spelling of a token from its pronunciation in context. In an ideal <a href=https://en.wikipedia.org/wiki/Phonogram_(linguistics)>phonographic system</a>, the model should need to attend to only the current token in order to compute how to spell it, and this would show in the attention matrix activations. In contrast, with a <a href=https://en.wikipedia.org/wiki/Logogram>logographic system</a>, where a given <a href=https://en.wikipedia.org/wiki/Pronunciation>pronunciation</a> might correspond to several different spellings, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> would need to attend to a broader context. The ratio of the activation outside the token and the total activation forms the basis of our <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a>. We compare this with a simple lexical measure, and an entropic measure, as well as several other neural models, and argue that on balance our attention-based measure accords best with intuition about how logographic various systems are.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-3.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-3--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-3.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-3.20/>Decoding Word Embeddings with Brain-Based Semantic Features</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-3--20><div class="card-body p-3 small">Word embeddings are vectorial semantic representations built with either counting or predicting techniques aimed at capturing shades of meaning from word co-occurrences. Since their introduction, these <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> have been criticized for lacking interpretable dimensions. This property of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> limits our understanding of the <a href=https://en.wikipedia.org/wiki/Semantic_feature>semantic features</a> they actually encode. Moreover, it contributes to the black box nature of the tasks in which they are used, since the reasons for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> performance often remain opaque to humans. In this contribution, we explore the semantic properties encoded in word embeddings by mapping them onto interpretable vectors, consisting of explicit and neurobiologically motivated semantic features (Binder et al. Our exploration takes into account different types of embeddings, including factorized count vectors and predict models (Skip-Gram, GloVe, etc.), as well as the most recent contextualized representations (i.e., ELMo and BERT). In our analysis, we first evaluate the quality of the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> in a retrieval task, then we shed light on the semantic features that are better encoded in each embedding type. A large number of probing tasks is finally set to assess how the original and the mapped embeddings perform in discriminating semantic categories. For each probing task, we identify the most relevant semantic features and we show that there is a correlation between the <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> performance and how they encode those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. This study sets itself as a step forward in understanding which aspects of meaning are captured by <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a>, by proposing a new and simple method to carve human-interpretable semantic representations from <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional vectors</a>.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>