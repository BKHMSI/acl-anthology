<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W19-54.pdf>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></h2><p class=lead><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>,
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>,
<a href=/people/c/christian-federmann/>Christian Federmann</a>,
<a href=/people/m/mark-fishel/>Mark Fishel</a>,
<a href=/people/y/yvette-graham/>Yvette Graham</a>,
<a href=/people/b/barry-haddow/>Barry Haddow</a>,
<a href=/people/m/matthias-huck/>Matthias Huck</a>,
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>,
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>,
<a href=/people/a/andre-f-t-martins/>André Martins</a>,
<a href=/people/c/christof-monz/>Christof Monz</a>,
<a href=/people/m/matteo-negri/>Matteo Negri</a>,
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>,
<a href=/people/m/mariana-neves/>Mariana Neves</a>,
<a href=/people/m/matt-post/>Matt Post</a>,
<a href=/people/m/marco-turchi/>Marco Turchi</a>,
<a href=/people/k/karin-verspoor/>Karin Verspoor</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W19-54</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Florence, Italy</dd><dt>Venues:</dt><dd><a href=/venues/acl/>ACL</a>
| <a href=/venues/wmt/>WMT</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigmt/>SIGMT</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W19-54>https://aclanthology.org/W19-54</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W19-54.pdf>https://aclanthology.org/W19-54.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W19-54.pdf title="Open PDF of 'Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Fourth+Conference+on+Machine+Translation+%28Volume+3%3A+Shared+Task+Papers%2C+Day+2%29" title="Search for 'Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5400/>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5404/>Findings of the WMT 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions<span class=acl-fixed-case>WMT</span> 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions</a></strong><br><a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5404><div class="card-body p-3 small">Following the WMT 2018 Shared Task on Parallel Corpus Filtering, we posed the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting 2 % and 10 % of the highest-quality data to be used to train machine translation systems. This year, the task tackled the low resource condition of <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-English</a> and Sinhala-English. Eleven participants from companies, national research labs, and universities participated in this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5405/>RTM Stacking Results for Machine Translation Performance Prediction<span class=acl-fixed-case>RTM</span> Stacking Results for Machine Translation Performance Prediction</a></strong><br><a href=/people/e/ergun-bicici/>Ergun Biçici</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5405><div class="card-body p-3 small">We obtain new results using referential translation machines with increased number of learning models in the set of results that are stacked to obtain a better mixture of experts prediction. We combine <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted from the word-level predictions with the sentence- or document-level features, which significantly improve the results on the training sets but decrease the test set results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5407/>QE BERT : Bilingual BERT Using Multi-task Learning for Neural Quality Estimation<span class=acl-fixed-case>QE</span> <span class=acl-fixed-case>BERT</span>: Bilingual <span class=acl-fixed-case>BERT</span> Using Multi-task Learning for Neural Quality Estimation</a></strong><br><a href=/people/h/hyun-kim/>Hyun Kim</a>
|
<a href=/people/j/joon-ho-lim/>Joon-Ho Lim</a>
|
<a href=/people/h/hyun-ki-kim/>Hyun-Ki Kim</a>
|
<a href=/people/s/seung-hoon-na/>Seung-Hoon Na</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5407><div class="card-body p-3 small">For translation quality estimation at word and sentence levels, this paper presents a novel approach based on BERT that recently has achieved impressive results on various natural language processing tasks. Our proposed model is re-purposed BERT for the translation quality estimation and uses <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> for the sentence-level task and word-level subtasks (i.e., source word, target word, and target gap). Experimental results on Quality Estimation shared task of WMT19 show that our systems show competitive results and provide significant improvements over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5408/>MIPT System for World-Level Quality Estimation<span class=acl-fixed-case>MIPT</span> System for World-Level Quality Estimation</a></strong><br><a href=/people/m/mikhail-mosyagin/>Mikhail Mosyagin</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5408><div class="card-body p-3 small">We explore different model architectures for the WMT 19 shared task on word-level quality estimation of automatic translation. We start with a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> similar to Shef-bRNN, which we modify by using conditional random fields for sequence labelling. Additionally, we use a different approach for labelling gaps and source words. We further develop this model by including features from different sources such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, baseline features for the task and transformer encoders. We evaluate the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the English-German dataset for the corresponding <a href=https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5409/>NJU Submissions for the WMT19 Quality Estimation Shared Task<span class=acl-fixed-case>NJU</span> Submissions for the <span class=acl-fixed-case>WMT</span>19 Quality Estimation Shared Task</a></strong><br><a href=/people/h/hou-qi/>Hou Qi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5409><div class="card-body p-3 small">In this paper, we describe the submissions of the team from Nanjing University for the WMT19 sentence-level Quality Estimation (QE) shared task on English-German language pair. We develop two approaches based on a two-stage neural QE model consisting of a <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extractor</a> and a quality estimator. More specifically, one of the proposed approaches employs the translation knowledge between the two languages from two different translation directions ; while the other one employs extra monolingual knowledge from both source and target sides, obtained by pre-training deep self-attention networks. To efficiently train these two-stage models, a joint learning training method is applied. Experiments show that the ensemble model of the above two models achieves the best results on the benchmark dataset of the WMT17 sentence-level QE shared task and obtains competitive results in WMT19, ranking 3rd out of 10 submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5410 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5410/>Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings</a></strong><br><a href=/people/e/elizaveta-yankovskaya/>Elizaveta Yankovskaya</a>
|
<a href=/people/a/andre-tattar/>Andre Tättar</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5410><div class="card-body p-3 small">We propose the use of pre-trained embeddings as features of a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> for sentence-level quality estimation of machine translation. In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model. In the second proposed method we use as an input features not only pre-trained embeddings, but also <a href=https://en.wikipedia.org/wiki/Log_probability>log probability</a> of any <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT) system</a>. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5411/>SOURCE : SOURce-Conditional Elmo-style Model for Machine Translation Quality Estimation<span class=acl-fixed-case>SOURCE</span>: <span class=acl-fixed-case>SOUR</span>ce-Conditional Elmo-style Model for Machine Translation Quality Estimation</a></strong><br><a href=/people/j/junpei-zhou/>Junpei Zhou</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/z/zecong-hu/>Zecong Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5411><div class="card-body p-3 small">Quality estimation (QE) of machine translation (MT) systems is a task of growing importance. It reduces the cost of <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>, allowing machine-translated text to be used in formal occasions. In this work, we describe our submission system in WMT 2019 sentence-level QE task. We mainly explore the utilization of pre-trained translation models in <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a> and adopt a bi-directional translation-like strategy. The <a href=https://en.wikipedia.org/wiki/Strategy_(game_theory)>strategy</a> is similar to ELMo, but additionally conditions on source sentences. Experiments on WMT QE dataset show that our <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a>, which makes the pre-training slightly harder, can bring improvements for <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a>. In WMT-2019 QE task, our system ranked in the second place on En-De NMT dataset and the third place on En-Ru NMT dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5416/>Effort-Aware Neural Automatic Post-Editing</a></strong><br><a href=/people/a/amirhossein-tebbifakhr/>Amirhossein Tebbifakhr</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5416><div class="card-body p-3 small">For this round of the WMT 2019 APE shared task, our submission focuses on addressing the over-correction problem in <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE</a>. Over-correction occurs when the APE system tends to rephrase an already correct MT output, and the resulting sentence is penalized by a reference-based evaluation against human post-edits. Our intuition is that this problem can be prevented by informing the <a href=https://en.wikipedia.org/wiki/System>system</a> about the predicted quality of the MT output or, in other terms, the expected amount of needed corrections. For this purpose, following the common approach in multilingual NMT, we prepend a special token to the beginning of both the source text and the MT output indicating the required amount of <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>. Following the best submissions to the WMT 2018 APE shared task, our backbone architecture is based on multi-source Transformer to encode both the MT output and the corresponding source text. We participated both in the English-German and English-Russian subtasks. In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and -0.47 TER. In the second subtask, where the higher quality of the MT output increases the risk of over-correction, none of our submitted runs was able to improve the MT output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5417 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5417.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5417/>UdS Submission for the WMT 19 Automatic Post-Editing Task<span class=acl-fixed-case>U</span>d<span class=acl-fixed-case>S</span> Submission for the <span class=acl-fixed-case>WMT</span> 19 Automatic Post-Editing Task</a></strong><br><a href=/people/h/hongfei-xu/>Hongfei Xu</a>
|
<a href=/people/q/qiuhui-liu/>Qiuhui Liu</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5417><div class="card-body p-3 small">In this paper, we describe our submission to the English-German APE shared task at WMT 2019. We utilize and adapt an NMT architecture originally developed for exploiting context information to <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE</a>, implement this in our own transformer model and explore joint training of the <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE task</a> with a de-noising encoder.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5418/>Terminology-Aware Segmentation and Domain Feature for the WMT19 Biomedical Translation Task<span class=acl-fixed-case>WMT</span>19 Biomedical Translation Task</a></strong><br><a href=/people/c/casimiro-pio-carrino/>Casimiro Pio Carrino</a>
|
<a href=/people/b/bardia-rafieian/>Bardia Rafieian</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5418><div class="card-body p-3 small">In this work, we give a description of the TALP-UPC systems submitted for the WMT19 Biomedical Translation Task. Our proposed strategy is NMT model-independent and relies only on one ingredient, a biomedical terminology list. We first extracted such a terminology list by labelling biomedical words in our training dataset using the BabelNet API. Then, we designed a data preparation strategy to insert the <a href=https://en.wikipedia.org/wiki/Term_(logic)>terms information</a> at a token level. Finally, we trained the Transformer model with this terms-informed data. Our best-submitted system ranked 2nd and 3rd for Spanish-English and English-Spanish translation directions, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5420/>Huawei’s NMT Systems for the WMT 2019 Biomedical Translation Task<span class=acl-fixed-case>NMT</span> Systems for the <span class=acl-fixed-case>WMT</span> 2019 Biomedical Translation Task</a></strong><br><a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/j/jianfeng-liu/>Jianfeng Liu</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5420><div class="card-body p-3 small">This paper describes Huawei&#8217;s neural machine translation systems for the WMT 2019 biomedical translation shared task. We trained and fine-tuned our systems on a combination of out-of-domain and in-domain parallel corpora for six translation directions covering EnglishChinese, EnglishFrench and EnglishGerman language pairs. Our submitted systems achieve the best BLEU scores on EnglishFrench and EnglishGerman language pairs according to the official evaluation results. In the EnglishChinese translation task, our <a href=https://en.wikipedia.org/wiki/System>systems</a> are in the second place. The enhanced performance is attributed to more in-domain training and more sophisticated <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> developed. Development of translation models and transfer learning (or domain adaptation) methods has significantly contributed to the progress of the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5421/>UCAM Biomedical Translation at WMT19 : Transfer Learning Multi-domain Ensembles<span class=acl-fixed-case>UCAM</span> Biomedical Translation at <span class=acl-fixed-case>WMT</span>19: Transfer Learning Multi-domain Ensembles</a></strong><br><a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5421><div class="card-body p-3 small">The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experimented with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of <a href=https://en.wikipedia.org/wiki/English_language>English-Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5425 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5425/>Machine Translation from an Intercomprehension Perspective</a></strong><br><a href=/people/y/yu-chen/>Yu Chen</a>
|
<a href=/people/t/tania-avgustinova/>Tania Avgustinova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5425><div class="card-body p-3 small">Within the first shared task on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> between similar languages, we present our first attempts on Czech to Polish machine translation from an intercomprehension perspective. We propose methods based on the <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>mutual intelligibility</a> of the two languages, taking advantage of their orthographic and phonological similarity, in the hope to improve over our baselines. The <a href=https://en.wikipedia.org/wiki/Translation>translation</a> results are evaluated using <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. On this <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, none of our <a href=https://en.wikipedia.org/wiki/Proposal_(business)>proposals</a> could outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on the final test set. The current setups are rather preliminary, and there are several potential improvements we can try in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5426 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5426/>Utilizing Monolingual Data in NMT for Similar Languages : Submission to Similar Language Translation Task<span class=acl-fixed-case>NMT</span> for Similar Languages: Submission to Similar Language Translation Task</a></strong><br><a href=/people/j/jyotsana-khatri/>Jyotsana Khatri</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5426><div class="card-body p-3 small">This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi-Nepali direction in which we have examined the performance of a RNN based NMT system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5427 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5427/>Neural Machine Translation : Hindi-Nepali<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>N</span>epali</a></strong><br><a href=/people/s/sahinur-rahman-laskar/>Sahinur Rahman Laskar</a>
|
<a href=/people/p/partha-pakray/>Partha Pakray</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5427><div class="card-body p-3 small">With the extensive use of Machine Translation (MT) technology, there is progressively interest in directly translating between pairs of similar languages. Because the main challenge is to overcome the limitation of available <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> to produce a precise MT output. Current work relies on the Neural Machine Translation (NMT) with attention mechanism for the similar language translation of WMT19 shared task in the context of Hindi-Nepali pair. The NMT systems trained the Hindi-Nepali parallel corpus and tested, analyzed in Hindi Nepali translation. The official result declared at WMT19 shared task, which shows that our NMT system obtained Bilingual Evaluation Understudy (BLEU) score 24.6 for primary configuration in Nepali to Hindi translation. Also, we have achieved <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a> 53.7 (Hindi to Nepali) and 49.1 (Nepali to Hindi) in contrastive system type.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5429/>Panlingua-KMI MT System for Similar Language Translation Task at WMT 2019<span class=acl-fixed-case>KMI</span> <span class=acl-fixed-case>MT</span> System for Similar Language Translation Task at <span class=acl-fixed-case>WMT</span> 2019</a></strong><br><a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/akanksha-bansal/>Akanksha Bansal</a>
|
<a href=/people/p/priya-rani/>Priya Rani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5429><div class="card-body p-3 small">The present paper enumerates the development of Panlingua-KMI Machine Translation (MT) systems for Hindi Nepali language pair, designed as part of the Similar Language Translation Task at the WMT 2019 Shared Task. The Panlingua-KMI team conducted a series of experiments to explore both the phrase-based statistical (PBSMT) and neural methods (NMT). Among the 11 MT systems prepared under this task, 6 PBSMT systems were prepared for <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-Hindi</a>, 1 PBSMT for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi-Nepali</a> and 2 NMT systems were developed for <a href=https://en.wikipedia.org/wiki/Nepali_language>NepaliHindi</a>. The results show that PBSMT could be an effective method for developing MT systems for <a href=https://en.wikipedia.org/wiki/Lingua_franca>closely-related languages</a>. Our Hindi-Nepali PBSMT system was ranked 2nd among the 13 systems submitted for the pair and our Nepali-Hindi PBSMTsystem was ranked 4th among the 12 systems submitted for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5430.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5430 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5430 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5430/>UDSDFKI Submission to the WMT2019 CzechPolish Similar Language Translation Shared Task<span class=acl-fixed-case>UDS</span>–<span class=acl-fixed-case>DFKI</span> Submission to the <span class=acl-fixed-case>WMT</span>2019 <span class=acl-fixed-case>C</span>zech–<span class=acl-fixed-case>P</span>olish Similar Language Translation Shared Task</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5430><div class="card-body p-3 small">In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages : <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> and <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and Nepali, and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Participants could choose to participate in any of these three tracks and submit system outputs in any translation direction. We report the results obtained by our <a href=https://en.wikipedia.org/wiki/System>system</a> in translating from <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and comment on the impact of out-of-domain test data in the performance of our <a href=https://en.wikipedia.org/wiki/System>system</a>. UDS-DFKI achieved competitive performance ranking second among ten teams in <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to Polish translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5431/>Neural Machine Translation of Low-Resource and Similar Languages with Backtranslation</a></strong><br><a href=/people/m/michael-przystupa/>Michael Przystupa</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5431><div class="card-body p-3 small">We present our contribution to the WMT19 Similar Language Translation shared task. We investigate the utility of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> on three low-resource, similar language pairs : <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech Polish</a>, and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi Nepali</a>. Since state-of-the-art neural machine translation systems still require large amounts of bitext, which we do not have for the pairs we consider, we focus primarily on incorporating monolingual data into our models with backtranslation. In our analysis, we found Transformer models to work best on Spanish Portuguese and Czech Polish translation, whereas LSTMs with global attention worked best on Hindi Nepali translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5433 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5433.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5433/>Dual Monolingual Cross-Entropy Delta Filtering of Noisy Parallel Data</a></strong><br><a href=/people/a/amittai-axelrod/>Amittai Axelrod</a>
|
<a href=/people/a/anish-kumar/>Anish Kumar</a>
|
<a href=/people/s/steve-sloto/>Steve Sloto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5433><div class="card-body p-3 small">We introduce a purely monolingual approach to <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filtering</a> for parallel data from a noisy corpus in a low-resource scenario. Our work is inspired by Junczysdowmunt:2018, but we relax the requirements to allow for cases where no parallel data is available. Our primary contribution is a dual monolingual cross-entropy delta criterion modified from Cynical data selection Axelrod:2017, and is competitive (within 1.8 BLEU) with the best bilingual filtering method when used to train SMT systems. Our approach is featherweight, and runs end-to-end on a standard laptop in three hours.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5434 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5434/>NRC Parallel Corpus Filtering System for WMT 2019<span class=acl-fixed-case>NRC</span> Parallel Corpus Filtering System for <span class=acl-fixed-case>WMT</span> 2019</a></strong><br><a href=/people/g/gabriel-bernier-colborne/>Gabriel Bernier-Colborne</a>
|
<a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5434><div class="card-body p-3 small">We describe the National Research Council Canada team&#8217;s submissions to the parallel corpus filtering task at the Fourth Conference on <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5436 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5436/>Quality and Coverage : The AFRL Submission to the WMT19 Parallel Corpus Filtering for Low-Resource Conditions Task<span class=acl-fixed-case>AFRL</span> Submission to the <span class=acl-fixed-case>WMT</span>19 Parallel Corpus Filtering for Low-Resource Conditions Task</a></strong><br><a href=/people/g/grant-erdmann/>Grant Erdmann</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5436><div class="card-body p-3 small">The WMT19 Parallel Corpus Filtering For Low-Resource Conditions Task aims to test various methods of filtering a noisy parallel corpora, to make them useful for training machine translation systems. This year the noisy corpora are the relatively low-resource language pairs of <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-English</a> and Sinhala-English. This papers describes the Air Force Research Laboratory (AFRL) submissions, including preprocessing methods and scoring metrics. Numerical results indicate a benefit over <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> and the relative benefits of different options.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5437 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5437/>Webinterpret Submission to the WMT2019 Shared Task on Parallel Corpus Filtering<span class=acl-fixed-case>WMT</span>2019 Shared Task on Parallel Corpus Filtering</a></strong><br><a href=/people/j/jesus-gonzalez-rubio/>Jesús González-Rubio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5437><div class="card-body p-3 small">This document describes the participation of Webinterpret in the shared task on parallel corpus filtering at the Fourth Conference on Machine Translation (WMT 2019). Here, we describe the main characteristics of our approach and discuss the results obtained on the data sets published for the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5439 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5439/>Filtering of Noisy Parallel Corpora Based on Hypothesis Generation</a></strong><br><a href=/people/z/zuzanna-parcheta/>Zuzanna Parcheta</a>
|
<a href=/people/g/german-sanchis-trilles/>Germán Sanchis-Trilles</a>
|
<a href=/people/f/francisco-casacuberta/>Francisco Casacuberta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5439><div class="card-body p-3 small">The filtering task of noisy parallel corpora in WMT2019 aims to challenge participants to create filtering methods to be useful for training machine translation systems. In this work, we introduce a noisy parallel corpora filtering system based on generating hypotheses by means of a translation model. We train translation models in both language pairs : NepaliEnglish and SinhalaEnglish using provided parallel corpora. We select the training subset for three language pairs (Nepali, <a href=https://en.wikipedia.org/wiki/Sinhala_language>Sinhala</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> to English) jointly using bilingual cross-entropy selection to create the best possible translation model for both language pairs. Once the translation models are trained, we translate the noisy corpora and generate a hypothesis for each sentence pair. We compute the smoothed BLEU score between the target sentence and generated hypothesis. In addition, we apply several rules to discard very noisy or inadequate sentences which can lower the translation score. These <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> are based on <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>, source and target similarity and source language detection. We compare our results with the baseline published on the shared task website, which uses the Zipporah model, over which we achieve significant improvements in one of the conditions in the shared task. The designed <a href=https://en.wikipedia.org/wiki/Filter_(software)>filtering system</a> is domain independent and all experiments are conducted using <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5441 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5441/>The University of Helsinki Submission to the WMT19 Parallel Corpus Filtering Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submission to the <span class=acl-fixed-case>WMT</span>19 Parallel Corpus Filtering Task</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5441><div class="card-body p-3 small">This paper describes the University of Helsinki Language Technology group&#8217;s participation in the WMT 2019 parallel corpus filtering task. Our scores were produced using a two-step strategy. First, we individually applied a series of <a href=https://en.wikipedia.org/wiki/Filter_(software)>filters</a> to remove the &#8216;bad&#8217; quality sentences. Then, we produced scores for each sentence by weighting these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with a <a href=https://en.wikipedia.org/wiki/Statistical_model>classification model</a>. This methodology allowed us to build a simple and reliable <a href=https://en.wikipedia.org/wiki/System>system</a> that is easily adaptable to other language pairs.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>