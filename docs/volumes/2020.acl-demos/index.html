<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2020.acl-demos.pdf>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></h2><p class=lead><a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>,
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.acl-demos</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venue:</dt><dd><a href=/venues/acl/>ACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.acl-demos>https://aclanthology.org/2020.acl-demos</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.acl-demos.pdf>https://aclanthology.org/2020.acl-demos.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.acl-demos.pdf title="Open PDF of 'Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+58th+Annual+Meeting+of+the+Association+for+Computational+Linguistics%3A+System+Demonstrations" title="Search for 'Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.acl-demos.0/>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></strong><br><a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928588 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.1/>Xiaomingbot : A Multilingual Robot News Reporter<span class=acl-fixed-case>X</span>iaomingbot: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>M</span>ultilingual <span class=acl-fixed-case>R</span>obot <span class=acl-fixed-case>N</span>ews <span class=acl-fixed-case>R</span>eporter</a></strong><br><a href=/people/r/runxin-xu/>Runxin Xu</a>
|
<a href=/people/j/jun-cao/>Jun Cao</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/j/jiaze-chen/>Jiaze Chen</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/y/ying-zeng/>Ying Zeng</a>
|
<a href=/people/y/yuping-wang/>Yuping Wang</a>
|
<a href=/people/l/li-chen/>Li Chen</a>
|
<a href=/people/x/xiang-yin/>Xiang Yin</a>
|
<a href=/people/x/xijin-zhang/>Xijin Zhang</a>
|
<a href=/people/s/songcheng-jiang/>Songcheng Jiang</a>
|
<a href=/people/y/yuxuan-wang/>Yuxuan Wang</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--1><div class="card-body p-3 small">This paper proposes the building of Xiaomingbot, an intelligent, multilingual and multimodal software robot equipped with four inte- gral capabilities : news generation, news translation, news reading and avatar animation. Its <a href=https://en.wikipedia.org/wiki/System>system</a> summarizes <a href=https://en.wikipedia.org/wiki/Media_of_China>Chinese news</a> that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> automatically generates from data tables. Next, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> translates the summary or the full article into multiple languages, and reads the multi- lingual rendition through <a href=https://en.wikipedia.org/wiki/Speech_synthesis>synthesized speech</a>. Notably, Xiaomingbot utilizes a voice cloning technology to synthesize the speech trained from a real person&#8217;s voice data in one input language. The proposed <a href=https://en.wikipedia.org/wiki/System>system</a> enjoys several merits : it has an <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>animated avatar</a>, and is able to generate and read multilingual news. Since <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> was put into practice, Xiaomingbot has written over 600,000 articles, and gained over 150,000 followers on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928594 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-demos.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.2/>TextBrewer : An Open-Source Knowledge Distillation Toolkit for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a><span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>B</span>rewer: <span class=acl-fixed-case>A</span>n <span class=acl-fixed-case>O</span>pen-<span class=acl-fixed-case>S</span>ource <span class=acl-fixed-case>K</span>nowledge <span class=acl-fixed-case>D</span>istillation <span class=acl-fixed-case>T</span>oolkit for <span class=acl-fixed-case>N</span>atural <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>P</span>rocessing</a></strong><br><a href=/people/z/ziqing-yang/>Ziqing Yang</a>
|
<a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/z/zhipeng-chen/>Zhipeng Chen</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--2><div class="card-body p-3 small">In this paper, we introduce TextBrewer, an open-source knowledge distillation toolkit designed for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. It works with different neural network models and supports various kinds of supervised learning tasks, such as <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>, <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. TextBrewer provides a simple and uniform workflow that enables quick setting up of <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> experiments with highly flexible configurations. It offers a set of predefined distillation methods and can be extended with custom code. As a case study, we use TextBrewer to distill BERT on several typical NLP tasks. With simple configurations, we achieve results that are comparable with or even higher than the public distilled BERT models with similar numbers of parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928604 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.6/>Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation<span class=acl-fixed-case>P</span>age<span class=acl-fixed-case>R</span>ank with Syntagmatic Information for Multilingual Word Sense Disambiguation</a></strong><br><a href=/people/f/federico-scozzafava/>Federico Scozzafava</a>
|
<a href=/people/m/marco-maru/>Marco Maru</a>
|
<a href=/people/f/fabrizio-brignone/>Fabrizio Brignone</a>
|
<a href=/people/g/giovanni-torrisi/>Giovanni Torrisi</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--6><div class="card-body p-3 small">Exploiting syntagmatic information is an encouraging research focus to be pursued in an effort to close the gap between knowledge-based and supervised Word Sense Disambiguation (WSD) performance. We follow this direction in our next-generation knowledge-based WSD system, SyntagRank, which we make available via a Web interface and a RESTful API. SyntagRank leverages the disambiguated pairs of co-occurring words included in SyntagNet, a lexical-semantic combination resource, to perform state-of-the-art knowledge-based WSD in a multilingual setting. Our <a href=https://en.wikipedia.org/wiki/Service_(systems_architecture)>service</a> provides both a user-friendly interface, available at http://syntagnet.org/, and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928620 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-demos.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.14/>Stanza : A Python Natural Language Processing Toolkit for Many Human Languages<span class=acl-fixed-case>S</span>tanza: A Python Natural Language Processing Toolkit for Many Human Languages</a></strong><br><a href=/people/p/peng-qi/>Peng Qi</a>
|
<a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/y/yuhui-zhang/>Yuhui Zhang</a>
|
<a href=/people/j/jason-bolton/>Jason Bolton</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--14><div class="card-body p-3 small">We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a>, multi-word token expansion, <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a>, part-of-speech and morphological feature tagging, dependency parsing, and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928595 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-demos.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.15/>jiant : A Software Toolkit for Research on General-Purpose Text Understanding Models</a></strong><br><a href=/people/y/yada-pruksachatkun/>Yada Pruksachatkun</a>
|
<a href=/people/p/phil-yeres/>Phil Yeres</a>
|
<a href=/people/h/haokun-liu/>Haokun Liu</a>
|
<a href=/people/j/jason-phang/>Jason Phang</a>
|
<a href=/people/p/phu-mon-htut/>Phu Mon Htut</a>
|
<a href=/people/a/alex-wang/>Alex Wang</a>
|
<a href=/people/i/ian-tenney/>Ian Tenney</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--15><div class="card-body p-3 small">We introduce <a href=https://en.wikipedia.org/wiki/Jiant>jiant</a>, an open source toolkit for conducting multitask and transfer learning experiments on English NLU tasks. jiant enables modular and configuration driven experimentation with state-of-the-art models and a broad set of tasks for probing, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, and multitask training experiments. jiant implements over 50 NLU tasks, including all GLUE and SuperGLUE benchmark tasks. We demonstrate that <a href=https://en.wikipedia.org/wiki/Jiant>jiant</a> reproduces published performance on a variety of <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> and models, e.g., RoBERTa and <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928611 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-demos.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.16/>The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding<span class=acl-fixed-case>M</span>icrosoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding</a></strong><br><a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/y/yu-wang/>Yu Wang</a>
|
<a href=/people/j/jianshu-ji/>Jianshu Ji</a>
|
<a href=/people/h/hao-cheng/>Hao Cheng</a>
|
<a href=/people/x/xueyun-zhu/>Xueyun Zhu</a>
|
<a href=/people/e/emmanuel-awa/>Emmanuel Awa</a>
|
<a href=/people/p/pengcheng-he/>Pengcheng He</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a>
|
<a href=/people/h/hoifung-poon/>Hoifung Poon</a>
|
<a href=/people/g/guihong-cao/>Guihong Cao</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--16><div class="card-body p-3 small">We present MT-DNN, an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models. Built upon <a href=https://en.wikipedia.org/wiki/PyTorch>PyTorch</a> and Transformers, MT-DNN is designed to facilitate rapid customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and <a href=https://en.wikipedia.org/wiki/Character_encoding>text encoders</a> (e.g., <a href=https://en.wikipedia.org/wiki/Random-access_memory>RNNs</a>, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is its built-in support for robust and transferable learning using the adversarial multi-task learning paradigm. To enable efficient production deployment, MT-DNN supports multi-task knowledge distillation, which can substantially compress a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural model</a> without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The software and pre-trained models will be publicly available at https://github.com/namisan/mt-dnn.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928597 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-demos.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.19/>ConvLab-2 : An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems<span class=acl-fixed-case>C</span>onv<span class=acl-fixed-case>L</span>ab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems</a></strong><br><a href=/people/q/qi-zhu/>Qi Zhu</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/y/yan-fang/>Yan Fang</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/r/ryuichi-takanobu/>Ryuichi Takanobu</a>
|
<a href=/people/j/jinchao-li/>Jinchao Li</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--19><div class="card-body p-3 small">We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab, ConvLab-2 inherits ConvLab&#8217;s framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an <a href=https://en.wikipedia.org/wiki/Analysis>analysis tool</a> and an <a href=https://en.wikipedia.org/wiki/Interactive_computing>interactive tool</a> to assist researchers in diagnosing <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and <a href=https://en.wikipedia.org/wiki/Systems_engineering>system improvement</a>. The interactive tool provides an <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a> that allows developers to diagnose an assembled dialogue system by interacting with the <a href=https://en.wikipedia.org/wiki/System>system</a> and modifying the output of each system component.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928609 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.21/>Label Noise in Context</a></strong><br><a href=/people/m/michael-desmond/>Michael Desmond</a>
|
<a href=/people/c/catherine-finegan-dollak/>Catherine Finegan-Dollak</a>
|
<a href=/people/j/jeff-boston/>Jeff Boston</a>
|
<a href=/people/m/matt-arnold/>Matt Arnold</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--21><div class="card-body p-3 small">Label noiseincorrectly or ambiguously labeled training examplescan negatively impact model performance. Although noise detection techniques have been around for decades, practitioners rarely apply them, as manual noise remediation is a tedious process. Examples incorrectly flagged as <a href=https://en.wikipedia.org/wiki/Noise_(electronics)>noise</a> waste reviewers&#8217; time, and correcting label noise without guidance can be difficult. We propose LNIC, a noise-detection method that uses an example&#8217;s neighborhood within the training set to (a) reduce <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positives</a> and (b) provide an explanation as to why the ex- ample was flagged as noise. We demonstrate on several short-text classification datasets that LNIC outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> on measures of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and <a href=https://en.wikipedia.org/wiki/F-number>F0.5-score</a>. We also show how LNIC&#8217;s training set context helps a reviewer to understand and correct label noise in a dataset. The LNIC tool lowers the barriers to label noise remediation, increasing its utility for NLP practitioners.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928624 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.24/>Photon : A Robust Cross-Domain Text-to-SQL System<span class=acl-fixed-case>P</span>hoton: A Robust Cross-Domain Text-to-<span class=acl-fixed-case>SQL</span> System</a></strong><br><a href=/people/j/jichuan-zeng/>Jichuan Zeng</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/s/steven-c-h-hoi/>Steven C.H. Hoi</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/m/michael-lyu/>Michael Lyu</a>
|
<a href=/people/i/irwin-king/>Irwin King</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--24><div class="card-body p-3 small">Natural language interfaces to databases(NLIDB) democratize end user access to <a href=https://en.wikipedia.org/wiki/Relational_model>relational data</a>. Due to fundamental differences between <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language communication</a> and <a href=https://en.wikipedia.org/wiki/Computer_programming>programming</a>, it is common for end users to issue questions that are ambiguous to the system or fall outside the semantic scope of its underlying <a href=https://en.wikipedia.org/wiki/Query_language>query language</a>. We present PHOTON, a robust, modular, cross-domain NLIDB that can flag natural language input to which a SQL mapping can not be immediately determined. PHOTON consists of a strong neural semantic parser (63.2 % structure accuracy on the Spider dev benchmark), a human-in-the-loop question corrector, a <a href=https://en.wikipedia.org/wiki/Executable>SQL executor</a> and a response generator. The question corrector isa discriminative neural sequence editor which detects confusion span(s) in the input question and suggests rephrasing until a translatable input is given by the user or a maximum number of iterations are conducted. Experiments on simulated data show that the proposed method effectively improves the robustness of text-to-SQL system against untranslatable user input. The live demo of our system is available at http://www.naturalsql.com</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928590 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.27/>NLP Scholar : An Interactive Visual Explorer for Natural Language Processing Literature<span class=acl-fixed-case>NLP</span> Scholar: An Interactive Visual Explorer for Natural Language Processing Literature</a></strong><br><a href=/people/s/saif-mohammad/>Saif M. Mohammad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--27><div class="card-body p-3 small">As part of the NLP Scholar project, we created a single unified dataset of NLP papers and their meta-information (including citation numbers), by extracting and aligning information from the <a href=https://en.wikipedia.org/wiki/ACL_Anthology>ACL Anthology</a> and <a href=https://en.wikipedia.org/wiki/Google_Scholar>Google Scholar</a>. In this paper, we describe several interconnected interactive visualizations (dashboards) that present various aspects of the data. Clicking on an item within a <a href=https://en.wikipedia.org/wiki/Data_visualization>visualization</a> or entering query terms in the search boxes filters the data in all visualizations in the <a href=https://en.wikipedia.org/wiki/Dashboard_(business)>dashboard</a>. This allows users to search for papers in the area of their interest, published within specific time periods, published by specified authors, etc. The interactive visualizations presented here, and the associated dataset of papers mapped to citations, have additional uses as well including understanding how the field is growing (both overall and across sub-areas), as well as quantifying the impact of different types of papers on subsequent publications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable Mention for Best Demonstration Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928622 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.32/>Prta : A System to Support the Analysis of Propaganda Techniques in the News<span class=acl-fixed-case>P</span>rta: A System to Support the Analysis of Propaganda Techniques in the News</a></strong><br><a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a>
|
<a href=/people/s/shaden-shaar/>Shaden Shaar</a>
|
<a href=/people/y/yifan-zhang/>Yifan Zhang</a>
|
<a href=/people/s/seunghak-yu/>Seunghak Yu</a>
|
<a href=/people/a/alberto-barron-cedeno/>Alberto Barrón-Cedeño</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--32><div class="card-body p-3 small">Recent events, such as the 2016 US Presidential Campaign, <a href=https://en.wikipedia.org/wiki/Brexit>Brexit</a> and the COVID-19 infodemic, have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a> and disinformation detection. However, little attention has been paid to the specific <a href=https://en.wikipedia.org/wiki/Rhetorical_techniques>rhetorical and psychological techniques</a> used to convey <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda messages</a>. Revealing the use of such techniques can help promote <a href=https://en.wikipedia.org/wiki/Media_literacy>media literacy</a> and <a href=https://en.wikipedia.org/wiki/Critical_thinking>critical thinking</a>, and eventually contribute to limiting the impact of fake news and <a href=https://en.wikipedia.org/wiki/Disinformation>disinformation campaigns</a>. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a>, and/or political orientation of the media. Moreover, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> allows users to analyze any text or URL through a dedicated interface or via an API. The <a href=https://en.wikipedia.org/wiki/System>system</a> is available online : https://www.tanbih.org/prta.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928587 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.37/>MMPE : A Multi-Modal Interface using Handwriting, Touch Reordering, and Speech Commands for Post-Editing Machine Translation<span class=acl-fixed-case>MMPE</span>: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>M</span>ulti-<span class=acl-fixed-case>M</span>odal <span class=acl-fixed-case>I</span>nterface using <span class=acl-fixed-case>H</span>andwriting, <span class=acl-fixed-case>T</span>ouch <span class=acl-fixed-case>R</span>eordering, and <span class=acl-fixed-case>S</span>peech <span class=acl-fixed-case>C</span>ommands for <span class=acl-fixed-case>P</span>ost-<span class=acl-fixed-case>E</span>diting <span class=acl-fixed-case>M</span>achine <span class=acl-fixed-case>T</span>ranslation</a></strong><br><a href=/people/n/nico-herbig/>Nico Herbig</a>
|
<a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/t/tim-duwel/>Tim Düwel</a>
|
<a href=/people/k/kalliopi-meladaki/>Kalliopi Meladaki</a>
|
<a href=/people/m/mahsa-monshizadeh/>Mahsa Monshizadeh</a>
|
<a href=/people/v/vladislav-hnatovskiy/>Vladislav Hnatovskiy</a>
|
<a href=/people/a/antonio-kruger/>Antonio Krüger</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--37><div class="card-body p-3 small">The shift from traditional <a href=https://en.wikipedia.org/wiki/Translation>translation</a> to post-editing (PE) of machine-translated (MT) text can save time and reduce errors, but it also affects the design of <a href=https://en.wikipedia.org/wiki/Translation>translation interfaces</a>, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Since this paradigm shift offers potential for modalities other than <a href=https://en.wikipedia.org/wiki/Computer_mouse>mouse</a> and <a href=https://en.wikipedia.org/wiki/Computer_keyboard>keyboard</a>, we present MMPE, the first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT. Users can directly cross out or hand-write new text, drag and drop words for reordering, or use spoken commands to update the text in place. All text manipulations are logged in an easily interpretable format to simplify subsequent translation process research. The results of an evaluation with professional translators suggest that pen and touch interaction are suitable for deletion and reordering tasks, while speech and multi-modal combinations of select & speech are considered suitable for replacements and insertions. Overall, experiment participants were enthusiastic about the new modalities and saw them as useful extensions to mouse & keyboard, but not as a complete substitute.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928605 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.39/>Conversation Learner-A Machine Teaching Tool for Building Dialog Managers for Task-Oriented Dialog Systems<span class=acl-fixed-case>C</span>onversation <span class=acl-fixed-case>L</span>earner - A Machine Teaching Tool for Building Dialog Managers for Task-Oriented Dialog Systems</a></strong><br><a href=/people/s/swadheen-shukla/>Swadheen Shukla</a>
|
<a href=/people/l/lars-liden/>Lars Liden</a>
|
<a href=/people/s/shahin-shayandeh/>Shahin Shayandeh</a>
|
<a href=/people/e/eslam-kamal/>Eslam Kamal</a>
|
<a href=/people/j/jinchao-li/>Jinchao Li</a>
|
<a href=/people/m/matt-mazzola/>Matt Mazzola</a>
|
<a href=/people/t/thomas-park/>Thomas Park</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--39><div class="card-body p-3 small">Traditionally, industry solutions for building a task-oriented dialog system have relied on helping dialog authors define rule-based dialog managers, represented as dialog flows. While dialog flows are intuitively interpretable and good for simple scenarios, they fall short of performance in terms of the flexibility needed to handle complex dialogs. On the other hand, purely machine-learned models can handle complex dialogs, but they are considered to be black boxes and require large amounts of training data. In this demonstration, we showcase Conversation Learner, a machine teaching tool for building dialog managers. It combines the best of both approaches by enabling dialog authors to create a dialog flow using familiar tools, converting the dialog flow into a parametric model (e.g., neural networks), and allowing dialog authors to improve the <a href=https://en.wikipedia.org/wiki/Dialog_manager>dialog manager</a> (i.e., the parametric model) over time by leveraging user-system dialog logs as training data through a machine teaching interface.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928607 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-demos.41" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.41/>SUPP.AI : finding evidence for supplement-drug interactions<span class=acl-fixed-case>SUPP</span>.<span class=acl-fixed-case>AI</span>: finding evidence for supplement-drug interactions</a></strong><br><a href=/people/l/lucy-wang/>Lucy Wang</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/s/sarthak-jain/>Sarthak Jain</a>
|
<a href=/people/s/sam-skjonsberg/>Sam Skjonsberg</a>
|
<a href=/people/c/carissa-schoenick/>Carissa Schoenick</a>
|
<a href=/people/n/nick-botner/>Nick Botner</a>
|
<a href=/people/w/waleed-ammar/>Waleed Ammar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--41><div class="card-body p-3 small">Dietary supplements are used by a large portion of the population, but information on their <a href=https://en.wikipedia.org/wiki/Drug_interaction>pharmacologic interactions</a> is incomplete. To address this challenge, we present SUPP.AI, an <a href=https://en.wikipedia.org/wiki/Application_software>application</a> for browsing evidence of supplement-drug interactions (SDIs) extracted from the <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical literature</a>. We train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to automatically extract <a href=https://en.wikipedia.org/wiki/Supplement_(publishing)>supplement information</a> and identify such <a href=https://en.wikipedia.org/wiki/Interaction_(statistics)>interactions</a> from the <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific literature</a>. To address the lack of labeled data for SDI identification, we use labels of the closely related task of identifying drug-drug interactions (DDIs) for supervision. We fine-tune the contextualized word representations of the RoBERTa language model using labeled DDI data, and apply the fine-tuned <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to identify supplement interactions. We extract 195k evidence sentences from 22 M articles (P=0.82, R=0.58, F1=0.68) for 60k interactions. We create the SUPP.AI application for users to search evidence sentences extracted by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. SUPP.AI is an attempt to close the information gap on <a href=https://en.wikipedia.org/wiki/Dietary_supplement>dietary supplements</a> by making up-to-date evidence on SDIs more discoverable for researchers, clinicians, and consumers. An informational video on how to use SUPP.AI is available at : https://youtu.be/dR0ucKdORwc</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>