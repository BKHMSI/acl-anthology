<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W18-54.pdf>Proceedings of the 2018 <span class=acl-fixed-case>EMNLP</span> Workshop <span class=acl-fixed-case>B</span>lackbox<span class=acl-fixed-case>NLP</span>: Analyzing and Interpreting Neural Networks for <span class=acl-fixed-case>NLP</span></a></h2><p class=lead><a href=/people/t/tal-linzen/>Tal Linzen</a>,
<a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a>,
<a href=/people/a/afra-alishahi/>Afra Alishahi</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W18-54</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Brussels, Belgium</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W18-54>https://aclanthology.org/W18-54</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W18-54.pdf>https://aclanthology.org/W18-54.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W18-54.pdf title="Open PDF of 'Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2018+EMNLP+Workshop+BlackboxNLP%3A+Analyzing+and+Interpreting+Neural+Networks+for+NLP" title="Search for 'Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5400/>Proceedings of the 2018 <span class=acl-fixed-case>EMNLP</span> Workshop <span class=acl-fixed-case>B</span>lackbox<span class=acl-fixed-case>NLP</span>: Analyzing and Interpreting Neural Networks for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a>
|
<a href=/people/a/afra-alishahi/>Afra Alishahi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5403/>Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures</a></strong><br><a href=/people/d/danilo-croce/>Danilo Croce</a>
|
<a href=/people/d/daniele-rossini/>Daniele Rossini</a>
|
<a href=/people/r/roberto-basili/>Roberto Basili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5403><div class="card-body p-3 small">Nonlinear methods such as <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> achieve state-of-the-art performances in several semantic NLP tasks. However epistemologically transparent decisions are not provided as for the limited interpretability of the underlying acquired neural models. In neural-based semantic inference tasks epistemological transparency corresponds to the ability of tracing back causal connections between the linguistic properties of a input instance and the produced classification output. In this paper, we propose the use of a methodology, called Layerwise Relevance Propagation, over linguistically motivated neural architectures, namely Kernel-based Deep Architectures (KDA), to guide argumentations and explanation inferences. In such a way, each decision provided by a KDA can be linked to real examples, linguistically related to the input instance : these can be used to motivate the network output. Quantitative analysis shows that richer explanations about the semantic and syntagmatic structures of the examples characterize more convincing arguments in two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, i.e. question classification and <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>.<i>Layerwise Relevance Propagation</i>, over linguistically motivated neural architectures, namely <i>Kernel-based Deep Architectures</i> (KDA), to guide argumentations and explanation inferences. In such a way, each decision provided by a KDA can be linked to real examples, linguistically related to the input instance: these can be used to motivate the network output. Quantitative analysis shows that richer explanations about the semantic and syntagmatic structures of the examples characterize more convincing arguments in two tasks, i.e. question classification and semantic role labeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5405/>Evaluating Textual Representations through Image Generation</a></strong><br><a href=/people/g/graham-spinks/>Graham Spinks</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5405><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for determining the quality of textual representations through the ability to generate <a href=https://en.wikipedia.org/wiki/Image>images</a> from them. Continuous representations of textual input are ubiquitous in modern Natural Language Processing techniques either at the core of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> or as the by-product at any given layer of a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. While current techniques to evaluate such representations focus on their performance on particular tasks, they do n&#8217;t provide a clear understanding of the level of informational detail that is stored within them, especially their ability to represent spatial information. The central premise of this paper is that visual inspection or analysis is the most convenient method to quickly and accurately determine information content. Through the use of text-to-image neural networks, we propose a new technique to compare the quality of textual representations by visualizing their information content. The method is illustrated on a <a href=https://en.wikipedia.org/wiki/Medical_imaging>medical dataset</a> where the correct representation of spatial information and <a href=https://en.wikipedia.org/wiki/Shorthand>shorthands</a> are of particular importance. For four different well-known textual representations, we show with a quantitative analysis that some <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> are consistently able to deliver higher quality visualizations of the information content. Additionally, we show that the quantitative analysis technique correlates with the judgment of a human expert evaluator in terms of alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5409/>Linguistic representations in multi-task neural networks for ellipsis resolution</a></strong><br><a href=/people/o/ola-ronning/>Ola Rønning</a>
|
<a href=/people/d/daniel-hardt/>Daniel Hardt</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5409><div class="card-body p-3 small">Sluicing resolution is the task of identifying the antecedent to a question ellipsis. Antecedents are often <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>sentential constituents</a>, and previous work has therefore relied on <a href=https://en.wikipedia.org/wiki/Parsing>syntactic parsing</a>, together with complex linguistic features. A recent model instead used partial parsing as an auxiliary task in sequential neural network architectures to inject <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>. We explore the linguistic information being brought to bear by such <a href=https://en.wikipedia.org/wiki/Social_network>networks</a>, both by defining subsets of the data exhibiting relevant linguistic characteristics, and by examining the internal representations of the <a href=https://en.wikipedia.org/wiki/Social_network>network</a>. Both perspectives provide evidence for substantial <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic knowledge</a> being deployed by the <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5413/>Rearranging the Familiar : Testing Compositional Generalization in Recurrent Networks</a></strong><br><a href=/people/j/joao-loula/>João Loula</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a>
|
<a href=/people/b/brenden-lake/>Brenden Lake</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5413><div class="card-body p-3 small">Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it&#8217;s seen as key to the human capacity for generalization in language. Recent work (Lake and Baroni, 2018) has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool. Lake and Baroni&#8217;s main experiment required the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as around and right) in novel contexts. Our findings confirm and strengthen the earlier ones : seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of X around right to jump around right), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of around right from those of right and around).<i>around</i>&#8221; and &#8220;<i>right</i>&#8221;) in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of &#8220;X <i>around right</i>&#8221; to &#8220;<i>jump around right</i>&#8221;), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of &#8220;<i>around right</i>&#8221; from those of &#8220;<i>right</i>&#8221; and &#8220;<i>around</i>&#8221;).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5415/>Interpretable Neural Architectures for Attributing an Ad’s Performance to its Writing Style</a></strong><br><a href=/people/r/reid-pryzant/>Reid Pryzant</a>
|
<a href=/people/s/sugato-basu/>Sugato Basu</a>
|
<a href=/people/k/kazoo-sone/>Kazoo Sone</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5415><div class="card-body p-3 small">How much does free shipping ! help an advertisement&#8217;s ability to persuade? This paper presents two methods for <a href=https://en.wikipedia.org/wiki/Performance_attribution>performance attribution</a> : finding the degree to which an outcome can be attributed to parts of a text while controlling for potential confounders. Both <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> are based on interpreting the behaviors and parameters of trained <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. One method uses a CNN to encode the text, an adversarial objective function to control for confounders, and projects its weights onto its activations to interpret the importance of each phrase towards each output class. The other method leverages <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>residualization</a> to control for <a href=https://en.wikipedia.org/wiki/Confounding>confounds</a> and performs <a href=https://en.wikipedia.org/wiki/Interpretation_(logic)>interpretation</a> by aggregating over learned word vectors. We demonstrate these algorithms&#8217; efficacy on 118,000 internet search advertisements and outcomes, finding language indicative of high and low click through rate (CTR) regardless of who the ad is by or what it is for. Our results suggest the proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> are high performance and data efficient, able to glean actionable insights from fewer than 10,000 data points. We find that quick, easy, and authoritative language is associated with success, while lackluster embellishment is related to failure. These findings agree with the advertising industry&#8217;s emperical wisdom, automatically revealing insights which previously required manual A / B testing to discover.<i>performance attribution</i>: finding the degree to which an outcome can be attributed to parts of a text while controlling for potential confounders. Both algorithms are based on interpreting the behaviors and parameters of trained neural networks. One method uses a CNN to encode the text, an adversarial objective function to control for confounders, and projects its weights onto its activations to interpret the importance of each phrase towards each output class. The other method leverages residualization to control for confounds and performs interpretation by aggregating over learned word vectors. We demonstrate these algorithms&#8217; efficacy on 118,000 internet search advertisements and outcomes, finding language indicative of high and low click through rate (CTR) regardless of who the ad is by or what it is for. Our results suggest the proposed algorithms are high performance and data efficient, able to glean actionable insights from fewer than 10,000 data points. We find that quick, easy, and authoritative language is associated with success, while lackluster embellishment is related to failure. These findings agree with the advertising industry&#8217;s emperical wisdom, automatically revealing insights which previously required manual A/B testing to discover.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5418/>LISA : Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation<span class=acl-fixed-case>LISA</span>: Explaining Recurrent Neural Network Judgments via Layer-w<span class=acl-fixed-case>I</span>se Semantic Accumulation and Example to Pattern Transformation</a></strong><br><a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5418><div class="card-body p-3 small">Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of <a href=https://en.wikipedia.org/wiki/Neural_network>RNN</a> via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>. We demonstrate (1) LISA : How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response (2) Example2pattern : How the saliency patterns look like for each category in the data according to the network in decision making. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the <a href=https://en.wikipedia.org/wiki/Neural_network>network</a>. We employ two relation classification datasets : SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern.<i>Layer-wIse-Semantic-Accumulation</i> (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making. We demonstrate (1) <i>LISA</i>: &#8220;How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response&#8221; (2) <i>Example2pattern</i>: &#8220;How the saliency patterns look like for each category in the data according to the network in decision making&#8221;. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the <i>LISA</i> and <i>example2pattern</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5420/>An Operation Sequence Model for Explainable Neural Machine Translation</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5420><div class="card-body p-3 small">We propose to achieve explainable neural machine translation (NMT) by changing the output representation to explain itself. We present a novel approach to NMT which generates the target sentence by monotonically walking through the source sentence. Word reordering is modeled by operations which allow setting markers in the target sentence and move a target-side write head between those markers. In contrast to many modern neural models, our system emits explicit word alignment information which is often crucial to practical <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> as it improves explainability. Our technique can outperform a plain text system in terms of BLEU score under the recent Transformer architecture on Japanese-English and Portuguese-English, and is within 0.5 BLEU difference on Spanish-English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5421/>Introspection for convolutional automatic speech recognition</a></strong><br><a href=/people/a/andreas-krug/>Andreas Krug</a>
|
<a href=/people/s/sebastian-stober/>Sebastian Stober</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5421><div class="card-body p-3 small">Artificial Neural Networks (ANNs) have experienced great success in the past few years. The increasing <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> leads to less understanding about their <a href=https://en.wikipedia.org/wiki/Decision-making>decision processes</a>. Therefore, introspection techniques have been proposed, mostly for <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> as input data. Patterns or relevant regions in <a href=https://en.wikipedia.org/wiki/Image>images</a> can be intuitively interpreted by a human observer. This is not the case for more complex data like speech recordings. In this work, we investigate the application of common introspection techniques from <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> to an Automatic Speech Recognition (ASR) task. To this end, we use a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> similar to <a href=https://en.wikipedia.org/wiki/Image_classification>image classification</a>, which predicts <a href=https://en.wikipedia.org/wiki/Letter_(alphabet)>letters</a> from <a href=https://en.wikipedia.org/wiki/Optical_spectrometer>spectrograms</a>. We show difficulties in applying image introspection to ASR. To tackle these problems, we propose normalized averaging of aligned inputs (NAvAI): a data-driven method to reveal learned patterns for prediction of specific classes. Our method integrates information from many data examples through local introspection techniques for Convolutional Neural Networks (CNNs). We demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> provides better interpretability of letter-specific patterns than existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5422/>Learning and Evaluating Sparse Interpretable Sentence Embeddings</a></strong><br><a href=/people/v/valentin-trifonov/>Valentin Trifonov</a>
|
<a href=/people/o/octavian-eugen-ganea/>Octavian-Eugen Ganea</a>
|
<a href=/people/a/anna-potapenko/>Anna Potapenko</a>
|
<a href=/people/t/thomas-hofmann/>Thomas Hofmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5422><div class="card-body p-3 small">Previous research on word embeddings has shown that sparse representations, which can be either learned on top of existing dense embeddings or obtained through model constraints during training time, have the benefit of increased interpretability properties : to some degree, each dimension can be understood by a human and associated with a recognizable feature in the data. In this paper, we transfer this idea to <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> and explore several approaches to obtain a <a href=https://en.wikipedia.org/wiki/Sparse_representation>sparse representation</a>. We further introduce a novel, quantitative and automated evaluation metric for sentence embedding interpretability, based on topic coherence methods. We observe an increase in <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> compared to dense models, on a dataset of movie dialogs and on the scene descriptions from the MS COCO dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5425 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5425/>Closing Brackets with Recurrent Neural Networks</a></strong><br><a href=/people/n/natalia-skachkova/>Natalia Skachkova</a>
|
<a href=/people/t/thomas-alexander-trost/>Thomas Trost</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5425><div class="card-body p-3 small">Many natural and formal languages contain words or symbols that require a matching counterpart for making an expression well-formed. The combination of opening and closing brackets is a typical example of such a construction. Due to their commonness, the ability to follow such <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> is important for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. Currently, recurrent neural networks (RNNs) are extensively used for this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. We investigate whether they are capable of learning the rules of opening and closing brackets by applying them to synthetic Dyck languages that consist of different types of <a href=https://en.wikipedia.org/wiki/Bracket>brackets</a>. We provide an analysis of the statistical properties of these languages as a baseline and show strengths and limits of Elman-RNNs, GRUs and LSTMs in experiments on random samples of these languages. In terms of perplexity and prediction accuracy, the RNNs get close to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>theoretical baseline</a> in most cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5426 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5426/>Under the Hood : Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information</a></strong><br><a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>
|
<a href=/people/j/jack-harding/>Jack Harding</a>
|
<a href=/people/f/florian-mohnert/>Florian Mohnert</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/w/willem-zuidema/>Willem Zuidema</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5426><div class="card-body p-3 small">How do neural language models keep track of <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>number agreement</a> between subject and verb? We show that &#8216;diagnostic classifiers&#8217;, trained to predict number from the internal states of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>, provide a detailed understanding of how, when, and where this information is represented. Moreover, they give us insight into when and where <a href=https://en.wikipedia.org/wiki/Number>number information</a> is corrupted in cases where the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> ends up making <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>agreement errors</a>. To demonstrate the causal role played by the representations we find, we then use agreement information to influence the course of the LSTM during the processing of difficult sentences. Results from such an intervention reveal a large increase in the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>&#8217;s accuracy. Together, these results show that diagnostic classifiers give us an unrivalled detailed look into the representation of linguistic information in neural models, and demonstrate that this knowledge can be used to improve their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5427 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5427/>Iterative Recursive Attention Model for Interpretable Sequence Classification</a></strong><br><a href=/people/m/martin-tutek/>Martin Tutek</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5427><div class="card-body p-3 small">Natural language processing has greatly benefited from the introduction of the attention mechanism. However, standard attention models are of limited interpretability for tasks that involve a series of <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference steps</a>. We describe an iterative recursive attention model, which constructs incremental representations of input data through reusing results of previously computed queries. We train our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on sentiment classification datasets and demonstrate its capacity to identify and combine different aspects of the input in an easily interpretable manner, while obtaining performance close to the state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5429/>Importance of Self-Attention for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/g/gael-letarte/>Gaël Letarte</a>
|
<a href=/people/f/frederik-paradis/>Frédérik Paradis</a>
|
<a href=/people/p/philippe-giguere/>Philippe Giguère</a>
|
<a href=/people/f/francois-laviolette/>François Laviolette</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5429><div class="card-body p-3 small">Despite their superior performance, deep learning models often lack <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. In this paper, we explore the modeling of insightful relations between words, in order to understand and enhance predictions. To this effect, we propose the Self-Attention Network (SANet), a flexible and interpretable architecture for text classification. Experiments indicate that gains obtained by self-attention is task-dependent. For instance, experiments on sentiment analysis tasks showed an improvement of around 2 % when using self-attention compared to a baseline without <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, while topic classification showed no gain. Interpretability brought forward by our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> highlighted the importance of neighboring word interactions to extract <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5431/>An Analysis of Encoder Representations in Transformer-Based Machine Translation</a></strong><br><a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5431><div class="card-body p-3 small">The attention mechanism is a successful technique in modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, especially in tasks like <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The recently proposed <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a> of the Transformer is based entirely on <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> and achieves new state of the art results in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> while higher layers tend to encode more <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>.<i>Transformer</i> is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5432 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5432/>Evaluating Grammaticality in Seq2seq Models with a Broad Coverage HPSG Grammar : A Case Study on Machine Translation<span class=acl-fixed-case>HPSG</span> Grammar: A Case Study on Machine Translation</a></strong><br><a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/k/khiem-pham/>Khiem Pham</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a>
|
<a href=/people/b/brian-w-dillon/>Brian Dillon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5432><div class="card-body p-3 small">Sequence to sequence (seq2seq) models are often employed in settings where the target output is <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. However, the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic properties</a> of the language generated from these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are not well understood. We explore whether such output belongs to a formal and realistic grammar, by employing the English Resource Grammar (ERG), a broad coverage, linguistically precise HPSG-based grammar of English. From a French to English parallel corpus, we analyze the parseability and <a href=https://en.wikipedia.org/wiki/Grammar>grammatical constructions</a> occurring in output from a seq2seq translation model. Over 93 % of the model translations are parseable, suggesting that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> learns to generate conforming to a <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has trouble learning the distribution of rarer syntactic rules, and we pinpoint several constructions that differentiate translations between the references and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5434 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5434" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5434/>Learning Explanations from Language Data</a></strong><br><a href=/people/d/david-harbecke/>David Harbecke</a>
|
<a href=/people/r/robert-schwarzenberg/>Robert Schwarzenberg</a>
|
<a href=/people/c/christoph-alt/>Christoph Alt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5434><div class="card-body p-3 small">PatternAttribution is a recent method, introduced in the <a href=https://en.wikipedia.org/wiki/Visual_system>vision domain</a>, that explains classifications of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. We demonstrate that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> also generates meaningful interpretations in the language domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5435.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5435 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5435 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5435/>How much should you ask? On the question structure in <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a>.<span class=acl-fixed-case>QA</span> systems.</a></strong><br><a href=/people/b/barbara-rychalska/>Barbara Rychalska</a>
|
<a href=/people/d/dominika-basaj/>Dominika Basaj</a>
|
<a href=/people/a/anna-wroblewska/>Anna Wróblewska</a>
|
<a href=/people/p/przemyslaw-biecek/>Przemyslaw Biecek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5435><div class="card-body p-3 small">Datasets that boosted state-of-the-art solutions for Question Answering (QA) systems prove that it is possible to ask questions in natural language manner. However, users are still used to query-like systems where they type in <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> to search for answer. In this study we validate which parts of questions are essential for obtaining valid answer. In order to conclude that, we take advantage of LIME-a framework that explains <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> by local approximation. We find that <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> and <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> is disregarded by QA. State-of-the-art model can answer properly even if&#8217; asked&#8217; only with a few words with high coefficients calculated with <a href=https://en.wikipedia.org/wiki/LIME>LIME</a>. According to our knowledge, it is the first time that QA model is being explained by LIME.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5438 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5438/>Language Models Learn POS First<span class=acl-fixed-case>POS</span> First</a></strong><br><a href=/people/n/naomi-saphra/>Naomi Saphra</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5438><div class="card-body p-3 small">A glut of recent research shows that <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> capture linguistic structure. Such work answers the question of whether a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> represents linguistic structure. But how and when are these <a href=https://en.wikipedia.org/wiki/List_of_nonbuilding_structure_types>structures</a> acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with <a href=https://en.wikipedia.org/wiki/Part_of_speech_tagging>part of speech tagging</a> acquired early and global topic information learned continuously.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5439 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5439/>Predicting and interpreting embeddings for out of vocabulary words in downstream tasks</a></strong><br><a href=/people/n/nicolas-garneau/>Nicolas Garneau</a>
|
<a href=/people/j/jean-samuel-leboeuf/>Jean-Samuel Leboeuf</a>
|
<a href=/people/l/luc-lamontagne/>Luc Lamontagne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5439><div class="card-body p-3 small">We propose a novel way to handle out of vocabulary (OOV) words in downstream natural language processing (NLP) tasks. We implement a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> that predicts useful embeddings for OOV words based on their <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> and on the context in which they appear. Our model also incorporates an attention mechanism indicating the focus allocated to the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>left context words</a>, the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>right context words</a> or the word&#8217;s characters, hence making the prediction more interpretable. The model is a drop-in module that is jointly trained with the downstream task&#8217;s <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, thus producing embeddings specialized for the task at hand. When the task is mostly syntactical, we observe that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> aims most of its attention on surface form characters. On the other hand, for tasks more semantical, the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> allocates more attention to the surrounding words. In all our tests, the <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> helps the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> to achieve better performances in comparison to the use of simple random embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5441 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305194062 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-5441/>Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation</a></strong><br><a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/a/aparajita-haldar/>Aparajita Haldar</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/j/j-edward-hu/>J. Edward Hu</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a>
|
<a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5441><div class="card-body p-3 small">We present a large scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence representation</a> encoded by a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> captures distinct types of <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. Our collection of diverse datasets is available at, and will grow over time as additional resources are recast and added from novel sources.<url>http://www.decomp.net/</url>, and will grow over time as additional resources are recast and added from novel sources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5442 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5442/>Interpretable Word Embedding Contextualization</a></strong><br><a href=/people/k/kyoung-rok-jang/>Kyoung-Rok Jang</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a>
|
<a href=/people/s/sang-bum-kim/>Sang-Bum Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5442><div class="card-body p-3 small">In this paper, we propose a method of calibrating a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, so that the semantic it conveys becomes more relevant to the context. Our method is novel because the output shows clearly which senses that were originally presented in a target <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> become stronger or weaker. This is possible by utilizing the technique of using <a href=https://en.wikipedia.org/wiki/Sparse_coding>sparse coding</a> to recover senses that comprises a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5444.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5444 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5444 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5444/>Extracting Syntactic Trees from Transformer Encoder Self-Attentions</a></strong><br><a href=/people/d/david-marecek/>David Mareček</a>
|
<a href=/people/r/rudolf-rosa/>Rudolf Rosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5444><div class="card-body p-3 small">This is a work in progress about extracting the sentence tree structures from the encoder&#8217;s self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing <a href=https://en.wikipedia.org/wiki/Syntax>syntactic theories</a> and <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5445 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5445/>Portable, layer-wise task performance monitoring for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a><span class=acl-fixed-case>NLP</span> models</a></strong><br><a href=/people/t/tom-lippincott/>Tom Lippincott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5445><div class="card-body p-3 small">There is a long-standing interest in understanding the internal behavior of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Deep neural architectures for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> are often accompanied by explanations for their effectiveness, from general observations (e.g. RNNs can represent unbounded dependencies in a sequence) to specific arguments about linguistic phenomena (early layers encode lexical information, deeper layers syntactic). The recent ascendancy of DNNs is fueling efforts in the NLP community to explore these claims. Previous work has tended to focus on easily-accessible representations like word or sentence embeddings, with deeper structure requiring more ad hoc methods to extract and examine. In this work, we introduce Vivisect, a toolkit that aims at a general solution for broad and fine-grained monitoring in the major DNN frameworks, with minimal change to research patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5447 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5447/>Explicitly modeling case improves neural dependency parsing</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5447><div class="card-body p-3 small">Neural dependency parsing models that compose word representations from <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> can presumably exploit <a href=https://en.wikipedia.org/wiki/Morphosyntax>morphosyntax</a> when making attachment decisions. How much do they know about <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphology</a>? We investigate how well they handle <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological case</a>, which is important for <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Our experiments on <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> suggest that adding explicit morphological caseeither oracle or predictedimproves neural dependency parsing, indicating that the learned representations in these models do not fully encode the morphological knowledge that they need, and can still benefit from targeted forms of explicit linguistic modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5449 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5449/>Representation of Word Meaning in the Intermediate Projection Layer of a Neural Language Model</a></strong><br><a href=/people/s/steven-derby/>Steven Derby</a>
|
<a href=/people/p/paul-miller/>Paul Miller</a>
|
<a href=/people/b/brian-murphy/>Brian Murphy</a>
|
<a href=/people/b/barry-devereux/>Barry Devereux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5449><div class="card-body p-3 small">Performance in <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a> has been significantly improved by training <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> on large corpora. This progress has come at the cost of interpretability and an understanding of how these <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> function, making principled development of better <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> more difficult. We look inside a state-of-the-art neural language model to analyse how this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> represents high-level lexico-semantic information. In particular, we investigate how the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> represents words by extracting activation patterns where they occur in the text, and compare these <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> directly to human semantic knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5450 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5450/>Interpretable Structure Induction via Sparse Attention</a></strong><br><a href=/people/b/ben-peters/>Ben Peters</a>
|
<a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5450><div class="card-body p-3 small">Neural network methods are experiencing wide adoption in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, thanks to their empirical performance on many tasks. Modern neural architectures go way beyond simple feedforward and recurrent models : they are complex pipelines that perform soft, differentiable computation instead of <a href=https://en.wikipedia.org/wiki/Discrete_mathematics>discrete logic</a>. The price of such <a href=https://en.wikipedia.org/wiki/Soft_computing>soft computing</a> is the introduction of dense dependencies, which make it hard to disentangle the patterns that trigger a prediction. Our recent work on sparse and structured latent computation presents a promising avenue for enhancing interpretability of such neural pipelines. Through this extended abstract, we aim to discuss and explore the potential and impact of our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5451 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5451/>Debugging Sequence-to-Sequence Models with Seq2Seq-Vis<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq-Vis</a></strong><br><a href=/people/h/hendrik-strobelt/>Hendrik Strobelt</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/m/michael-behrisch/>Michael Behrisch</a>
|
<a href=/people/a/adam-perer/>Adam Perer</a>
|
<a href=/people/h/hanspeter-pfister/>Hanspeter Pfister</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5451><div class="card-body p-3 small">Neural attention-based sequence-to-sequence models (seq2seq) (Sutskever et al., 2014 ; Bahdanau et al., 2014) have proven to be accurate and robust for many sequence prediction tasks. They have become the standard approach for automatic translation of text, at the cost of increased model complexity and <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a>. End-to-end trained neural models act as a black box, which makes it difficult to examine model decisions and attribute errors to a specific part of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. The highly connected and high-dimensional internal representations pose a challenge for analysis and visualization tools. The development of methods to understand seq2seq predictions is crucial for systems in production settings, as mistakes involving language are often very apparent to human readers. For instance, a widely publicized incident resulted from a translation system mistakenly translating good morning into attack them leading to a wrongful arrest (Hern, 2017).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5453.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5453 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5453 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5453/>Does Syntactic Knowledge in Multilingual Language Models Transfer Across Languages?</a></strong><br><a href=/people/p/prajit-dhar/>Prajit Dhar</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5453><div class="card-body p-3 small">Recent work has shown that neural models can be successfully trained on multiple languages simultaneously. We investigate whether such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> learn to share and exploit common syntactic knowledge among the languages on which they are trained. This extended abstract presents our preliminary results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5455.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5455 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5455 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5455" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5455/>End-to-end Image Captioning Exploits Distributional Similarity in Multimodal Space</a></strong><br><a href=/people/p/pranava-swaroop-madhyastha/>Pranava Swaroop Madhyastha</a>
|
<a href=/people/j/josiah-wang/>Josiah Wang</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5455><div class="card-body p-3 small">We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn &#8216;distributional similarity&#8217; in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the &#8216;image&#8217; side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. Our analysis indicates that image captioning models (i) are capable of separating structure from noisy input representations ; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space ; (iii) cluster images with similar visual and linguistic information together. Our experiments all point to one fact : that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>