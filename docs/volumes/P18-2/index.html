<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/P18-2.pdf>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></h2><p class=lead><a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>,
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>P18-2</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Melbourne, Australia</dd><dt>Venue:</dt><dd><a href=/venues/acl/>ACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/P18-2>https://aclanthology.org/P18-2</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/P18-2.pdf>https://aclanthology.org/P18-2.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/P18-2.pdf title="Open PDF of 'Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+56th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+%28Volume+2%3A+Short+Papers%29" title="Search for 'Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2000/>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></strong><br><a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2002.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2002/>Restricted Recurrent Neural Tensor Networks : Exploiting Word Frequency and Compositionality</a></strong><br><a href=/people/a/alexandre-salle/>Alexandre Salle</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2002><div class="card-body p-3 small">Increasing the capacity of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNN)</a> usually involves augmenting the size of the hidden layer, with significant increase of <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>. Recurrent neural tensor networks (RNTN) increase capacity using distinct hidden layer weights for each word, but with greater costs in memory usage. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> performance over RNNs using only a small fraction of the parameters of unrestricted RNTNs. These results hold for r-RNTNs using <a href=https://en.wikipedia.org/wiki/Gated_recurrent_unit>Gated Recurrent Units</a> and <a href=https://en.wikipedia.org/wiki/Long_short-term_memory>Long Short-Term Memory</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2003.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2003.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2003/>Deep RNNs Encode Soft Hierarchical Syntax<span class=acl-fixed-case>RNN</span>s Encode Soft Hierarchical Syntax</a></strong><br><a href=/people/t/terra-blevins/>Terra Blevins</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2003><div class="card-body p-3 small">We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a> ; for each word, we predict its <a href=https://en.wikipedia.org/wiki/Part_of_speech>part of speech</a> as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives : dependency parsing, <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, or <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> encode significant amounts of <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> even in the absence of an explicit syntactic training supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2004/>Word Error Rate Estimation for <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Recognition</a> : e-WER<span class=acl-fixed-case>WER</span></a></strong><br><a href=/people/a/ahmed-ali/>Ahmed Ali</a>
|
<a href=/people/s/steve-renals/>Steve Renals</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2004><div class="card-body p-3 small">Measuring the performance of automatic speech recognition (ASR) systems requires manually transcribed data in order to compute the word error rate (WER), which is often time-consuming and expensive. In this paper, we propose a novel approach to estimate WER, or e-WER, which does not require a gold-standard transcription of the test set. Our e-WER framework uses a comprehensive set of <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a> : ASR recognised text, <a href=https://en.wikipedia.org/wiki/Computer_vision>character recognition</a> results to complement recognition output, and internal decoder features. We report results for the two features ; black-box and glass-box using unseen 24 Arabic broadcast programs. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves 16.9 % <a href=https://en.wikipedia.org/wiki/Root-mean-square_deviation>WER root mean squared error (RMSE)</a> across 1,400 sentences. The estimated overall WER e-WER was 25.3 % for the three hours test set, while the actual WER was 28.5 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2006.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2006/>HotFlip : White-Box Adversarial Examples for Text Classification<span class=acl-fixed-case>H</span>ot<span class=acl-fixed-case>F</span>lip: White-Box Adversarial Examples for Text Classification</a></strong><br><a href=/people/j/javid-ebrahimi/>Javid Ebrahimi</a>
|
<a href=/people/a/anyi-rao/>Anyi Rao</a>
|
<a href=/people/d/daniel-lowd/>Daniel Lowd</a>
|
<a href=/people/d/dejing-dou/>Dejing Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2006><div class="card-body p-3 small">We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2008.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2008.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2008/>Active learning for deep semantic parsing</a></strong><br><a href=/people/l/long-duong/>Long Duong</a>
|
<a href=/people/h/hadi-afshar/>Hadi Afshar</a>
|
<a href=/people/d/dominique-estival/>Dominique Estival</a>
|
<a href=/people/g/glen-pink/>Glen Pink</a>
|
<a href=/people/p/philip-r-cohen/>Philip Cohen</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2008><div class="card-body p-3 small">Semantic parsing requires <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> that is expensive and slow to collect. We apply <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> to both traditional and overnight data collection approaches. We show that it is possible to obtain good training hyperparameters from seed data which is only a small fraction of the full dataset. We show that uncertainty sampling based on least confidence score is competitive in traditional <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> but not applicable for overnight collection. We propose several active learning strategies for overnight data collection and show that different example selection strategies per domain perform best.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2010.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2010.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2010/>Unsupervised Semantic Frame Induction using Triclustering</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2010><div class="card-body p-3 small">We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2011.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2011/>Identification of Alias Links among Participants in Narratives</a></strong><br><a href=/people/s/sangameshwar-patil/>Sangameshwar Patil</a>
|
<a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/g/girish-palshikar/>Girish Palshikar</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2011><div class="card-body p-3 small">Identification of distinct and independent participants (entities of interest) in a narrative is an important task for many NLP applications. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> becomes challenging because these participants are often referred to using multiple aliases. In this paper, we propose an approach based on linguistic knowledge for identification of aliases mentioned using <a href=https://en.wikipedia.org/wiki/Proper_noun>proper nouns</a>, <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> or <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a> with common noun headword. We use Markov Logic Network (MLN) to encode the linguistic knowledge for identification of aliases. We evaluate on four <a href=https://en.wikipedia.org/wiki/Multiculturalism>diverse history narratives</a> of varying complexity. Our approach performs better than the state-of-the-art approach as well as a combination of standard <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and coreference resolution techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2012.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2012/>Named Entity Recognition With Parallel Recurrent Neural Networks</a></strong><br><a href=/people/a/andrej-zukov-gregoric/>Andrej Žukov-Gregorič</a>
|
<a href=/people/y/yoram-bachrach/>Yoram Bachrach</a>
|
<a href=/people/s/sam-coope/>Sam Coope</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2012><div class="card-body p-3 small">We present a new <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Our model employs multiple independent bidirectional LSTM units across the same input and promotes diversity among them by employing an inter-model regularization term. By distributing computation across multiple smaller <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> we find a significant reduction in the total number of parameters. We find our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> achieves state-of-the-art performance on the CoNLL 2003 NER dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2014.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2014" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2014/>A Walk-based Model on <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>Entity Graphs</a> for Relation Extraction</a></strong><br><a href=/people/f/fenia-christopoulou/>Fenia Christopoulou</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2014><div class="card-body p-3 small">We present a novel graph-based neural network model for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> treats multiple pairs in a sentence simultaneously and considers interactions among them. All the entities in a sentence are placed as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> in a fully-connected graph structure. The <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> are represented with <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>position-aware contexts</a> around the entity pairs. In order to consider different <a href=https://en.wikipedia.org/wiki/Path_(graph_theory)>relation paths</a> between two entities, we construct up to l-length walks between each pair. The resulting <a href=https://en.wikipedia.org/wiki/Walk_(graph_theory)>walks</a> are merged and iteratively used to update the edge representations into longer walks representations. We show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools.<tex-math>l</tex-math>-length walks between each pair. The resulting walks are merged and iteratively used to update the edge representations into longer walks representations. We show that the model achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2016.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2016/>Automatic Extraction of Commonsense LocatedNear Knowledge<span class=acl-fixed-case>L</span>ocated<span class=acl-fixed-case>N</span>ear Knowledge</a></strong><br><a href=/people/f/frank-f-xu/>Frank F. Xu</a>
|
<a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/k/kenny-zhu/>Kenny Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2016><div class="card-body p-3 small">LocatedNear relation is a kind of <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> describing two physical objects that are typically found near each other in real life. In this paper, we study how to automatically extract such relationship through a sentence-level relation classifier and aggregating the scores of entity pairs from a large corpus. Also, we release two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> for evaluation and future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2020.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2020/>A Named Entity Recognition Shootout for German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2020><div class="card-body p-3 small">We ask how to practically build a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for German named entity recognition (NER) that performs at the state of the art for both <a href=https://en.wikipedia.org/wiki/Contemporary_history>contemporary and historical texts</a>, i.e., a big-data and a small-data scenario. The two best-performing model families are pitted against each other (linear-chain CRFs and BiLSTM) to observe the trade-off between expressiveness and data requirements. BiLSTM outperforms the <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>CRF</a> when large datasets are available and performs inferior for the smallest dataset. BiLSTMs profit substantially from <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, which enables them to be trained on multiple corpora, resulting in a new state-of-the-art model for German NER on two contemporary German corpora (CoNLL 2003 and GermEval 2014) and two historic corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2021/>A dataset for identifying actionable feedback in <a href=https://en.wikipedia.org/wiki/Collaborative_software_development>collaborative software development</a></a></strong><br><a href=/people/b/benjamin-s-meyers/>Benjamin S. Meyers</a>
|
<a href=/people/n/nuthan-munaiah/>Nuthan Munaiah</a>
|
<a href=/people/e/emily-prudhommeaux/>Emily Prud’hommeaux</a>
|
<a href=/people/a/andrew-meneely/>Andrew Meneely</a>
|
<a href=/people/j/josephine-wolff/>Josephine Wolff</a>
|
<a href=/people/c/cecilia-ovesdotter-alm/>Cecilia Ovesdotter Alm</a>
|
<a href=/people/p/pradeep-murukannaiah/>Pradeep Murukannaiah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2021><div class="card-body p-3 small">Software developers and testers have long struggled with how to elicit proactive responses from their coworkers when reviewing code for <a href=https://en.wikipedia.org/wiki/Vulnerability_(computing)>security vulnerabilities</a> and errors. For a <a href=https://en.wikipedia.org/wiki/Code_review>code review</a> to be successful, it must not only identify potential problems but also elicit an active response from the colleague responsible for modifying the code. To understand the factors that contribute to this outcome, we analyze a novel dataset of more than one million code reviews for the Google Chromium project, from which we extract linguistic features of feedback that elicited responsive actions from coworkers. Using a manually-labeled subset of reviewer comments, we trained a highly accurate <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to identify acted-upon comments (AUC = 0.85). Our results demonstrate the utility of our dataset, the feasibility of using <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for this new task, and the potential of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> to improve our understanding of how communications between colleagues can be authored to elicit positive, proactive responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2023.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2023" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2023/>Analogical Reasoning on Chinese Morphological and Semantic Relations<span class=acl-fixed-case>C</span>hinese Morphological and Semantic Relations</a></strong><br><a href=/people/s/shen-li/>Shen Li</a>
|
<a href=/people/z/zhe-zhao/>Zhe Zhao</a>
|
<a href=/people/r/renfen-hu/>Renfen Hu</a>
|
<a href=/people/w/wensi-li/>Wensi Li</a>
|
<a href=/people/t/tao-liu/>Tao Liu</a>
|
<a href=/people/x/xiaoyong-du/>Xiaoyong Du</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2023><div class="card-body p-3 small">Analogical reasoning is effective in capturing linguistic regularities. This paper proposes an analogical reasoning task on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. After delving into Chinese lexical knowledge, we sketch 68 implicit morphological relations and 28 explicit semantic relations. A big and balanced dataset CA8 is then built for this task, including 17813 questions. Furthermore, we systematically explore the influences of <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representations</a>, context features, and <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> on <a href=https://en.wikipedia.org/wiki/Analogy>analogical reasoning</a>. With the experiments, <a href=https://en.wikipedia.org/wiki/CA8>CA8</a> is proved to be a reliable benchmark for evaluating Chinese word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2024.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2024.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2024/>Construction of a Chinese Corpus for the Analysis of the Emotionality of Metaphorical Expressions<span class=acl-fixed-case>C</span>hinese Corpus for the Analysis of the Emotionality of Metaphorical Expressions</a></strong><br><a href=/people/d/dongyu-zhang/>Dongyu Zhang</a>
|
<a href=/people/h/hongfei-lin/>Hongfei Lin</a>
|
<a href=/people/l/liang-yang/>Liang Yang</a>
|
<a href=/people/s/shaowu-zhang/>Shaowu Zhang</a>
|
<a href=/people/b/bo-xu/>Bo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2024><div class="card-body p-3 small">Metaphors are frequently used to convey emotions. However, there is little research on the construction of metaphor corpora annotated with <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> for the analysis of emotionality of metaphorical expressions. Furthermore, most studies focus on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and few in other languages, particularly <a href=https://en.wikipedia.org/wiki/Sino-Tibetan_languages>Sino-Tibetan languages</a> such as <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, for emotion analysis from metaphorical texts, although there are likely to be many differences in emotional expressions of metaphorical usages across different languages. We therefore construct a significant new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> on <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a>, with 5,605 manually annotated sentences in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We present an annotation scheme that contains annotations of linguistic metaphors, emotional categories (joy, <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, <a href=https://en.wikipedia.org/wiki/Sadness>sadness</a>, <a href=https://en.wikipedia.org/wiki/Fear>fear</a>, <a href=https://en.wikipedia.org/wiki/Love>love</a>, disgust and surprise), and intensity. The annotation agreement analyses for multiple annotators are described. We also use the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to explore and analyze the emotionality of metaphors. To the best of our knowledge, this is the first relatively large metaphor corpus with an annotation of emotions in Chinese.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2028.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2028.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2028/>A Language Model based Evaluator for Sentence Compression</a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/z/zhiyuan-luo/>Zhiyuan Luo</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2028><div class="card-body p-3 small">We herein present a language-model-based evaluator for deletion-based sentence compression and view this task as a series of deletion-and-evaluation operations using the evaluator. More specifically, the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words. Subsequently, a series of trial-and-error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression. An empirical study shows that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can effectively generate more readable compression, comparable or superior to several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Furthermore, we introduce a 200-sentence test set for a large-scale dataset, setting a new baseline for the future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2029/>Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources</a></strong><br><a href=/people/m/maria-glenski/>Maria Glenski</a>
|
<a href=/people/t/tim-weninger/>Tim Weninger</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2029><div class="card-body p-3 small">In the age of <a href=https://en.wikipedia.org/wiki/Social_news_website>social news</a>, it is important to understand the types of <a href=https://en.wikipedia.org/wiki/Reactionary>reactions</a> that are evoked from <a href=https://en.wikipedia.org/wiki/Source_(journalism)>news sources</a> with various levels of <a href=https://en.wikipedia.org/wiki/Credibility>credibility</a>. In the present work we seek to better understand how users react to trusted and deceptive news sources across two popular, and very different, <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. To that end, (1) we develop a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sources for 10.8 M <a href=https://en.wikipedia.org/wiki/Twitter>Twitter posts</a> and 6.2 M <a href=https://en.wikipedia.org/wiki/Reddit>Reddit comments</a>. We show that there are significant differences in the speed and the type of reactions between trusted and deceptive news sources on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, but far smaller differences on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2031 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2031/>Fighting Offensive Language on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> with Unsupervised Text Style Transfer</a></strong><br><a href=/people/c/cicero-dos-santos/>Cicero Nogueira dos Santos</a>
|
<a href=/people/i/igor-melnyk/>Igor Melnyk</a>
|
<a href=/people/i/inkit-padhi/>Inkit Padhi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2031><div class="card-body p-3 small">We introduce a new approach to tackle the problem of <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a> in <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a>. Our approach uses unsupervised text style transfer to translate offensive sentences into non-offensive ones. We propose a new method for training encoder-decoders using non-parallel data that combines a collaborative classifier, attention and the cycle consistency loss. Experimental results on data from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> show that our method outperforms a state-of-the-art text style transfer system in two out of three quantitative metrics and produces reliable non-offensive transferred sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2033.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2033/>Task-oriented Dialogue System for Automatic Diagnosis</a></strong><br><a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/q/qianlong-liu/>Qianlong Liu</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/h/huaixiao-tou/>Huaixiao Tou</a>
|
<a href=/people/t/ting-chen/>Ting Chen</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>
|
<a href=/people/k/kam-fai-wong/>Kam-fai Wong</a>
|
<a href=/people/x/xiang-dai/>Xiangying Dai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2033><div class="card-body p-3 small">In this paper, we make a move to build a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> for automatic diagnosis. We first build a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> collected from an online medical forum by extracting symptoms from both patients&#8217; self-reports and conversational data between patients and doctors. Then we propose a task-oriented dialogue system framework to make <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnosis</a> for patients automatically, which can converse with patients to collect additional <a href=https://en.wikipedia.org/wiki/Symptom>symptoms</a> beyond their self-reports. Experimental results on our dataset show that additional <a href=https://en.wikipedia.org/wiki/Symptom>symptoms</a> extracted from conversation can greatly improve the accuracy for disease identification and our dialogue system is able to collect these <a href=https://en.wikipedia.org/wiki/Symptom>symptoms</a> automatically and make a better <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnosis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2034 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2034/>Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce<span class=acl-fixed-case>E</span>-commerce</a></strong><br><a href=/people/m/minghui-qiu/>Minghui Qiu</a>
|
<a href=/people/l/liu-yang/>Liu Yang</a>
|
<a href=/people/f/feng-ji/>Feng Ji</a>
|
<a href=/people/w/wei-zhou/>Wei Zhou</a>
|
<a href=/people/j/jun-huang/>Jun Huang</a>
|
<a href=/people/h/haiqing-chen/>Haiqing Chen</a>
|
<a href=/people/w/w-bruce-croft/>Bruce Croft</a>
|
<a href=/people/w/wei-lin/>Wei Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2034><div class="card-body p-3 small">Building multi-turn information-seeking conversation systems is an important and challenging research topic. Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications. Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications. To alleviate these problems, we study <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for multi-turn information seeking conversations in this paper. We first propose an efficient and effective multi-turn conversation model based on <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. After that, we extend our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to adapt the knowledge learned from a resource-rich domain to enhance the performance. Finally, we deployed our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> in an industrial chatbot called AliMe Assist and observed a significant improvement over the existing online model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2035.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2035/>A Multi-task Approach to Learning Multilingual Representations</a></strong><br><a href=/people/k/karan-singla/>Karan Singla</a>
|
<a href=/people/d/dogan-can/>Dogan Can</a>
|
<a href=/people/s/shrikanth-narayanan/>Shrikanth Narayanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2035><div class="card-body p-3 small">We present a novel multi-task modeling approach to learning multilingual distributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. Our architecture can transparently use both monolingual and sentence aligned bilingual corpora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> shows competitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in a <a href=https://en.wikipedia.org/wiki/Scenario_analysis>limited resource scenario</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2036 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2036/>Characterizing Departures from <a href=https://en.wikipedia.org/wiki/Linearity>Linearity</a> in Word Translation</a></strong><br><a href=/people/n/ndapandula-nakashole/>Ndapa Nakashole</a>
|
<a href=/people/r/raphael-flauger/>Raphael Flauger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2036><div class="card-body p-3 small">We investigate the behavior of <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>maps</a> learned by <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation methods</a>. The <a href=https://en.wikipedia.org/wiki/Map>maps</a> translate words by projecting between word embedding spaces of different languages. We locally approximate these <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>maps</a> using linear maps, and find that they vary across the word embedding space. This demonstrates that the underlying <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>maps</a> are non-linear. Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained. Our results can be used to test <a href=https://en.wikipedia.org/wiki/Nonlinear_system>non-linear methods</a>, and to drive the design of more accurate <a href=https://en.wikipedia.org/wiki/Map>maps</a> for <a href=https://en.wikipedia.org/wiki/Translation>word translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2038 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2038.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2038/>Hybrid semi-Markov CRF for Neural Sequence Labeling<span class=acl-fixed-case>M</span>arkov <span class=acl-fixed-case>CRF</span> for Neural Sequence Labeling</a></strong><br><a href=/people/z/zhixiu-ye/>Zhixiu Ye</a>
|
<a href=/people/z/zhen-hua-ling/>Zhen-Hua Ling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2038><div class="card-body p-3 small">This paper proposes hybrid semi-Markov conditional random fields (SCRFs) for neural sequence labeling in natural language processing. Based on conventional conditional random fields (CRFs), SCRFs have been designed for the tasks of assigning labels to segments by extracting <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from and describing transitions between segments instead of words. In this paper, we improve the existing SCRF methods by employing word-level and segment-level information simultaneously. First, word-level labels are utilized to derive the segment scores in SCRFs. Second, a CRF output layer and an SCRF output layer are integrated into a unified neural network and trained jointly. Experimental results on CoNLL 2003 named entity recognition (NER) shared task show that our model achieves state-of-the-art performance when no external knowledge is used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2042 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2042.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2042" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2042/>Paper Abstract Writing through Editing Mechanism</a></strong><br><a href=/people/q/qingyun-wang/>Qingyun Wang</a>
|
<a href=/people/z/zhihao-zhou/>Zhihao Zhou</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/s/spencer-whitehead/>Spencer Whitehead</a>
|
<a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2042><div class="card-body p-3 small">We present a paper abstract writing system based on an attentive neural sequence-to-sequence model that can take a title as input and automatically generate an abstract. We design a novel Writing-editing Network that can attend to both the title and the previously generated abstract drafts and then iteratively revise and polish the abstract. With two series of <a href=https://en.wikipedia.org/wiki/Turing_test>Turing tests</a>, where the human judges are asked to distinguish the system-generated abstracts from human-written ones, our system passes <a href=https://en.wikipedia.org/wiki/Turing_test>Turing tests</a> by junior domain experts at a rate up to 30 % and by non-expert at a rate up to 80 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2043" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2043/>Conditional Generators of Words Definitions</a></strong><br><a href=/people/a/artyom-gadetsky/>Artyom Gadetsky</a>
|
<a href=/people/i/ilya-yakubovskiy/>Ilya Yakubovskiy</a>
|
<a href=/people/d/dmitry-vetrov/>Dmitry Vetrov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2043><div class="card-body p-3 small">We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words&#8217; ambiguity and <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> leads to performance improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2045 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2045" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2045/>Narrative Modeling with Memory Chains and Semantic Supervision</a></strong><br><a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2045><div class="card-body p-3 small">Story comprehension requires a deep semantic understanding of the narrative, making it a challenging task. Inspired by previous studies on ROC Story Cloze Test, we propose a novel method, tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect. Evaluated on the task of story ending prediction, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> demonstrates superior performance to a collection of competitive baselines, setting a new state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2046 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2046.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2046" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2046/>Injecting <a href=https://en.wikipedia.org/wiki/Relational_structure>Relational Structural Representation</a> in <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> for Question Similarity</a></strong><br><a href=/people/a/antonio-uva/>Antonio Uva</a>
|
<a href=/people/d/daniele-bonadiman/>Daniele Bonadiman</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2046><div class="card-body p-3 small">Effectively using full syntactic parsing information in Neural Networks (NNs) for solving <a href=https://en.wikipedia.org/wiki/Relational_model>relational tasks</a>, e.g., question similarity, is still an open problem. In this paper, we propose to inject structural representations in NNs by (i) learning a model with Tree Kernels (TKs) on relatively few pairs of questions (few thousands) as gold standard (GS) training data is typically scarce, (ii) predicting labels on a very large corpus of question pairs, and (iii) pre-training NNs on such large corpus. The results on Quora and SemEval question similarity datasets show that NNs using our approach can learn more accurate models, especially after fine tuning on GS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2047 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2047.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2047.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2047/>A Simple and Effective Approach to Coverage-Aware Neural Machine Translation</a></strong><br><a href=/people/y/yanyang-li/>Yanyang Li</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/y/yinqiao-li/>Yinqiao Li</a>
|
<a href=/people/q/qiang-wang/>Qiang Wang</a>
|
<a href=/people/c/changming-xu/>Changming Xu</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2047><div class="card-body p-3 small">We offer a simple and effective method to seek a better balance between model confidence and length preference for Neural Machine Translation (NMT). Unlike the popular length normalization and coverage models, our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> does not require training nor reranking the limited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>approach</a> yields +0.4 1.5 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> improvements over the state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2048 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2048.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2048/>Dynamic Sentence Sampling for Efficient Training of Neural Machine Translation</a></strong><br><a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2048><div class="card-body p-3 small">Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs ; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2050 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2050.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2050" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2050/>Extreme Adaptation for Personalized Neural Machine Translation</a></strong><br><a href=/people/p/paul-michel/>Paul Michel</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2050><div class="card-body p-3 small">Every person speaks or writes their own flavor of their native language, influenced by a number of factors : the content they tend to talk about, their gender, their social status, or their geographical origin. When attempting to perform Machine Translation (MT), these variations have a significant effect on how the <a href=https://en.wikipedia.org/wiki/System>system</a> should perform <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, but this is not captured well by standard one-size-fits-all models. In this paper, we propose a simple and parameter-efficient adaptation technique that only requires adapting the bias of the output softmax to each particular user of the MT system, either directly or through a factored approximation. Experiments on <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED talks</a> in three languages demonstrate improvements in translation accuracy, and better reflection of speaker traits in the target text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2052/>Learning from Chunk-based Feedback in Neural Machine Translation</a></strong><br><a href=/people/p/pavel-petrushkov/>Pavel Petrushkov</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/e/evgeny-matusov/>Evgeny Matusov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2052><div class="card-body p-3 small">We empirically investigate learning from partial feedback in neural machine translation (NMT), when partial feedback is collected by asking users to highlight a correct chunk of a translation. We propose a simple and effective way of utilizing such <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> in NMT training. We demonstrate how the common machine translation problem of domain mismatch between training and deployment can be reduced solely based on chunk-level user feedback. We conduct a series of simulation experiments to test the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. Our results show that chunk-level feedback outperforms sentence based feedback by up to 2.61 % BLEU absolute.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2053 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2053/>Bag-of-Words as Target for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2053><div class="card-body p-3 small">A sentence can be translated into more than one correct sentences. However, most of the existing neural machine translation models only use one of the correct translations as the targets, and the other correct sentences are punished as the incorrect sentences in the training stage. Since most of the correct translations for one sentence share the similar <a href=https://en.wikipedia.org/wiki/Bag-of-words>bag-of-words</a>, it is possible to distinguish the correct translations from the incorrect ones by the <a href=https://en.wikipedia.org/wiki/Bag-of-words>bag-of-words</a>. In this paper, we propose an approach that uses both the sentences and the <a href=https://en.wikipedia.org/wiki/Bag-of-words>bag-of-words</a> as targets in the training stage, in order to encourage the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to generate the potentially correct sentences that are not appeared in the training set. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a Chinese-English translation dataset, and experiments show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the strong baselines by the BLEU score of 4.55.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2054 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2054.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2054.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2054/>Improving Beam Search by Removing Monotonic Constraint for Neural Machine Translation</a></strong><br><a href=/people/r/raphael-shu/>Raphael Shu</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2054><div class="card-body p-3 small">To achieve high translation performance, neural machine translation models usually rely on the beam search algorithm for decoding sentences. The <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> finds good candidate translations by considering multiple hypotheses of translations simultaneously. However, as the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> produces hypotheses in a monotonic left-to-right order, a hypothesis can not be revisited once it is discarded. We found such <a href=https://en.wikipedia.org/wiki/Monotonic_function>monotonicity</a> forces the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to sacrifice some good decoding paths. To mitigate this problem, we relax the <a href=https://en.wikipedia.org/wiki/Monotonic_function>monotonic constraint</a> of the beam search by maintaining all found hypotheses in a single <a href=https://en.wikipedia.org/wiki/Priority_queue>priority queue</a> and using a universal score function for hypothesis selection. The proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> allows discarded hypotheses to be recovered in a later step. Despite its simplicity, we show that the proposed decoding algorithm enhances the quality of selected hypotheses and improve the <a href=https://en.wikipedia.org/wiki/Translation>translations</a> even for high-performance models in English-Japanese translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2055.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803899 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2055/>Leveraging <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> and lexico-syntactic fixedness for token-level prediction of the idiomaticity of English verb-noun combinations<span class=acl-fixed-case>E</span>nglish verb-noun combinations</a></strong><br><a href=/people/m/milton-king/>Milton King</a>
|
<a href=/people/p/paul-cook/>Paul Cook</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2055><div class="card-body p-3 small">Verb-noun combinations (VNCs)-e.g., blow the whistle, hit the roof, and see stars-are a common type of <a href=https://en.wikipedia.org/wiki/List_of_English_idioms>English idiom</a> that are ambiguous with literal usages. In this paper we propose and evaluate models for classifying VNC usages as idiomatic or literal, based on a variety of approaches to forming <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a>. Our results show that a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on averaging word embeddings performs on par with, or better than, a previously-proposed approach based on skip-thoughts. Idiomatic usages of <a href=https://en.wikipedia.org/wiki/Virtual_Network_Computing>VNCs</a> are known to exhibit lexico-syntactic fixedness. We further incorporate this information into our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, demonstrating that this rich linguistic knowledge is complementary to the information carried by <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2056 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2056.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803915 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2056/>Using pseudo-senses for improving the extraction of synonyms from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a></a></strong><br><a href=/people/o/olivier-ferret/>Olivier Ferret</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2056><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> proposed recently for specializing word embeddings according to a particular perspective generally rely on <a href=https://en.wikipedia.org/wiki/Knowledge>external knowledge</a>. In this article, we propose Pseudofit, a new method for specializing word embeddings according to <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> without any external knowledge. Pseudofit exploits the notion of pseudo-sense for building several <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> for each word and uses these <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> for making the initial embeddings more generic. We illustrate the interest of Pseudofit for acquiring synonyms and study several variants of Pseudofit according to this perspective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2057.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803929 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2057" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2057/>Hearst Patterns Revisited : Automatic Hypernym Detection from Large Text Corpora</a></strong><br><a href=/people/s/stephen-roller/>Stephen Roller</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/m/maximilian-nickel/>Maximilian Nickel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2057><div class="card-body p-3 small">Methods for unsupervised hypernym detection may broadly be categorized according to two paradigms : pattern-based and distributional methods. In this paper, we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional methods</a> on common benchmark datasets. Our results show that pattern-based models provide important contextual constraints which are not yet captured in <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2059.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2059.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803960 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2059/>Sparse and Constrained Attention for Neural Machine Translation</a></strong><br><a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/p/pedro-ferreira/>Pedro Ferreira</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2059><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, words are sometimes dropped from the source or generated repeatedly in the <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We explore novel <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2060.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803972 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2060/>Neural Hidden Markov Model for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a><span class=acl-fixed-case>M</span>arkov Model for Machine Translation</a></strong><br><a href=/people/w/weiyue-wang/>Weiyue Wang</a>
|
<a href=/people/d/derui-zhu/>Derui Zhu</a>
|
<a href=/people/t/tamer-alkhouli/>Tamer Alkhouli</a>
|
<a href=/people/z/zixuan-gan/>Zixuan Gan</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2060><div class="card-body p-3 small">Attention-based neural machine translation (NMT) models selectively focus on specific source positions to produce a <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a>, which brings significant improvements over pure encoder-decoder sequence-to-sequence models. This work investigates <a href=https://en.wikipedia.org/wiki/Attention>NMT</a> while replacing the <a href=https://en.wikipedia.org/wiki/Attention>attention component</a>. We study a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models, which are trained jointly using the <a href=https://en.wikipedia.org/wiki/Forward&#8211;backward_algorithm>forward-backward algorithm</a>. We show that the attention component can be effectively replaced by the neural network alignment model and the neural HMM approach is able to provide comparable performance with the state-of-the-art attention-based models on the WMT 2017 GermanEnglish and ChineseEnglish translation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2062 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2062.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804010 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2062/>Orthographic Features for Bilingual Lexicon Induction</a></strong><br><a href=/people/p/parker-riley/>Parker Riley</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2062><div class="card-body p-3 small">Recent embedding-based methods in bilingual lexicon induction show good results, but do not take advantage of orthographic features, such as <a href=https://en.wikipedia.org/wiki/Edit_distance>edit distance</a>, which can be helpful for pairs of related languages. This work extends embedding-based methods to incorporate these features, resulting in significant accuracy gains for related languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2063 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804022 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2063/>Neural Cross-Lingual Coreference Resolution And Its Application To Entity Linking</a></strong><br><a href=/people/g/gourab-kundu/>Gourab Kundu</a>
|
<a href=/people/a/avirup-sil/>Avi Sil</a>
|
<a href=/people/r/radu-florian/>Radu Florian</a>
|
<a href=/people/w/wael-hamza/>Wael Hamza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2063><div class="card-body p-3 small">We propose an entity-centric neural crosslingual coreference model that builds on multi-lingual embeddings and language independent features. We perform both intrinsic and extrinsic evaluations of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. In the intrinsic evaluation, we show that our model, when trained on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and tested on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, achieves competitive results to the models trained directly on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> respectively. In the extrinsic evaluation, we show that our English model helps achieve superior entity linking accuracy on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese and Spanish test sets</a> than the top 2015 TAC system without using any annotated data from <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> or Spanish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2065 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804053 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2065/>Neural Open Information Extraction</a></strong><br><a href=/people/l/lei-cui/>Lei Cui</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2065><div class="card-body p-3 small">Conventional Open Information Extraction (Open IE) systems are usually built on hand-crafted patterns from other NLP tools such as <a href=https://en.wikipedia.org/wiki/Syntactic_parsing>syntactic parsing</a>, yet they face problems of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. In this paper, we propose a neural Open IE approach with an encoder-decoder framework. Distinct from existing methods, the neural Open IE approach learns highly confident arguments and relation tuples bootstrapped from a state-of-the-art Open IE system. An empirical study on a large benchmark dataset shows that the neural Open IE system significantly outperforms several <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, while maintaining comparable computational efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2066 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2066.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804071 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2066/>Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention</a></strong><br><a href=/people/y/yue-zhao/>Yue Zhao</a>
|
<a href=/people/x/xiaolong-jin/>Xiaolong Jin</a>
|
<a href=/people/y/yuanzhuo-wang/>Yuanzhuo Wang</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2066><div class="card-body p-3 small">Document-level information is very important for event detection even at <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level</a>. In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences. This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences. Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2068 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804096 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2068/>Improving Slot Filling in Spoken Language Understanding with Joint Pointer and Attention</a></strong><br><a href=/people/l/lin-zhao/>Lin Zhao</a>
|
<a href=/people/z/zhe-feng/>Zhe Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2068><div class="card-body p-3 small">We present a generative neural network model for slot filling based on a sequence-to-sequence (Seq2Seq) model together with a pointer network, in the situation where only sentence-level slot annotations are available in the spoken dialogue data. This model predicts slot values by jointly learning to copy a word which may be out-of-vocabulary (OOV) from an input utterance through a pointer network, or generate a word within the vocabulary through an attentional Seq2Seq model. Experimental results show the effectiveness of our slot filling model, especially at addressing the OOV problem. Additionally, we integrate the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> into a spoken language understanding system and achieve the state-of-the-art performance on the <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2070 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2070.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804139 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2070/>Modeling discourse cohesion for discourse parsing via memory network</a></strong><br><a href=/people/y/yanyan-jia/>Yanyan Jia</a>
|
<a href=/people/y/yuan-ye/>Yuan Ye</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/y/yuxuan-lai/>Yuxuan Lai</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2070><div class="card-body p-3 small">Identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance. Most existing approaches design sophisticated <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> or exploit various off-the-shelf tools, but achieve little success. In this paper, we propose a new transition-based discourse parser that makes use of memory networks to take discourse cohesion into account. The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios. Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2071 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2071.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804155 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2071" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2071/>SciDTB : Discourse Dependency TreeBank for Scientific Abstracts<span class=acl-fixed-case>S</span>ci<span class=acl-fixed-case>DTB</span>: Discourse Dependency <span class=acl-fixed-case>T</span>ree<span class=acl-fixed-case>B</span>ank for Scientific Abstracts</a></strong><br><a href=/people/a/an-yang/>An Yang</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2071><div class="card-body p-3 small">Annotation corpus for <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> benefits NLP tasks such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. In this paper, we present SciDTB, a domain-specific discourse treebank annotated on <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific articles</a>. Different from widely-used RST-DT and <a href=https://en.wikipedia.org/wiki/PDTB>PDTB</a>, SciDTB uses dependency trees to represent discourse structure, which is flexible and simplified to some extent but do not sacrifice structural integrity. We discuss the labeling framework, annotation workflow and some statistics about SciDTB. Furthermore, our <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> is made as a benchmark for evaluating discourse dependency parsers, on which we provide several baselines as fundamental work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2075.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2075 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2075 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2075.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2075.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804205 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2075/>Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2075><div class="card-body p-3 small">Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser&#8217;s transition system. We explore using a <a href=https://en.wikipedia.org/wiki/Policy_gradient_method>policy gradient method</a> as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a> by allowing exploration during training ; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle</a> which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2076 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2076.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804218 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2076/>Linear-time Constituency Parsing with <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNNs</a> and <a href=https://en.wikipedia.org/wiki/Dynamic_programming>Dynamic Programming</a><span class=acl-fixed-case>RNN</span>s and Dynamic Programming</a></strong><br><a href=/people/j/juneki-hong/>Juneki Hong</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2076><div class="card-body p-3 small">Recently, span-based constituency parsing has achieved competitive accuracies with extremely simple models by using bidirectional RNNs to model spans. However, the minimal span parser of Stern et al. (2017a) which holds the current state of the art accuracy is a <a href=https://en.wikipedia.org/wiki/Chart_parser>chart parser</a> running in cubic time, O(n^3), which is too slow for longer sentences and for applications beyond sentence boundaries such as end-to-end discourse parsing and joint sentence boundary detection and parsing. We propose a linear-time constituency parser with RNNs and <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a> using graph-structured stack and beam search, which runs in time O(n b^2) where b is the beam size. We further speed this up to O(n b log b) by integrating cube pruning. Compared with chart parsing baselines, this linear-time parser is substantially faster for long sentences on the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> and orders of magnitude faster for discourse parsing, and achieves the highest <a href=https://en.wikipedia.org/wiki/F-number>F1 accuracy</a> on the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> among single model end-to-end systems.<tex-math>O(n^3)</tex-math>, which is too slow for longer sentences and for applications beyond sentence boundaries such as end-to-end discourse parsing and joint sentence boundary detection and parsing. We propose a linear-time constituency parser with RNNs and dynamic programming using graph-structured stack and beam search, which runs in time <tex-math>O(n b^2)</tex-math> where <tex-math>b</tex-math> is the beam size. We further speed this up to <tex-math>O(n b log b)</tex-math> by integrating cube pruning. Compared with chart parsing baselines, this linear-time parser is substantially faster for long sentences on the Penn Treebank and orders of magnitude faster for discourse parsing, and achieves the highest F1 accuracy on the Penn Treebank among single model end-to-end systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2077 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804230 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2077" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2077/>Simpler but More Accurate Semantic Dependency Parsing</a></strong><br><a href=/people/t/timothy-dozat/>Timothy Dozat</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2077><div class="card-body p-3 small">While syntactic dependency annotations concentrate on the surface or functional structure of a sentence, semantic dependency annotations aim to capture between-word relationships that are more closely related to the meaning of a sentence, using graph-structured representations. We extend the LSTM-based syntactic parser of Dozat and Manning (2017) to train on and generate these graph structures. The resulting <a href=https://en.wikipedia.org/wiki/System>system</a> on its own achieves state-of-the-art performance, beating the previous, substantially more complex state-of-the-art system by 0.6 % labeled F1. Adding linguistically richer input representations pushes the margin even higher, allowing us to beat it by 1.9 % labeled F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2079 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2079.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2079" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2079/>Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network</a></strong><br><a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2079><div class="card-body p-3 small">As more and more academic papers are being submitted to conferences and journals, evaluating all these papers by professionals is time-consuming and can cause inequality due to the personal factors of the reviewers. In this paper, in order to assist professionals in evaluating academic papers, we propose a novel task : automatic academic paper rating (AAPR), which automatically determine whether to accept academic papers. We build a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this task and propose a novel modularized hierarchical convolutional neural network to achieve automatic academic paper rating. Evaluation results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by a large margin. The dataset and code are available at<url>https://github.com/lancopku/AAPR</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2080 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2080.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2080/>Automated essay scoring with string kernels and word embeddings</a></strong><br><a href=/people/m/madalina-cozma/>Mădălina Cozma</a>
|
<a href=/people/a/andrei-butnaru/>Andrei Butnaru</a>
|
<a href=/people/r/radu-tudor-ionescu/>Radu Tudor Ionescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2080><div class="card-body p-3 small">In this work, we present an approach based on combining <a href=https://en.wikipedia.org/wiki/String_kernel>string kernels</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for automatic essay scoring. String kernels capture the similarity among strings based on counting common character n-grams, which are a low-level yet powerful type of feature, demonstrating state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification. To our best knowledge, we are the first to apply <a href=https://en.wikipedia.org/wiki/String_(computer_science)>string kernels</a> to automatically score essays. We are also the first to combine them with a high-level semantic feature representation, namely the bag-of-super-word-embeddings. We report the best performance on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings, surpassing recent state-of-the-art deep learning approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2081 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-2081/>Party Matters : Enhancing Legislative Embeddings with Author Attributes for Vote Prediction</a></strong><br><a href=/people/a/anastassia-kornilova/>Anastassia Kornilova</a>
|
<a href=/people/d/daniel-argyle/>Daniel Argyle</a>
|
<a href=/people/v/vladimir-eidelman/>Vladimir Eidelman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2081><div class="card-body p-3 small">Predicting how Congressional legislators will vote is important for understanding their past and future behavior. However, previous work on roll-call prediction has been limited to single session settings, thus not allowing for generalization across sessions. In this paper, we show that text alone is insufficient for modeling voting outcomes in new contexts, as session changes lead to changes in the underlying data generation process. We propose a novel neural method for encoding documents alongside additional <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>, achieving an average of a 4 % boost in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2082 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2082.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2082/>Dynamic and Static Topic Model for Analyzing Time-Series Document Collections</a></strong><br><a href=/people/r/rem-hida/>Rem Hida</a>
|
<a href=/people/n/naoya-takeishi/>Naoya Takeishi</a>
|
<a href=/people/t/takehisa-yairi/>Takehisa Yairi</a>
|
<a href=/people/k/koichi-hori/>Koichi Hori</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2082><div class="card-body p-3 small">For extracting meaningful topics from texts, their structures should be considered properly. In this paper, we aim to analyze structured time-series documents such as a collection of news articles and a series of scientific papers, wherein topics evolve along time depending on multiple topics in the past and are also related to each other at each time. To this end, we propose a dynamic and static topic model, which simultaneously considers the dynamic structures of the temporal topic evolution and the static structures of the topic hierarchy at each time. We show the results of experiments on collections of scientific papers, in which the proposed <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> outperformed conventional <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Moreover, we show an example of extracted topic structures, which we found helpful for analyzing research activities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2083 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2083.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2083/>PhraseCTM : Correlated Topic Modeling on Phrases within <a href=https://en.wikipedia.org/wiki/Markov_random_field>Markov Random Fields</a><span class=acl-fixed-case>P</span>hrase<span class=acl-fixed-case>CTM</span>: Correlated Topic Modeling on Phrases within <span class=acl-fixed-case>M</span>arkov Random Fields</a></strong><br><a href=/people/w/weijing-huang/>Weijing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2083><div class="card-body p-3 small">Recent emerged phrase-level topic models are able to provide topics of phrases, which are easy to read for humans. But these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are lack of the ability to capture the <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation structure</a> among the discovered numerous topics. We propose a novel topic model PhraseCTM and a two-stage method to find out the correlated topics at phrase level. In the first stage, we train PhraseCTM, which models the generation of words and phrases simultaneously by linking the phrases and component words within <a href=https://en.wikipedia.org/wiki/Markov_random_field>Markov Random Fields</a> when they are semantically coherent. In the second stage, we generate the correlation of topics from PhraseCTM. We evaluate our method by a quantitative experiment and a human study, showing the correlated topic modeling on phrases is a good and practical way to interpret the underlying themes of a corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2085 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2085.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2085.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2085/>Learning with Structured Representations for Negation Scope Extraction</a></strong><br><a href=/people/h/hao-li/>Hao Li</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2085><div class="card-body p-3 small">We report an empirical study on the task of negation scope extraction given the negation cue. Our key observation is that certain useful information such as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> related to negation cue, long-distance dependencies as well as some latent structural information can be exploited for such a task. We design approaches based on conditional random fields (CRF), semi-Markov CRF, as well as latent-variable CRF models to capture such information. Extensive experiments on several standard datasets demonstrate that our approaches are able to achieve better results than existing approaches reported in the literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2086 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2086.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2086" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2086/>End-Task Oriented Textual Entailment via Deep Explorations of Inter-Sentence Interactions</a></strong><br><a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2086><div class="card-body p-3 small">This work deals with SciTail, a natural entailment challenge derived from a multi-choice question answering problem. The premises and hypotheses in SciTail were generated with no awareness of each other, and did not specifically aim at the entailment task. This makes it more challenging than other entailment data sets and more directly useful to the end-task question answering. We propose DEISTE (deep explorations of inter-sentence interactions for textual entailment) for this entailment task. Given word-to-word interactions between the premise-hypothesis pair (P, H), DEISTE consists of : (i) a parameter-dynamic convolution to make important words in P and H play a dominant role in learnt representations ; and (ii) a position-aware attentive convolution to encode the representation and position information of the aligned word pairs. Experiments show that DEISTE gets 5 % improvement over prior state of the art and that the pretrained DEISTE on SciTail generalizes well on RTE-5.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2088 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2088.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2088.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2088/>A Rank-Based Similarity Metric for Word Embeddings</a></strong><br><a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/h/hongmin-wang/>Hongmin Wang</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2088><div class="card-body p-3 small">Word Embeddings have recently imposed themselves as a standard for representing word meaning in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. Semantic similarity between word pairs has become the most common evaluation benchmark for these representations, with <a href=https://en.wikipedia.org/wiki/Trigonometric_functions>vector cosine</a> being typically used as the only similarity metric. In this paper, we report experiments with a rank-based metric for WE, which performs comparably to vector cosine in similarity estimation and outperforms it in the recently-introduced and challenging task of <a href=https://en.wikipedia.org/wiki/Outlier_detection>outlier detection</a>, thus suggesting that rank-based measures can improve clustering quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2089 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2089.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2089/>Addressing Noise in Multidialectal Word Embeddings</a></strong><br><a href=/people/a/alexander-erdmann/>Alexander Erdmann</a>
|
<a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2089><div class="card-body p-3 small">Word embeddings are crucial to many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>. The quality of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> relies on large non-noisy corpora. Arabic dialects lack <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>large corpora</a> and are noisy, being linguistically disparate with no <a href=https://en.wikipedia.org/wiki/Standard_language>standardized spelling</a>. We make three contributions to address this noise. First, we describe simple but effective adaptations to word embedding tools to maximize the informative content leveraged in each training sentence. Second, we analyze methods for representing disparate dialects in one embedding space, either by mapping individual dialects into a shared space or learning a joint model of all dialects. Finally, we evaluate via dictionary induction, showing that two metrics not typically reported in the task enable us to analyze our contributions&#8217; effects on low and high frequency words. In addition to boosting performance between 2-53 %, we specifically improve on noisy, low frequency forms without compromising <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on high frequency forms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2090 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2090.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2090/>GNEG : Graph-Based Negative Sampling for word2vec<span class=acl-fixed-case>GNEG</span>: Graph-Based Negative Sampling for word2vec</a></strong><br><a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2090><div class="card-body p-3 small">Negative sampling is an important component in <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> for distributed word representation learning. We hypothesize that taking into account global, corpus-level information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution. In this purpose we pre-compute word co-occurrence statistics from the corpus and apply to it network algorithms such as <a href=https://en.wikipedia.org/wiki/Random_walk>random walk</a>. We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5 % and improves the performance on word similarity tasks by about 1 % compared to the skip-gram negative sampling baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2091 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2091.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2091/>Unsupervised Learning of Style-sensitive Word Vectors</a></strong><br><a href=/people/r/reina-akama/>Reina Akama</a>
|
<a href=/people/k/kento-watanabe/>Kento Watanabe</a>
|
<a href=/people/s/sho-yokoi/>Sho Yokoi</a>
|
<a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2091><div class="card-body p-3 small">This paper presents the first study aimed at capturing stylistic similarity between words in an unsupervised manner. We propose extending the continuous bag of words (CBOW) embedding model (Mikolov et al., 2013b) to learn style-sensitive word vectors using a wider context window under the assumption that the style of all the words in an utterance is consistent. In addition, we introduce a novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to predict lexical stylistic similarity and to create a benchmark dataset for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our experiment with this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> supports our assumption and demonstrates that the proposed <a href=https://en.wikipedia.org/wiki/Plug-in_(computing)>extensions</a> contribute to the acquisition of style-sensitive word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2094.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2094" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2094/>Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction<span class=acl-fixed-case>CNN</span>-based Sequence Labeling for Aspect Extraction</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/p/philip-s-yu/>Philip S. Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2094><div class="card-body p-3 small">One key task of fine-grained sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on. This paper focuses on supervised aspect extraction using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. Unlike other highly sophisticated supervised deep learning models, this paper proposes a novel and yet simple CNN model employing two types of pre-trained embeddings for <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> : general-purpose embeddings and domain-specific embeddings. Without using any additional <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>, this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves surprisingly good results, outperforming state-of-the-art sophisticated existing methods. To our knowledge, this paper is the first to report such double embeddings based CNN model for <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> and achieve very good results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2095 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2095.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2095/>Will it Blend? Blending Weak and Strong Labeled Data in a <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Network</a> for Argumentation Mining</a></strong><br><a href=/people/e/eyal-shnarch/>Eyal Shnarch</a>
|
<a href=/people/c/carlos-alzate/>Carlos Alzate</a>
|
<a href=/people/l/lena-dankin/>Lena Dankin</a>
|
<a href=/people/m/martin-gleize/>Martin Gleize</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2095><div class="card-body p-3 small">The process of obtaining high quality <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding tasks</a> is often slow, error-prone, complicated and expensive. With the vast usage of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, this issue becomes more notorious since these networks require a large amount of labeled data to produce satisfactory results. We propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to blend high quality but scarce strong labeled data with noisy but abundant weak labeled data during the training of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Experiments in the context of topic-dependent evidence detection with two forms of weak labeled data show the advantages of the blending scheme. In addition, we provide a manually annotated data set for the task of topic-dependent evidence detection. We believe that blending weak and strong labeled data is a general notion that may be applicable to many language understanding tasks, and can especially assist researchers who wish to train a <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> but have a small amount of high quality <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> for their task of interest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2096 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2096.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2096/>Investigating Audio, Video, and Text Fusion Methods for End-to-End Automatic Personality Prediction</a></strong><br><a href=/people/o/onno-kampman/>Onno Kampman</a>
|
<a href=/people/e/elham-j-barezi/>Elham J. Barezi</a>
|
<a href=/people/d/dario-bertero/>Dario Bertero</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2096><div class="card-body p-3 small">We propose a tri-modal architecture to predict Big Five personality trait scores from video clips with different channels for audio, text, and video data. For each <a href=https://en.wikipedia.org/wiki/Communication_channel>channel</a>, stacked Convolutional Neural Networks are employed. The <a href=https://en.wikipedia.org/wiki/Communication_channel>channels</a> are fused both on <a href=https://en.wikipedia.org/wiki/Decision-making>decision-level</a> and by concatenating their respective fully connected layers. It is shown that a multimodal fusion approach outperforms each single modality channel, with an improvement of 9.4 % over the best individual modality (video). Full backpropagation is also shown to be better than a linear combination of modalities, meaning complex interactions between modalities can be leveraged to build better models. Furthermore, we can see the prediction relevance of each <a href=https://en.wikipedia.org/wiki/Locus_(genetics)>modality</a> for each trait. The described <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can be used to increase the <a href=https://en.wikipedia.org/wiki/Emotional_intelligence>emotional intelligence</a> of <a href=https://en.wikipedia.org/wiki/Virtual_agent>virtual agents</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2098 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2098.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2098" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2098/>Parser Training with Heterogeneous Treebanks</a></strong><br><a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/a/aaron-smith/>Aaron Smith</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2098><div class="card-body p-3 small">How to make the most of multiple heterogeneous treebanks when training a monolingual dependency parser is an open question. We start by investigating previously suggested, but little evaluated, strategies for exploiting multiple treebanks based on concatenating training sets, with or without <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. We go on to propose a new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> based on treebank embeddings. We perform experiments for several languages and show that in many cases <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and treebank embeddings lead to substantial improvements over single treebanks or <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation</a>, with average gains of 2.03.5 LAS points. We argue that treebank embeddings should be preferred due to their conceptual simplicity, flexibility and extensibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2099 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2099.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2099/>Generalized chart constraints for efficient PCFG and TAG parsing<span class=acl-fixed-case>PCFG</span> and <span class=acl-fixed-case>TAG</span> parsing</a></strong><br><a href=/people/s/stefan-grunewald/>Stefan Grünewald</a>
|
<a href=/people/s/sophie-henning/>Sophie Henning</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2099><div class="card-body p-3 small">Chart constraints, which specify at which string positions a constituent may begin or end, have been shown to speed up chart parsers for PCFGs. We generalize chart constraints to more expressive grammar formalisms and describe a neural tagger which predicts chart constraints at very high precision. Our constraints accelerate both PCFG and TAG parsing, and combine effectively with other pruning techniques (coarse-to-fine and supertagging) for an overall speedup of two orders of magnitude, while improving accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2100 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2100.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805830 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2100/>Exploring Semantic Properties of Sentence Embeddings</a></strong><br><a href=/people/x/xunjie-zhu/>Xunjie Zhu</a>
|
<a href=/people/t/tingfeng-li/>Tingfeng Li</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2100><div class="card-body p-3 small">Neural vector representations are ubiquitous throughout all subfields of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. While word vectors have been studied in much detail, thus far only little light has been shed on the properties of <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>. In this paper, we assess to what extent prominent sentence embedding methods exhibit select semantic properties. We propose a framework that generate triplets of sentences to explore how changes in the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a> or <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of a given sentence affect the similarities obtained between their sentence embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2103 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2103.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805874 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2103" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2103/>Breaking NLI Systems with Sentences that Require Simple <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Inferences</a><span class=acl-fixed-case>NLI</span> Systems with Sentences that Require Simple Lexical Inferences</a></strong><br><a href=/people/m/max-glockner/>Max Glockner</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2103><div class="card-body p-3 small">We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> is substantially worse across <a href=https://en.wikipedia.org/wiki/System>systems</a> trained on SNLI, demonstrating that these <a href=https://en.wikipedia.org/wiki/System>systems</a> are limited in their generalization ability, failing to capture many simple inferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2106 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285805925 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2106/>Polyglot Semantic Role Labeling</a></strong><br><a href=/people/p/phoebe-mulcaire/>Phoebe Mulcaire</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2106><div class="card-body p-3 small">Previous approaches to multilingual semantic dependency parsing treat languages independently, without exploiting the similarities between semantic structures across languages. We experiment with a new approach where we combine resources from different languages in the CoNLL 2009 shared task to build a single polyglot semantic dependency parser. Notwithstanding the absence of parallel data, and the dissimilarity in annotations between languages, our approach results in improvement in <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance on several languages over a monolingual baseline. Analysis of the polyglot models&#8217; performance provides a new understanding of the similarities and differences between languages in the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2111 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2111.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806005 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2111/>Personalized Language Model for Query Auto-Completion</a></strong><br><a href=/people/a/aaron-jaech/>Aaron Jaech</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2111><div class="card-body p-3 small">Query auto-completion is a <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine feature</a> whereby the <a href=https://en.wikipedia.org/wiki/System>system</a> suggests completed queries as the user types. Recently, the use of a recurrent neural network language model was suggested as a method of generating query completions. We show how an adaptable language model can be used to generate personalized completions and how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can use online updating to make predictions for users not seen during training. The personalized predictions are significantly better than a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> that uses no <a href=https://en.wikipedia.org/wiki/User_(computing)>user information</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2112 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2112.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806016 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2112/>Personalized Review Generation By Expanding Phrases and Attending on Aspect-Aware Representations</a></strong><br><a href=/people/j/jianmo-ni/>Jianmo Ni</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2112><div class="card-body p-3 small">In this paper, we focus on the problem of building <a href=https://en.wikipedia.org/wiki/Assistive_technology>assistive systems</a> that can help users to write reviews. We cast this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries, product titles) provided as input to the <a href=https://en.wikipedia.org/wiki/System>system</a>. We incorporate aspect-level information via an aspect encoder that learns aspect-aware user and item representations. An attention fusion layer is applied to control generation by attending on the outputs of multiple <a href=https://en.wikipedia.org/wiki/Code>encoders</a>. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> successfully learns <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> capable of generating coherent and diverse reviews. In addition, the learned aspect-aware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2113 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2113.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2113.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806034 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2113/>Learning Simplifications for Specific Target Audiences</a></strong><br><a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2113><div class="card-body p-3 small">Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text. Most recent work is based on sequence-to-sequence neural models similar to those used for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>. Different from MT, TS data comprises more elaborate <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a>, such as sentence splitting. It can also contain multiple <a href=https://en.wikipedia.org/wiki/Simplification>simplifications</a> of the same original text targeting different audiences, such as <a href=https://en.wikipedia.org/wiki/Educational_stage>school grade levels</a>. We explore these two features of TS to build <a href=https://en.wikipedia.org/wiki/Physical_model>models</a> tailored for specific grade levels. Our approach uses a standard sequence-to-sequence architecture where the original sequence is annotated with information about the target audience and/or the (predicted) type of simplification operation. We show that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> outperforms state-of-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2114 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2114.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2114.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806055 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2114/>Split and Rephrase : Better Evaluation and Stronger Baselines</a></strong><br><a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2114><div class="card-body p-3 small">Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al., 2017), they suffer from memorization of the training set which contains more than 89 % of the unique simple sentences from the validation and test sets. To aid this, we present a new train-development-test data split and neural models augmented with a copy-mechanism, outperforming the best reported <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by 8.68 BLEU and fostering further progress on the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2115 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806074 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2115/>Autoencoder as Assistant Supervisor : Improving Text Representation for Chinese Social Media Text Summarization<span class=acl-fixed-case>C</span>hinese Social Media Text Summarization</a></strong><br><a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2115><div class="card-body p-3 small">Most of the current abstractive text summarization models are based on the sequence-to-sequence model (Seq2Seq). The source content of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation. Compared with the source content, the annotated summary is short and well written. Moreover, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> shares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a popular Chinese social media dataset. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the state-of-the-art performances on the <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2117 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2117.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2117.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806108 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2117/>On the Practical Computational Power of Finite Precision RNNs for <a href=https://en.wikipedia.org/wiki/Language_recognition>Language Recognition</a><span class=acl-fixed-case>RNN</span>s for Language Recognition</a></strong><br><a href=/people/g/gail-weiss/>Gail Weiss</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/e/eran-yahav/>Eran Yahav</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2117><div class="card-body p-3 small">While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>finite precision</a> whose <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time</a> is linear in the input length. Under these limitations, we show that different RNN variants have different <a href=https://en.wikipedia.org/wiki/Computational_power>computational power</a>. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> and <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>ReLU-RNNs</a> can easily implement counting behavior. We show empirically that the <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTM</a> does indeed learn to effectively use the <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>counting mechanism</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2121 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2121.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2121.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806152 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2121/>Pretraining Sentiment Classifiers with Unlabeled Dialog Data</a></strong><br><a href=/people/t/tohru-shimizu/>Toru Shimizu</a>
|
<a href=/people/n/nobuyuki-shimizu/>Nobuyuki Shimizu</a>
|
<a href=/people/h/hayato-kobayashi/>Hayato Kobayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2121><div class="card-body p-3 small">The huge cost of creating labeled training data is a common problem for supervised learning tasks such as sentiment classification. Recent studies showed that pretraining with unlabeled data via a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> can improve the performance of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification models</a>. In this paper, we take the concept a step further by using a conditional language model, instead of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. Specifically, we address a sentiment classification task for a tweet analysis service as a case study and propose a pretraining strategy with unlabeled dialog data (tweet-reply pairs) via an encoder-decoder model. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> can improve the performance of sentiment classifiers and outperform several state-of-the-art strategies including language model pretraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2122 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2122.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285806165 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-2122/>Disambiguating False-Alarm Hashtag Usages in Tweets for Irony Detection</a></strong><br><a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/c/chiao-chen-chen/>Chiao-Chen Chen</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2122><div class="card-body p-3 small">The reliability of self-labeled data is an important issue when the <a href=https://en.wikipedia.org/wiki/Data>data</a> are regarded as ground-truth for training and testing <a href=https://en.wikipedia.org/wiki/Machine_learning>learning-based models</a>. This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection. We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> that are widely used to collect the training data for irony detection. Furthermore, we apply our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to prune the self-labeled training data. Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on all data.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>