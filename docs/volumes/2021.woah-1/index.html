<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2021.woah-1.pdf>Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)</a></h2><p class=lead><a href=/people/a/aida-mostafazadeh-davani/>Aida Mostafazadeh Davani</a>,
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>,
<a href=/people/m/mathias-lambert/>Mathias Lambert</a>,
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>,
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>,
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.woah-1</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/acl/>ACL</a>
| <a href=/venues/ijcnlp/>IJCNLP</a>
| <a href=/venues/woah/>WOAH</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.woah-1>https://aclanthology.org/2021.woah-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.woah-1.pdf>https://aclanthology.org/2021.woah-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.woah-1.pdf title="Open PDF of 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+5th+Workshop+on+Online+Abuse+and+Harms+%28WOAH+2021%29" title="Search for 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.woah-1.0/>Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)</a></strong><br><a href=/people/a/aida-mostafazadeh-davani/>Aida Mostafazadeh Davani</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/m/mathias-lambert/>Mathias Lambert</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--woah-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.woah-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.woah-1.10/>Improving Counterfactual Generation for Fair Hate Speech Detection</a></strong><br><a href=/people/a/aida-mostafazadeh-davani/>Aida Mostafazadeh Davani</a>
|
<a href=/people/a/ali-omrani/>Ali Omrani</a>
|
<a href=/people/b/brendan-kennedy/>Brendan Kennedy</a>
|
<a href=/people/m/mohammad-atari/>Mohammad Atari</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/m/morteza-dehghani/>Morteza Dehghani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--woah-1--10><div class="card-body p-3 small">Bias mitigation approaches reduce models&#8217; dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features. In <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech detection</a>, however, equalizing model predictions may ignore important differences among targeted social groups, as <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> can contain stereotypical language specific to each SGT. Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs. Our method evaluates the similarity in sentence likelihoods (via pre-trained language models) among <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a>, to treat SGTs equally only within interchangeable contexts. By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--woah-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.woah-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.woah-1.15/>Context Sensitivity Estimation in Toxicity Detection</a></strong><br><a href=/people/a/alexandros-xenos/>Alexandros Xenos</a>
|
<a href=/people/j/john-pavlopoulos/>John Pavlopoulos</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--woah-1--15><div class="card-body p-3 small">User posts whose perceived toxicity depends on the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversational context</a> are rare in current toxicity detection datasets. Hence, toxicity detectors trained on current datasets will also disregard <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>, making the detection of context-sensitive toxicity a lot harder when it occurs. We constructed and publicly release a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 10k posts with two kinds of toxicity labels per post, obtained from annotators who considered (i) both the current post and the previous one as context, or (ii) only the current post. We introduce a new task, context-sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered. Using the new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we show that <a href=https://en.wikipedia.org/wiki/System>systems</a> can be developed for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce additional costs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--woah-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.woah-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.woah-1.18.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.woah-1.18/>When the Echo Chamber Shatters : Examining the Use of Community-Specific Language Post-Subreddit Ban</a></strong><br><a href=/people/m/milo-trujillo/>Milo Trujillo</a>
|
<a href=/people/s/sam-rosenblatt/>Sam Rosenblatt</a>
|
<a href=/people/g/guillermo-de-anda-jauregui/>Guillermo de Anda Jáuregui</a>
|
<a href=/people/e/emily-moog/>Emily Moog</a>
|
<a href=/people/b/briane-paul-v-samson/>Briane Paul V. Samson</a>
|
<a href=/people/l/laurent-hebert-dufresne/>Laurent Hébert-Dufresne</a>
|
<a href=/people/a/allison-m-roth/>Allison M. Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--woah-1--18><div class="card-body p-3 small">Community-level bans are a common tool against groups that enable <a href=https://en.wikipedia.org/wiki/Cyberbullying>online harassment</a> and <a href=https://en.wikipedia.org/wiki/Cyberbullying>harmful speech</a>. Unfortunately, the efficacy of community bans has only been partially studied and with mixed results. Here, we provide a flexible <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methodology</a> to identify in-group language and track user activity on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> both before and after the ban of a community (subreddit). We use a simple word frequency divergence to identify uncommon words overrepresented in a given <a href=https://en.wikipedia.org/wiki/Community>community</a>, not as a proxy for harmful speech but as a linguistic signature of the community. We apply our method to 15 banned subreddits, and find that community response is heterogeneous between <a href=https://en.wikipedia.org/wiki/Reddit>subreddits</a> and between users of a <a href=https://en.wikipedia.org/wiki/Reddit>subreddit</a>. Top users were more likely to become less active overall, while random users often reduced use of in-group language without decreasing activity. Finally, we find some evidence that the effectiveness of <a href=https://en.wikipedia.org/wiki/Ban_(law)>bans</a> aligns with the content of a community. Users of dark humor communities were largely unaffected by bans while users of communities organized around <a href=https://en.wikipedia.org/wiki/White_supremacy>white supremacy</a> and <a href=https://en.wikipedia.org/wiki/Fascism>fascism</a> were the most affected. Altogether, our results show that <a href=https://en.wikipedia.org/wiki/Ban_(law)>bans</a> do not affect all groups or users equally, and pave the way to understanding the effect of <a href=https://en.wikipedia.org/wiki/Ban_(law)>bans</a> across communities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--woah-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.woah-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.woah-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.woah-1.19/>Targets and Aspects in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> Hate Speech</a></strong><br><a href=/people/a/alexander-shvets/>Alexander Shvets</a>
|
<a href=/people/p/paula-fortuna/>Paula Fortuna</a>
|
<a href=/people/j/juan-soler/>Juan Soler</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--woah-1--19><div class="card-body p-3 small">Mainstream research on <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> focused so far predominantly on the task of classifying mainly social media posts with respect to predefined typologies of rather coarse-grained hate speech categories. This may be sufficient if the goal is to detect and delete abusive language posts. However, removal is not always possible due to the legislation of a country. Also, there is evidence that <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> can not be successfully combated by merely removing <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech posts</a> ; they should be countered by <a href=https://en.wikipedia.org/wiki/Education>education</a> and counter-narratives. For this purpose, we need to identify (i) who is the target in a given hate speech post, and (ii) what aspects (or characteristics) of the target are attributed to the target in the post. As the first approximation, we propose to adapt a generic state-of-the-art concept extraction model to the hate speech domain. The outcome of the experiments is promising and can serve as inspiration for further work on the task</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--woah-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.woah-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.woah-1.23/>Racist or Sexist Meme? Classifying Memes beyond Hateful</a></strong><br><a href=/people/h/haris-bin-zia/>Haris Bin Zia</a>
|
<a href=/people/i/ignacio-castro/>Ignacio Castro</a>
|
<a href=/people/g/gareth-tyson/>Gareth Tyson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--woah-1--23><div class="card-body p-3 small">Memes are the combinations of text and images that are often humorous in nature. But, that may not always be the case, and certain combinations of <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> and <a href=https://en.wikipedia.org/wiki/Image>images</a> may depict <a href=https://en.wikipedia.org/wiki/Hatred>hate</a>, referred to as hateful memes. This work presents a multimodal pipeline that takes both visual and textual features from <a href=https://en.wikipedia.org/wiki/Meme>memes</a> into account to (1) identify the protected category (e.g. race, sex etc.) that has been attacked ; and (2) detect the type of attack (e.g. contempt, <a href=https://en.wikipedia.org/wiki/Pejorative>slurs</a> etc.). Our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> uses state-of-the-art pre-trained visual and textual representations, followed by a simple <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression classifier</a>. We employ our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> on the Hateful Memes Challenge dataset with additional newly created fine-grained labels for protected category and type of attack. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an AUROC of 0.96 for identifying the protected category, and 0.97 for detecting the type of attack. We release our code at https://github.com/harisbinzia/HatefulMemes</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>