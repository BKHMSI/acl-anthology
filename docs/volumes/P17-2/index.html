<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/P17-2.pdf>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></h2><p class=lead><a href=/people/r/regina-barzilay/>Regina Barzilay</a>,
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>P17-2</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Vancouver, Canada</dd><dt>Venue:</dt><dd><a href=/venues/acl/>ACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/P17-2>https://aclanthology.org/P17-2</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/P17-2 title="To the current version of the paper by DOI">10.18653/v1/P17-2</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/P17-2.pdf>https://aclanthology.org/P17-2.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/P17-2.pdf title="Open PDF of 'Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+55th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+%28Volume+2%3A+Short+Papers%29" title="Search for 'Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2000/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></strong><br><a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953227 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2001/>Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths<span class=acl-fixed-case>LSTM</span> over Dependency Paths</a></strong><br><a href=/people/f/fei-cheng/>Fei Cheng</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2001><div class="card-body p-3 small">Temporal relation classification is becoming an active research field. Lots of methods have been proposed, while most of them focus on extracting <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> from <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>external resources</a>. Less attention has been paid to a significant advance in a closely related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> : <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. In this work, we borrow a state-of-the-art method in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> by adopting bidirectional long short-term memory (Bi-LSTM) along dependency paths (DP). We make a common root assumption to extend DP representations of cross-sentence links. In the final comparison to two state-of-the-art systems on TimeBank-Dense, our model achieves comparable performance, without using external knowledge, as well as manually annotated attributes of entities (class, tense, polarity, etc.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954327 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2002/>AMR-to-text Generation with Synchronous Node Replacement Grammar<span class=acl-fixed-case>AMR</span>-to-text Generation with Synchronous Node Replacement Grammar</a></strong><br><a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/x/xiaochang-peng/>Xiaochang Peng</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/z/zhiguo-wang/>Zhiguo Wang</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2002><div class="card-body p-3 small">This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, graph-to-string rules are learned using a <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic extraction algorithm</a>. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> gives the state-of-the-art result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953454 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2003/>Lexical Features in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a> : To be Used With Caution</a></strong><br><a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2003><div class="card-body p-3 small">Lexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the <a href=https://en.wikipedia.org/wiki/Phenomenon>linguistic phenomena</a> at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical features</a>, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234951559 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2004/>Alternative Objective Functions for Training MT Evaluation Metrics<span class=acl-fixed-case>MT</span> Evaluation Metrics</a></strong><br><a href=/people/m/milos-stanojevic/>Miloš Stanojević</a>
|
<a href=/people/k/khalil-simaan/>Khalil Sima’an</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2004><div class="card-body p-3 small">MT evaluation metrics are tested for correlation with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> either at the sentence- or the corpus-level. Trained <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> ignore <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus-level judgments</a> and are trained for high sentence-level correlation only. We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized. To this end we present a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> trained for <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus-level</a> and show empirical comparison against a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> trained for <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence-level</a> exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> trained to optimize both objectives simultaneously and show that it is far more stable thanand on average outperformsboth <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on both objectives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2006/>Vector space models for evaluating semantic fluency in autism</a></strong><br><a href=/people/e/emily-prudhommeaux/>Emily Prud’hommeaux</a>
|
<a href=/people/j/jan-van-santen/>Jan van Santen</a>
|
<a href=/people/d/douglas-gliner/>Douglas Gliner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2006><div class="card-body p-3 small">A common <a href=https://en.wikipedia.org/wiki/Test_(assessment)>test</a> administered during <a href=https://en.wikipedia.org/wiki/Neurological_examination>neurological examination</a> is the semantic fluency test, in which the patient must list as many examples of a given semantic category as possible under timed conditions. Poor performance is associated with neurological conditions characterized by impairments in <a href=https://en.wikipedia.org/wiki/Executive_functions>executive function</a>, such as <a href=https://en.wikipedia.org/wiki/Dementia>dementia</a>, <a href=https://en.wikipedia.org/wiki/Schizophrenia>schizophrenia</a>, and <a href=https://en.wikipedia.org/wiki/Autism_spectrum>autism spectrum disorder (ASD)</a>. Methods for analyzing semantic fluency responses at the level of detail necessary to uncover these differences have typically relied on subjective manual annotation. In this paper, we explore automated approaches for scoring semantic fluency responses that leverage ontological resources and distributional semantic models to characterize the semantic fluency responses produced by young children with and without ASD. Using these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>, we find significant differences in the semantic fluency responses of children with ASD, demonstrating the utility of using objective methods for clinical language analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954045 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2008/>Incorporating <a href=https://en.wikipedia.org/wiki/Uncertainty>Uncertainty</a> into <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> for Spoken Language Assessment</a></strong><br><a href=/people/a/andrey-malinin/>Andrey Malinin</a>
|
<a href=/people/a/anton-ragni/>Anton Ragni</a>
|
<a href=/people/k/kate-knill/>Kate Knill</a>
|
<a href=/people/m/mark-gales/>Mark Gales</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2008><div class="card-body p-3 small">There is a growing demand for automatic assessment of spoken English proficiency. These systems need to handle large variations in input data owing to the wide range of candidate skill levels and L1s, and errors from ASR. Some candidates will be a poor match to the training data set, undermining the validity of the predicted grade. For high stakes tests it is essential for such systems not only to grade well, but also to provide a measure of their uncertainty in their predictions, enabling rejection to human graders. Previous work examined Gaussian Process (GP) graders which, though successful, do not scale well with large data sets. Deep Neural Network (DNN) may also be used to provide <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> using Monte-Carlo Dropout (MCD). This paper proposes a novel method to yield uncertainty and compares it to GPs and DNNs with MCD. The proposed approach explicitly teaches a DNN to have low uncertainty on training data and high uncertainty on generated artificial data. On experiments conducted on data from the Business Language Testing Service (BULATS), the proposed approach is found to outperform GPs and DNNs with MCD in uncertainty-based rejection whilst achieving comparable grading performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2009.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952193 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2009/>Incorporating Dialectal Variability for Socially Equitable Language Identification</a></strong><br><a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2009><div class="card-body p-3 small">Language identification (LID) is a critical first step for processing <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual text</a>. Yet most LID systems are not designed to handle the linguistic diversity of global platforms like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, where local dialects and rampant code-switching lead language classifiers to systematically miss minority dialect speakers and multilingual speakers. We propose a new dataset and a character-based sequence-to-sequence model for LID designed to support dialectal and multilingual language varieties. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on multiple LID benchmarks. Furthermore, in a case study using <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> for health tracking, our method substantially increases the availability of texts written by underrepresented populations, enabling the development of socially inclusive NLP tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952906 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2010/>Evaluating Compound Splitters Extrinsically with Textual Entailment</a></strong><br><a href=/people/g/glorianna-jagfeld/>Glorianna Jagfeld</a>
|
<a href=/people/p/patrick-ziering/>Patrick Ziering</a>
|
<a href=/people/l/lonneke-van-der-plas/>Lonneke van der Plas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2010><div class="card-body p-3 small">Traditionally, compound splitters are evaluated intrinsically on <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold-standard data</a> or extrinsically on the task of <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>. We explore a novel way for the extrinsic evaluation of compound splitters, namely recognizing textual entailment. Compound splitting has great potential for this novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that is both transparent and well-defined. Moreover, we show that it addresses certain aspects that are either ignored in intrinsic evaluations or compensated for by taskinternal mechanisms in <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>. We show significant improvements using different compound splitting methods on a German textual entailment dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955092 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2012/>Learning to Parse and Translate Improves <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/a/akiko-eriguchi/>Akiko Eriguchi</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2012><div class="card-body p-3 small">There has been relatively little attention to incorporating linguistic prior to <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2013 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955925 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2013/>On the Distribution of Lexical Features at Multiple Levels of Analysis</a></strong><br><a href=/people/f/fatemeh-almodaresi/>Fatemeh Almodaresi</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a>
|
<a href=/people/v/vivek-kulkarni/>Vivek Kulkarni</a>
|
<a href=/people/m/mohsen-zakeri/>Mohsen Zakeri</a>
|
<a href=/people/s/salvatore-giorgi/>Salvatore Giorgi</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2013><div class="card-body p-3 small">Natural language processing has increasingly moved from modeling documents and words toward studying the people behind the language. This move to working with data at the user or community level has presented the field with different characteristics of linguistic data. In this paper, we empirically characterize various lexical distributions at different levels of analysis, showing that, while most features are decidedly sparse and non-normal at the message-level (as with traditional NLP), they follow the <a href=https://en.wikipedia.org/wiki/Central_limit_theorem>central limit theorem</a> to become much more <a href=https://en.wikipedia.org/wiki/Log-normal_distribution>Log-normal</a> or even Normal at the user- and county-levels. Finally, we demonstrate that modeling lexical features for the correct level of analysis leads to marked improvements in common social scientific prediction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956698 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2014/>Exploring Neural Text Simplification Models</a></strong><br><a href=/people/s/sergiu-nisioi/>Sergiu Nisioi</a>
|
<a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2014><div class="card-body p-3 small">We present the first attempt at using sequence to sequence neural networks to model text simplification (TS). Unlike the previously proposed automated TS systems, our neural text simplification (NTS) systems are able to simultaneously perform lexical simplification and content reduction. An extensive human evaluation of the output has shown that NTS systems achieve almost perfect <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a> and meaning preservation of output sentences and higher level of simplification than the state-of-the-art automated TS systems</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2017.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2017.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958413 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2017/>Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection</a></strong><br><a href=/people/y/youxuan-jiang/>Youxuan Jiang</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/w/walter-lasecki/>Walter S. Lasecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2017><div class="card-body p-3 small">Linguistically diverse datasets are critical for training and evaluating robust <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning systems</a>, but <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> is a costly process that often requires experts. Crowdsourcing the process of <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> is an effective means of expanding <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language datasets</a>, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>instructions</a>, <a href=https://en.wikipedia.org/wiki/Incentive>incentives</a>, <a href=https://en.wikipedia.org/wiki/Data_domain>data domains</a>, and <a href=https://en.wikipedia.org/wiki/Workflow>workflows</a>. We manually analyzed paraphrases for correctness, <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, and <a href=https://en.wikipedia.org/wiki/Linguistic_diversity>linguistic diversity</a>. Our observations provide new insight into the trade-offs between <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and diversity in crowd responses that arise as a result of <a href=https://en.wikipedia.org/wiki/Design_of_experiments>task design</a>, providing guidance for future paraphrase generation procedures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957642 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2018/>Arc-swift : A Novel Transition System for Dependency Parsing</a></strong><br><a href=/people/p/peng-qi/>Peng Qi</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2018><div class="card-body p-3 small">Transition-based dependency parsers often need sequences of local shift and reduce operations to produce certain attachments. Correct individual decisions hence require global information about the sentence context and mistakes cause <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. This paper proposes a novel <a href=https://en.wikipedia.org/wiki/Transition_(computer_science)>transition system</a>, arc-swift, that enables direct attachments between tokens farther apart with a single <a href=https://en.wikipedia.org/wiki/Transition_(computer_science)>transition</a>. This allows the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to leverage <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical information</a> more directly in transition decisions. Hence, arc-swift can achieve significantly better performance with a very small <a href=https://en.wikipedia.org/wiki/Beam_diameter>beam size</a>. Our <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> reduce error by 3.77.6 % relative to those using existing transition systems on the Penn Treebank dependency parsing task and English Universal Dependencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957682 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2019/>A Generative Parser with a Discriminative Recognition Algorithm</a></strong><br><a href=/people/j/jianpeng-cheng/>Jianpeng Cheng</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2019><div class="card-body p-3 small">Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and are often outperformed by discriminative models. We propose a framework for parsing and language modeling which marries a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on <a href=https://en.wikipedia.org/wiki/Expectation_maximization>expectation maximization</a> and <a href=https://en.wikipedia.org/wiki/Variational_inference>variational inference</a>, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treen-bank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art single-model language modeling score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2021.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955359 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2021/>Towards String-To-Tree Neural Machine Translation</a></strong><br><a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2021><div class="card-body p-3 small">We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. An experiment on the WMT16 German-English news translation task resulted in an improved BLEU score when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during <a href=https://en.wikipedia.org/wiki/Translation>translation</a> in comparison to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. A small-scale human evaluation also showed an advantage to the syntax-aware system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2022 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956090 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2022/>Learning Lexico-Functional Patterns for First-Person Affect</a></strong><br><a href=/people/l/lena-reed/>Lena Reed</a>
|
<a href=/people/j/jiaqi-wu/>Jiaqi Wu</a>
|
<a href=/people/s/shereen-oraby/>Shereen Oraby</a>
|
<a href=/people/p/pranav-anand/>Pranav Anand</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2022><div class="card-body p-3 small">Informal first-person narratives are a unique resource for computational models of everyday events and people&#8217;s affective reactions to them. People blogging about their day tend not to explicitly say I am happy. Instead they describe situations from which other humans can readily infer their <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective reactions</a>. However current sentiment dictionaries are missing much of the information needed to make similar inferences. We build on recent work that models affect in terms of lexical predicate functions and affect on the predicate&#8217;s arguments. We present a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> to learn proxies for these <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a> from <a href=https://en.wikipedia.org/wiki/First-person_narrative>first-person narratives</a>. We construct a novel fine-grained test set, and show that the patterns we learn improve our ability to predict first-person affective reactions to everyday events, from a Stanford sentiment baseline of.67F to.75F.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956125 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2023/>Lifelong Learning CRF for Supervised Aspect Extraction<span class=acl-fixed-case>CRF</span> for Supervised Aspect Extraction</a></strong><br><a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2023><div class="card-body p-3 small">This paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can still improve its extraction with experiences in its applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956168 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2024/>Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization</a></strong><br><a href=/people/y/ye-zhang/>Ye Zhang</a>
|
<a href=/people/m/matthew-lease/>Matthew Lease</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2024><div class="card-body p-3 small">A fundamental advantage of neural models for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is their ability to learn <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> or <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>domain specific ontologies</a> such as the <a href=https://en.wikipedia.org/wiki/Unified_Medical_Language_System>Unified Medical Language System (UMLS)</a>. We propose a general, novel method for exploiting such <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resources</a> via weight sharing. Prior work on weight sharing in <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> has considered it largely as a means of <a href=https://en.wikipedia.org/wiki/Data_compression>model compression</a>. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957130 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2025/>Improving Neural Parsing by Disentangling Model Combination and Reranking Effects</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/m/mitchell-stern/>Mitchell Stern</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2025><div class="card-body p-3 small">Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for direct search in these <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a>. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955606 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2026/>Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function</a></strong><br><a href=/people/o/oren-melamud/>Oren Melamud</a>
|
<a href=/people/j/jacob-goldberger/>Jacob Goldberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2026><div class="card-body p-3 small">In this paper we define a measure of dependency between two <a href=https://en.wikipedia.org/wiki/Random_variable>random variables</a>, based on the Jensen-Shannon (JS) divergence between their <a href=https://en.wikipedia.org/wiki/Joint_probability_distribution>joint distribution</a> and the product of their <a href=https://en.wikipedia.org/wiki/Marginal_distribution>marginal distributions</a>. Then, we show that word2vec&#8217;s skip-gram with negative sampling embedding algorithm finds the optimal low-dimensional approximation of this JS dependency measure between the words and their contexts. The gap between the optimal score and the low-dimensional approximation is demonstrated on a standard <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234959028 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2028/>The Role of <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>Prosody</a> and <a href=https://en.wikipedia.org/wiki/Speech_register>Speech Register</a> in <a href=https://en.wikipedia.org/wiki/Word_segmentation>Word Segmentation</a> : A Computational Modelling Perspective</a></strong><br><a href=/people/b/bogdan-ludusan/>Bogdan Ludusan</a>
|
<a href=/people/r/reiko-mazuka/>Reiko Mazuka</a>
|
<a href=/people/m/mathieu-bernard/>Mathieu Bernard</a>
|
<a href=/people/a/alejandrina-cristia/>Alejandrina Cristia</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2028><div class="card-body p-3 small">This study explores the role of <a href=https://en.wikipedia.org/wiki/Speech_register>speech register</a> and <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosody</a> for the task of <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>. Since these two factors are thought to play an important role in early language acquisition, we aim to quantify their contribution for this task. We study a Japanese corpus containing both infant- and adult-directed speech and we apply four different word segmentation models, with and without knowledge of <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosodic boundaries</a>. The results showed that the difference between registers is smaller than previously reported and that prosodic boundary information helps more adult- than infant-directed speech.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234959057 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2029/>A Two-Stage Parsing Method for Text-Level Discourse Analysis</a></strong><br><a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2029><div class="card-body p-3 small">Previous work introduced transition-based algorithms to form a unified architecture of parsing rhetorical structures (including span, nuclearity and relation), but did not achieve satisfactory performance. In this paper, we propose that transition-based model is more appropriate for parsing the naked discourse tree (i.e., identifying span and nuclearity) due to data sparsity. At the same time, we argue that relation labeling can benefit from naked tree structure and should be treated elaborately with consideration of three kinds of <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> including within-sentence, across-sentence and across-paragraph relations. Thus, we design a pipelined two-stage parsing method for generating an RST tree from text. Experimental results show that our method achieves state-of-the-art performance, especially on span and nuclearity identification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2030 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234959088 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2030/>Error-repair Dependency Parsing for Ungrammatical Texts</a></strong><br><a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2030><div class="card-body p-3 small">We propose a new dependency parsing scheme which jointly parses a sentence and repairs grammatical errors by extending the non-directional transition-based formalism of Goldberg and Elhadad (2010) with three additional actions : SUBSTITUTE, DELETE, INSERT. Because these actions may cause an infinite loop in derivation, we also introduce simple constraints that ensure the <a href=https://en.wikipedia.org/wiki/Parsing>parser termination</a>. We evaluate our model with respect to dependency accuracy and grammaticality improvements for ungrammatical sentences, demonstrating the robustness and applicability of our scheme.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2031 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2031.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234946385 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2031/>Attention Strategies for Multi-Source Sequence-to-Sequence Learning</a></strong><br><a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/j/jindrich-helcl/>Jindřich Helcl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2031><div class="card-body p-3 small">Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> over each source sequence, flat and hierarchical. We compare the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> with existing techniques and present results of systematic evaluation of those <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on the WMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> achieve competitive results on both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234946757 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2033/>A Neural Model for User Geolocation and Lexical Dialectology</a></strong><br><a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2033><div class="card-body p-3 small">We propose a simple yet effective text-based user geolocation model based on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> with one hidden layer, which achieves state of the art performance over three Twitter benchmark geolocation datasets, in addition to producing word and phrase embeddings in the hidden layer that we show to be useful for detecting dialectal terms. As part of our analysis of dialectal terms, we release DAREDS, a dataset for evaluating dialect term detection methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2035/>Neural Architecture for Temporal Relation Extraction : A Bi-LSTM Approach for Detecting Narrative Containers<span class=acl-fixed-case>B</span>i-<span class=acl-fixed-case>LSTM</span> Approach for Detecting Narrative Containers</a></strong><br><a href=/people/j/julien-tourille/>Julien Tourille</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2035><div class="card-body p-3 small">We present a neural architecture for containment relation identification between medical events and/or temporal expressions. We experiment on a corpus of de-identified clinical notes in English from the Mayo Clinic, namely the THYME corpus. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> of 0.613 and outperforms the best result reported on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to date.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2036 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2036/>How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models</a></strong><br><a href=/people/z/zhiliang-tian/>Zhiliang Tian</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/y/yiping-song/>Yiping Song</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2036><div class="card-body p-3 small">Generative conversational systems are attracting increasing attention in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. Recently, researchers have noticed the importance of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> in dialog processing, and built various <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>. However, there is no systematic comparison to analyze how to use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> effectively. In this paper, we conduct an empirical study to compare various <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and investigate the effect of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> in <a href=https://en.wikipedia.org/wiki/Dialogue>dialog systems</a>. We also propose a variant that explicitly weights context vectors by context-query relevance, outperforming the other baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2037 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2037/>Cross-lingual and cross-domain discourse segmentation of entire documents</a></strong><br><a href=/people/c/chloe-braud/>Chloé Braud</a>
|
<a href=/people/o/ophelie-lacroix/>Ophélie Lacroix</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2037><div class="card-body p-3 small">Discourse segmentation is a crucial step in building end-to-end discourse parsers. However, discourse segmenters only exist for a few languages and domains. Typically they only detect intra-sentential segment boundaries, assuming gold standard sentence and token segmentation, and relying on high-quality syntactic parses and rich heuristics that are not generally available across languages and domains. In this paper, we propose statistical discourse segmenters for five languages and three domains that do not rely on gold pre-annotations. We also consider the problem of learning discourse segmenters when no labeled data is available for a language. Our <a href=https://en.wikipedia.org/wiki/Supervised_learning>fully supervised system</a> obtains 89.5 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> for <a href=https://en.wikipedia.org/wiki/News_agency>English newswire</a>, with slight drops in performance on other domains, and we report supervised and unsupervised (cross-lingual) results for five languages in total.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2039 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2039/>Argumentation Quality Assessment : Theory vs. Practice</a></strong><br><a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/n/nona-naderi/>Nona Naderi</a>
|
<a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2039><div class="card-body p-3 small">Argumentation quality is viewed differently in <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation theory</a> and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by <a href=https://en.wikipedia.org/wiki/Theory>theory</a>. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2040 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2040/>A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations<span class=acl-fixed-case>C</span>hinese Implicit Discourse Relations</a></strong><br><a href=/people/s/samuel-ronnqvist/>Samuel Rönnqvist</a>
|
<a href=/people/n/niko-schenk/>Niko Schenk</a>
|
<a href=/people/c/christian-chiarcos/>Christian Chiarcos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2040><div class="card-body p-3 small">We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to selectively focus on the relevant parts of an input sequence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2042 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2042/>Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings</a></strong><br><a href=/people/c/changxing-wu/>Changxing Wu</a>
|
<a href=/people/x/xiaodong-shi/>Xiaodong Shi</a>
|
<a href=/people/y/yidong-chen/>Yidong Chen</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a>
|
<a href=/people/b/boli-wang/>Boli Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2042><div class="card-body p-3 small">We introduce a simple and effective method to learn discourse-specific word embeddings (DSWE) for implicit discourse relation recognition. Specifically, DSWE is learned by performing connective classification on massive explicit discourse data, and capable of capturing discourse relationships between words. On the PDTB data set, using DSWE as <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> achieves significant improvements over <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2043/>Oracle Summaries of Compressive Summarization</a></strong><br><a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nishino/>Masaaki Nishino</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2043><div class="card-body p-3 small">This paper derives an Integer Linear Programming (ILP) formulation to obtain an oracle summary of the compressive summarization paradigm in terms of ROUGE. The oracle summary is essential to reveal the upper bound performance of the <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a>. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2044 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2044/>Japanese Sentence Compression with a Large Training Dataset<span class=acl-fixed-case>J</span>apanese Sentence Compression with a Large Training Dataset</a></strong><br><a href=/people/s/shun-hasegawa/>Shun Hasegawa</a>
|
<a href=/people/y/yuta-kikuchi/>Yuta Kikuchi</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2044><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/English_language>English</a>, high-quality sentence compression models by deleting words have been trained on automatically created large training datasets. We work on Japanese sentence compression by a similar approach. To create a large Japanese training dataset, a method of creating English training dataset is modified based on the characteristics of the <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese language</a>. The created <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is used to train Japanese sentence compression models based on the <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2046 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2046/>English Event Detection With Translated Language Features<span class=acl-fixed-case>E</span>nglish Event Detection With Translated Language Features</a></strong><br><a href=/people/s/sam-wei/>Sam Wei</a>
|
<a href=/people/i/igor-korostil/>Igor Korostil</a>
|
<a href=/people/j/joel-nothman/>Joel Nothman</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2046><div class="card-body p-3 small">We propose novel radical features from <a href=https://en.wikipedia.org/wiki/Automatic_translation>automatic translation</a> for <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a>. Event detection is a complex language processing task for which it is expensive to collect training data, making <a href=https://en.wikipedia.org/wiki/Generalization>generalisation</a> challenging. We derive meaningful subword features from <a href=https://en.wikipedia.org/wiki/Automatic_translation>automatic translations</a> into target language. Results suggest this <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is particularly useful when using languages with writing systems that facilitate easy decomposition into subword features, e.g., logograms and <a href=https://en.wikipedia.org/wiki/Cangjie>Cangjie</a>. The best result combines logogram features from <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> with syllable features from <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>, providing an additional 3.0 points <a href=https://en.wikipedia.org/wiki/F-score>f-score</a> when added to state-of-the-art generalisation features on the TAC KBP 2015 Event Nugget task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2047 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2047/>EviNets : <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> for Combining Evidence Signals for Factoid Question Answering<span class=acl-fixed-case>E</span>vi<span class=acl-fixed-case>N</span>ets: Neural Networks for Combining Evidence Signals for Factoid Question Answering</a></strong><br><a href=/people/d/denis-savenkov/>Denis Savenkov</a>
|
<a href=/people/e/eugene-agichtein/>Eugene Agichtein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2047><div class="card-body p-3 small">A critical task for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> is the final answer selection stage, which has to combine multiple signals available about each answer candidate. This paper proposes EviNets : a novel <a href=https://en.wikipedia.org/wiki/Neural_network>neural network architecture</a> for factoid question answering. EviNets scores candidate answer entities by combining the available supporting evidence, e.g., <a href=https://en.wikipedia.org/wiki/Knowledge_base>structured knowledge bases</a> and <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text documents</a>. EviNets represents each piece of evidence with a dense embeddings vector, scores their relevance to the question, and aggregates the support for each candidate to predict their final scores. Each of the <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> is generic and allows plugging in a variety of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for semantic similarity scoring and <a href=https://en.wikipedia.org/wiki/Information_aggregation>information aggregation</a>. We demonstrate the effectiveness of EviNets in experiments on the existing TREC QA and WikiMovies benchmarks, and on the new Yahoo ! Answers dataset introduced in this paper. EviNets can be extended to other information types and could facilitate future work on combining evidence signals for joint reasoning in question answering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2048 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2048/>Pocket Knowledge Base Population</a></strong><br><a href=/people/t/travis-wolfe/>Travis Wolfe</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2048><div class="card-body p-3 small">Existing Knowledge Base Population methods extract relations from a closed relational schema with limited coverage leading to sparse KBs. We propose Pocket Knowledge Base Population (PKBP), the task of dynamically constructing a KB of entities related to a query and finding the best characterization of relationships between entities. We describe novel Open Information Extraction methods which leverage the PKB to find informative trigger words. We evaluate using existing KBP shared-task data as well anew annotations collected for this work. Our methods produce high quality KB from just text with many more entities and relationships than existing KBP systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2049 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2049.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2049/>Answering Complex Questions Using Open Information Extraction</a></strong><br><a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2049><div class="card-body p-3 small">While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>, but to date such <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> has only been used to answer simple questions with retrieval-based methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for <a href=https://en.wikipedia.org/wiki/Open_IE>Open IE</a>, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2050 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2050/>Bootstrapping for Numerical Open IE<span class=acl-fixed-case>IE</span></a></strong><br><a href=/people/s/swarnadeep-saha/>Swarnadeep Saha</a>
|
<a href=/people/h/harinder-pal/>Harinder Pal</a>
|
<a href=/people/m/mausam/>Mausam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2050><div class="card-body p-3 small">We design and release BONIE, the first open numerical relation extractor, for extracting Open IE tuples where one of the arguments is a number or a quantity-unit phrase. BONIE uses <a href=https://en.wikipedia.org/wiki/Bootstrapping_(compilers)>bootstrapping</a> to learn the specific dependency patterns that express numerical relations in a sentence. BONIE&#8217;s novelty lies in task-specific customizations, such as inferring implicit relations, which are clear due to context such as units (for e.g., &#8216;square kilometers&#8217; suggests area, even if the word &#8216;area&#8217; is missing in the sentence). BONIE obtains 1.5x <a href=https://en.wikipedia.org/wiki/Yield_(engineering)>yield</a> and 15 point precision gain on numerical facts over a state-of-the-art Open IE system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2051 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2051/>Feature-Rich Networks for Knowledge Base Completion</a></strong><br><a href=/people/a/alexandros-komninos/>Alexandros Komninos</a>
|
<a href=/people/s/suresh-manandhar/>Suresh Manandhar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2051><div class="card-body p-3 small">We propose jointly modelling <a href=https://en.wikipedia.org/wiki/Knowledge_base>Knowledge Bases</a> and aligned text with Feature-Rich Networks. Our models perform Knowledge Base Completion by learning to represent and compose diverse feature types from partially aligned and noisy resources. We perform experiments on <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> utilizing additional entity type information and syntactic textual relations. Our evaluation suggests that the proposed models can better incorporate side information than previously proposed combinations of bilinear models with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>, showing large improvements when scoring the plausibility of unobserved facts with associated textual mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2052/>Fine-Grained Entity Typing with High-Multiplicity Assignments</a></strong><br><a href=/people/m/maxim-rabinovich/>Maxim Rabinovich</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2052><div class="card-body p-3 small">As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2053 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2053/>Group Sparse CNNs for Question Classification with Answer Sets<span class=acl-fixed-case>CNN</span>s for Question Classification with Answer Sets</a></strong><br><a href=/people/m/mingbo-ma/>Mingbo Ma</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a>
|
<a href=/people/b/bowen-zhou/>Bowen Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2053><div class="card-body p-3 small">Question classification is an important task with wide applications. However, traditional techniques treat questions as general sentences, ignoring the corresponding answer data. In order to consider answer information into question modeling, we first introduce novel group sparse autoencoders which refine question representation by utilizing group information in the answer set. We then propose novel group sparse CNNs which naturally learn question representation with respect to their answers by implanting group sparse autoencoders into traditional CNNs. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperform strong <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a> on four datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2055.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2055.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2055/>Cardinal Virtues : Extracting Relation Cardinalities from Text</a></strong><br><a href=/people/p/paramita-mirza/>Paramita Mirza</a>
|
<a href=/people/s/simon-razniewski/>Simon Razniewski</a>
|
<a href=/people/f/fariz-darari/>Fariz Darari</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2055><div class="card-body p-3 small">Information extraction (IE) from text has largely focused on relations between individual entities, such as who has won which award. However, some facts are never fully mentioned, and no IE method has perfect <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>. Thus, it is beneficial to also tap contents about the cardinalities of these relations, for example, how many awards someone has won. We introduce this novel <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> of extracting cardinalities and discusses the specific challenges that set it apart from standard IE. We present a distant supervision method using <a href=https://en.wikipedia.org/wiki/Conditional_random_field>conditional random fields</a>. A preliminary evaluation results in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> between 3 % and 55 %, depending on the difficulty of relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2056 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2056/>Integrating <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Linguistic Features</a> in Factuality Prediction over Unified Datasets</a></strong><br><a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/j/judith-eckle-kohler/>Judith Eckle-Kohler</a>
|
<a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2056><div class="card-body p-3 small">Previous models for the assessment of commitment towards a predicate in a sentence (also known as factuality prediction) were trained and tested against a specific annotated dataset, subsequently limiting the generality of their results. In this work we propose an intuitive method for mapping three previously annotated corpora onto a single factuality scale, thereby enabling <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to be tested across these <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>. In addition, we design a novel model for factuality prediction by first extending a previous rule-based factuality prediction system and applying it over an abstraction of dependency trees, and then using the output of this system in a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifier</a>. We show that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on all three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We make both the unified factuality corpus and our new model publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2057/>Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks</a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2057><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Question_answering>question answering methods</a> infer answers either from a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> or from <a href=https://en.wikipedia.org/wiki/Text_corpus>raw text</a>. While knowledge base (KB) methods are good at answering compositional questions, their performance is often affected by the incompleteness of the KB. Au contraire, <a href=https://en.wikipedia.org/wiki/Web_page>web text</a> contains millions of facts that are absent in the KB, however in an <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured form</a>. Universal schema can support reasoning on the union of both structured KBs and <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a> by aligning them in a common embedded space. In this paper we extend universal schema to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language question answering</a>, employing Memory networks to attend to the large body of facts in the combination of text and KB. Our <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> can be trained in an end-to-end fashion on question-answer pairs. Evaluation results on Spades fill-in-the-blank question answering dataset show that exploiting universal schema for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> is better than using either a KB or text alone. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> also outperforms the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by 8.5 F1 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2059/>A <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Network</a> with Visual Text Composition Behavior</a></strong><br><a href=/people/h/hongyu-guo/>Hongyu Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2059><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a> are compositional, how state-of-the-art neural models achieve <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a> is still unclear. We propose a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep network</a>, which not only achieves competitive <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the <a href=https://en.wikipedia.org/wiki/Social_network>network</a> distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2060.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2060/>Neural System Combination for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/w/wenpeng-hu/>Wenpeng Hu</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2060><div class="card-body p-3 small">Neural machine translation (NMT) becomes a new approach to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy. It is therefore a promising direction to combine the advantages of both <a href=https://en.wikipedia.org/wiki/Network_topology>NMT</a> and <a href=https://en.wikipedia.org/wiki/Network_topology>SMT</a>. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chinese-to-English translation task show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2061 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2061.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2061/>An Empirical Comparison of Domain Adaptation Methods for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/c/chenhui-chu/>Chenhui Chu</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2061><div class="card-body p-3 small">In this paper, we propose a novel domain adaptation method named mixed fine tuning for neural machine translation (NMT). We combine two existing approaches namely <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine tuning</a> and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> which is a mix of the in-domain and out-of-domain corpora. All <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> are augmented with <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>artificial tags</a> to indicate specific domains. We empirically compare our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> against <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine tuning</a> and multi domain methods and discuss its benefits and shortcomings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2062 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2062/>Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings</a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2062><div class="card-body p-3 small">We propose a new method for extracting pseudo-parallel sentences from a pair of large monolingual corpora, without relying on any document-level information. Our method first exploits <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in order to efficiently evaluate trillions of candidate sentence pairs and then a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to find the most reliable ones. We report significant improvements in <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> when using a <a href=https://en.wikipedia.org/wiki/Machine_translation>translation model</a> trained on the sentence pairs extracted from in-domain monolingual corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2063 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2063/>Feature Hashing for Language and Dialect Identification</a></strong><br><a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/m/mark-dras/>Mark Dras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2063><div class="card-body p-3 small">We evaluate <a href=https://en.wikipedia.org/wiki/Feature_hashing>feature hashing</a> for language identification (LID), a <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> not previously used for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Using a standard dataset, we first show that while <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> performance is high, LID data is highly dimensional and mostly sparse (99.5 %) as it includes large vocabularies for many languages ; memory requirements grow as languages are added. Next we apply <a href=https://en.wikipedia.org/wiki/Hash_function>hashing</a> using various hash sizes, demonstrating that there is no performance loss with <a href=https://en.wikipedia.org/wiki/Dimensionality_reduction>dimensionality reductions</a> of up to 86 %. We also show that using an ensemble of low-dimension hash-based classifiers further boosts performance. Feature hashing is highly useful for LID and holds great promise for future work in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2064 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2064.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2064/>Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM<span class=acl-fixed-case>C</span>hinese Word Usage Errors for Non-Native <span class=acl-fixed-case>C</span>hinese Learners with Bidirectional <span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/y/yow-ting-shiue/>Yow-Ting Shiue</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2064><div class="card-body p-3 small">Selecting appropriate words to compose a sentence is one common problem faced by <a href=https://en.wikipedia.org/wiki/Foreign_language>non-native Chinese learners</a>. In this paper, we propose (bidirectional) LSTM sequence labeling models and explore various <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to detect word usage errors in Chinese sentences. By combining CWINDOW word embedding features and POS information, the best bidirectional LSTM model achieves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> 0.5138 and MRR 0.6789 on the HSK dataset. For 80.79 % of the test data, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> ranks the ground-truth within the top two at position level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2065 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2065.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2065/>Automatic Compositor Attribution in the First Folio of Shakespeare</a></strong><br><a href=/people/m/maria-ryskina/>Maria Ryskina</a>
|
<a href=/people/h/hannah-alpert-abrams/>Hannah Alpert-Abrams</a>
|
<a href=/people/d/dan-garrette/>Dan Garrette</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2065><div class="card-body p-3 small">Compositor attribution, the clustering of pages in a historical printed document by the individual who set the type, is a bibliographic task that relies on analysis of orthographic variation and inspection of visual details of the printed page. In this paper, we introduce a novel <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised model</a> that jointly describes the textual and visual features needed to distinguish compositors. Applied to images of Shakespeare&#8217;s First Folio, our model predicts attributions that agree with the manual judgements of bibliographers with an accuracy of 87 %, even on text that is the output of <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2066 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2066/>STAIR Captions : Constructing a Large-Scale Japanese Image Caption Dataset<span class=acl-fixed-case>STAIR</span> Captions: Constructing a Large-Scale <span class=acl-fixed-case>J</span>apanese Image Caption Dataset</a></strong><br><a href=/people/y/yuya-yoshikawa/>Yuya Yoshikawa</a>
|
<a href=/people/y/yutaro-shigeto/>Yutaro Shigeto</a>
|
<a href=/people/a/akikazu-takeuchi/>Akikazu Takeuchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2066><div class="card-body p-3 small">In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating <a href=https://en.wikipedia.org/wiki/Japanese_writing_system>Japanese captions</a> for <a href=https://en.wikipedia.org/wiki/Image>images</a>. Since most available caption datasets have been constructed for <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, there are few datasets for <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIR Captions. STAIR Captions consists of 820,310 <a href=https://en.wikipedia.org/wiki/Japanese_writing_system>Japanese captions</a> for 164,062 images. In the experiment, we show that a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2067 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2067" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2067/>Liar, Liar Pants on Fire : A New Benchmark Dataset for Fake News Detection</a></strong><br><a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2067><div class="card-body p-3 small">Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR : a new, publicly available <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for fake news detection. We collected a decade-long, 12.8 K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can be used for <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking research</a> as well. Notably, this new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate <a href=https://en.wikipedia.org/wiki/Meta_data>meta-data</a> with text. We show that this hybrid approach can improve a text-only deep learning model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2068 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2068/>English Multiword Expression-aware Dependency Parsing Including Named Entities<span class=acl-fixed-case>E</span>nglish Multiword Expression-aware Dependency Parsing Including Named Entities</a></strong><br><a href=/people/a/akihiko-kato/>Akihiko Kato</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2068><div class="card-body p-3 small">Because syntactic structures and spans of multiword expressions (MWEs) are independently annotated in many English syntactic corpora, they are generally inconsistent with respect to one another, which is harmful to the implementation of an aggregate system. In this work, we construct a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> that ensures consistency between dependency structures and MWEs, including <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>. Further, we explore <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that predict both MWE-spans and an MWE-aware dependency structure. Experimental results show that our joint model using additional MWE-span features achieves an MWE recognition improvement of 1.35 points over a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2069 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2069.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2069.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2069" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2069/>Improving Semantic Composition with Offset Inference</a></strong><br><a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/j/julie-weeds/>Julie Weeds</a>
|
<a href=/people/j/jeremy-reffin/>Jeremy Reffin</a>
|
<a href=/people/d/david-weir/>David Weir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2069><div class="card-body p-3 small">Count-based distributional semantic models suffer from sparsity due to unobserved but plausible co-occurrences in any text collection. This problem is amplified for models like Anchored Packed Trees (APTs), that take the grammatical type of a co-occurrence into account. We therefore introduce a novel form of distributional inference that exploits the rich type structure in APTs and infers <a href=https://en.wikipedia.org/wiki/Missing_data>missing data</a> by the same mechanism that is used for semantic composition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2071 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2071" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2071/>Temporal Word Analogies : Identifying Lexical Replacement with Diachronic Word Embeddings</a></strong><br><a href=/people/t/terrence-szymanski/>Terrence Szymanski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2071><div class="card-body p-3 small">This paper introduces the concept of temporal word analogies : pairs of words which occupy the same <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> at different points in time. One well-known property of word embeddings is that they are able to effectively model traditional word analogies (word w_1 is to word w_2 as word w_3 is to word w_4) through vector addition. Here, I show that temporal word analogies (word w_1 at time t _ is like word w_2 at time t _) can effectively be modeled with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space. When applied to a diachronic corpus of news articles, this method is able to identify temporal word analogies such as <a href=https://en.wikipedia.org/wiki/Ronald_Reagan>Ronald Reagan</a> in 1987 is like Bill Clinton in 1997, or <a href=https://en.wikipedia.org/wiki/Walkman>Walkman</a> in 1987 is like <a href=https://en.wikipedia.org/wiki/IPod>iPod</a> in 2007.<tex-math>w_1</tex-math> is to word <tex-math>w_2</tex-math> as word <tex-math>w_3</tex-math> is to word <tex-math>w_4</tex-math>&#8221;) through vector addition. Here, I show that temporal word analogies (&#8220;word <tex-math>w_1</tex-math> at time <tex-math>t_\\alpha</tex-math> is like word <tex-math>w_2</tex-math> at time <tex-math>t_\\beta</tex-math>&#8221;) can effectively be modeled with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space. When applied to a diachronic corpus of news articles, this method is able to identify temporal word analogies such as &#8220;Ronald Reagan in 1987 is like Bill Clinton in 1997&#8221;, or &#8220;Walkman in 1987 is like iPod in 2007&#8221;.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2072 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2072/>Methodical Evaluation of Arabic Word Embeddings<span class=acl-fixed-case>A</span>rabic Word Embeddings</a></strong><br><a href=/people/m/mohammed-elrazzaz/>Mohammed Elrazzaz</a>
|
<a href=/people/s/shady-elbassuoni/>Shady Elbassuoni</a>
|
<a href=/people/k/khaled-shaban/>Khaled Shaban</a>
|
<a href=/people/c/chadi-helwe/>Chadi Helwe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2072><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning techniques</a> have been proposed to obtain meaningful representations of words from text. In this study, we evaluate these various techniques when used to generate Arabic word embeddings. We first build a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> for the <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a> that can be utilized to perform intrinsic evaluation of different <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We then perform additional extrinsic evaluations of the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> based on two NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2073 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2073/>Multilingual Connotation Frames : A Case Study on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> for Targeted Sentiment Analysis and Forecast</a></strong><br><a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/e/eric-bell/>Eric Bell</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2073><div class="card-body p-3 small">People around the globe respond to major real world events through <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. To study targeted public sentiments across many languages and geographic locations, we introduce multilingual connotation frames : an extension from English connotation frames of Rashkin et al. (2016) with 10 additional <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European languages</a>, focusing on the implied sentiments among event participants engaged in a frame. As a case study, we present large scale analysis on targeted public sentiments toward salient events and <a href=https://en.wikipedia.org/wiki/Non-physical_entity>entities</a> using 1.2 million multilingual connotation frames extracted from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2074 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2074/>Best-Worst Scaling More Reliable than Rating Scales : A Case Study on Sentiment Intensity Annotation</a></strong><br><a href=/people/s/svetlana-kiritchenko/>Svetlana Kiritchenko</a>
|
<a href=/people/s/saif-mohammad/>Saif Mohammad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2074><div class="card-body p-3 small">Rating scales are a widely used method for <a href=https://en.wikipedia.org/wiki/Annotation>data annotation</a> ; however, they present several challenges, such as difficulty in maintaining inter- and intra-annotator consistency. Bestworst scaling (BWS) is an alternative method of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> that is claimed to produce high-quality annotations while keeping the required number of annotations similar to that of <a href=https://en.wikipedia.org/wiki/Scale_(social_sciences)>rating scales</a>. However, the veracity of this claim has never been systematically established. Here for the first time, we set up an experiment that directly compares the rating scale method with BWS. We show that with the same total number of annotations, BWS produces significantly more reliable results than the <a href=https://en.wikipedia.org/wiki/Rating_scale>rating scale</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2078 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2078.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2078.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2078/>Parser Adaptation for <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> by Integrating Normalization</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2078><div class="card-body p-3 small">This work explores different approaches of using normalization for parser adaptation. Traditionally, <a href=https://en.wikipedia.org/wiki/Normalization_(image_processing)>normalization</a> is used as separate pre-processing step. We show that integrating the <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization model</a> into the <a href=https://en.wikipedia.org/wiki/Parsing>parsing algorithm</a> is more beneficial. This way, multiple normalization candidates can be leveraged, which improves <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We test this hypothesis by modifying the Berkeley parser ; out-of-the-box it achieves an <a href=https://en.wikipedia.org/wiki/Feasible_region>F1 score</a> of 66.52. Our integrated approach reaches a significant improvement with an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 67.36, while using the best <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization sequence</a> results in an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of only 66.94.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2079 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2079/>AliMe Chat : A Sequence to Sequence and Rerank based Chatbot Engine<span class=acl-fixed-case>A</span>li<span class=acl-fixed-case>M</span>e Chat: A Sequence to Sequence and Rerank based Chatbot Engine</a></strong><br><a href=/people/m/minghui-qiu/>Minghui Qiu</a>
|
<a href=/people/f/feng-lin-li/>Feng-Lin Li</a>
|
<a href=/people/s/siyu-wang/>Siyu Wang</a>
|
<a href=/people/x/xing-gao/>Xing Gao</a>
|
<a href=/people/y/yan-chen/>Yan Chen</a>
|
<a href=/people/w/weipeng-zhao/>Weipeng Zhao</a>
|
<a href=/people/h/haiqing-chen/>Haiqing Chen</a>
|
<a href=/people/j/jun-huang/>Jun Huang</a>
|
<a href=/people/w/wei-chu/>Wei Chu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2079><div class="card-body p-3 small">We propose AliMe Chat, an open-domain chatbot engine that integrates the joint results of Information Retrieval (IR) and Sequence to Sequence (Seq2Seq) based generation models. AliMe Chat uses an attentive Seq2Seq based rerank model to optimize the joint results. Extensive experiments show our <a href=https://en.wikipedia.org/wiki/Engine>engine</a> outperforms both <a href=https://en.wikipedia.org/wiki/Infrared>IR</a> and generation based models. We launch AliMe Chat for a real-world industrial application and observe better results than another public chatbot.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2080 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2080/>A Conditional Variational Framework for Dialog Generation</a></strong><br><a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/y/yanran-li/>Yanran Li</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/s/shuzi-niu/>Shuzi Niu</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a>
|
<a href=/people/g/guoping-long/>Guoping Long</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2080><div class="card-body p-3 small">Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> allowing conditional response generation based on specific attributes. These <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a> can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> on two different scenarios, where the attribute refers to <a href=https://en.wikipedia.org/wiki/Generic_property>genericness</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment states</a> respectively. The experiment result testified the potential of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, where meaningful responses can be generated in accordance with the specified attributes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2081 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2081/>Question Answering through <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a> from Large Fine-grained Supervision Data</a></strong><br><a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2081><div class="card-body p-3 small">We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the previous best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by more than 8 %. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2082 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2082/>Self-Crowdsourcing Training for <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/a/azad-abad/>Azad Abad</a>
|
<a href=/people/m/moin-nabi/>Moin Nabi</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2082><div class="card-body p-3 small">In this paper we introduce a self-training strategy for <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. The training examples are automatically selected to train the <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd workers</a>. Our experimental results show an impact of 5 % Improvement in terms of <a href=https://en.wikipedia.org/wiki/F-number>F1</a> for relation extraction task, compared to the method based on distant supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2083 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2083/>A Generative Attentional Neural Network Model for Dialogue Act Classification</a></strong><br><a href=/people/q/quan-hung-tran/>Quan Hung Tran</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/i/ingrid-zukerman/>Ingrid Zukerman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2083><div class="card-body p-3 small">We propose a novel generative neural network architecture for Dialogue Act classification. Building upon the Recurrent Neural Network framework, our model incorporates a novel attentional technique and a label to label connection for <a href=https://en.wikipedia.org/wiki/Sequence_learning>sequence learning</a>, akin to Hidden Markov Models. The experiments show that both of these innovations lead our model to outperform strong baselines for dialogue act classification on MapTask and Switchboard corpora. We further empirically analyse the effectiveness of each of the new <a href=https://en.wikipedia.org/wiki/Innovation>innovations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2084 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2084/>Salience Rank : Efficient <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>Keyphrase Extraction</a> with Topic Modeling</a></strong><br><a href=/people/n/nedelina-teneva/>Nedelina Teneva</a>
|
<a href=/people/w/weiwei-cheng/>Weiwei Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2084><div class="card-body p-3 small">Topical PageRank (TPR) uses latent topic distribution inferred by Latent Dirichlet Allocation (LDA) to perform ranking of noun phrases extracted from documents. The ranking procedure consists of running <a href=https://en.wikipedia.org/wiki/PageRank>PageRank</a> K times, where K is the number of topics used in the LDA model. In this paper, we propose a modification of <a href=https://en.wikipedia.org/wiki/Time_complexity>TPR</a>, called Salience Rank. Salience Rank only needs to run <a href=https://en.wikipedia.org/wiki/PageRank>PageRank</a> once and extracts comparable or better <a href=https://en.wikipedia.org/wiki/String_(computer_science)>keyphrases</a> on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. In addition to quality and efficiency benefit, our method has the flexibility to extract keyphrases with varying tradeoffs between topic specificity and corpus specificity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2085 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2085/>List-only Entity Linking</a></strong><br><a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2085><div class="card-body p-3 small">Traditional Entity Linking (EL) technologies rely on rich structures and properties in the target knowledge base (KB). However, in many applications, the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> as List-only Entity Linking problem. Fortunately, some mentions may have more cues for linking, which can be used as seed mentions to bridge other mentions and the uninformative entities. In this work, we select most linkable mentions as <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(S)>seed mentions</a> and disambiguate other mentions by comparing them with the <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(S)>seed mentions</a> rather than directly with the entities. Our experiments on linking mentions to seven automatically mined lists show promising results and demonstrate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2087 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2087/>Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model</a></strong><br><a href=/people/p/paria-jamshid-lou/>Paria Jamshid Lou</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2087><div class="card-body p-3 small">This paper presents a model for disfluency detection in spontaneous speech transcripts called LSTM Noisy Channel Model. The model uses a Noisy Channel Model (NCM) to generate n-best candidate disfluency analyses and a Long Short-Term Memory (LSTM) language model to score the underlying fluent sentences of each analysis. The LSTM language model scores, along with other <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, are used in a MaxEnt reranker to identify the most plausible analysis. We show that using an LSTM language model in the reranking process of noisy channel disfluency model improves the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in disfluency detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2088 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2088.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2088/>On the Equivalence of Holographic and Complex Embeddings for Link Prediction</a></strong><br><a href=/people/k/katsuhiko-hayashi/>Katsuhiko Hayashi</a>
|
<a href=/people/m/masashi-shimbo/>Masashi Shimbo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2088><div class="card-body p-3 small">We show the equivalence of two state-of-the-art models for link prediction / knowledge graph completion : Nickel et al&#8217;s holographic embeddings and Trouillon et al.&#8217;s complex embeddings. We first consider a spectral version of the holographic embeddings, exploiting the <a href=https://en.wikipedia.org/wiki/Frequency_domain>frequency domain</a> in the <a href=https://en.wikipedia.org/wiki/Fourier_transform>Fourier transform</a> for efficient computation. The analysis of the resulting <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> reveals that it can be viewed as an instance of the complex embeddings with a certain constraint imposed on the initial vectors upon training. Conversely, any set of complex embeddings can be converted to a set of equivalent holographic embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2089 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2089/>Sentence Embedding for Neural Machine Translation Domain Adaptation</a></strong><br><a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2089><div class="card-body p-3 small">Although new corpora are becoming increasingly available for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, only those that belong to the same or similar domains are typically able to improve <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. Recently Neural Machine Translation (NMT) has become prominent in the field. However, most of the existing domain adaptation methods only focus on phrase-based machine translation. In this paper, we exploit the NMT&#8217;s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU points</a>, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2090 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2090.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2090" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2090/>Data Augmentation for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/m/marzieh-fadaee/>Marzieh Fadaee</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2090><div class="card-body p-3 small">The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2092 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2092" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2092/>Chunk-Based Bi-Scale Decoder for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/x/xiaohua-liu/>Xiaohua Liu</a>
|
<a href=/people/h/hang-li/>Hang Li</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2092><div class="card-body p-3 small">In typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all <a href=https://en.wikipedia.org/wiki/Granularity>linguistic granularities</a> in the same time-scale of <a href=https://en.wikipedia.org/wiki/Neural_network>RNN</a>. In this paper, we propose a new type of <a href=https://en.wikipedia.org/wiki/Code>decoder</a> for NMT, which splits the decode state into two parts and updates them in two different <a href=https://en.wikipedia.org/wiki/Time_complexity>time-scales</a>. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improves the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance over the state-of-the-art NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2093 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2093.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2093" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2093/>Model Transfer for Tagging Low-resource Languages using a <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>Bilingual Dictionary</a></a></strong><br><a href=/people/m/meng-fang/>Meng Fang</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2093><div class="card-body p-3 small">Cross-lingual model transfer is a compelling and popular method for predicting annotations in a low-resource language, whereby parallel corpora provide a bridge to a high-resource language, and its associated annotated corpora. However, parallel data is not readily available for many languages, limiting the applicability of these approaches. We address these drawbacks in our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> which takes advantage of cross-lingual word embeddings trained solely on a high coverage dictionary. We propose a novel neural network model for joint training from both sources of data based on cross-lingual word embeddings, and show substantial empirical improvements over baseline techniques. We also propose several active learning heuristics, which result in improvements over competitive benchmark methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2094/>EuroSense : Automatic Harvesting of Multilingual Sense Annotations from Parallel Text<span class=acl-fixed-case>E</span>uro<span class=acl-fixed-case>S</span>ense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text</a></strong><br><a href=/people/c/claudio-delli-bovi/>Claudio Delli Bovi</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2094><div class="card-body p-3 small">Parallel corpora are widely used in a variety of Natural Language Processing tasks, from <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> to cross-lingual Word Sense Disambiguation, where parallel sentences can be exploited to automatically generate high-quality sense annotations on a large scale. In this paper we present EuroSense, a multilingual sense-annotated resource based on the joint disambiguation of the Europarl parallel corpus, with almost 123 million sense annotations for over 155 thousand distinct concepts and entities from a language-independent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2095 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2095/>Challenging Language-Dependent Segmentation for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> : An Application to Machine Translation and Part-of-Speech Tagging<span class=acl-fixed-case>A</span>rabic: An Application to Machine Translation and Part-of-Speech Tagging</a></strong><br><a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/s/stephan-vogel/>Stephan Vogel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2095><div class="card-body p-3 small">Word segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Off-the-shelf tools, however, are : i) complicated to use and ii) domain / dialect dependent. We explore three language-independent alternatives to morphological segmentation using : i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> and POS tagging, we found these <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a <a href=https://en.wikipedia.org/wiki/Ratio>ratio</a> close to 1 or greater, gives optimal performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2096 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2096" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2096/>Fast and Accurate Neural Word Segmentation for Chinese<span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/y/yuan-xin/>Yuan Xin</a>
|
<a href=/people/y/yongjian-wu/>Yongjian Wu</a>
|
<a href=/people/f/feiyue-huang/>Feiyue Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2096><div class="card-body p-3 small">Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of <a href=https://en.wikipedia.org/wiki/Chinese_word_segmentation>Chinese word segmentation</a>. However, both training and working procedures of the current neural models are computationally inefficient. In this paper, we propose a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2097 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2097/>Pay Attention to the Ending : Strong Neural Baselines for the ROC Story Cloze Task<span class=acl-fixed-case>ROC</span> Story Cloze Task</a></strong><br><a href=/people/z/zheng-cai/>Zheng Cai</a>
|
<a href=/people/l/lifu-tu/>Lifu Tu</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2097><div class="card-body p-3 small">We consider the ROC story cloze task (Mostafazadeh et al., 2016) and present several findings. We develop a model that uses hierarchical recurrent networks with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> to encode the sentences in the story and score candidate endings. By discarding the large training set and only training on the validation set, we achieve an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 74.7 %. Even when we discard the <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>story plots</a> (sentences before the ending) and only train to choose the better of two endings, we can still reach 72.5 %. We then analyze this ending-only task setting. We estimate human accuracy to be 78 % and find several types of clues that lead to this high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, including those related to <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, <a href=https://en.wikipedia.org/wiki/Negation>negation</a>, and general ending likelihood regardless of the story context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2099 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2099/>Representing Sentences as Low-Rank Subspaces</a></strong><br><a href=/people/j/jiaqi-mu/>Jiaqi Mu</a>
|
<a href=/people/s/suma-bhat/>Suma Bhat</a>
|
<a href=/people/p/pramod-viswanath/>Pramod Viswanath</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2099><div class="card-body p-3 small">Sentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications. We observe a simple geometry of sentences the word representations of a given sentence (on average 10.23 words in all SemEval datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation, we represent a sentence by the low-rank subspace spanned by its <a href=https://en.wikipedia.org/wiki/Word_(group_theory)>word vectors</a>. Such an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised representation</a> is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models, including skip-thought vectors, by 15 % on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2100 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2100/>Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization<span class=acl-fixed-case>C</span>hinese Social Media Text Summarization</a></strong><br><a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/q/qi-su/>Qi Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2100><div class="card-body p-3 small">Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve <a href=https://en.wikipedia.org/wiki/Relevance_(information_retrieval)>semantic relevance</a> between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a>. Besides, the <a href=https://en.wikipedia.org/wiki/Similarity_score>similarity score</a> between the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> is maximized during training. Our experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a> on a <a href=https://en.wikipedia.org/wiki/Social_media>social media corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2102 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2102.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2102/>Separating Facts from Fiction : Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/s/svitlana-volkova/>Svitlana Volkova</a>
|
<a href=/people/k/kyle-shaffer/>Kyle Shaffer</a>
|
<a href=/people/j/jin-yea-jang/>Jin Yea Jang</a>
|
<a href=/people/n/nathan-hodas/>Nathan Hodas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2102><div class="card-body p-3 small">Pew research polls report 62 percent of U.S. adults get news on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> (Gottfried and Shearer, 2016). In a December poll, 64 percent of U.S. adults said that made-up news has caused a great deal of confusion about the facts of current events (Barthel et al., 2016). Fabricated stories in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, ranging from <a href=https://en.wikipedia.org/wiki/Propaganda>deliberate propaganda</a> to <a href=https://en.wikipedia.org/wiki/Hoax>hoaxes</a> and <a href=https://en.wikipedia.org/wiki/Satire>satire</a>, contributes to this confusion in addition to having serious effects on global stability. In this work we build predictive models to classify 130 thousand news posts as suspicious or verified, and predict four sub-types of suspicious news satire, <a href=https://en.wikipedia.org/wiki/Hoax>hoaxes</a>, <a href=https://en.wikipedia.org/wiki/Clickbait>clickbait</a> and <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>. We show that <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a> trained on tweet content and <a href=https://en.wikipedia.org/wiki/Social_network>social network interactions</a> outperform <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical models</a>. Unlike previous work on deception detection, we find that adding syntax and grammar features to our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> does not improve performance. Incorporating linguistic features improves <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results, however, social interaction features are most informative for finer-grained separation between four types of suspicious news posts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2103 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2103.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2103/>Recognizing Counterfactual Thinking in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media Texts</a></a></strong><br><a href=/people/y/youngseo-son/>Youngseo Son</a>
|
<a href=/people/a/anneke-buffone/>Anneke Buffone</a>
|
<a href=/people/j/joe-raso/>Joe Raso</a>
|
<a href=/people/a/allegra-larche/>Allegra Larche</a>
|
<a href=/people/a/anthony-janocko/>Anthony Janocko</a>
|
<a href=/people/k/kevin-zembroski/>Kevin Zembroski</a>
|
<a href=/people/h/h-andrew-schwartz/>H Andrew Schwartz</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2103><div class="card-body p-3 small">Counterfactual statements, describing events that did not occur and their consequents, have been studied in areas including <a href=https://en.wikipedia.org/wiki/Problem_solving>problem-solving</a>, affect management, and behavior regulation. People with more <a href=https://en.wikipedia.org/wiki/Counterfactual_thinking>counterfactual thinking</a> tend to perceive life events as more personally meaningful. Nevertheless, <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a> have not been studied in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. We create a counterfactual tweet dataset and explore approaches for detecting counterfactuals using rule-based and supervised statistical approaches. A combined rule-based and statistical approach yielded the best results (F1 = 0.77) outperforming either <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>approach</a> used alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2104 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2104/>Temporal Orientation of Tweets for Predicting Income of Users</a></strong><br><a href=/people/m/mohammed-hasanuzzaman/>Mohammed Hasanuzzaman</a>
|
<a href=/people/s/sabyasachi-kamila/>Sabyasachi Kamila</a>
|
<a href=/people/m/mandeep-kaur/>Mandeep Kaur</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2104><div class="card-body p-3 small">Automatically estimating a user&#8217;s socio-economic profile from their language use in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> can significantly help <a href=https://en.wikipedia.org/wiki/Social_science>social science research</a> and various downstream applications ranging from <a href=https://en.wikipedia.org/wiki/Business>business</a> to <a href=https://en.wikipedia.org/wiki/Politics>politics</a>. The current paper presents the first study where user cognitive structure is used to build a predictive model of income. In particular, we first develop a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> using a weakly supervised learning framework to automatically time-tag tweets as past, present, or future. We quantify a user&#8217;s overall temporal orientation based on their distribution of tweets, and use it to build a predictive model of income. Our analysis uncovers a correlation between future temporal orientation and <a href=https://en.wikipedia.org/wiki/Income>income</a>. Finally, we measure the predictive power of future temporal orientation on <a href=https://en.wikipedia.org/wiki/Income>income</a> by performing <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2105 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2105/>Character-Aware Neural Morphological Disambiguation</a></strong><br><a href=/people/a/alymzhan-toleu/>Alymzhan Toleu</a>
|
<a href=/people/g/gulmira-tolegen/>Gulmira Tolegen</a>
|
<a href=/people/a/aibek-makazhanov/>Aibek Makazhanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2105><div class="card-body p-3 small">We develop a language-independent, deep learning-based approach to the task of morphological disambiguation. Guided by the intuition that the correct analysis should be most similar to the context, we propose dense representations for morphological analyses and surface context and a simple yet effective way of combining the two to perform disambiguation. Our approach improves on the language-dependent state of the art for two <a href=https://en.wikipedia.org/wiki/Agglutinative_language>agglutinative languages</a> (Turkish and Kazakh) and can be potentially applied to other morphologically complex languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2106 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2106/>Character Composition Model with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for Dependency Parsing on Morphologically Rich Languages</a></strong><br><a href=/people/x/xiang-yu/>Xiang Yu</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2106><div class="card-body p-3 small">We present a transition-based dependency parser that uses a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> to compose word representations from characters. The character composition model shows great improvement over the word-lookup model, especially for parsing <a href=https://en.wikipedia.org/wiki/Agglutinative_language>agglutinative languages</a>. These improvements are even better than using pre-trained word embeddings from extra data. On the SPMRL data sets, our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms the previous best greedy parser (Ballesteros et. al, 2015) by a margin of 3 % on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2107 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2107/>How (not) to train a dependency parser : The curious case of jackknifing part-of-speech taggers</a></strong><br><a href=/people/z/zeljko-agic/>Željko Agić</a>
|
<a href=/people/n/natalie-schluter/>Natalie Schluter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2107><div class="card-body p-3 small">In dependency parsing, jackknifing taggers is indiscriminately used as a simple adaptation strategy. Here, we empirically evaluate when and how (not) to use <a href=https://en.wikipedia.org/wiki/Jackknifing>jackknifing</a> in <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. On 26 languages, we reveal a preference that conflicts with, and surpasses the ubiquitous ten-folding. We show no clear benefits of tagging the training data in cross-lingual parsing.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>