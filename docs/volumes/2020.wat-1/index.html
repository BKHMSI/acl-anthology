<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 7th Workshop on Asian Translation - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the 7th Workshop on Asian Translation</h2><p class=lead><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>,
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a>,
<a href=/people/c/chenchen-ding/>Chenchen Ding</a>,
<a href=/people/r/raj-dabre/>Raj Dabre</a>,
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>,
<a href=/people/w/win-pa-pa/>Win Pa Pa</a>,
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>,
<a href=/people/s/shantipriya-parida/>Shantipriya Parida</a>,
<a href=/people/i/isao-goto/>Isao Goto</a>,
<a href=/people/h/hidaya-mino/>Hidaya Mino</a>,
<a href=/people/h/hiroshi-manabe/>Hiroshi Manabe</a>,
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>,
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a>,
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.wat-1</dd><dt>Month:</dt><dd>December</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Suzhou, China</dd><dt>Venues:</dt><dd><a href=/venues/aacl/>AACL</a>
| <a href=/venues/wat/>WAT</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.wat-1>https://aclanthology.org/2020.wat-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+7th+Workshop+on+Asian+Translation" title="Search for 'Proceedings of the 7th Workshop on Asian Translation' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.0/>Proceedings of the 7th Workshop on Asian Translation</a></strong><br><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a>
|
<a href=/people/c/chenchen-ding/>Chenchen Ding</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/w/win-pa-pa/>Win Pa Pa</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/s/shantipriya-parida/>Shantipriya Parida</a>
|
<a href=/people/i/isao-goto/>Isao Goto</a>
|
<a href=/people/h/hidaya-mino/>Hidaya Mino</a>
|
<a href=/people/h/hiroshi-manabe/>Hiroshi Manabe</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.5/>Meta Ensemble for Japanese-Chinese Neural Machine Translation : Kyoto-U+ECNU Participation to WAT 2020<span class=acl-fixed-case>J</span>apanese-<span class=acl-fixed-case>C</span>hinese Neural Machine Translation: <span class=acl-fixed-case>K</span>yoto-<span class=acl-fixed-case>U</span>+<span class=acl-fixed-case>ECNU</span> Participation to <span class=acl-fixed-case>WAT</span> 2020</a></strong><br><a href=/people/z/zhuoyuan-mao/>Zhuoyuan Mao</a>
|
<a href=/people/y/yibin-shen/>Yibin Shen</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a>
|
<a href=/people/c/cheqing-jin/>Cheqing Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--5><div class="card-body p-3 small">This paper describes the Japanese-Chinese Neural Machine Translation (NMT) system submitted by the joint team of Kyoto University and East China Normal University (Kyoto-U+ECNU) to WAT 2020 (Nakazawa et al.,2020). We participate in APSEC Japanese-Chinese translation task. We revisit several techniques for NMT including various architectures, different data selection and augmentation methods, denoising pre-training, and also some specific tricks for Japanese-Chinese translation. We eventually perform a meta ensemble to combine all of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> into a single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese-Chinese translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.8/>HW-TSC’s Participation in the WAT 2020 Indic Languages Multilingual Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WAT</span> 2020 Indic Languages Multilingual Task</a></strong><br><a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--8><div class="card-body p-3 small">This paper describes our work in the WAT 2020 Indic Multilingual Translation Task. We participated in all 7 language pairs (En-Bn / Hi / Gu / Ml / Mr / Ta / Te) in both directions under the constrained conditionusing only the officially provided data. Using <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a> as a baseline, our Multi-En and En-Multi translation systems achieve the best performances. Detailed data filtering and data domain selection are the keys to performance enhancement in our experiment, with an average improvement of 2.6 BLEU scores for each language pair in the En-Multi system and an average improvement of 4.6 BLEU scores regarding the Multi-En. In addition, we employed language independent adapter to further improve the <a href=https://en.wikipedia.org/wiki/System>system</a> performances. Our submission obtains competitive results in the final evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.11/>Multimodal Neural Machine Translation for English to Hindi<span class=acl-fixed-case>E</span>nglish to <span class=acl-fixed-case>H</span>indi</a></strong><br><a href=/people/s/sahinur-rahman-laskar/>Sahinur Rahman Laskar</a>
|
<a href=/people/a/abdullah-faiz-ur-rahman-khilji/>Abdullah Faiz Ur Rahman Khilji</a>
|
<a href=/people/p/partha-pakray/>Partha Pakray</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--11><div class="card-body p-3 small">Machine translation (MT) focuses on the <a href=https://en.wikipedia.org/wiki/Machine_translation>automatic translation</a> of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-the-art results in the task of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> because of utilizing advanced deep learning techniques and handles issues like long-term dependency, and context-analysis. Nevertheless, NMT still suffers low translation quality for <a href=https://en.wikipedia.org/wiki/Linguistic_conservatism>low resource languages</a>. To encounter this challenge, the multi-modal concept comes in. The multi-modal concept combines textual and visual features to improve the translation quality of low resource languages. Moreover, the utilization of monolingual data in the pre-training step can improve the performance of the <a href=https://en.wikipedia.org/wiki/System>system</a> for low resource language translations. Workshop on Asian Translation 2020 (WAT2020) organized a translation task for multimodal translation in <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. We have participated in the same in two-track submission, namely text-only and multi-modal translation with team name CNLP-NITS. The evaluated results are declared at the WAT2020 translation task, which reports that our multi-modal NMT system attained higher scores than our text-only NMT on both challenge and evaluation test set. For the challenge test data, our multi-modal neural machine translation system achieves <a href=https://en.wikipedia.org/wiki/Bilingual_Evaluation_Understudy>Bilingual Evaluation Understudy (BLEU) score</a> of 33.57, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.754141, Adequacy-Fluency Metrics (AMFM) score 0.787320 and for evaluation test data, BLEU, RIBES, and, AMFM score of 40.51, 0.803208, and 0.820980 for English to Hindi translation respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.14/>WT : Wipro AI Submissions to the WAT 2020<span class=acl-fixed-case>WT</span>: Wipro <span class=acl-fixed-case>AI</span> Submissions to the <span class=acl-fixed-case>WAT</span> 2020</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--14><div class="card-body p-3 small">In this paper we present an EnglishHindi and HindiEnglish neural machine translation (NMT) system, submitted to the Translation shared Task organized at WAT 2020. We trained a multilingual NMT system based on transformer architecture. In this paper we show : (i) how effective pre-processing helps to improve performance, (ii) how synthetic data through <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> from available monolingual data can help in overall translation performance, (iii) how language similarity can aid more onto it. Our submissions ranked 1st in both English to Hindi and Hindi to English translation achieving <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> 20.80 and 29.59 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.17/>The ADAPT Centre’s Neural MT Systems for the WAT 2020 Document-Level Translation Task<span class=acl-fixed-case>ADAPT</span> Centre’s Neural <span class=acl-fixed-case>MT</span> Systems for the <span class=acl-fixed-case>WAT</span> 2020 Document-Level Translation Task</a></strong><br><a href=/people/w/wandri-jooste/>Wandri Jooste</a>
|
<a href=/people/r/rejwanul-haque/>Rejwanul Haque</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--17><div class="card-body p-3 small">In this paper we describe the ADAPT Centre&#8217;s submissions to the WAT 2020 document-level Business Scene Dialogue (BSD) Translation task. We only consider translating from <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a> for this task and we use the MarianNMT toolkit to train Transformer models. In order to improve the translation quality, we made use of both in-domain and out-of-domain data for training our Machine Translation (MT) systems, as well as various data augmentation techniques for fine-tuning the model parameters. This paper outlines the experiments we ran to train our <a href=https://en.wikipedia.org/wiki/System>systems</a> and report the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> achieved through these various experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.19/>Improving NMT via Filtered Back Translation<span class=acl-fixed-case>NMT</span> via Filtered Back Translation</a></strong><br><a href=/people/n/nikhil-jaiswal/>Nikhil Jaiswal</a>
|
<a href=/people/m/mayur-patidar/>Mayur Patidar</a>
|
<a href=/people/s/surabhi-kumari/>Surabhi Kumari</a>
|
<a href=/people/m/manasi-patwardhan/>Manasi Patwardhan</a>
|
<a href=/people/s/shirish-karande/>Shirish Karande</a>
|
<a href=/people/p/puneet-agarwal/>Puneet Agarwal</a>
|
<a href=/people/l/lovekesh-vig/>Lovekesh Vig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--19><div class="card-body p-3 small">Document-Level Machine Translation (MT) has become an active research area among the NLP community in recent years. Unlike sentence-level MT, which translates the sentences independently, document-level MT aims to utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> while translating a given source sentence. This paper demonstrates our submission (Team ID-DEEPNLP) to the Document-Level Translation task organized by WAT 2020. This task focuses on translating texts from a business dialog corpus while optionally utilizing the context present in the dialog. In our proposed approach, we utilize publicly available <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> from different domains to train an open domain base NMT model. We then use monolingual target data to create filtered pseudo parallel data and employ <a href=https://en.wikipedia.org/wiki/Back-translation>Back-Translation</a> to fine-tune the base model. This is further followed by <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on the domain-specific corpus. We also ensemble various <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to improvise the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve a <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a> of 26.59 and 22.83 in an <a href=https://en.wikipedia.org/wiki/BLEU>unconstrained setting</a> and 15.10 and 10.91 in the <a href=https://en.wikipedia.org/wiki/BLEU>constrained settings</a> for <a href=https://en.wikipedia.org/wiki/BLEU>En-Ja & Ja-En direction</a>, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wat-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.20/>A Parallel Evaluation Data Set of <a href=https://en.wikipedia.org/wiki/Software_documentation>Software Documentation</a> with Document Structure Annotation</a></strong><br><a href=/people/b/bianka-buschbeck/>Bianka Buschbeck</a>
|
<a href=/people/m/miriam-exel/>Miriam Exel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--20><div class="card-body p-3 small">This paper accompanies the software documentation data set for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, a parallel evaluation data set of data originating from the SAP Help Portal, that we released to the machine translation community for research purposes. It offers the possibility to tune and evaluate <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> in the domain of corporate software documentation and contributes to the availability of a wider range of evaluation scenarios. The data set comprises of the language pairs English to Hindi, <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian</a>, <a href=https://en.wikipedia.org/wiki/Malay_language>Malay</a> and <a href=https://en.wikipedia.org/wiki/Thai_language>Thai</a>, and thus also increases the test coverage for the many low-resource language pairs. Unlike most evaluation data sets that consist of plain parallel text, the segments in this <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> come with additional metadata that describes structural information of the document context. We provide insights into the origin and creation, the particularities and characteristics of the <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> as well as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> results.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>