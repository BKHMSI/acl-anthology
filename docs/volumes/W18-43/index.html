<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Workshop Events and Stories in the News 2018 - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W18-43.pdf>Proceedings of the Workshop Events and Stories in the News 2018</a></h2><p class=lead><a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>,
<a href=/people/b/ben-miller/>Ben Miller</a>,
<a href=/people/m/marieke-van-erp/>Marieke van Erp</a>,
<a href=/people/p/piek-vossen/>Piek Vossen</a>,
<a href=/people/m/martha-palmer/>Martha Palmer</a>,
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>,
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>,
<a href=/people/d/david-caswell/>David Caswell</a>,
<a href=/people/s/susan-windisch-brown/>Susan W. Brown</a>,
<a href=/people/c/claire-bonial/>Claire Bonial</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W18-43</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Santa Fe, New Mexico, U.S.A</dd><dt>Venues:</dt><dd><a href=/venues/coling/>COLING</a>
| <a href=/venues/eventstory/>EventStory</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W18-43>https://aclanthology.org/W18-43</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W18-43.pdf>https://aclanthology.org/W18-43.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W18-43.pdf title="Open PDF of 'Proceedings of the Workshop Events and Stories in the News 2018'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Workshop+Events+and+Stories+in+the+News+2018" title="Search for 'Proceedings of the Workshop Events and Stories in the News 2018' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4300/>Proceedings of the Workshop Events and Stories in the News 2018</a></strong><br><a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/b/ben-miller/>Ben Miller</a>
|
<a href=/people/m/marieke-van-erp/>Marieke van Erp</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>
|
<a href=/people/d/david-caswell/>David Caswell</a>
|
<a href=/people/s/susan-windisch-brown/>Susan W. Brown</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4301/>Every Object Tells a Story</a></strong><br><a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/n/nikhil-krishnaswamy/>Nikhil Krishnaswamy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4301><div class="card-body p-3 small">Most work within the computational event modeling community has tended to focus on the interpretation and ordering of events that are associated with <a href=https://en.wikipedia.org/wiki/Verb>verbs</a> and event nominals in linguistic expressions. What is often overlooked in the construction of a global interpretation of a narrative is the role contributed by the objects participating in these structures, and the latent events and activities conventionally associated with them. Recently, the analysis of visual images has also enriched the scope of how events can be identified, by anchoring both linguistic expressions and ontological labels to segments, subregions, and properties of <a href=https://en.wikipedia.org/wiki/Image>images</a>. By semantically grounding <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>event descriptions</a> in their visualization, the importance of <a href=https://en.wikipedia.org/wiki/Object-oriented_programming>object-based attributes</a> becomes more apparent. In this position paper, we look at the narrative structure of objects : that is, how objects reference events through their intrinsic attributes, such as <a href=https://en.wikipedia.org/wiki/Affordance>affordances</a>, purposes, and <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a>. We argue that, not only do objects encode conventionalized events, but that when they are composed within specific habitats, the ensemble can be viewed as modeling coherent event sequences, thereby enriching the global interpretation of the evolving narrative being constructed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4302/>A Rich Annotation Scheme for Mental Events</a></strong><br><a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/p/pavlina-peskova/>Pavlína Pešková</a>
|
<a href=/people/m/michael-regan/>Michael Regan</a>
|
<a href=/people/s/sook-kyung-lee/>Sook-kyung Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4302><div class="card-body p-3 small">We present a rich annotation scheme for the structure of mental events. Mental events are those in which the verb describes a mental state or process, usually oriented towards an external situation. While <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>physical events</a> have been described in detail and there are numerous studies of their <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a> and <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, <a href=https://en.wikipedia.org/wiki/Mental_event>mental events</a> are less thoroughly studied. The annotation scheme proposed here is based on decompositional analyses in the semantic and typological linguistic literature. The scheme was applied to the <a href=https://en.wikipedia.org/wiki/Text_corpus>news corpus</a> from the 2016 Events workshop, and <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> of the test annotation provides suggestions for refinement and clarification of the annotation scheme.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4304/>Identifying the Discourse Function of News Article Paragraphs</a></strong><br><a href=/people/w/w-victor-yarlott/>W. Victor Yarlott</a>
|
<a href=/people/c/cristina-cornelio/>Cristina Cornelio</a>
|
<a href=/people/t/tian-gao/>Tian Gao</a>
|
<a href=/people/m/mark-finlayson/>Mark Finlayson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4304><div class="card-body p-3 small">Discourse structure is a key aspect of all forms of <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, providing valuable information both to humans and machines. We applied the hierarchical theory of news discourse developed by van Dijk to examine how paragraphs operate as units of discourse structure within news articleswhat we refer to here as document-level discourse. This document-level discourse provides a characterization of the content of each paragraph that describes its relation to the events presented in the article (such as main events, backgrounds, and consequences) as well as to other components of the story (such as commentary and evaluation). The purpose of a news discourse section is of great utility to story understanding as it affects both the importance and temporal order of items introduced in the texttherefore, if we know the news discourse purpose for different sections, we should be able to better rank events for their importance and better construct timelines. We test two hypotheses : first, that people can reliably annotate news articles with van Dijk&#8217;s theory ; second, that we can reliably predict these labels using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. We show that people have a high degree of agreement with each other when annotating the theory (F1 0.8, Cohen&#8217;s kappa 0.6), demonstrating that it can be both learned and reliably applied by human annotators. Additionally, we demonstrate first steps toward <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> of the <a href=https://en.wikipedia.org/wiki/Theory>theory</a>, achieving a performance of <a href=https://en.wikipedia.org/wiki/F-number>F1</a> = 0.54, which is 65 % of human performance. Moreover, we have generated a gold-standard, adjudicated corpus of 50 documents for document-level discourse annotation based on the ACE Phase 2 corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4305/>An Evaluation of Information Extraction Tools for Identifying Health Claims in News Headlines</a></strong><br><a href=/people/s/shi-yuan/>Shi Yuan</a>
|
<a href=/people/b/bei-yu/>Bei Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4305><div class="card-body p-3 small">This study evaluates the performance of four information extraction tools (extractors) on identifying health claims in health news headlines. A <a href=https://en.wikipedia.org/wiki/Health_claim>health claim</a> is defined as a triplet : IV (what is being manipulated), DV (what is being measured) and their relation. Tools that can identify <a href=https://en.wikipedia.org/wiki/Health_claim>health claims</a> provide the foundation for evaluating the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of these <a href=https://en.wikipedia.org/wiki/Health_claim>claims</a> against authoritative resources. The evaluation result shows that 26 % headlines do not in-clude <a href=https://en.wikipedia.org/wiki/Health_claim>health claims</a>, and all extractors face difficulty separating them from the rest. For those with <a href=https://en.wikipedia.org/wiki/Health_claim>health claims</a>, OPENIE-5.0 performed the best with <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> at 0.6 level for ex-tracting IV-relation-DV. However, the characteristic linguistic structures in health news headlines, such as <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>incomplete sentences</a> and non-verb relations, pose particular challenge to existing tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4307/>Can You Spot the Semantic Predicate in this Video?</a></strong><br><a href=/people/c/christopher-reale/>Christopher Reale</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/h/heesung-kwon/>Heesung Kwon</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4307><div class="card-body p-3 small">We propose a method to improve human activity recognition in video by leveraging semantic information about the target activities from an expert-defined linguistic resource, <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a>. Our hypothesis is that activities that share similar event semantics, as defined by the semantic predicates of <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a>, will be more likely to share some visual components. We use a deep convolutional neural network approach as a baseline and incorporate linguistic information from <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a> through <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. We present results of experiments showing the added information has negligible impact on <a href=https://en.wikipedia.org/wiki/Computer_vision>recognition</a> performance. We discuss how this may be because the lexical semantic information defined by <a href=https://en.wikipedia.org/wiki/VerbNet>VerbNet</a> is generally not visually salient given the video processing approach used here, and how we may handle this in future approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4309/>On Training Classifiers for Linking Event Templates</a></strong><br><a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/f/fredi-saric/>Fredi Šarić</a>
|
<a href=/people/v/vanni-zavarella/>Vanni Zavarella</a>
|
<a href=/people/m/martin-atkinson/>Martin Atkinson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4309><div class="card-body p-3 small">The paper reports on exploring various machine learning techniques and a range of textual and meta-data features to train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> for linking related event templates automatically extracted from <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news</a>. With the best <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>textual features</a> only we achieved 94.7 % (92.9 %) <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> on GOLD (SILVER) dataset. These figures were further improved to 98.6 % (GOLD) and 97 % (SILVER) F1 score by adding meta-data features, mainly thanks to the strong discriminatory power of automatically extracted geographical information related to <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4310/>HEI : Hunter Events Interface A <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> based on <a href=https://en.wikipedia.org/wiki/Service_(systems_architecture)>services</a> for the detection and reasoning about events<span class=acl-fixed-case>HEI</span>: Hunter Events Interface A platform based on services for the detection and reasoning about events</a></strong><br><a href=/people/a/antonio-sorgente/>Antonio Sorgente</a>
|
<a href=/people/a/antonio-calabrese/>Antonio Calabrese</a>
|
<a href=/people/g/gianluca-coda/>Gianluca Coda</a>
|
<a href=/people/p/paolo-vanacore/>Paolo Vanacore</a>
|
<a href=/people/f/francesco-mele/>Francesco Mele</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4310><div class="card-body p-3 small">In this paper we present the definition and implementation of the Hunter Events Interface (HEI) System. The HEI System is a system for events annotation and temporal reasoning in Natural Language Texts and media, mainly oriented to texts of historical and cultural contents available on the Web. In this work we assume that <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> are defined through various components : <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>actions</a>, participants, <a href=https://en.wikipedia.org/wiki/Location>locations</a>, and occurrence intervals. The HEI system, through independent services, locates (annotates) the various components, and successively associates them to a specific event. The objective of this work is to build a system integrating <a href=https://en.wikipedia.org/wiki/Service_(systems_architecture)>services</a> for the identification of events, the discovery of their connections, and the evaluation of their consistency. We believe this interface is useful to develop applications that use the notion of story, to integrate data of digital cultural archives, and to build systems of fruition in the same field. The HEI system has been partially developed within the TrasTest project</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>