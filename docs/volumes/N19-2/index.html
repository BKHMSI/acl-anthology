<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/N19-2.pdf>Proceedings of the 2019 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)</a></h2><p class=lead><a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>,
<a href=/people/m/michelle-morales/>Michelle Morales</a>,
<a href=/people/r/rohit-kumar/>Rohit Kumar</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>N19-2</dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Minneapolis, Minnesota</dd><dt>Venue:</dt><dd><a href=/venues/naacl/>NAACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/N19-2>https://aclanthology.org/N19-2</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/N19-2.pdf>https://aclanthology.org/N19-2.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/N19-2.pdf title="Open PDF of 'Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2019+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics%3A+Human+Language+Technologies%2C+Volume+2+%28Industry+Papers%29" title="Search for 'Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2000/>Proceedings of the 2019 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)</a></strong><br><a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/m/michelle-morales/>Michelle Morales</a>
|
<a href=/people/r/rohit-kumar/>Rohit Kumar</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-2001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-2001/>Enabling Real-time Neural IME with Incremental Vocabulary Selection<span class=acl-fixed-case>IME</span> with Incremental Vocabulary Selection</a></strong><br><a href=/people/j/jiali-yao/>Jiali Yao</a>
|
<a href=/people/r/raphael-shu/>Raphael Shu</a>
|
<a href=/people/x/xinjian-li/>Xinjian Li</a>
|
<a href=/people/k/katsutoshi-ohtsuki/>Katsutoshi Ohtsuki</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2001><div class="card-body p-3 small">Input method editor (IME) converts sequential alphabet key inputs to words in a target language. It is an indispensable service for billions of Asian users. Although the neural-based language model is extensively studied and shows promising results in sequence-to-sequence tasks, applying a neural-based language model to IME was not considered feasible due to high <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a> when converting words on user devices. In this work, we articulate the bottleneck of neural IME decoding to be the heavy softmax computation over a large vocabulary. We propose an approach that incrementally builds a subset vocabulary from the word lattice. Our approach always computes the probability with a selected subset vocabulary. When the selected vocabulary is updated, the stale probabilities in previous steps are fixed by recomputing the missing logits. The experiments on Japanese IME benchmark shows an over 50x speedup for the softmax computations comparing to the baseline, reaching real-time speed even on commodity CPU without losing conversion accuracy. The approach is potentially applicable to other incremental sequence-to-sequence decoding tasks such as real-time continuous speech recognition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2011/>Neural Lexicons for Slot Tagging in Spoken Language Understanding</a></strong><br><a href=/people/k/kyle-williams/>Kyle Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2011><div class="card-body p-3 small">We explore the use of <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> or <a href=https://en.wikipedia.org/wiki/Gazette>gazettes</a> in neural models for slot tagging in spoken language understanding. We develop models that encode lexicon information as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>neural features</a> for use in a Long-short term memory neural network. Experiments are performed on <a href=https://en.wikipedia.org/wiki/Data>data</a> from 4 domains from an intelligent assistant under conditions that often occur in an industry setting, where there may be : 1) large amounts of training data, 2) limited amounts of training data for new domains, and 3) cross domain training. Results show that the use of neural lexicon information leads to a significant improvement in slot tagging, with improvements in the <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of up to 12 %. Our findings have implications for how <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> can be used to improve the performance of neural slot tagging models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2012/>Active Learning for New Domains in Natural Language Understanding</a></strong><br><a href=/people/s/stanislav-peshterliev/>Stanislav Peshterliev</a>
|
<a href=/people/j/john-kearney/>John Kearney</a>
|
<a href=/people/a/abhyuday-jagannatha/>Abhyuday Jagannatha</a>
|
<a href=/people/i/imre-kiss/>Imre Kiss</a>
|
<a href=/people/s/spyros-matsoukas/>Spyros Matsoukas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2012><div class="card-body p-3 small">We explore active learning (AL) for improving the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of new domains in a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding (NLU) system</a>. We propose an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> called Majority-CRF that uses an ensemble of classification models to guide the selection of relevant utterances, as well as a sequence labeling model to help prioritize informative examples. Experiments with three domains show that Majority-CRF achieves 6.6%-9 % relative error rate reduction compared to <a href=https://en.wikipedia.org/wiki/Simple_random_sample>random sampling</a> with the same annotation budget, and statistically significant improvements compared to other AL approaches. Additionally, case studies with human-in-the-loop AL on six <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>new domains</a> show 4.6%-9 % improvement on an existing NLU system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2014/>Are the Tools up to the Task? an Evaluation of Commercial Dialog Tools in Developing Conversational Enterprise-grade Dialog Systems</a></strong><br><a href=/people/m/marie-meteer/>Marie Meteer</a>
|
<a href=/people/m/meghan-hickey/>Meghan Hickey</a>
|
<a href=/people/c/carmi-rothberg/>Carmi Rothberg</a>
|
<a href=/people/d/david-nahamoo/>David Nahamoo</a>
|
<a href=/people/e/ellen-eide-kislal/>Ellen Eide Kislal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2014><div class="card-body p-3 small">There has been a significant investment in dialog systems (tools and runtime) for building conversational systems by major companies including <a href=https://en.wikipedia.org/wiki/Google>Google</a>, <a href=https://en.wikipedia.org/wiki/IBM>IBM</a>, <a href=https://en.wikipedia.org/wiki/Microsoft>Microsoft</a>, and <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon</a>. The question remains whether these tools are up to the task of building conversational, task-oriented dialog applications at the enterprise level. In our company, we are exploring and comparing several toolsets in an effort to determine their strengths and weaknesses in meeting our goals for dialog system development : <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, time to market, ease of replicating and extending applications, and efficiency and ease of use by developers. In this paper, we provide both quantitative and qualitative results in three main areas : <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, <a href=https://en.wikipedia.org/wiki/Dialogue>dialog</a>, and text generation. While existing toolsets were all incomplete, we hope this paper will provide a roadmap of where they need to go to meet the goal of building effective dialog systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2015/>Development and Deployment of a Large-Scale Dialog-based Intelligent Tutoring System</a></strong><br><a href=/people/s/shazia-afzal/>Shazia Afzal</a>
|
<a href=/people/t/tejas-dhamecha/>Tejas Dhamecha</a>
|
<a href=/people/n/nirmal-mukhi/>Nirmal Mukhi</a>
|
<a href=/people/r/renuka-sindhgatta/>Renuka Sindhgatta</a>
|
<a href=/people/s/smit-marvaniya/>Smit Marvaniya</a>
|
<a href=/people/m/matthew-ventura/>Matthew Ventura</a>
|
<a href=/people/j/jessica-yarbro/>Jessica Yarbro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2015><div class="card-body p-3 small">There are significant challenges involved in the design and implementation of a dialog-based tutoring system (DBT) ranging from <a href=https://en.wikipedia.org/wiki/Domain_engineering>domain engineering</a> to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language classification</a> and eventually instantiating an adaptive, personalized dialog strategy. These issues are magnified when implementing such a <a href=https://en.wikipedia.org/wiki/System>system</a> at scale and across domains. In this paper, we describe and reflect on the design, methods, decisions and assessments that led to the successful deployment of our AI driven DBT currently being used by several hundreds of college level students for practice and self-regulated study in diverse subjects like <a href=https://en.wikipedia.org/wiki/Sociology>Sociology</a>, <a href=https://en.wikipedia.org/wiki/Communication>Communications</a>, and <a href=https://en.wikipedia.org/wiki/Federal_government_of_the_United_States>American Government</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2016/>Learning When Not to Answer : a Ternary Reward Structure for Reinforcement Learning Based Question Answering</a></strong><br><a href=/people/f/frederic-godin/>Fréderic Godin</a>
|
<a href=/people/a/anjishnu-kumar/>Anjishnu Kumar</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2016><div class="card-body p-3 small">In this paper, we investigate the challenges of using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning agents</a> for question-answering over knowledge graphs for real-world applications. We examine the <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a> used by state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> and determine that they are inadequate for such settings. More specifically, they do not evaluate the <a href=https://en.wikipedia.org/wiki/System>systems</a> correctly for situations when there is no answer available and thus <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> optimized for these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are poor at modeling confidence. We introduce a simple new <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metric</a> for evaluating question-answering agents that is more representative of practical usage conditions, and optimize for this metric by extending the binary reward structure used in prior work to a ternary reward structure which also rewards an agent for not answering a question rather than giving an incorrect answer. We show that this can drastically improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of answered questions while only not answering a limited number of previously correctly answered questions. Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2017/>Extraction of Message Sequence Charts from Software Use-Case Descriptions</a></strong><br><a href=/people/g/girish-palshikar/>Girish Palshikar</a>
|
<a href=/people/n/nitin-ramrakhiyani/>Nitin Ramrakhiyani</a>
|
<a href=/people/s/sangameshwar-patil/>Sangameshwar Patil</a>
|
<a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2017><div class="card-body p-3 small">Software Requirement Specification documents provide natural language descriptions of the core functional requirements as a set of <a href=https://en.wikipedia.org/wiki/Use_case>use-cases</a>. Essentially, each use-case contains a set of <a href=https://en.wikipedia.org/wiki/Actor_(disambiguation)>actors</a> and sequences of steps describing the interactions among them. Goals of use-case reviews and analyses include their correctness, completeness, detection of ambiguities, <a href=https://en.wikipedia.org/wiki/Software_prototyping>prototyping</a>, <a href=https://en.wikipedia.org/wiki/Software_verification>verification</a>, <a href=https://en.wikipedia.org/wiki/Test_case>test case generation</a> and <a href=https://en.wikipedia.org/wiki/Traceability>traceability</a>. Message Sequence Chart (MSC) have been proposed as a expressive, rigorous yet intuitive visual representation of use-cases. In this paper, we describe a linguistic knowledge-based approach to extract MSCs from <a href=https://en.wikipedia.org/wiki/Use_case>use-cases</a>. Compared to existing techniques, we extract richer constructs of the MSC notation such as <a href=https://en.wikipedia.org/wiki/Timer>timers</a>, <a href=https://en.wikipedia.org/wiki/Conditional_(computer_programming)>conditions</a> and alt-boxes. We apply this <a href=https://en.wikipedia.org/wiki/Tool>tool</a> to extract MSCs from several real-life software use-case descriptions and show that it performs better than the existing techniques. We also discuss the benefits and limitations of the extracted MSCs to meet the above goals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2018/>Improving Knowledge Base Construction from Robust Infobox Extraction</a></strong><br><a href=/people/b/boya-peng/>Boya Peng</a>
|
<a href=/people/y/yejin-huh/>Yejin Huh</a>
|
<a href=/people/x/xiao-ling/>Xiao Ling</a>
|
<a href=/people/m/michele-banko/>Michele Banko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2018><div class="card-body p-3 small">A capable, automatic Question Answering (QA) system can provide more complete and accurate answers using a comprehensive knowledge base (KB). One important approach to constructing a comprehensive knowledge base is to extract information from Wikipedia infobox tables to populate an existing KB. Despite previous successes in the Infobox Extraction (IBE) problem (e.g., DBpedia), three major challenges remain : 1) Deterministic extraction patterns used in DBpedia are vulnerable to template changes ; 2) Over-trusting Wikipedia anchor links can lead to entity disambiguation errors ; 3) Heuristic-based extraction of unlinkable entities yields low precision, hurting both accuracy and completeness of the final KB. This paper presents a <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robust approach</a> that tackles all three challenges. We build probabilistic models to predict relations between entity mentions directly from the infobox tables in <a href=https://en.wikipedia.org/wiki/HTML>HTML</a>. The entity mentions are linked to <a href=https://en.wikipedia.org/wiki/Identifier>identifiers</a> in an existing <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> if possible. The unlinkable ones are also parsed and preserved in the final output. Training data for both the <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> and the entity linking models are automatically generated using distant supervision. We demonstrate the empirical effectiveness of the proposed method in both <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> compared to a strong IBE baseline, <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a>, with an absolute improvement of 41.3 % in average F1. We also show that our <a href=https://en.wikipedia.org/wiki/Information_extraction>extraction</a> makes the final KB significantly more complete, improving the <a href=https://en.wikipedia.org/wiki/Completeness_(logic)>completeness score</a> of list-value relation types by 61.4 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-2019.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-2019/>A k-Nearest Neighbor Approach towards Multi-level Sequence Labeling</a></strong><br><a href=/people/y/yue-chen/>Yue Chen</a>
|
<a href=/people/j/john-chen/>John Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2019><div class="card-body p-3 small">In this paper we present a new method for intent recognition for complex dialog management in low resource situations. Complex dialog management is required because our target domain is real world mixed initiative food ordering between agents and their customers, where individual customer utterances may contain multiple intents and refer to food items with complex structure. For example, a customer might say Can I get a deluxe burger with large fries and oh put extra mayo on the burger would you? We approach this task as a multi-level sequence labeling problem, with the constraint of limited real training data. Both traditional methods like HMM, <a href=https://en.wikipedia.org/wiki/Microelectromechanical_systems>MEMM</a>, or CRF and newer methods like <a href=https://en.wikipedia.org/wiki/Deep_learning>DNN</a> or BiLSTM use only homogeneous feature sets. Newer <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> perform better but also require considerably more data. Previous research has done pseudo-data synthesis to obtain the required amounts of training data. We propose to use a k-NN learner with heterogeneous feature set. We used windowed word n-grams, POS tag n-grams and pre-trained word embeddings as features. For the experiments we perform a comparison between using pseudo-data and real world data. We also perform <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised self-training</a> to obtain additional labeled data, in order to better model real world scenarios. Instead of using massive pseudo-data, we show that with only less than 1 % of the data size, we can achieve better result than any of the methods above by annotating real world data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2024/>Neural Text Normalization with Subword Units</a></strong><br><a href=/people/c/courtney-mansfield/>Courtney Mansfield</a>
|
<a href=/people/m/ming-sun/>Ming Sun</a>
|
<a href=/people/y/yuzong-liu/>Yuzong Liu</a>
|
<a href=/people/a/ankur-gandhe/>Ankur Gandhe</a>
|
<a href=/people/b/bjorn-hoffmeister/>Björn Hoffmeister</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2024><div class="card-body p-3 small">Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech synthesis</a>. Finite state transducers (FSTs) are commonly used to build <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammars</a> that handle <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a>. However, translating <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic knowledge</a> into <a href=https://en.wikipedia.org/wiki/Grammar>grammars</a> requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2026/>In Other News : a Bi-style Text-to-speech Model for Synthesizing Newscaster Voice with Limited Data</a></strong><br><a href=/people/n/nishant-prateek/>Nishant Prateek</a>
|
<a href=/people/m/mateusz-lajszczak/>Mateusz Łajszczak</a>
|
<a href=/people/r/roberto-barra-chicote/>Roberto Barra-Chicote</a>
|
<a href=/people/t/thomas-drugman/>Thomas Drugman</a>
|
<a href=/people/j/jaime-lorenzo-trueba/>Jaime Lorenzo-Trueba</a>
|
<a href=/people/t/thomas-merritt/>Thomas Merritt</a>
|
<a href=/people/s/srikanth-ronanki/>Srikanth Ronanki</a>
|
<a href=/people/t/trevor-wood/>Trevor Wood</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2026><div class="card-body p-3 small">Neural text-to-speech synthesis (NTTS) models have shown significant progress in generating high-quality speech, however they require a large quantity of training data. This makes creating <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> for multiple styles expensive and time-consuming. In this paper different styles of speech are analysed based on <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosodic variations</a>, from this a model is proposed to synthesise <a href=https://en.wikipedia.org/wiki/Speech>speech</a> in the style of a <a href=https://en.wikipedia.org/wiki/News_presenter>newscaster</a>, with just a few hours of supplementary data. We pose the problem of synthesising in a target style using limited data as that of creating a bi-style model that can synthesise both neutral-style and newscaster-style speech via a one-hot vector which factorises the two styles. We also propose conditioning the model on contextual word embeddings, and extensively evaluate it against neutral NTTS, and neutral concatenative-based synthesis. This model closes the gap in perceived style-appropriateness between <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural recordings</a> for <a href=https://en.wikipedia.org/wiki/News_style>newscaster-style of speech</a>, and neutral speech synthesis by approximately two-thirds.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2028/>Content-based Dwell Time Engagement Prediction Model for News Articles</a></strong><br><a href=/people/h/heidar-davoudi/>Heidar Davoudi</a>
|
<a href=/people/a/aijun-an/>Aijun An</a>
|
<a href=/people/g/gordon-edall/>Gordon Edall</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2028><div class="card-body p-3 small">The article dwell time (i.e., expected time that users spend on an article) is among the most important factors showing the article engagement. It is of great interest to predict the dwell time of an article before its release. This allows <a href=https://en.wikipedia.org/wiki/Digital_newspaper>digital newspapers</a> to make informed decisions and publish more engaging articles. In this paper, we propose a novel content-based approach based on a deep neural network architecture for predicting article dwell times. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> extracts <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>, event and entity features from an article, learns interactions among them, and combines the interactions with the word-based features of the article to learn a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for predicting the dwell time. The experimental results on a real dataset from a major newspaper show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms other state-of-the-art baselines.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>