<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/P19-2.pdf>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></h2><p class=lead><a href=/people/f/fernando-alva-manchego/>Fernando Alva-Manchego</a>,
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>,
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>P19-2</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Florence, Italy</dd><dt>Venue:</dt><dd><a href=/venues/acl/>ACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/P19-2>https://aclanthology.org/P19-2</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/P19-2.pdf>https://aclanthology.org/P19-2.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/P19-2.pdf title="Open PDF of 'Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+57th+Annual+Meeting+of+the+Association+for+Computational+Linguistics%3A+Student+Research+Workshop" title="Search for 'Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2000/>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></strong><br><a href=/people/f/fernando-alva-manchego/>Fernando Alva-Manchego</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-2002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-2002/>Robust to Noise Models in Natural Language Processing Tasks</a></strong><br><a href=/people/v/valentin-malykh/>Valentin Malykh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2002><div class="card-body p-3 small">There are a lot of noise texts surrounding a person in modern life. The traditional approach is to use spelling correction, yet the existing solutions are far from perfect. We propose robust to noise word embeddings model, which outperforms existing commonly used models, like fasttext and <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> in different tasks. In addition, we investigate the noise robustness of current <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in different natural language processing tasks. We propose extensions for modern <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in three <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a>, i.e. text classification, named entity recognition and aspect extraction, which shows improvement in noise robustness over existing solutions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2004/>Measuring the Value of <a href=https://en.wikipedia.org/wiki/Linguistics>Linguistics</a> : A Case Study from St. Lawrence Island Yupik<span class=acl-fixed-case>S</span>t. <span class=acl-fixed-case>L</span>awrence <span class=acl-fixed-case>I</span>sland <span class=acl-fixed-case>Y</span>upik</a></strong><br><a href=/people/e/emily-chen/>Emily Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2004><div class="card-body p-3 small">The adaptation of neural approaches to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is a landmark achievement that has called into question the utility of <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> in the development of <a href=https://en.wikipedia.org/wiki/Computational_science>computational systems</a>. This research proposal consequently explores this question in the context of a neural morphological analyzer for a <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic language</a>, <a href=https://en.wikipedia.org/wiki/St._Lawrence_Island_Yupik>St. Lawrence Island Yupik</a>. It asks whether incorporating elements of <a href=https://en.wikipedia.org/wiki/Yupik_languages>Yupik linguistics</a> into the implementation of the analyzer can improve performance, both in low-resource settings and in high-resource settings, where rich quantities of data are readily available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2005/>Not All Reviews Are Equal : Towards Addressing Reviewer Biases for Opinion Summarization</a></strong><br><a href=/people/w/wenyi-tay/>Wenyi Tay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2005><div class="card-body p-3 small">Consumers read online reviews for insights which help them to make decisions. Given the large volumes of reviews, succinct review summaries are important for many applications. Existing research has focused on mining for opinions from only review texts and largely ignores the reviewers. However, reviewers have biases and may write lenient or harsh reviews ; they may also have preferences towards some topics over others. Therefore, not all reviews are equal. Ignoring the biases in reviews can generate misleading summaries. We aim for summarization of reviews to include balanced opinions from reviewers of different biases and preferences. We propose to model reviewer biases from their review texts and rating distributions, and learn a bias-aware opinion representation. We further devise an approach for balanced opinion summarization of reviews using our bias-aware opinion representation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2006/>Towards Turkish Abstract Meaning Representation<span class=acl-fixed-case>T</span>urkish <span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation</a></strong><br><a href=/people/z/zahra-azin/>Zahra Azin</a>
|
<a href=/people/g/gulsen-eryigit/>Gülşen Eryiğit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2006><div class="card-body p-3 small">Using rooted, directed and labeled graphs, Abstract Meaning Representation (AMR) abstracts away from syntactic features such as word order and does not annotate every constituent in a sentence. AMR has been specified for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and was not supposed to be an <a href=https://en.wikipedia.org/wiki/Interlingua>Interlingua</a>. However, several studies strived to overcome divergences in the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> between English AMRs and those of their target languages by refining the annotation specification. Following this line of research, we have started to build the first Turkish AMR corpus by hand-annotating 100 sentences of the Turkish translation of the novel The Little Prince and comparing the results with the English AMRs available for the same <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. The next step is to prepare the Turkish AMR annotation specification for training future annotators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2009/>Knowledge Discovery and Hypothesis Generation from Online Patient Forums : A Research Proposal</a></strong><br><a href=/people/a/anne-dirkson/>Anne Dirkson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2009><div class="card-body p-3 small">The unprompted patient experiences shared on patient forums contain a wealth of unexploited knowledge. Mining this knowledge and cross-linking it with biomedical literature, could expose novel insights, which could subsequently provide hypotheses for further clinical research. As of yet, <a href=https://en.wikipedia.org/wiki/Automation>automated methods</a> for open knowledge discovery on patient forum text are lacking. Thus, in this research proposal, we outline future research into methods for mining, aggregating and cross-linking patient knowledge from <a href=https://en.wikipedia.org/wiki/Internet_forum>online forums</a>. Additionally, we aim to address how one could measure the <a href=https://en.wikipedia.org/wiki/Credibility>credibility</a> of this extracted knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2011/>Natural Language Generation : Recently Learned Lessons, Directions for Semantic Representation-based Approaches, and the Case of Brazilian Portuguese Language<span class=acl-fixed-case>B</span>razilian <span class=acl-fixed-case>P</span>ortuguese Language</a></strong><br><a href=/people/m/marco-antonio-sobrevilla-cabezudo/>Marco Antonio Sobrevilla Cabezudo</a>
|
<a href=/people/t/thiago-pardo/>Thiago Pardo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2011><div class="card-body p-3 small">This paper presents a more recent literature review on <a href=https://en.wikipedia.org/wiki/Natural_language_generation>Natural Language Generation</a>. In particular, we highlight the efforts for <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a> in order to show the available resources and the existent approaches for this <a href=https://en.wikipedia.org/wiki/Language>language</a>. We also focus on the approaches for generation from semantic representations (emphasizing the Abstract Meaning Representation formalism) as well as their advantages and limitations, including possible future directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2014/>Active Reading Comprehension : A Dataset for Learning the Question-Answer Relationship Strategy</a></strong><br><a href=/people/d/diana-galvan-sosa/>Diana Galván-Sosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2014><div class="card-body p-3 small">Reading comprehension (RC) through <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> is a useful method for evaluating if a reader understands a text. Standard <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy metrics</a> are used for evaluation, where high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> is taken as indicative of a good understanding. However, literature in quality learning suggests that task performance should also be evaluated on the undergone process to answer. The Question-Answer Relationship (QAR) is one of the strategies for evaluating a reader&#8217;s understanding based on their ability to select different sources of information depending on the question type. We propose the creation of a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to learn the QAR strategy with weak supervision. We expect to complement current work on <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> by introducing a new setup for evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2015/>Paraphrases as Foreign Languages in Multilingual Neural Machine Translation</a></strong><br><a href=/people/z/zhong-zhou/>Zhong Zhou</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2015><div class="card-body p-3 small">Paraphrases, rewordings of the same semantic meaning, are useful for improving <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. Unlike previous works that only explore <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> at the word or phrase level, we use different translations of the whole training data that are consistent in structure as <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> at the corpus level. We treat <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> as foreign languages, tag source sentences with paraphrase labels, and train on parallel paraphrases in the style of multilingual Neural Machine Translation (NMT). Our multi-paraphrase NMT that trains only on two languages outperforms the multilingual baselines. Adding <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> improves the rare word translation and increases <a href=https://en.wikipedia.org/wiki/Entropy>entropy</a> and diversity in <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a>. Adding the source paraphrases boosts performance better than adding the target ones, while adding both lifts performance further. We achieve a BLEU score of 57.2 for <a href=https://en.wikipedia.org/wiki/Bible_translations_into_French>French-to-English translation</a> using 24 corpus-level paraphrases of the <a href=https://en.wikipedia.org/wiki/Bible>Bible</a>, which outperforms the multilingual baselines and is +34.7 above the single-source single-target NMT baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2017/>Unsupervised Pretraining for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> Using Elastic Weight Consolidation</a></strong><br><a href=/people/d/dusan-varis/>Dušan Variš</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2017><div class="card-body p-3 small">This work presents our ongoing research of unsupervised pretraining in neural machine translation (NMT). In our method, we initialize the weights of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Code>decoder</a> with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation (EWC) to avoid forgetting of the original language modeling task. We compare the <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> by EWC with the previous work that focuses on <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> by language modeling objectives. The positive result is that using EWC with the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> achieves BLEU scores similar to the previous work. However, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> converges 2-3 times faster and does not require the original unlabeled training data during the fine-tuning stage. In contrast, the <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> using EWC is less effective if the original and new tasks are not closely related. We show that initializing the bidirectional NMT encoder with a left-to-right language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> for the whole bidirectional context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-2019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-2019/>Ranking of Potential Questions</a></strong><br><a href=/people/l/luise-schricker/>Luise Schricker</a>
|
<a href=/people/t/tatjana-scheffler/>Tatjana Scheffler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2019><div class="card-body p-3 small">Questions are an integral part of <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>. They provide structure and support the exchange of information. One <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic theory</a>, the Questions Under Discussion model, takes question structures as integral to the functioning of a coherent discourse. This <a href=https://en.wikipedia.org/wiki/Theory>theory</a> has not been tested on the count of its validity for predicting observations in real dialogue data, however. In this submission, a system for ranking explicit and implicit questions by their appropriateness in a dialogue is presented. This <a href=https://en.wikipedia.org/wiki/System>system</a> implements constraints and principles put forward in the linguistic literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2020/>Controlling Grammatical Error Correction Using Word Edit Rate</a></strong><br><a href=/people/k/kengo-hotate/>Kengo Hotate</a>
|
<a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/s/satoru-katsumata/>Satoru Katsumata</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2020><div class="card-body p-3 small">When professional English teachers correct grammatically erroneous sentences written by English learners, they use various methods. The correction method depends on how much corrections a learner requires. In this paper, we propose a method for neural grammar error correction (GEC) that can control the degree of correction. We show that it is possible to actually control the degree of GEC by using new training data annotated with word edit rate. Thereby, diverse corrected sentences is obtained from a single erroneous sentence. Moreover, compared to a GEC model that does not use information on the degree of correction, the proposed method improves correction accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-2021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-2021/>From Brain Space to Distributional Space : The Perilous Journeys of fMRI Decoding<span class=acl-fixed-case>MRI</span> Decoding</a></strong><br><a href=/people/g/gosse-minnema/>Gosse Minnema</a>
|
<a href=/people/a/aurelie-herbelot/>Aurélie Herbelot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2021><div class="card-body p-3 small">Recent work in <a href=https://en.wikipedia.org/wiki/Cognitive_neuroscience>cognitive neuroscience</a> has introduced <a href=https://en.wikipedia.org/wiki/Scientific_modelling>models</a> for predicting distributional word meaning representations from <a href=https://en.wikipedia.org/wiki/Neuroimaging>brain imaging data</a>. Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have great potential, but the quality of their predictions has not yet been thoroughly evaluated from a computational linguistics point of view. Due to the limited size of available brain imaging datasets, standard <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality metrics</a> (e.g. similarity judgments and analogies) can not be used. Instead, we investigate the use of several alternative <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a> for evaluating the predicted distributional space against a corpus-derived distributional space. We show that a state-of-the-art decoder, while performing impressively on <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that are commonly used in <a href=https://en.wikipedia.org/wiki/Cognitive_neuroscience>cognitive neuroscience</a>, performs unexpectedly poorly on our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. To address this, we propose <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> for improving the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance. Despite returning promising results, our experiments also demonstrate that much work remains to be done before distributional representations can reliably be predicted from brain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2023/>A Strong and Robust Baseline for Text-Image Matching</a></strong><br><a href=/people/f/fangyu-liu/>Fangyu Liu</a>
|
<a href=/people/r/rongtian-ye/>Rongtian Ye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2023><div class="card-body p-3 small">We review the current schemes of text-image matching models and propose improvements for both <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. First, we empirically show limitations of two popular loss (sum and max-margin loss) widely used in training text-image embeddings and propose a trade-off : a kNN-margin loss which 1) utilizes information from hard negatives and 2) is robust to noise as all K-most hardest samples are taken into account, tolerating pseudo negatives and outliers. Second, we advocate the use of Inverted Softmax (IS) and Cross-modal Local Scaling (CSLS) during inference to mitigate the so-called hubness problem in high-dimensional embedding space, enhancing scores of all metrics by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2025/>Corpus Creation and Analysis for Named Entity Recognition in Telugu-English Code-Mixed Social Media Data<span class=acl-fixed-case>T</span>elugu-<span class=acl-fixed-case>E</span>nglish Code-Mixed Social Media Data</a></strong><br><a href=/people/v/vamshi-krishna-srirangam/>Vamshi Krishna Srirangam</a>
|
<a href=/people/a/appidi-abhinav-reddy/>Appidi Abhinav Reddy</a>
|
<a href=/people/v/vinay-singh/>Vinay Singh</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2025><div class="card-body p-3 small">Named Entity Recognition(NER) is one of the important tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing(NLP)</a> and also is a subtask of <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a>. In this paper we present our work on NER in Telugu-English code-mixed social media data. Code-Mixing, a progeny of <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingualism</a> is a way in which multilingual people express themselves on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> by using linguistics units from different languages within a sentence or speech context. Entity Extraction from social media data such as tweets(twitter) is in general difficult due to its informal nature, code-mixed data further complicates the problem due to its informal, unstructured and incomplete information. We present a Telugu-English code-mixed corpus with the corresponding named entity tags. The <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> used to tag data are Person(&#8216;Per&#8217;), Organization(&#8216;Org&#8217;) and Location(&#8216;Loc&#8217;). We experimented with the machine learning models Conditional Random Fields(CRFs), <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>Decision Trees</a> and BiLSTMs on our <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus</a> which resulted in a <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 0.96, 0.94 and 0.95 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2026/>Joint Learning of Named Entity Recognition and Entity Linking</a></strong><br><a href=/people/p/pedro-henrique-martins/>Pedro Henrique Martins</a>
|
<a href=/people/z/zita-marinho/>Zita Marinho</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2026><div class="card-body p-3 small">Named entity recognition (NER) and entity linking (EL) are two fundamentally related tasks, since in order to perform EL, first the mentions to entities have to be detected. However, most entity linking approaches disregard the mention detection part, assuming that the correct mentions have been previously detected. In this paper, we perform joint learning of NER and EL to leverage their relatedness and obtain a more robust and generalisable system. For that, we introduce a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> inspired by the Stack-LSTM approach. We observe that, in fact, doing <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> of NER and EL improves the performance in both tasks when comparing with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with individual objectives. Furthermore, we achieve results competitive with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in both NER and EL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2027 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2027/>Dialogue-Act Prediction of Future Responses Based on Conversation History</a></strong><br><a href=/people/k/koji-tanaka/>Koji Tanaka</a>
|
<a href=/people/j/junya-takayama/>Junya Takayama</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2027><div class="card-body p-3 small">Sequence-to-sequence models are a common approach to develop a <a href=https://en.wikipedia.org/wiki/Chatbot>chatbot</a>. They can train a <a href=https://en.wikipedia.org/wiki/Conversational_model>conversational model</a> in an end-to-end manner. One significant drawback of such a neural network based approach is that the response generation process is a black-box, and how a specific response is generated is unclear. To tackle this problem, an interpretable response generation mechanism is desired. As a step toward this direction, we focus on dialogue-acts (DAs) that may provide insight to understand the response generation process. In particular, we propose a method to predict a DA of the next response based on the history of previous utterances and their DAs. Experiments using a Switch Board Dialogue Act corpus show that compared to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> considering only a single utterance, our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves 10.8 % higher <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> and 3.0 % higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on DA prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2029/>Multiple Character Embeddings for Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/j/jianing-zhou/>Jianing Zhou</a>
|
<a href=/people/j/jingkang-wang/>Jingkang Wang</a>
|
<a href=/people/g/gongshen-liu/>Gongshen Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2029><div class="card-body p-3 small">Chinese word segmentation (CWS) is often regarded as a character-based sequence labeling task in most current works which have achieved great success with the help of powerful <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. However, these works neglect an important clue : <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a> incorporate both semantic and phonetic meanings. In this paper, we introduce multiple character embeddings including <a href=https://en.wikipedia.org/wiki/Pinyin_Romanization>Pinyin Romanization</a> and Wubi Input, both of which are easily accessible and effective in depicting semantics of characters. We propose a novel shared Bi-LSTM-CRF model to fuse linguistic features efficiently by sharing the LSTM network during the training procedure. Extensive experiments on five corpora show that extra embeddings help obtain a significant improvement in labeling accuracy. Specifically, we achieve the state-of-the-art performance in AS and CityU corpora with F1 scores of 96.9 and 97.3, respectively without leveraging any external lexical resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2033/>From Bilingual to Multilingual Neural Machine Translation by Incremental Training</a></strong><br><a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2033><div class="card-body p-3 small">Multilingual Neural Machine Translation approaches are based on the use of task specific models and the addition of one more language can only be done by retraining the whole system. In this work, we propose a new training schedule that allows the system to scale to more languages without modification of the previous components based on joint training and language-independent encoder / decoder modules allowing for zero-shot translation. This work in progress shows close results to state-of-the-art in the WMT task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2034 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-2034" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-2034/>STRASS : A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings<span class=acl-fixed-case>STRASS</span>: A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings</a></strong><br><a href=/people/l/leo-bouscarrat/>Léo Bouscarrat</a>
|
<a href=/people/a/antoine-bonnefoy/>Antoine Bonnefoy</a>
|
<a href=/people/t/thomas-peel/>Thomas Peel</a>
|
<a href=/people/c/cecile-pereira/>Cécile Pereira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2034><div class="card-body p-3 small">This paper introduces STRASS : Summarization by TRAnsformation Selection and Scoring. It is an extractive text summarization method which leverages the semantic information in existing sentence embedding spaces. Our method creates an extractive summary by selecting the sentences with the closest embeddings to the document embedding. The model earns a transformation of the document embedding to minimize the similarity between the extractive summary and the ground truth summary. As the <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformation</a> is only composed of a dense layer, the training can be done on <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPU</a>, therefore, inexpensive. Moreover, <a href=https://en.wikipedia.org/wiki/Time_complexity>inference time</a> is short and linear according to the number of sentences. As a second contribution, we introduce the French CASS dataset, composed of judgments from the <a href=https://en.wikipedia.org/wiki/Court_of_Cassation_(France)>French Court of cassation</a> and their corresponding summaries. On this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, our results show that our method performs similarly to the state of the art extractive methods with effective training and inferring time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2035/>Attention and Lexicon Regularized LSTM for Aspect-based Sentiment Analysis<span class=acl-fixed-case>LSTM</span> for Aspect-based Sentiment Analysis</a></strong><br><a href=/people/l/lingxian-bao/>Lingxian Bao</a>
|
<a href=/people/p/patrik-lambert/>Patrik Lambert</a>
|
<a href=/people/t/toni-badia/>Toni Badia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2035><div class="card-body p-3 small">Abstract Attention based deep learning systems have been demonstrated to be the state of the art approach for aspect-level sentiment analysis, however, end-to-end deep neural networks lack flexibility as one can not easily adjust the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> to fix an obvious problem, especially when more training data is not available : e.g. when it always predicts positive when seeing the word disappointed. Meanwhile, it is less stressed that attention mechanism is likely to over-focus on particular parts of a sentence, while ignoring positions which provide key information for judging the polarity. In this paper, we describe a simple yet effective approach to leverage <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon information</a> so that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> becomes more flexible and robust. We also explore the effect of regularizing attention vectors to allow the <a href=https://en.wikipedia.org/wiki/Neural_network>network</a> to have a broader focus on different parts of the sentence. The experimental results demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a>.<i>positive</i> when seeing the word <i>disappointed</i>. Meanwhile, it is less stressed that attention mechanism is likely to &#8220;over-focus&#8221; on particular parts of a sentence, while ignoring positions which provide key information for judging the polarity. In this paper, we describe a simple yet effective approach to leverage lexicon information so that the model becomes more flexible and robust. We also explore the effect of regularizing attention vectors to allow the network to have a broader &#8220;focus&#8221; on different parts of the sentence. The experimental results demonstrate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2037 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2037/>Normalizing Non-canonical Turkish Texts Using Machine Translation Approaches<span class=acl-fixed-case>T</span>urkish Texts Using Machine Translation Approaches</a></strong><br><a href=/people/t/talha-colakoglu/>Talha Çolakoğlu</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/a/ahmet-cuneyd-tantug/>Ahmet Cüneyd Tantuğ</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2037><div class="card-body p-3 small">With the growth of the <a href=https://en.wikipedia.org/wiki/Social_web>social web</a>, user-generated text data has reached unprecedented sizes. Non-canonical text normalization provides a way to exploit this as a practical source of training data for <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing systems</a>. The state of the art in Turkish text normalization is composed of a token level pipeline of modules, heavily dependent on external linguistic resources and manually defined rules. Instead, we propose a fully automated, context-aware machine translation approach with fewer stages of processing. Experiments with various implementations of our approach show that we are able to surpass the current best-performing <a href=https://en.wikipedia.org/wiki/System>system</a> by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2039 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2039/>Investigating Political Herd Mentality : A Community Sentiment Based Approach</a></strong><br><a href=/people/a/anjali-bhavan/>Anjali Bhavan</a>
|
<a href=/people/r/rohan-mishra/>Rohan Mishra</a>
|
<a href=/people/p/pradyumna-prakhar-sinha/>Pradyumna Prakhar Sinha</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2039><div class="card-body p-3 small">Analyzing polarities and sentiments inherent in <a href=https://en.wikipedia.org/wiki/Public_speaking>political speeches</a> and <a href=https://en.wikipedia.org/wiki/Debate>debates</a> poses an important problem today. This experiment aims to address this issue by analyzing publicly-available Hansard transcripts of the debates conducted in the UK Parliament. Our proposed approach, which uses community-based graph information to augment hand-crafted features based on <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a> and <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a> on debate transcripts, currently surpasses the benchmark results on the same dataset. Such sentiment classification systems could prove to be of great use in today&#8217;s politically turbulent times, for public knowledge of politicians&#8217; stands on various relevant issues proves vital for <a href=https://en.wikipedia.org/wiki/Good_governance>good governance</a> and citizenship. The experiments also demonstrate that continuous feature representations learned from <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> can improve performance on sentiment classification tasks significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2041 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2041/>Embedding Strategies for Specialized Domains : Application to Clinical Entity Recognition</a></strong><br><a href=/people/h/hicham-el-boukkouri/>Hicham El Boukkouri</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2041><div class="card-body p-3 small">Using pre-trained word embeddings in conjunction with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning models</a> has become the de facto approach in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. While this usually yields satisfactory results, off-the-shelf <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> tend to perform poorly on texts from specialized domains such as clinical reports. Moreover, training specialized word representations from scratch is often either impossible or ineffective due to the lack of large enough in-domain data. In this work, we focus on the clinical domain for which we study embedding strategies that rely on general-domain resources only. We show that by combining off-the-shelf contextual embeddings (ELMo) with static word2vec embeddings trained on a small in-domain corpus built from the task data, we manage to reach and sometimes outperform representations learned from a large corpus in the medical domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2044 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2044/>Improving Neural Entity Disambiguation with Graph Embeddings</a></strong><br><a href=/people/o/ozge-sevgili/>Özge Sevgili</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2044><div class="card-body p-3 small">Entity Disambiguation (ED) is the task of linking an ambiguous entity mention to a corresponding entry in a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. Current methods have mostly focused on unstructured text data to learn representations of entities, however, there is structured information in the knowledge base itself that should be useful to disambiguate entities. In this work, we propose a method that uses <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embeddings</a> for integrating structured information from the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> with <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured information</a> from text-based representations. Our experiments confirm that graph embeddings trained on a graph of hyperlinks between Wikipedia articles improve the performances of simple feed-forward neural ED model and a state-of-the-art neural ED system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2046 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2046/>Convolutional Neural Networks for Financial Text Regression</a></strong><br><a href=/people/n/nesat-dereli/>Neşat Dereli</a>
|
<a href=/people/m/murat-saraclar/>Murat Saraclar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2046><div class="card-body p-3 small">Forecasting financial volatility of a publicly-traded company from its <a href=https://en.wikipedia.org/wiki/Annual_report>annual reports</a> has been previously defined as a text regression problem. Recent studies use a manually labeled lexicon to filter the annual reports by keeping sentiment words only. In order to remove the lexicon dependency without decreasing the performance, we replace bag-of-words model word features by word embedding vectors. Using word vectors increases the number of parameters. Considering the increase in number of parameters and excessive lengths of annual reports, a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network model</a> is proposed and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> is applied. Experimental results show that the convolutional neural network model provides more accurate volatility predictions than lexicon based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2049 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-2049" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-2049/>Scheduled Sampling for Transformers</a></strong><br><a href=/people/t/tsvetomila-mihaylova/>Tsvetomila Mihaylova</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2049><div class="card-body p-3 small">Scheduled sampling is a technique for avoiding one of the known problems in sequence-to-sequence generation : <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a>. It consists of feeding the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> a mix of the teacher forced embeddings and the model predictions from the previous step in training time. The <a href=https://en.wikipedia.org/wiki/Scientific_technique>technique</a> has been used for improving <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNN)</a>. In the Transformer model, unlike the RNN, the generation of a new word attends to the full sentence generated so far, not only to the last word, and it is not straightforward to apply the scheduled sampling technique. We propose some structural changes to allow scheduled sampling to be applied to Transformer architectures, via a two-pass decoding strategy. Experiments on two language pairs achieve performance close to a teacher-forcing baseline and show that this technique is promising for further exploration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2051 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2051/>Cross-domain and Cross-lingual Abusive Language Detection : A Hybrid Approach with Deep Learning and a Multilingual Lexicon</a></strong><br><a href=/people/e/endang-wahyu-pamungkas/>Endang Wahyu Pamungkas</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2051><div class="card-body p-3 small">The development of <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational methods</a> to detect abusive language in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> within variable and multilingual contexts has recently gained significant traction. The growing interest is confirmed by the large number of benchmark corpora for different languages developed in the latest years. However, abusive language behaviour is multifaceted and available datasets are featured by different topical focuses. This makes abusive language detection a domain-dependent task, and building a robust system to detect general abusive content a first challenge. Moreover, most resources are available for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, which makes detecting abusive language in low-resource languages a further challenge. We address both challenges by considering ten publicly available datasets across different domains and languages. A hybrid approach with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and a multilingual lexicon to cross-domain and cross-lingual detection of abusive content is proposed and compared with other simpler models. We show that training a <a href=https://en.wikipedia.org/wiki/System>system</a> on general abusive language datasets will produce a cross-domain robust system, which can be used to detect other more specific types of abusive content. We also found that using the domain-independent lexicon HurtLex is useful to transfer knowledge between domains and languages. In the cross-lingual experiment, we demonstrate the effectiveness of our jointlearning model also in out-domain scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2052/>De-Mixing Sentiment from Code-Mixed Text</a></strong><br><a href=/people/y/yash-kumar-lal/>Yash Kumar Lal</a>
|
<a href=/people/v/vaibhav-kumar/>Vaibhav Kumar</a>
|
<a href=/people/m/mrinal-dhar/>Mrinal Dhar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2052><div class="card-body p-3 small">Code-mixing is the phenomenon of mixing the vocabulary and syntax of multiple languages in the same sentence. It is an increasingly common occurrence in today&#8217;s multilingual society and poses a big challenge when encountered in different downstream tasks. In this paper, we present a hybrid architecture for the task of Sentiment Analysis of English-Hindi code-mixed data. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> consists of three components, each seeking to alleviate different issues. We first generate subword level representations for the sentences using a CNN architecture. The generated representations are used as inputs to a Dual Encoder Network which consists of two different BiLSTMs-the Collective and Specific Encoder. The Collective Encoder captures the overall sentiment of the sentence, while the Specific <a href=https://en.wikipedia.org/wiki/Encoder>Encoder</a> utilizes an attention mechanism in order to focus on individual sentiment-bearing sub-words. This, combined with a Feature Network consisting of orthographic features and specially trained <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, achieves state-of-the-art results-83.54 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and 0.827 F1 score-on a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2055/>Deep Neural Models for Medical Concept Normalization in <a href=https://en.wikipedia.org/wiki/User-generated_content>User-Generated Texts</a></a></strong><br><a href=/people/z/zulfat-miftahutdinov/>Zulfat Miftahutdinov</a>
|
<a href=/people/e/elena-tutubalina/>Elena Tutubalina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2055><div class="card-body p-3 small">In this work, we consider the medical concept normalization problem, i.e., the problem of mapping a health-related entity mention in a <a href=https://en.wikipedia.org/wiki/Formal_language>free-form text</a> to a concept in a <a href=https://en.wikipedia.org/wiki/Controlled_vocabulary>controlled vocabulary</a>, usually to the standard thesaurus in the <a href=https://en.wikipedia.org/wiki/Unified_Medical_Language_System>Unified Medical Language System (UMLS)</a>. This is a challenging task since <a href=https://en.wikipedia.org/wiki/Medical_terminology>medical terminology</a> is very different when coming from health care professionals or from the general public in the form of social media texts. We approach it as a sequence learning problem with powerful <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> such as <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> and contextualized word representation models trained to obtain semantic representations of social media expressions. Our experimental evaluation over three different benchmarks shows that neural architectures leverage the semantic meaning of the entity mention and significantly outperform existing state of the art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2056 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2056/>Using <a href=https://en.wikipedia.org/wiki/Semantic_similarity>Semantic Similarity</a> as Reward for <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> in Sentence Generation</a></strong><br><a href=/people/g/go-yasui/>Go Yasui</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2056><div class="card-body p-3 small">Traditional model training for sentence generation employs cross-entropy loss as the <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>. While cross-entropy loss has convenient properties for <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>, it is unable to evaluate sentences as a whole, and lacks flexibility. We present the approach of training the generation model using the estimated semantic similarity between the output and reference sentences to alleviate the problems faced by the training with cross-entropy loss. We use the BERT-based scorer fine-tuned to the Semantic Textual Similarity (STS) task for semantic similarity estimation, and train the model with the estimated scores through reinforcement learning (RL). Our experiments show that <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> with semantic similarity reward improves the BLEU scores from the baseline LSTM NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2058 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2058/>Detecting Adverse Drug Reactions from Biomedical Texts with Neural Networks</a></strong><br><a href=/people/i/ilseyar-alimova/>Ilseyar Alimova</a>
|
<a href=/people/e/elena-tutubalina/>Elena Tutubalina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2058><div class="card-body p-3 small">Detection of adverse drug reactions in postapproval periods is a crucial challenge for <a href=https://en.wikipedia.org/wiki/Pharmacology>pharmacology</a>. Social media and electronic clinical reports are becoming increasingly popular as a source for obtaining health related information. In this work, we focus on extraction information of adverse drug reactions from various sources of biomedical textbased information, including <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical literature</a> and <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We formulate the problem as a binary classification task and compare the performance of four state-of-the-art attention-based neural networks in terms of the <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>. We show the effectiveness of these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on four different <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2059/>Annotating and Analyzing Semantic Role of Elementary Units and Relations in Online Persuasive Arguments</a></strong><br><a href=/people/r/ryo-egawa/>Ryo Egawa</a>
|
<a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/k/katsuhide-fujita/>Katsuhide Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2059><div class="card-body p-3 small">For analyzing online persuasions, one of the important goals is to semantically understand how people construct comments to persuade others. However, analyzing the semantic role of arguments for online persuasion has been less emphasized. Therefore, in this study, we propose a novel annotation scheme that captures the semantic role of arguments in a popular online persuasion forum, so-called ChangeMyView. Through this study, we have made the following contributions : (i) proposing a <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a> that includes five types of elementary units (EUs) and two types of <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. (ii) annotating ChangeMyView which results in 4612 EUs and 2713 relations in 345 posts. (iii) analyzing the semantic role of persuasive arguments. Our analyses captured certain characteristic phenomena for <a href=https://en.wikipedia.org/wiki/Persuasion>online persuasion</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2060/>A Japanese Word Segmentation Proposal<span class=acl-fixed-case>J</span>apanese Word Segmentation Proposal</a></strong><br><a href=/people/s/stalin-aguirre/>Stalin Aguirre</a>
|
<a href=/people/j/josafa-aguiar/>Josafá Aguiar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2060><div class="card-body p-3 small">Current Japanese word segmentation methods, that use a morpheme-based approach, may produce different segmentations for the same strings. This occurs when these strings appear in different sentences. The cause is the influence of different contexts around these strings affecting the <a href=https://en.wikipedia.org/wiki/Statistical_model>probabilistic models</a> used in segmentation algorithms. This paper presents an alternative to the current morpheme-based scheme for Japanese word segmentation. The proposed scheme focuses on segmenting inflections as single words instead of separating the <a href=https://en.wikipedia.org/wiki/Auxiliary_verb>auxiliary verbs</a> and other morphemes from the stems. Some morphological segmentation rules are presented for each type of word and these <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> are implemented in a <a href=https://en.wikipedia.org/wiki/Computer_program>program</a> which is properly described. The program is used to generate a segmentation of a sentence corpus, whose consistency is calculated and compared with the current morpheme-based segmentation of the same corpus. The experiments show that this <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> produces a much more consistent segmentation than the morpheme-based one.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>