<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/S17-1.pdf>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*<span class=acl-fixed-case>SEM</span> 2017)</a></h2><p class=lead><a href=/people/n/nancy-ide/>Nancy Ide</a>,
<a href=/people/a/aurelie-herbelot/>Aurélie Herbelot</a>,
<a href=/people/l/lluis-marquez/>Lluís Màrquez</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>S17-1</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Vancouver, Canada</dd><dt>Venues:</dt><dd><a href=/venues/starsem/>*SEM</a>
| <a href=/venues/semeval/>SemEval</a></dd><dt>SIGs:</dt><dd><a href=/sigs/siglex/>SIGLEX</a>
|
<a href=/sigs/sigsem/>SIGSEM</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/S17-1>https://aclanthology.org/S17-1</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/S17-1 title="To the current version of the paper by DOI">10.18653/v1/S17-1</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/S17-1.pdf>https://aclanthology.org/S17-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/S17-1.pdf title="Open PDF of 'Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+6th+Joint+Conference+on+Lexical+and+Computational+Semantics+%28%2ASEM+2017%29" title="Search for 'Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1000/>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*<span class=acl-fixed-case>SEM</span> 2017)</a></strong><br><a href=/people/n/nancy-ide/>Nancy Ide</a>
|
<a href=/people/a/aurelie-herbelot/>Aurélie Herbelot</a>
|
<a href=/people/l/lluis-marquez/>Lluís Màrquez</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1001/>What Analogies Reveal about Word Vectors and their Compositionality</a></strong><br><a href=/people/g/gregory-finley/>Gregory Finley</a>
|
<a href=/people/s/stephanie-farmer/>Stephanie Farmer</a>
|
<a href=/people/s/serguei-pakhomov/>Serguei Pakhomov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1001><div class="card-body p-3 small">Analogy completion via <a href=https://en.wikipedia.org/wiki/Vector_arithmetic>vector arithmetic</a> has become a common means of demonstrating the compositionality of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Previous work have shown that this <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> works more reliably for certain types of analogical word relationships than for others, but these studies have not offered a convincing account for why this is the case. We arrive at such an account through an experiment that targets a wide variety of analogy questions and defines a baseline condition to more accurately measure the efficacy of our <a href=https://en.wikipedia.org/wiki/System>system</a>. We find that the most reliably solvable analogy categories involve either 1) the application of a <a href=https://en.wikipedia.org/wiki/Morpheme>morpheme</a> with clear syntactic effects, 2) malefemale alternations, or 3) <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>. These broader types do not pattern cleanly along a syntacticsemantic divide. We suggest instead that their commonality is distributional, in that the difference between the distributions of two words in any given pair encompasses a relatively small number of word types. Our study offers a needed explanation for why analogy tests succeed and fail where they do and provides nuanced insight into the relationship between word distributions and the <a href=https://en.wikipedia.org/wiki/Semantics>theoretical linguistic domains of syntax and semantics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1003/>Decoding Sentiment from Distributed Representations of Sentences</a></strong><br><a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1003><div class="card-body p-3 small">Distributed representations of sentences have been developed recently to represent their meaning as real-valued vectors. However, it is not clear how much information such <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> retain about the polarity of sentences. To study this question, we decode sentiment from <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised sentence representations</a> learned with different architectures (sensitive to the order of words, the order of sentences, or none) in 9 typologically diverse languages. Sentiment results from the (recursive) composition of lexical items and grammatical strategies such as <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> and concession. The results are manifold : we show that there is no &#8216;one-size-fits-all&#8217; representation architecture outperforming the others across the board. Rather, the top-ranking architectures depend on the language at hand. Moreover, we find that in several cases the additive composition model based on skip-gram word vectors may surpass supervised state-of-art architectures such as bi-directional LSTMs. Finally, we provide a possible explanation of the observed variation based on the type of negative constructions in each language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1004/>Detecting Asymmetric Semantic Relations in Context : A Case-Study on Hypernymy Detection</a></strong><br><a href=/people/y/yogarshi-vyas/>Yogarshi Vyas</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1004><div class="card-body p-3 small">We introduce WHiC, a challenging testbed for detecting hypernymy, an asymmetric relation between words. While previous work has focused on detecting hypernymy between word types, we ground the meaning of words in specific contexts drawn from WordNet examples, and require predictions to be sensitive to changes in contexts. WHiC lets us analyze complementary properties of two approaches of inducing vector representations of word meaning in context. We show that such contextualized word representations also improve detection of a wider range of semantic relations in context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S17-1005" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S17-1005/>Domain-Specific New Words Detection in Chinese<span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/a/ao-chen/>Ao Chen</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1005><div class="card-body p-3 small">With the explosive growth of <a href=https://en.wikipedia.org/wiki/Internet>Internet</a>, more and more domain-specific environments appear, such as <a href=https://en.wikipedia.org/wiki/Internet_forum>forums</a>, <a href=https://en.wikipedia.org/wiki/Blog>blogs</a>, <a href=https://en.wikipedia.org/wiki/Massive_open_online_course>MOOCs</a> and etc. Domain-specific words appear in these areas and always play a critical role in the domain-specific NLP tasks. This paper aims at extracting Chinese domain-specific new words automatically. The extraction of domain-specific new words has two parts including both new words in this domain and the especially important words. In this work, we propose a joint statistical model to perform these two works simultaneously. Compared to traditional new words detection models, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> does n&#8217;t need handcraft features which are labor intensive. Experimental results demonstrate that our joint model achieves a better performance compared with the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1006/>Deep Learning Models For Multiword Expression Identification</a></strong><br><a href=/people/w/waseem-gharbieh/>Waseem Gharbieh</a>
|
<a href=/people/v/virendrakumar-bhavsar/>Virendrakumar Bhavsar</a>
|
<a href=/people/p/paul-cook/>Paul Cook</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1006><div class="card-body p-3 small">Multiword expressions (MWEs) are lexical items that can be decomposed into multiple component words, but have properties that are unpredictable with respect to their component words. In this paper we propose the first <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for token-level identification of MWEs. Specifically, we consider a layered feedforward network, a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>, and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. In experimental results we show that <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> are able to outperform the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for MWE identification, with a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> with three hidden layers giving the best performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S17-1007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S17-1007/>Emotion Intensities in Tweets</a></strong><br><a href=/people/s/saif-mohammad/>Saif Mohammad</a>
|
<a href=/people/f/felipe-bravo-marquez/>Felipe Bravo-Marquez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1007><div class="card-body p-3 small">This paper examines the task of detecting intensity of emotion from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>. We create the first datasets of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> annotated for <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, <a href=https://en.wikipedia.org/wiki/Fear>fear</a>, <a href=https://en.wikipedia.org/wiki/Joy>joy</a>, and sadness intensities. We use a technique called bestworst scaling (BWS) that improves annotation consistency and obtains reliable fine-grained scores. We show that emotion-word hashtags often impact emotion intensity, usually conveying a more intense emotion. Finally, we create a benchmark regression system and conduct experiments to determine : which <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are useful for detecting emotion intensity ; and, the extent to which two emotions are similar in terms of how they manifest in language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1008/>Deep Active Learning for Dialogue Generation</a></strong><br><a href=/people/n/nabiha-asghar/>Nabiha Asghar</a>
|
<a href=/people/p/pascal-poupart/>Pascal Poupart</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/h/hang-li/>Hang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1008><div class="card-body p-3 small">We propose an online, end-to-end, neural generative conversational model for open-domain dialogue. It is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning. While most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character user-feedback at each step. Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> with customized personas, <a href=https://en.wikipedia.org/wiki/Mood_(psychology)>moods</a> and conversational styles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1009/>Mapping the Paraphrase Database to <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a><span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1009><div class="card-body p-3 small">WordNet has facilitated important research in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> but its usefulness is somewhat limited by its relatively small lexical coverage. The Paraphrase Database (PPDB) covers 650 times more words, but lacks the semantic structure of <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> that would make it more directly useful for downstream tasks. We present a method for mapping words from PPDB to WordNet synsets with 89 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. The mapping also lays important groundwork for incorporating WordNet&#8217;s relations into PPDB so as to increase its utility for semantic reasoning in applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1010/>Semantic Frame Labeling with Target-based Neural Model</a></strong><br><a href=/people/y/yukun-feng/>Yukun Feng</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a>
|
<a href=/people/j/jian-xu/>Jian Xu</a>
|
<a href=/people/c/chunhua-liu/>Chunhua Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1010><div class="card-body p-3 small">This paper explores the automatic learning of <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> of the target&#8217;s context for semantic frame labeling with target-based neural model. We constrain the whole sentence as the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s input without <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> from the sentence. This is different from many previous works in which local feature extraction of the targets is widely used. This constraint makes the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> harder, especially with long sentences, but also makes our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> easily applicable to a range of resources and other similar tasks. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on several resources and get the state-of-the-art result on subtask 2 of SemEval 2015 task 15. Finally, we extend the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to word-sense disambiguation task and we also achieve a strong result in comparison to state-of-the-art work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S17-1011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S17-1011/>Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles</a></strong><br><a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1011><div class="card-body p-3 small">We study how different frame annotations complement one another when learning continuous lexical semantics. We learn the representations from a tensorized skip-gram model that consistently encodes syntactic-semantic content better, with multiple 10 % gains over baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1013/>Comparing Approaches for Automatic Question Identification</a></strong><br><a href=/people/a/angel-maredia/>Angel Maredia</a>
|
<a href=/people/k/kara-schechtman/>Kara Schechtman</a>
|
<a href=/people/s/sarah-ita-levitan/>Sarah Ita Levitan</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1013><div class="card-body p-3 small">Collecting spontaneous speech corpora that are open-ended, yet topically constrained, is increasingly popular for research in spoken dialogue systems and speaker state, inter alia. Typically, these <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> are labeled by human annotators, either in the lab or through <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> ; however, this is cumbersome and time-consuming for large corpora. We present four different approaches to automatically tagging a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> when general topics of the conversations are known. We develop these approaches on the Columbia X-Cultural Deception corpus and find <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> that significantly exceeds the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a>. Finally, we conduct a cross-corpus evaluation by testing the best performing approach on the Columbia / SRI / Colorado corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/S17-1014.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/S17-1014/>Does Free Word Order Hurt? Assessing the Practical Lexical Function Model for Croatian<span class=acl-fixed-case>C</span>roatian</a></strong><br><a href=/people/z/zoran-medic/>Zoran Medić</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1014><div class="card-body p-3 small">The Practical Lexical Function (PLF) model is a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> of computational distributional semantics that attempts to strike a balance between expressivity and learnability in predicting phrase meaning and shows competitive results. We investigate how well the PLF carries over to free word order languages, given that it builds on observations of predicate-argument combinations that are harder to recover in free word order languages. We evaluate variants of the PLF for <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, using a new lexical substitution dataset. We find that the PLF works about as well for <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a> as for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, but demonstrate that its strength lies in modeling verbs, and that the free word order affects the less robust PLF variant.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1015/>A <a href=https://en.wikipedia.org/wiki/Mixture_model>Mixture Model</a> for Learning Multi-Sense Word Embeddings</a></strong><br><a href=/people/d/dai-quoc-nguyen/>Dai Quoc Nguyen</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a>
|
<a href=/people/s/stefan-thater/>Stefan Thater</a>
|
<a href=/people/m/manfred-pinkal/>Manfred Pinkal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1015><div class="card-body p-3 small">Word embeddings are now a standard technique for inducing <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning representations</a> for words. For getting good <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a>, it is important to take into account different senses of a word. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mixture_model>mixture model</a> for learning multi-sense word embeddings. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on standard evaluation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1018/>Semantic Frames and Visual Scenes : Learning Semantic Role Inventories from Image and Video Descriptions</a></strong><br><a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a>
|
<a href=/people/a/andreas-wundsam/>Andreas Wundsam</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1018><div class="card-body p-3 small">Frame-semantic parsing and semantic role labelling, that aim to automatically assign semantic roles to arguments of verbs in a sentence, have become an active strand of research in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. However, to date these methods have relied on a predefined inventory of semantic roles. In this paper, we present a method to automatically learn argument role inventories for verbs from large corpora of text, images and videos. We evaluate the method against manually constructed role inventories in <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> and show that the <a href=https://en.wikipedia.org/wiki/Visual_model>visual model</a> outperforms the language-only model and operates with a high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1019/>Acquiring Predicate Paraphrases from News Tweets</a></strong><br><a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1019><div class="card-body p-3 small">We present a simple method for ever-growing extraction of predicate paraphrases from <a href=https://en.wikipedia.org/wiki/Headline>news headlines</a> in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Analysis of the output of ten weeks of collection shows that the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> with different support levels is estimated between 60-86 %. We also demonstrate that our <a href=https://en.wikipedia.org/wiki/Resource>resource</a> is to a large extent complementary to existing resources, providing many novel <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>. Our <a href=https://en.wikipedia.org/wiki/Resource>resource</a> is publicly available, continuously expanding based on <a href=https://en.wikipedia.org/wiki/News>daily news</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S17-1020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S17-1020/>Evaluating <a href=https://en.wikipedia.org/wiki/Semantic_parsing>Semantic Parsing</a> against a Simple Web-based Question Answering Model</a></strong><br><a href=/people/a/alon-talmor/>Alon Talmor</a>
|
<a href=/people/m/mor-geva/>Mor Geva</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1020><div class="card-body p-3 small">Semantic parsing shines at analyzing complex natural language that involves <a href=https://en.wikipedia.org/wiki/Composition_(language)>composition</a> and computation over multiple pieces of evidence. However, <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> contain many factoid questions that can be answered from a single <a href=https://en.wikipedia.org/wiki/Web_page>web document</a>. In this paper, we propose to evaluate semantic parsing-based question answering models by comparing them to a question answering baseline that queries the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> and extracts the answer only from <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web snippets</a>, without access to the target knowledge-base. We investigate this approach on COMPLEXQUESTIONS, a dataset designed to focus on compositional language, and find that our model obtains reasonable performance (35 F1 compared to 41 F1 of state-of-the-art). We find in our analysis that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs well on complex questions involving <a href=https://en.wikipedia.org/wiki/Logical_conjunction>conjunctions</a>, but struggles on questions that involve relation composition and <a href=https://en.wikipedia.org/wiki/Superlative>superlatives</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1021/>Logical Metonymy in a Distributional Model of Sentence Comprehension</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/p/philippe-blache/>Philippe Blache</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1021><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Theoretical_linguistics>theoretical linguistics</a>, logical metonymy is defined as the combination of an event-subcategorizing verb with an <a href=https://en.wikipedia.org/wiki/Object_(grammar)>entity-denoting direct object</a> (e.g., The author began the book), so that the interpretation of the VP requires the retrieval of a covert event (e.g., writing). Psycholinguistic studies have revealed extra processing costs for logical metonymy, a phenomenon generally explained with the introduction of new semantic structure. In this paper, we present a general <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional model</a> for <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence comprehension</a> inspired by the Memory, Unification and Control model by Hagoort (2013,2016). We show that our distributional framework can account for the extra processing costs of logical metonymy and can identify the covert event in a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/S17-1022.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/S17-1022/>Double Trouble : The Problem of Construal in Semantic Annotation of Adpositions</a></strong><br><a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/n/na-rae-han/>Na-Rae Han</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1022><div class="card-body p-3 small">We consider the semantics of prepositions, revisiting a broad-coverage <a href=https://en.wikipedia.org/wiki/Annotation>annotation scheme</a> used for annotating all 4,250 <a href=https://en.wikipedia.org/wiki/Preposition_and_postposition>preposition tokens</a> in a 55,000 word corpus of English. Attempts to apply the scheme to adpositions and <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>case markers</a> in other languages, as well as some problematic cases in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, have led us to reconsider the assumption that an adposition&#8217;s lexical contribution is equivalent to the role / relation that it mediates. Our proposal is to embrace the potential for construal in adposition use, expressing such phenomena directly at the token level to manage complexity and avoid sense proliferation. We suggest a framework to represent both the scene role and the adposition&#8217;s lexical function so they can be annotated at scalesupporting automatic, statistical processing of domain-general languageand discuss how this representation would allow for a simpler inventory of labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1023/>Issues of Mass and Count : Dealing with ‘Dual-Life’ Nouns</a></strong><br><a href=/people/t/tibor-kiss/>Tibor Kiss</a>
|
<a href=/people/f/francis-jeffry-pelletier/>Francis Jeffry Pelletier</a>
|
<a href=/people/h/halima-husic/>Halima Husić</a>
|
<a href=/people/j/johanna-poppek/>Johanna Poppek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1023><div class="card-body p-3 small">The topics of mass and count have been studied for many decades in <a href=https://en.wikipedia.org/wiki/Philosophy>philosophy</a> (e.g., Quine, 1960 ; Pelletier, 1975), <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> (e.g., McCawley, 1975 ; Allen, 1980 ; Krifka, 1991) and <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a> (e.g., Middleton et al, 2004 ; Barner et al, 2009). More recently, interest from within <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> has studied the issues involved (e.g., Pustejovsky, 1991 ; Bond, 2005 ; Schmidtke & Kuperman, 2016), to name just a few. As is pointed out in these works, there are many difficult conceptual issues involved in the study of this contrast. In this article we study one of these issues the Dual-Life of being simultaneously + mass and + count by means of an unusual combination of human annotation, online lexical resources, and online corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1024/>Parsing Graphs with Regular Graph Grammars</a></strong><br><a href=/people/s/sorcha-gilroy/>Sorcha Gilroy</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/s/sebastian-maneth/>Sebastian Maneth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1024><div class="card-body p-3 small">Recently, several <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> have become available which represent <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language phenomena</a> as <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>. Hyperedge Replacement Languages (HRL) have been the focus of much attention as a formalism to represent the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> in these datasets. Chiang et al. (2013) prove that HRL graphs can be parsed in <a href=https://en.wikipedia.org/wiki/Time_complexity>polynomial time</a> with respect to the size of the input graph. We believe that HRL are more expressive than is necessary to represent semantic graphs and we propose the use of Regular Graph Languages (RGL ; Courcelle 1991), which is a subfamily of HRL, as a possible alternative. We provide a top-down parsing algorithm for RGL that runs in time linear in the size of the input graph.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1025/>Embedded Semantic Lexicon Induction with Joint Global and Local Optimization</a></strong><br><a href=/people/s/sujay-kumar-jauhar/>Sujay Kumar Jauhar</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1025><div class="card-body p-3 small">Creating annotated frame lexicons such as <a href=https://en.wikipedia.org/wiki/PropBank>PropBank</a> and <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> is expensive and labor intensive. We present a method to induce an embedded frame lexicon in an minimally supervised fashion using nothing more than unlabeled predicate-argument word pairs. We hypothesize that aggregating such pair selectional preferences across training leads us to a global understanding that captures predicate-argument frame structure. Our approach revolves around a novel integration between a predictive embedding model and an Indian Buffet Process posterior regularizer. We show, through our experimental evaluation, that we outperform baselines on two tasks and can learn an embedded frame lexicon that is able to capture some interesting generalities in relation to hand-crafted semantic frames.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1026/>Generating Pattern-Based Entailment Graphs for Relation Extraction</a></strong><br><a href=/people/k/kathrin-eichler/>Kathrin Eichler</a>
|
<a href=/people/f/feiyu-xu/>Feiyu Xu</a>
|
<a href=/people/h/hans-uszkoreit/>Hans Uszkoreit</a>
|
<a href=/people/s/sebastian-krause/>Sebastian Krause</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1026><div class="card-body p-3 small">Relation extraction is the task of recognizing and extracting relations between entities or concepts in texts. A common approach is to exploit existing knowledge to learn linguistic patterns expressing the target relation and use these <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> for extracting new relation mentions. Deriving relation patterns automatically usually results in large numbers of candidates, which need to be filtered to derive a subset of <a href=https://en.wikipedia.org/wiki/Pattern_matching>patterns</a> that reliably extract correct relation mentions. We address the pattern selection task by exploiting the knowledge represented by entailment graphs, which capture semantic relationships holding among the learned pattern candidates. This is motivated by the fact that a <a href=https://en.wikipedia.org/wiki/Pattern_matching>pattern</a> may not express the target relation explicitly, but still be useful for extracting instances for which the relation holds, because its meaning entails the meaning of the target relation. We evaluate the usage of both automatically generated and gold-standard entailment graphs in a relation extraction scenario and present favorable experimental results, exhibiting the benefits of structuring and selecting patterns based on entailment graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1027/>Classifying Semantic Clause Types : Modeling Context and Genre Characteristics with Recurrent Neural Networks and Attention</a></strong><br><a href=/people/m/maria-becker/>Maria Becker</a>
|
<a href=/people/m/michael-staniek/>Michael Staniek</a>
|
<a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1027><div class="card-body p-3 small">Detecting aspectual properties of clauses in the form of situation entity types has been shown to depend on a combination of syntactic-semantic and contextual features. We explore this task in a deep-learning framework, where tuned word representations capture lexical, syntactic and semantic features. We introduce an attention mechanism that pinpoints relevant context not only for the current instance, but also for the larger context. Apart from implicitly capturing task relevant features, the advantage of our neural model is that it avoids the need to reproduce linguistic features for other languages and is thus more easily transferable. We present experiments for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a> that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our <a href=https://en.wikipedia.org/wiki/System>system</a> from one language to another.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-1030/>Ways of Asking and Replying in Duplicate Question Detection</a></strong><br><a href=/people/j/joao-rodrigues/>João António Rodrigues</a>
|
<a href=/people/c/chakaveh-saedi/>Chakaveh Saedi</a>
|
<a href=/people/v/vladislav-maraev/>Vladislav Maraev</a>
|
<a href=/people/j/joao-silva/>João Silva</a>
|
<a href=/people/a/antonio-branco/>António Branco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1030><div class="card-body p-3 small">This paper presents the results of systematic experimentation on the impact in duplicate question detection of different types of questions across both a number of established approaches and a novel, superior one used to address this language processing task. This study permits to gain a novel insight on the different levels of <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of the diverse detection methods with respect to different conditions of their application, including the ones that approximate real usage scenarios.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>