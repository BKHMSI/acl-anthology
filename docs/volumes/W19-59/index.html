<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W19-59.pdf>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</a></h2><p class=lead><a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a>,
<a href=/people/m/milica-gasic/>Milica Gasic</a>,
<a href=/people/i/ingrid-zuckerman/>Ingrid Zuckerman</a>,
<a href=/people/g/gabriel-skantze/>Gabriel Skantze</a>,
<a href=/people/m/mikio-nakano/>Mikio Nakano</a>,
<a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>,
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>,
<a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W19-59</dd><dt>Month:</dt><dd>September</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Stockholm, Sweden</dd><dt>Venues:</dt><dd><a href=/venues/sigdial/>SIGDIAL</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigdial/>SIGDIAL</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W19-59>https://aclanthology.org/W19-59</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W19-59.pdf>https://aclanthology.org/W19-59.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W19-59.pdf title="Open PDF of 'Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+20th+Annual+SIGdial+Meeting+on+Discourse+and+Dialogue" title="Search for 'Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5900/>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</a></strong><br><a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a>
|
<a href=/people/i/ingrid-zuckerman/>Ingrid Zuckerman</a>
|
<a href=/people/g/gabriel-skantze/>Gabriel Skantze</a>
|
<a href=/people/m/mikio-nakano/>Mikio Nakano</a>
|
<a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5904/>Few-Shot Dialogue Generation Without Annotated Data : A Transfer Learning Approach</a></strong><br><a href=/people/i/igor-shalyminov/>Igor Shalyminov</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a>
|
<a href=/people/a/arash-eshghi/>Arash Eshghi</a>
|
<a href=/people/o/oliver-lemon/>Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5904><div class="card-body p-3 small">Learning with minimal data is one of the key challenges in the development of practical, production-ready goal-oriented dialogue systems. In a real-world enterprise setting where dialogue systems are developed rapidly and are expected to work robustly for an ever-growing variety of domains, products, and scenarios, efficient <a href=https://en.wikipedia.org/wiki/Learning>learning</a> from a limited number of examples becomes indispensable. In this paper, we introduce a technique to achieve state-of-the-art dialogue generation performance in a few-shot setup, without using any <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a>. We do this by leveraging background knowledge from a larger, more highly represented dialogue source namely, the MetaLWOz dataset. We evaluate our model on the Stanford Multi-Domain Dialogue Dataset, consisting of human-human goal-oriented dialogues in <a href=https://en.wikipedia.org/wiki/Automotive_navigation_system>in-car navigation</a>, appointment scheduling, and weather information domains. We show that our few-shot approach achieves state-of-the art results on that dataset by consistently outperforming the previous best model in terms of <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and Entity F1 scores, while being more data-efficient than it by not requiring any data annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5905/>SIM : A Slot-Independent Neural Model for Dialogue State Tracking<span class=acl-fixed-case>SIM</span>: A Slot-Independent Neural Model for Dialogue State Tracking</a></strong><br><a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/m/michael-zeng/>Michael Zeng</a>
|
<a href=/people/x/xuedong-huang/>Xuedong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5905><div class="card-body p-3 small">Dialogue state tracking is an important component in task-oriented dialogue systems to identify users&#8217; goals and requests as a dialogue proceeds. However, as most previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are dependent on dialogue slots, the model complexity soars when the number of slots increases. In this paper, we put forward a slot-independent neural model (SIM) to track dialogue states while keeping the model complexity invariant to the number of dialogue slots. The <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a> utilizes <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> between user utterance and <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>system actions</a>. SIM achieves state-of-the-art results on WoZ and DSTC2 tasks, with only 20 % of the model size of previous models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5907/>Time Masking : Leveraging Temporal Information in Spoken Dialogue Systems</a></strong><br><a href=/people/r/rylan-conway/>Rylan Conway</a>
|
<a href=/people/m/mathias-lambert/>Mathias Lambert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5907><div class="card-body p-3 small">In a spoken dialogue system, dialogue state tracker (DST) components track the state of the conversation by updating a distribution of values associated with each of the slots being tracked for the current user turn, using the interactions until then. Much of the previous work has relied on modeling the natural order of the conversation, using distance based offsets as an approximation of time. In this work, we hypothesize that leveraging the wall-clock temporal difference between turns is crucial for finer-grained control of dialogue scenarios. We develop a novel approach that applies a time mask, based on the wall-clock time difference, to the associated slot embeddings and empirically demonstrate that our proposed approach outperforms existing approaches that leverage distance offsets, on both an internal benchmark dataset as well as DSTC2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5908/>To Combine or Not To Combine? A Rainbow Deep Reinforcement Learning Agent for Dialog Policies</a></strong><br><a href=/people/d/dirk-vath/>Dirk Väth</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5908><div class="card-body p-3 small">In this paper, we explore state-of-the-art deep reinforcement learning methods for dialog policy training such as prioritized experience replay, double deep Q-Networks, dueling network architectures and distributional learning. Our main findings show that each individual method improves the rewards and the task success rate but combining these methods in a Rainbow agent, which performs best across tasks and environments, is a non-trivial task. We, therefore, provide insights about the influence of each method on the combination and how to combine them to form a Rainbow agent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5912" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5912/>Collaborative Multi-Agent Dialogue Model Training Via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>
|
<a href=/people/y/yi-chia-wang/>Yi-Chia Wang</a>
|
<a href=/people/p/piero-molino/>Piero Molino</a>
|
<a href=/people/g/gokhan-tur/>Gokhan Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5912><div class="card-body p-3 small">Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent&#8217;s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training pipeline</a> and model the <a href=https://en.wikipedia.org/wiki/Conversation>conversation</a> as a stochastic collaborative game. Each agent (player) has a role (assistant, tourist, eater, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> (its own LU and LG, the other agent&#8217;s LU, <a href=https://en.wikipedia.org/wiki/Policy>Policy</a>, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5913/>Scoring Interactional Aspects of Human-Machine Dialog for Language Learning and Assessment using Text Features</a></strong><br><a href=/people/v/vikram-ramanarayanan/>Vikram Ramanarayanan</a>
|
<a href=/people/m/matthew-mulholland/>Matthew Mulholland</a>
|
<a href=/people/y/yao-qian/>Yao Qian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5913><div class="card-body p-3 small">While there has been much work in the language learning and assessment literature on human and automated scoring of essays and short constructed responses, there is little to no work examining text features for scoring of dialog data, particularly interactional aspects thereof, to assess conversational proficiency over and above constructed response skills. Our work bridges this gap by investigating both human and automated approaches towards scoring humanmachine text dialog in the context of a real-world language learning application. We collected conversational data of human learners interacting with a cloud-based standards-compliant dialog system, triple-scored these <a href=https://en.wikipedia.org/wiki/Data>data</a> along multiple dimensions of conversational proficiency, and then analyzed the performance trends. We further examined two different approaches to automated scoring of such data and show that these approaches are able to perform at or above par with human agreement for a majority of dimensions of the scoring rubric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5914/>Spoken Conversational Search for General Knowledge</a></strong><br><a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas Barahona</a>
|
<a href=/people/p/pascal-bellec/>Pascal Bellec</a>
|
<a href=/people/b/benoit-besset/>Benoit Besset</a>
|
<a href=/people/m/martinho-dossantos/>Martinho Dossantos</a>
|
<a href=/people/j/johannes-heinecke/>Johannes Heinecke</a>
|
<a href=/people/m/munshi-asadullah/>Munshi Asadullah</a>
|
<a href=/people/o/olivier-leblouch/>Olivier Leblouch</a>
|
<a href=/people/j/jeanyves-lancien/>Jeanyves. Lancien</a>
|
<a href=/people/g/geraldine-damnati/>Geraldine Damnati</a>
|
<a href=/people/e/emmanuel-mory/>Emmanuel Mory</a>
|
<a href=/people/f/frederic-herledan/>Frederic Herledan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5914><div class="card-body p-3 small">We present a spoken conversational question answering proof of concept that is able to answer questions about <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a> from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a>. The dialogue agent does not only orchestrate various <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> but also solve <a href=https://en.wikipedia.org/wiki/Coreference>coreferences</a> and <a href=https://en.wikipedia.org/wiki/Ellipsis_(linguistics)>ellipsis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5915/>Graph2Bots, Unsupervised Assistance for Designing Chatbots<span class=acl-fixed-case>G</span>raph2<span class=acl-fixed-case>B</span>ots, Unsupervised Assistance for Designing Chatbots</a></strong><br><a href=/people/j/jean-leon-bouraoui/>Jean-Leon Bouraoui</a>
|
<a href=/people/s/sonia-le-meitour/>Sonia Le Meitour</a>
|
<a href=/people/r/romain-carbou/>Romain Carbou</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas Barahona</a>
|
<a href=/people/v/vincent-lemaire/>Vincent Lemaire</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5915><div class="card-body p-3 small">We present Graph2Bots, a tool for assisting conversational agent designers. It extracts a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph representation</a> from <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-human conversations</a> by using <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning</a>. The generated <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> contains the main stages of the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> and their inner transitions. The <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>graphical user interface (GUI)</a> then allows <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph editing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5916.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5916 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5916 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5916/>On a <a href=https://en.wikipedia.org/wiki/Chatbot>Chatbot</a> Conducting Dialogue-in-Dialogue</a></strong><br><a href=/people/b/boris-galitsky/>Boris Galitsky</a>
|
<a href=/people/d/dmitry-ilvovsky/>Dmitry Ilvovsky</a>
|
<a href=/people/e/elizaveta-goncharova/>Elizaveta Goncharova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5916><div class="card-body p-3 small">We demo a <a href=https://en.wikipedia.org/wiki/Chatbot>chatbot</a> that delivers content in the form of virtual dialogues automatically produced from plain texts extracted and selected from documents. This virtual dialogue content is provided in the form of answers derived from the found and selected documents split into fragments, and questions are automatically generated for these answers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5917/>DeepCopy : Grounded Response Generation with Hierarchical Pointer Networks<span class=acl-fixed-case>D</span>eep<span class=acl-fixed-case>C</span>opy: Grounded Response Generation with Hierarchical Pointer Networks</a></strong><br><a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a>
|
<a href=/people/g/guan-lin-chao/>Guan-Lin Chao</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5917><div class="card-body p-3 small">Recent advances in neural sequence-to-sequence models have led to promising results for several language generation-based tasks, including dialogue response generation, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. However, these models are known to have several problems, especially in the context of chit-chat based dialogue systems : they tend to generate short and dull responses that are often too generic. Furthermore, these models do not ground conversational responses on knowledge and facts, resulting in turns that are not accurate, informative and engaging for the users. In this paper, we propose and experiment with a series of response generation models that aim to serve in the general scenario where in addition to the dialogue context, relevant unstructured external knowledge in the form of text is also assumed to be available for <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to harness. Our proposed approach extends pointer-generator networks (See et al., 2017) by allowing the decoder to hierarchically attend and copy from external knowledge in addition to the dialogue context. We empirically show the effectiveness of the proposed model compared to several baselines including (Ghazvininejadet al., 2018 ; Zhang et al., 2018) through both automatic evaluation metrics and human evaluation on ConvAI2 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5918 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5918/>Towards End-to-End Learning for Efficient Dialogue Agent by Modeling Looking-ahead Ability</a></strong><br><a href=/people/z/zhuoxuan-jiang/>Zhuoxuan Jiang</a>
|
<a href=/people/x/xian-ling-mao/>Xian-Ling Mao</a>
|
<a href=/people/z/ziming-huang/>Ziming Huang</a>
|
<a href=/people/j/jie-ma/>Jie Ma</a>
|
<a href=/people/s/shaochun-li/>Shaochun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5918><div class="card-body p-3 small">Learning an efficient manager of dialogue agent from data with little manual intervention is important, especially for goal-oriented dialogues. However, existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> either take too many manual efforts (e.g. reinforcement learning methods) or can not guarantee the dialogue efficiency (e.g. sequence-to-sequence methods). In this paper, we address this problem by proposing a novel end-to-end learning model to train a <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue agent</a> that can look ahead for several future turns and generate an optimal response to make the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> efficient. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is data-driven and does not require too much manual work for intervention during <a href=https://en.wikipedia.org/wiki/Systems_design>system design</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on two datasets of different scenarios and the experimental results demonstrate the efficiency of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5919.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5919 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5919 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5919/>Unsupervised Dialogue Spectrum Generation for Log Dialogue Ranking</a></strong><br><a href=/people/x/xinnuo-xu/>Xinnuo Xu</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/l/lars-liden/>Lars Liden</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5919><div class="card-body p-3 small">Although the data-driven approaches of some recent bot building platforms make it possible for a wide range of users to easily create dialogue systems, those <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> do n&#8217;t offer tools for quickly identifying which log dialogues contain problems. This is important since corrections to log dialogues provide a means to improve performance after deployment. A log dialogue ranker, which ranks problematic dialogues higher, is an essential tool due to the sheer volume of log dialogues that could be generated. However, training a <a href=https://en.wikipedia.org/wiki/Ranker>ranker</a> typically requires labelling a substantial amount of data, which is not feasible for most users. In this paper, we present a novel <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> for dialogue ranking using GANs and release a corpus of labelled dialogues for evaluation and comparison with <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a>. The evaluation result shows that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> compares favorably to <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a> without any <a href=https://en.wikipedia.org/wiki/Data>labelled data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5920.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5920 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5920 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5920/>Tree-Structured Semantic Encoder with Knowledge Sharing for Domain Adaptation in Natural Language Generation</a></strong><br><a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/y/yen-chen-wu/>Yen-chen Wu</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5920><div class="card-body p-3 small">Domain adaptation in natural language generation (NLG) remains challenging because of the high complexity of input semantics across domains and limited data of a target domain. This is particularly the case for dialogue systems, where we want to be able to seamlessly include new domains into the conversation. Therefore, it is crucial for generation models to share knowledge across domains for the effective <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> from one domain to another. In this study, we exploit a tree-structured semantic encoder to capture the internal structure of complex semantic representations required for multi-domain dialogues in order to facilitate knowledge sharing across domains. In addition, a layer-wise attention mechanism between the <a href=https://en.wikipedia.org/wiki/Tree_traversal>tree encoder</a> and the <a href=https://en.wikipedia.org/wiki/Tree_traversal>decoder</a> is adopted to further improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s capability. The automatic evaluation results show that our model outperforms previous methods in terms of the BLEU score and the slot error rate, in particular when the <a href=https://en.wikipedia.org/wiki/Adaptation_data>adaptation data</a> is limited. In subjective evaluation, human judges tend to prefer the sentences generated by our model, rating them more highly on informativeness and <a href=https://en.wikipedia.org/wiki/Naturalness_(philosophy)>naturalness</a> than other <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5922.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5922 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5922 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5922" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5922/>Flexibly-Structured Model for Task-Oriented Dialogues</a></strong><br><a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/p/piero-molino/>Piero Molino</a>
|
<a href=/people/m/mahdi-namazifar/>Mahdi Namazifar</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/h/huaixiu-zheng/>Huaixiu Zheng</a>
|
<a href=/people/g/gokhan-tur/>Gokhan Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5922><div class="card-body p-3 small">This paper proposes a novel end-to-end architecture for task-oriented dialogue systems. It is based on a simple and practical yet very effective sequence-to-sequence approach, where <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and state tracking tasks are modeled jointly with a structured copy-augmented sequential decoder and a multi-label decoder for each slot. The policy engine and language generation tasks are modeled jointly following that. The copy-augmented sequential decoder deals with new or unknown values in the conversation, while the multi-label decoder combined with the <a href=https://en.wikipedia.org/wiki/Sequential_decoder>sequential decoder</a> ensures the explicit assignment of values to slots. On the generation part, slot binary classifiers are used to improve performance. This architecture is scalable to real-world scenarios and is shown through an empirical evaluation to achieve state-of-the-art performance on both the Cambridge Restaurant dataset and the Stanford in-car assistant dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5923.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5923 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5923 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5923/>FriendsQA : Open-Domain Question Answering on TV Show Transcripts<span class=acl-fixed-case>F</span>riends<span class=acl-fixed-case>QA</span>: Open-Domain Question Answering on <span class=acl-fixed-case>TV</span> Show Transcripts</a></strong><br><a href=/people/z/zhengzhe-yang/>Zhengzhe Yang</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5923><div class="card-body p-3 small">This paper presents FriendsQA, a challenging question answering dataset that contains 1,222 dialogues and 10,610 open-domain questions, to tackle machine comprehension on everyday conversations. Each dialogue, involving multiple speakers, is annotated with several types of questions regarding the dialogue contexts, and the answers are annotated with certain spans in the dialogue. A series of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing tasks</a> are conducted to ensure good annotation quality, resulting a high <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a> of 81.82 %. A comprehensive annotation analytics is provided for a deeper understanding in this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Three state-of-the-art QA systems are experimented, R-Net, QANet, and BERT, and evaluated on this dataset. BERT in particular depicts promising results, an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 74.2 % for answer utterance selection and an F1-score of 64.2 % for answer span selection, suggesting that the FriendsQA task is hard yet has a great potential of elevating QA research on multiparty dialogue to another level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5928.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5928 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5928 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5928/>A Quantitative Analysis of Patients’ Narratives of Heart Failure</a></strong><br><a href=/people/s/sabita-acharya/>Sabita Acharya</a>
|
<a href=/people/b/barbara-di-eugenio/>Barbara Di Eugenio</a>
|
<a href=/people/a/andrew-boyd/>Andrew Boyd</a>
|
<a href=/people/r/richard-cameron/>Richard Cameron</a>
|
<a href=/people/k/karen-dunn-lopez/>Karen Dunn Lopez</a>
|
<a href=/people/p/pamela-martyn-nemeth/>Pamela Martyn-Nemeth</a>
|
<a href=/people/d/debaleena-chattopadhyay/>Debaleena Chattopadhyay</a>
|
<a href=/people/p/pantea-habibi/>Pantea Habibi</a>
|
<a href=/people/c/carolyn-dickens/>Carolyn Dickens</a>
|
<a href=/people/h/haleh-vatani/>Haleh Vatani</a>
|
<a href=/people/a/amer-ardati/>Amer Ardati</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5928><div class="card-body p-3 small">Patients with <a href=https://en.wikipedia.org/wiki/Chronic_condition>chronic conditions</a> like <a href=https://en.wikipedia.org/wiki/Heart_failure>heart failure</a> are the most likely to be re-hospitalized. One step towards avoiding re-hospitalization is to devise strategies for motivating patients to take care of their own health. In this paper, we perform a quantitative analysis of patients&#8217; narratives of their experience with heart failure and explore the different topics that patients talk about. We compare two different groups of patients- those unable to take charge of their illness, and those who make efforts to improve their health. We will use the findings from our analysis to refine and personalize the summaries of hospitalizations that our <a href=https://en.wikipedia.org/wiki/System>system</a> automatically generates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5930.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5930 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5930 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5930/>Real Life Application of a <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering System</a> Using BERT Language Model<span class=acl-fixed-case>BERT</span> Language Model</a></strong><br><a href=/people/f/francesca-alloatti/>Francesca Alloatti</a>
|
<a href=/people/l/luigi-di-caro/>Luigi Di Caro</a>
|
<a href=/people/g/gianpiero-sportelli/>Gianpiero Sportelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5930><div class="card-body p-3 small">It is often hard to apply the newest advances in research to real life scenarios. They usually require the resolution of some specific <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> applied to a restricted domain, all the while providing small amounts of data to begin with. In this study we apply one of the newest innovations in <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> to a task of <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. We created a <a href=https://en.wikipedia.org/wiki/Question_answering>question answering system</a> in <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> that provides information about a specific subject, <a href=https://en.wikipedia.org/wiki/Electronic_invoicing>e-invoicing</a> and digital billing. Italy recently introduced a new legislation about <a href=https://en.wikipedia.org/wiki/Electronic_invoicing>e-invoicing</a> and people have some legit doubts, therefore a large share of professionals could benefit from this tool.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5932.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5932 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5932 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5932/>Dialog State Tracking : A Neural Reading Comprehension Approach</a></strong><br><a href=/people/s/shuyang-gao/>Shuyang Gao</a>
|
<a href=/people/a/abhishek-sethi/>Abhishek Sethi</a>
|
<a href=/people/s/sanchit-agarwal/>Sanchit Agarwal</a>
|
<a href=/people/t/tagyoung-chung/>Tagyoung Chung</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5932><div class="card-body p-3 small">Dialog state tracking is used to estimate the current belief state of a dialog given all the preceding conversation. Machine reading comprehension, on the other hand, focuses on building systems that read passages of text and answer questions that require some understanding of passages. We formulate dialog state tracking as a reading comprehension task to answer the question what is the state of the current dialog? after reading <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversational context</a>. In contrast to traditional state tracking methods where the dialog state is often predicted as a distribution over a closed set of all the possible slot values within an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a>, our method uses a simple attention-based neural network to point to the slot values within the conversation. Experiments on MultiWOZ-2.0 cross-domain dialog dataset show that our simple <a href=https://en.wikipedia.org/wiki/System>system</a> can obtain similar accuracies compared to the previous more complex methods. By exploiting recent advances in contextual word embeddings, adding a model that explicitly tracks whether a slot value should be carried over to the next turn, and combining our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> with a traditional joint state tracking method that relies on closed set vocabulary, we can obtain a joint-goal accuracy of 47.33 % on the standard test split, exceeding current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by 11.75 % * *.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5933.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5933 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5933 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5933/>Cross-Corpus Data Augmentation for Acoustic Addressee Detection</a></strong><br><a href=/people/o/oleg-akhtiamov/>Oleg Akhtiamov</a>
|
<a href=/people/i/ingo-siegert/>Ingo Siegert</a>
|
<a href=/people/a/alexey-karpov/>Alexey Karpov</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5933><div class="card-body p-3 small">Acoustic addressee detection (AD) is a modern paralinguistic and dialogue challenge that especially arises in <a href=https://en.wikipedia.org/wiki/Voice_assistant>voice assistants</a>. In the present study, we distinguish addressees in two settings (a conversation between several people and a spoken dialogue system, and a conversation between several adults and a child) and introduce the first competitive baseline (unweighted average recall equals 0.891) for the Voice Assistant Conversation Corpus that models the first setting. We jointly solve both classification problems, using three models : a linear support vector machine dealing with acoustic functionals and two <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> utilising raw waveforms alongside with acoustic low-level descriptors. We investigate how different corpora influence each other, applying the mixup approach to <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. We also study the influence of various acoustic context lengths on <a href=https://en.wikipedia.org/wiki/Anno_Domini>AD</a>. Two-second speech fragments turn out to be sufficient for reliable <a href=https://en.wikipedia.org/wiki/Anno_Domini>AD</a>. Mixup is shown to be beneficial for merging acoustic data (extracted features but not raw waveforms) from different domains that allows us to reach a higher classification performance on human-machine AD and also for training a multipurpose neural network that is capable of solving both human-machine and adult-child AD problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5935.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5935 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5935 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5935/>A Large-Scale User Study of an Alexa Prize Chatbot : Effect of TTS Dynamism on Perceived Quality of Social Dialog<span class=acl-fixed-case>A</span>lexa <span class=acl-fixed-case>P</span>rize Chatbot: Effect of <span class=acl-fixed-case>TTS</span> Dynamism on Perceived Quality of Social Dialog</a></strong><br><a href=/people/m/michelle-cohn/>Michelle Cohn</a>
|
<a href=/people/c/chun-yen-chen/>Chun-Yen Chen</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5935><div class="card-body p-3 small">This study tests the effect of cognitive-emotional expression in an Alexa text-to-speech (TTS) voice on users&#8217; experience with a social dialog system. We systematically introduced emotionally expressive interjections (e.g., Wow !) and <a href=https://en.wikipedia.org/wiki/Filler_(media)>filler words</a> (e.g., um, mhmm) in an Amazon Alexa Prize socialbot, Gunrock. We tested whether these TTS manipulations improved users&#8217; ratings of their conversation across thousands of real user interactions (n=5,527). Results showed that <a href=https://en.wikipedia.org/wiki/Interjection>interjections</a> and <a href=https://en.wikipedia.org/wiki/Filler_(media)>fillers</a> each improved users&#8217; holistic ratings, an improvement that further increased if the <a href=https://en.wikipedia.org/wiki/System>system</a> used both manipulations. A separate perception experiment corroborated the findings from the user study, with improved social ratings for conversations including interjections ; however, no positive effect was observed for fillers, suggesting that the role of the rater in the conversationas active participant or external listeneris an important factor in assessing social dialogs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5936.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5936 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5936 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5936/>Influence of Time and Risk on Response Acceptability in a Simple Spoken Dialogue System</a></strong><br><a href=/people/a/andisheh-partovi/>Andisheh Partovi</a>
|
<a href=/people/i/ingrid-zukerman/>Ingrid Zukerman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5936><div class="card-body p-3 small">We describe a <a href=https://en.wikipedia.org/wiki/Longitudinal_study>longitudinal user study</a> conducted in the context of a Spoken Dialogue System for a <a href=https://en.wikipedia.org/wiki/Home_robot>household robot</a>, where we examined the influence of <a href=https://en.wikipedia.org/wiki/Time_displacement>time displacement</a> and situational risk on users&#8217; preferred responses. To this effect, we employed a corpus of spoken requests that asked a robot to fetch or move objects in a room. In the first stage of our study, participants selected among four response types to these requests under two risk conditions : low and high. After some time, the same participants rated several responses to the previous requests these responses were instantiated from the four response types. Our results show that participants did not rate highly their own response types ; moreover, they rated their own response types similarly to different ones. This suggests that, at least in this context, people&#8217;s preferences at a particular point in time may not reflect their general attitudes, and that various reasonable response types may be equally acceptable. Our study also reveals that situational risk influences the acceptability of some response types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5937.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5937 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5937 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5937/>Characterizing the Response Space of Questions : a Corpus Study for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Polish<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>P</span>olish</a></strong><br><a href=/people/j/jonathan-ginzburg/>Jonathan Ginzburg</a>
|
<a href=/people/z/zulipiye-yusupujiang/>Zulipiye Yusupujiang</a>
|
<a href=/people/c/chuyuan-li/>Chuyuan Li</a>
|
<a href=/people/k/kexin-ren/>Kexin Ren</a>
|
<a href=/people/p/pawel-lupkowski/>Paweł Łupkowski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5937><div class="card-body p-3 small">The main aim of this paper is to provide a characterization of the response space for questions using a <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a> grounded in a dialogical formal semantics. As a starting point we take the <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typology</a> for responses in the form of questions provided in (Lupkowski and Ginzburg, 2016). This work develops a wide coverage taxonomy for question / question sequences observable in corpora including the BNC, <a href=https://en.wikipedia.org/wiki/CHILDES>CHILDES</a>, and BEE, as well as formal modelling of all the postulated classes. Our aim is to extend this work to cover all responses to questions. We present the extended typology of responses to questions based on a corpus studies of BNC, BEE and Maptask with include 506, 262, and 467 question / response pairs respectively. We compare the data for <a href=https://en.wikipedia.org/wiki/English_language>English</a> with data from <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> using the Spokes corpus (205 question / response pairs). We discuss annotation reliability and <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>disagreement analysis</a>. We sketch how each class can be formalized using a dialogical semantics appropriate for <a href=https://en.wikipedia.org/wiki/Dialogue_management>dialogue management</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5939.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5939 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5939 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5939/>Prediction of User Emotion and Dialogue Success Using Audio Spectrograms and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/a/athanasios-lykartsis/>Athanasios Lykartsis</a>
|
<a href=/people/m/margarita-kotti/>Margarita Kotti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5939><div class="card-body p-3 small">In this paper we aim to predict dialogue success and user satisfaction as well as <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> on a turn level. To achieve this, we investigate the use of <a href=https://en.wikipedia.org/wiki/Spectrogram>spectrogram representations</a>, extracted from <a href=https://en.wikipedia.org/wiki/Audio_file_format>audio files</a>, in combination with several types of <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. The experiments were performed on the Let&#8217;s Go V2 database, comprising 5065 audio files and having labels for subjective and objective dialogue turn success, as well as the emotional state of the user. Results show that by using only <a href=https://en.wikipedia.org/wiki/Sound>audio</a>, it is possible to predict turn success with very high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for all three labels (90 %). The best performing input representation were 1s long mel-spectrograms in combination with a CNN with a bottleneck architecture. The resulting <a href=https://en.wikipedia.org/wiki/System>system</a> has the potential to be used real-time. Our results significantly surpass the state of the art for dialogue success prediction based only on <a href=https://en.wikipedia.org/wiki/Sound>audio</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5941.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5941 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5941 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5941/>Coached Conversational Preference Elicitation : A Case Study in Understanding Movie Preferences</a></strong><br><a href=/people/f/filip-radlinski/>Filip Radlinski</a>
|
<a href=/people/k/krisztian-balog/>Krisztian Balog</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a>
|
<a href=/people/k/karthik-krishnamoorthi/>Karthik Krishnamoorthi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5941><div class="card-body p-3 small">Conversational recommendation has recently attracted significant attention. As systems must understand users&#8217; preferences, training them has called for conversational corpora, typically derived from task-oriented conversations. We observe that such <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> often do not reflect how people naturally describe preferences. We present a new approach to obtaining user preferences in dialogue : Coached Conversational Preference Elicitation. It allows collection of natural yet structured conversational preferences. Studying the dialogues in one domain, we present a brief quantitative analysis of how people describe movie preferences at scale. Demonstrating the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a>, we release the CCPE-M dataset to the community with over 500 movie preference dialogues expressing over 10,000 preferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5944.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5944 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5944 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5944" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5944/>Investigating Evaluation of Open-Domain Dialogue Systems With Human Generated Multiple References</a></strong><br><a href=/people/p/prakhar-gupta/>Prakhar Gupta</a>
|
<a href=/people/s/shikib-mehri/>Shikib Mehri</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/a/amy-pavel/>Amy Pavel</a>
|
<a href=/people/m/maxine-eskenazi/>Maxine Eskenazi</a>
|
<a href=/people/j/jeffrey-p-bigham/>Jeffrey Bigham</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5944><div class="card-body p-3 small">The aim of this paper is to mitigate the shortcomings of automatic evaluation of open-domain dialog systems through multi-reference evaluation. Existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> have been shown to correlate poorly with <a href=https://en.wikipedia.org/wiki/Judgement>human judgement</a>, particularly in open-domain dialog. One alternative is to collect <a href=https://en.wikipedia.org/wiki/Annotation>human annotations</a> for <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>, which can be expensive and time consuming. To demonstrate the effectiveness of multi-reference evaluation, we augment the test set of DailyDialog with multiple references. A series of experiments show that the use of multiple references results in improved correlation between several automatic metrics and human judgement for both the <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> and the diversity of system output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5950.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5950 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5950 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5950/>Which aspects of <a href=https://en.wikipedia.org/wiki/Discourse_relation>discourse relations</a> are hard to learn? Primitive decomposition for discourse relation classification</a></strong><br><a href=/people/c/charlotte-roze/>Charlotte Roze</a>
|
<a href=/people/c/chloe-braud/>Chloé Braud</a>
|
<a href=/people/p/philippe-muller/>Philippe Muller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5950><div class="card-body p-3 small">Discourse relation classification has proven to be a hard task, with rather low performance on several corpora that notably differ on the relation set they use. We propose to decompose the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> into smaller, mostly binary tasks corresponding to various primitive concepts encoded into the discourse relation definitions. More precisely, we translate the discourse relations into a set of values for attributes based on distinctions used in the mappings between discourse frameworks proposed by Sanders et al. This arguably allows for a more robust representation of <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>, and enables us to address usually ignored aspects of discourse relation prediction, namely multiple labels and underspecified annotations. We show experimentally which of the conceptual primitives are harder to learn from the Penn Discourse Treebank English corpus, and propose a correspondence to predict the original labels, with preliminary empirical comparisons with a direct model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5951.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5951 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5951 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5951/>Discourse Relation Prediction : Revisiting Word Pairs with Convolutional Networks</a></strong><br><a href=/people/s/siddharth-varia/>Siddharth Varia</a>
|
<a href=/people/c/christopher-hidey/>Christopher Hidey</a>
|
<a href=/people/t/tuhin-chakrabarty/>Tuhin Chakrabarty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5951><div class="card-body p-3 small">Word pairs across argument spans have been shown to be effective for predicting the <a href=https://en.wikipedia.org/wiki/Discourse_relation>discourse relation</a> between them. We propose an approach to distill knowledge from word pairs for discourse relation classification with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> by incorporating joint learning of implicit and explicit relations. Our novel approach of representing the input as word pairs achieves state-of-the-art results on four-way classification of both implicit and explicit relations as well as one of the binary classification tasks. For explicit relation prediction, we achieve around 20 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> on the four-way task. At the same time, compared to a two-layered Bi-LSTM-CRF model, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to achieve these results with half the number of learnable parameters and approximately half the amount of training time.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>