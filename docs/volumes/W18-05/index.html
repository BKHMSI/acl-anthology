<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W18-05.pdf>Proceedings of the Thirteenth Workshop on Innovative Use of <span class=acl-fixed-case>NLP</span> for Building Educational Applications</a></h2><p class=lead><a href=/people/j/joel-tetreault/>Joel Tetreault</a>,
<a href=/people/j/jill-burstein/>Jill Burstein</a>,
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>,
<a href=/people/c/claudia-leacock/>Claudia Leacock</a>,
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W18-05</dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>New Orleans, Louisiana</dd><dt>Venues:</dt><dd><a href=/venues/bea/>BEA</a>
| <a href=/venues/naacl/>NAACL</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigedu/>SIGEDU</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W18-05>https://aclanthology.org/W18-05</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/W18-05 title="To the current version of the paper by DOI">10.18653/v1/W18-05</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W18-05.pdf>https://aclanthology.org/W18-05.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W18-05.pdf title="Open PDF of 'Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Thirteenth+Workshop+on+Innovative+Use+of+NLP+for+Building+Educational+Applications" title="Search for 'Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0500/>Proceedings of the Thirteenth Workshop on Innovative Use of <span class=acl-fixed-case>NLP</span> for Building Educational Applications</a></strong><br><a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/j/jill-burstein/>Jill Burstein</a>
|
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>
|
<a href=/people/c/claudia-leacock/>Claudia Leacock</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0503/>Predicting misreadings from gaze in children with reading difficulties</a></strong><br><a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/m/maria-barrett/>Maria Barrett</a>
|
<a href=/people/s/sigrid-klerke/>Sigrid Klerke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0503><div class="card-body p-3 small">We present the first work on predicting reading mistakes in children with <a href=https://en.wikipedia.org/wiki/Reading_disability>reading difficulties</a> based on eye-tracking data from real-world reading teaching. Our approach employs several linguistic and gaze-based features to inform an ensemble of different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>, including multi-task learning models that let us transfer knowledge about individual readers to attain better predictions. Notably, the <a href=https://en.wikipedia.org/wiki/Data>data</a> we use in this work stems from noisy readings in the wild, outside of controlled lab conditions. Our experiments show that despite the <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and despite the small fraction of misreadings, gaze data improves the performance more than any other feature group and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve good performance. We further show that gaze patterns for misread words do not fully generalize across readers, but that we can transfer some knowledge between readers using <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> at least in some cases. Applications of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> include partial automation of reading assessment as well as personalized text simplification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0504/>Automatic Input Enrichment for Selecting Reading Material : An Online Study with English Teachers<span class=acl-fixed-case>E</span>nglish Teachers</a></strong><br><a href=/people/m/maria-chinkina/>Maria Chinkina</a>
|
<a href=/people/a/ankita-oswal/>Ankita Oswal</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0504><div class="card-body p-3 small">Input material at the appropriate level is crucial for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a>. Automating the search for such material can systematically and efficiently support teachers in their pedagogical practice. This is the goal of the computational linguistic task of automatic input enrichment (Chinkina & Meurers, 2016): It analyzes and re-ranks a collection of texts in order to prioritize those containing target linguistic forms. In the online study described in the paper, we collected 240 responses from English teachers in order to investigate whether they preferred automatic input enrichment over <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search</a> when selecting reading material for class. Participants demonstrated a general preference for the material provided by an automatic input enrichment system. It was also rated significantly higher than the texts retrieved by a standard <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search engine</a> with regard to the representation of linguistic forms and equivalent with regard to the relevance of the content to the topic. We discuss the implications of the results for <a href=https://en.wikipedia.org/wiki/Language_education>language teaching</a> and consider the potential strands of future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0506/>Second Language Acquisition Modeling</a></strong><br><a href=/people/b/burr-settles/>Burr Settles</a>
|
<a href=/people/c/chris-brust/>Chris Brust</a>
|
<a href=/people/e/erin-gustafson/>Erin Gustafson</a>
|
<a href=/people/m/masato-hagiwara/>Masato Hagiwara</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0506><div class="card-body p-3 small">We present the task of second language acquisition (SLA) modeling. Given a history of errors made by learners of a <a href=https://en.wikipedia.org/wiki/Second_language>second language</a>, the task is to predict errors that they are likely to make at arbitrary points in the future. We describe a large corpus of more than 7 M words produced by more than 6k learners of <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and <a href=https://en.wikipedia.org/wiki/French_language>French</a> using <a href=https://en.wikipedia.org/wiki/Duolingo>Duolingo</a>, a popular online language-learning app. Then we report on the results of a shared task challenge aimed studying the SLA task via this corpus, which attracted 15 teams and synthesized work from various fields including <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>, <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>, and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>.<i>second language acquisition (SLA) modeling</i>. Given a history of errors made by learners of a\n second language, the task is to predict errors that they are\n likely to make at arbitrary points in the future. We describe a\n large corpus of more than 7M words produced by more than 6k\n learners of English, Spanish, and French using Duolingo, a\n popular online language-learning app. Then we report on the\n results of a shared task challenge aimed studying the SLA task\n via this corpus, which attracted 15 teams and synthesized work\n from various fields including cognitive science, linguistics,\n and machine learning.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0507/>A Report on the Complex Word Identification Shared Task 2018</a></strong><br><a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/a/anais-tack/>Anaïs Tack</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0507><div class="card-body p-3 small">We report the findings of the second Complex Word Identification (CWI) shared task organized as part of the BEA workshop co-located with NAACL-HLT&#8217;2018. The second CWI shared task featured multilingual and multi-genre datasets divided into four tracks : English monolingual, German monolingual, Spanish monolingual, and a multilingual track with a French test set, and two tasks : <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a> and probabilistic classification. A total of 12 teams submitted their results in different task / track combinations and 11 of them wrote system description papers that are referred to in this report and appear in the BEA workshop proceedings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0508/>Towards Single Word Lexical Complexity Prediction</a></strong><br><a href=/people/d/david-alfter/>David Alfter</a>
|
<a href=/people/e/elena-volodina/>Elena Volodina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0508><div class="card-body p-3 small">In this paper we present work-in-progress where we investigate the usefulness of previously created word lists to the task of single-word lexical complexity analysis and prediction of the complexity level for learners of <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> as a second language. The word lists used map each word to a single CEFR level, and the task consists of predicting CEFR levels for unseen words. In contrast to previous work on word-level lexical complexity, we experiment with topics as additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and show that linking words to topics significantly increases <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0511 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0511/>Annotating Student Talk in Text-based Classroom Discussions</a></strong><br><a href=/people/l/luca-lugini/>Luca Lugini</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a>
|
<a href=/people/a/amanda-godley/>Amanda Godley</a>
|
<a href=/people/c/christopher-olshefski/>Christopher Olshefski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0511><div class="card-body p-3 small">Classroom discussions in English Language Arts have a positive effect on students&#8217; reading, writing and reasoning skills. Although prior work has largely focused on teacher talk and student-teacher interactions, we focus on three theoretically-motivated aspects of high-quality student talk : <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a>, <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>, and <a href=https://en.wikipedia.org/wiki/Knowledge_domain>knowledge domain</a>. We introduce an annotation scheme, then show that the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a> can be used to produce reliable <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> and that the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> are predictive of discussion quality. We also highlight opportunities provided by our scheme for education and natural language processing research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0512/>Toward Automatically Measuring Learner Ability from Human-Machine Dialog Interactions using Novel Psychometric Models</a></strong><br><a href=/people/v/vikram-ramanarayanan/>Vikram Ramanarayanan</a>
|
<a href=/people/m/michelle-lamar/>Michelle LaMar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0512><div class="card-body p-3 small">While dialog systems have been widely deployed for computer-assisted language learning (CALL) and formative assessment systems in recent years, relatively limited work has been done with respect to the psychometrics and validity of these technologies in evaluating and providing feedback regarding student learning and conversational ability. This paper formulates a Markov decision process based measurement model, and applies it to text chat data collected from crowdsourced native and non-native English language speakers interacting with an automated dialog agent. We investigate how well the model measures speaker conversational ability, and find that it effectively captures the differences in how native and non-native speakers of English accomplish the dialog task. Such models could have important implications for CALL systems of the future that effectively combine dialog management with measurement of learner conversational ability in real-time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0513/>Generating Feedback for English Foreign Language Exercises<span class=acl-fixed-case>E</span>nglish Foreign Language Exercises</a></strong><br><a href=/people/b/bjorn-rudzewitz/>Björn Rudzewitz</a>
|
<a href=/people/r/ramon-ziai/>Ramon Ziai</a>
|
<a href=/people/k/kordula-de-kuthy/>Kordula De Kuthy</a>
|
<a href=/people/v/verena-moller/>Verena Möller</a>
|
<a href=/people/f/florian-nuxoll/>Florian Nuxoll</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0513><div class="card-body p-3 small">While immediate feedback on learner language is often discussed in the Second Language Acquisition literature (e.g., Mackey 2006), few systems used in real-life educational settings provide helpful, metalinguistic feedback to learners. In this paper, we present a novel approach leveraging task information to generate the expected range of well-formed and ill-formed variability in learner answers along with the required diagnosis and <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a>. We combine this offline generation approach with an online component that matches the actual student answers against the pre-computed hypotheses. The results obtained for a set of 33 thousand answers of 7th grade German high school students learning English show that the approach successfully covers frequent answer patterns. At the same time, paraphrases and content errors require a more flexible alignment approach, for which we are planning to complement the method with the CoMiC approach successfully used for the analysis of reading comprehension answers (Meurers et al., 2011).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0523/>Grotoco@SLAM : Second Language Acquisition Modeling with Simple Features, Learners and Task-wise Models<span class=acl-fixed-case>SLAM</span>: Second Language Acquisition Modeling with Simple Features, Learners and Task-wise Models</a></strong><br><a href=/people/s/sigrid-klerke/>Sigrid Klerke</a>
|
<a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0523><div class="card-body p-3 small">We present our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We focus on evaluating a range of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for the task, including user-derived measures, while examining how far we can get with a simple <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a>. Our analysis reveals that errors differ per exercise format, which motivates our final and best-performing system : a task-wise (per exercise-format) model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0524 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-0524" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-0524/>Context Based Approach for Second Language Acquisition</a></strong><br><a href=/people/n/nihal-v-nayak/>Nihal V. Nayak</a>
|
<a href=/people/a/arjun-r-rao/>Arjun R. Rao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0524><div class="card-body p-3 small">SLAM 2018 focuses on predicting a student&#8217;s mistake while using the Duolingo application. In this paper, we describe the <a href=https://en.wikipedia.org/wiki/System>system</a> we developed for this shared <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our system uses a <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression model</a> to predict the likelihood of a student making a mistake while answering an exercise on <a href=https://en.wikipedia.org/wiki/Duolingo>Duolingo</a> in all three language tracks-English / Spanish (en / es), Spanish / English (es / en) and French / English (fr / en). We conduct an ablation study with several <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> during the development of this system and discover that context based features plays a major role in language acquisition modeling. Our model beats Duolingo&#8217;s baseline scores in all three language tracks (AUROC scores for en / es = 0.821, es / en = 0.790 and fr / en = 0.812). Our work makes a case for providing <a href=https://en.wikipedia.org/wiki/Context_(language_use)>favourable textual context</a> for students while learning second language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0525 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0525/>Second Language Acquisition Modeling : An Ensemble Approach</a></strong><br><a href=/people/a/anton-osika/>Anton Osika</a>
|
<a href=/people/s/susanna-nilsson/>Susanna Nilsson</a>
|
<a href=/people/a/andrii-sydorchuk/>Andrii Sydorchuk</a>
|
<a href=/people/f/faruk-sahin/>Faruk Sahin</a>
|
<a href=/people/a/anders-huss/>Anders Huss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0525><div class="card-body p-3 small">Accurate prediction of students&#8217; knowledge is a fundamental building block of <a href=https://en.wikipedia.org/wiki/Personalized_learning>personalized learning systems</a>. Here, we propose an ensemble model to predict student knowledge gaps. Applying our approach to student trace data from the online educational platform Duolingo we achieved highest score on all three datasets in the 2018 Shared Task on Second Language Acquisition Modeling. We describe our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and discuss relevance of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> compared to how it would be setup in a production environment for personalized education.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0526 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0526/>Modeling Second-Language Learning from a Psychological Perspective</a></strong><br><a href=/people/a/alexander-rich/>Alexander Rich</a>
|
<a href=/people/p/pamela-osborn-popp/>Pamela Osborn Popp</a>
|
<a href=/people/d/david-halpern/>David Halpern</a>
|
<a href=/people/a/anselm-rothe/>Anselm Rothe</a>
|
<a href=/people/t/todd-gureckis/>Todd Gureckis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0526><div class="card-body p-3 small">Psychological research on learning and memory has tended to emphasize small-scale laboratory studies. However, large datasets of people using <a href=https://en.wikipedia.org/wiki/Educational_software>educational software</a> provide opportunities to explore these issues from a new perspective. In this paper we describe our approach to the Duolingo Second Language Acquisition Modeling (SLAM) competition which was run in early 2018. We used a well-known class of <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> (gradient boosted decision trees), with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> partially informed by theories from the <a href=https://en.wikipedia.org/wiki/Psychology>psychological literature</a>. After detailing our modeling approach and a number of supplementary simulations, we reflect on the degree to which psychological theory aided the model, and the potential for cognitive science and predictive modeling competitions to gain from each other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0527 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0527/>A Memory-Sensitive Classification Model of Errors in Early Second Language Learning</a></strong><br><a href=/people/b/brendan-tomoschuk/>Brendan Tomoschuk</a>
|
<a href=/people/j/jarrett-lovelett/>Jarrett Lovelett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0527><div class="card-body p-3 small">In this paper, we explore a variety of linguistic and cognitive features to better understand <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>second language acquisition</a> in early users of the language learning app Duolingo. With these features, we trained a random forest classifier to predict errors in early learners of <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Of particular note was our finding that mean and variance in error for each user and token can be a memory efficient replacement for their respective dummy-encoded categorical variables. At test, these models improved over the baseline model with AUROC values of 0.803 for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, 0.823 for <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and 0.829 for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0529 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0529/>Language Model Based Grammatical Error Correction without Annotated Training Data</a></strong><br><a href=/people/c/christopher-bryant/>Christopher Bryant</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0529><div class="card-body p-3 small">Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0530 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0530/>A Semantic Role-based Approach to Open-Domain Automatic Question Generation</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/b/brian-riordan/>Brian Riordan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0530><div class="card-body p-3 small">We present a novel rule-based system for automatic generation of factual questions from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentences</a>, using semantic role labeling (SRL) as the main form of text analysis. The system is capable of generating both <a href=https://en.wikipedia.org/wiki/Questionnaire>wh-questions</a> and <a href=https://en.wikipedia.org/wiki/Yes&#8211;no_question>yes / no questions</a> from the same <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a>. We present an extensive evaluation of the <a href=https://en.wikipedia.org/wiki/System>system</a> and compare it to a recent neural network architecture for question generation. The SRL-based system outperforms the <a href=https://en.wikipedia.org/wiki/Nervous_system>neural system</a> in both average quality and variety of generated questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0531 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0531/>Automated Content Analysis : A Case Study of Computer Science Student Summaries</a></strong><br><a href=/people/y/yanjun-gao/>Yanjun Gao</a>
|
<a href=/people/p/patricia-m-davies/>Patricia M. Davies</a>
|
<a href=/people/r/rebecca-j-passonneau/>Rebecca J. Passonneau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0531><div class="card-body p-3 small">Technology is transforming Higher Education learning and teaching. This paper reports on a project to examine how and why automated content analysis could be used to assess precis writing by university students. We examine the case of one hundred and twenty-two summaries written by computer science freshmen. The texts, which had been hand scored using a teacher-designed rubric, were autoscored using the Natural Language Processing software, PyrEval. Pearson&#8217;s correlation coefficient and <a href=https://en.wikipedia.org/wiki/Spearman_rank_correlation>Spearman rank correlation</a> were used to analyze the relationship between the teacher score and the PyrEval score for each summary. Three content models automatically constructed by PyrEval from different sets of human reference summaries led to consistent correlations, showing that the approach is reliable. Also observed was that, in cases where the focus of student assessment centers on formative feedback, categorizing the PyrEval scores by examining the average and standard deviations could lead to novel interpretations of their relationships. It is suggested that this project has implications for the ways in which automated content analysis could be used to help university students improve their summarization skills.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0532 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0532/>Toward Data-Driven Tutorial Question Answering with Deep Learning Conversational Models</a></strong><br><a href=/people/m/mayank-kulkarni/>Mayank Kulkarni</a>
|
<a href=/people/k/kristy-boyer/>Kristy Boyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0532><div class="card-body p-3 small">There has been an increase in popularity of data-driven question answering systems given their recent success. This pa-per explores the possibility of building a tutorial question answering system for <a href=https://en.wikipedia.org/wiki/Java_(programming_language)>Java programming</a> from data sampled from a community-based question answering forum. This paper reports on the creation of a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that could support building such a tutorial question answering system and discusses the methodology to create the 106,386 question strong dataset. We investigate how retrieval-based and generative models perform on the given <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The work also investigates the usefulness of using hybrid approaches such as combining retrieval-based and generative models. The results indicate that building data-driven tutorial systems using community-based question answering forums holds significant promise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0533 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0533/>Distractor Generation for Multiple Choice Questions Using Learning to Rank</a></strong><br><a href=/people/c/chen-liang/>Chen Liang</a>
|
<a href=/people/x/xiao-yang/>Xiao Yang</a>
|
<a href=/people/n/neisarg-dave/>Neisarg Dave</a>
|
<a href=/people/d/drew-wham/>Drew Wham</a>
|
<a href=/people/b/bart-pursel/>Bart Pursel</a>
|
<a href=/people/c/c-lee-giles/>C. Lee Giles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0533><div class="card-body p-3 small">We investigate how <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>, specifically <a href=https://en.wikipedia.org/wiki/Ranking>ranking models</a>, can be used to select useful distractors for <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple choice questions</a>. Our proposed models can learn to select distractors that resemble those in actual exam questions, which is different from most existing unsupervised ontology-based and similarity-based methods. We empirically study feature-based and neural net (NN) based ranking models with experiments on the recently released SciQ dataset and our MCQL dataset. Experimental results show that feature-based ensemble learning methods (random forest and LambdaMART) outperform both the NN-based method and unsupervised baselines. These two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> can also be used as benchmarks for distractor generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0534 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0534/>A Portuguese Native Language Identification Dataset<span class=acl-fixed-case>P</span>ortuguese Native Language Identification Dataset</a></strong><br><a href=/people/i/iria-del-rio-gayo/>Iria del Río Gayo</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0534><div class="card-body p-3 small">In this paper we present NLI-PT, the first Portuguese dataset compiled for Native Language Identification (NLI), the task of identifying an author&#8217;s first language based on their second language writing. The dataset includes 1,868 student essays written by learners of <a href=https://en.wikipedia.org/wiki/European_Portuguese>European Portuguese</a>, native speakers of the following L1s : <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/Tetum_language>Tetum</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a>, and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. NLI-PT includes the original student text and four different types of annotation : POS, fine-grained POS, constituency parses, and dependency parses. NLI-PT can be used not only in NLI but also in research on several topics in the field of <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>Second Language Acquisition</a> and educational NLP. We discuss possible applications of this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and present the results obtained for the first lexical baseline system for Portuguese NLI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0536 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0536/>The Effect of Adding Authorship Knowledge in Automated Text Scoring</a></strong><br><a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/x/xie-chen/>Xie Chen</a>
|
<a href=/people/r/ronan-cummins/>Ronan Cummins</a>
|
<a href=/people/o/oistein-e-andersen/>Øistein E. Andersen</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0536><div class="card-body p-3 small">Some <a href=https://en.wikipedia.org/wiki/Test_(assessment)>language exams</a> have multiple writing tasks. When a learner writes multiple texts in a language exam, it is not surprising that the quality of these texts tends to be similar, and the existing automated text scoring (ATS) systems do not explicitly model this similarity. In this paper, we suggest that it could be useful to include the other texts written by this learner in the same exam as extra references in an ATS system. We propose various approaches of fusing information from multiple tasks and pass this authorship knowledge into our ATS model on six different datasets. We show that this can positively affect the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance at a global level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0537 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0537/>SB@GU at the Complex Word Identification 2018 Shared Task<span class=acl-fixed-case>SB</span>@<span class=acl-fixed-case>GU</span> at the Complex Word Identification 2018 Shared Task</a></strong><br><a href=/people/d/david-alfter/>David Alfter</a>
|
<a href=/people/i/ildiko-pilan/>Ildikó Pilán</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0537><div class="card-body p-3 small">In this paper, we describe our experiments for the Shared Task on Complex Word Identification (CWI) 2018 (Yimam et al., 2018), hosted by the 13th Workshop on Innovative Use of NLP for Building Educational Applications (BEA) at NAACL 2018. Our system for <a href=https://en.wikipedia.org/wiki/English_language>English</a> builds on previous work for <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> concerning the classification of words into proficiency levels. We investigate different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and compare their usefulness using <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection methods</a>. For the German, Spanish and French data we use simple <a href=https://en.wikipedia.org/wiki/System>systems</a> based on character n-gram models and show that sometimes simple <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve comparable results to fully feature-engineered systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0539.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0539 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0539 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0539/>Deep Learning Architecture for Complex Word Identification</a></strong><br><a href=/people/d/dirk-de-hertog/>Dirk De Hertog</a>
|
<a href=/people/a/anais-tack/>Anaïs Tack</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0539><div class="card-body p-3 small">We describe a system for the CWI-task that includes information on 5 aspects of the (complex) lexical item, namely distributional information of the item itself, <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological structure</a>, psychological measures, corpus-counts and topical information. We constructed a deep learning architecture that combines those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and apply it to the probabilistic and binary classification task for all <a href=https://en.wikipedia.org/wiki/English_language>English sets</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. We achieved reasonable performance on all sets with best performances seen on the <a href=https://en.wikipedia.org/wiki/Randomized_controlled_trial>probabilistic task</a>, particularly on the English news set (MAE 0.054 and F1-score of 0.872). An analysis of the results shows that reasonable performance can be achieved with a single <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> without any domain-specific tweaking of the parameter settings and that distributional features capture almost all of the information also found in hand-crafted features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0540 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0540/>NILC at CWI 2018 : Exploring <a href=https://en.wikipedia.org/wiki/Feature_engineering>Feature Engineering</a> and Feature Learning<span class=acl-fixed-case>NILC</span> at <span class=acl-fixed-case>CWI</span> 2018: Exploring Feature Engineering and Feature Learning</a></strong><br><a href=/people/n/nathan-hartmann/>Nathan Hartmann</a>
|
<a href=/people/l/leandro-borges-dos-santos/>Leandro Borges dos Santos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0540><div class="card-body p-3 small">This paper describes the results of NILC team at CWI 2018. We developed solutions following three approaches : (i) a feature engineering method using lexical, n-gram and psycholinguistic features, (ii) a shallow neural network method using only word embeddings, and (iii) a Long Short-Term Memory (LSTM) language model, which is pre-trained on a large text corpus to produce a contextualized word vector. The feature engineering method obtained our best results for the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a> and the LSTM model achieved the best results for the probabilistic classification task. Our results show that deep neural networks are able to perform as well as traditional machine learning methods using manually engineered features for the task of complex word identification in <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0541 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0541/>Complex Word Identification Using Character n-grams</a></strong><br><a href=/people/m/maja-popovic/>Maja Popović</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0541><div class="card-body p-3 small">This paper investigates the use of character n-gram frequencies for identifying complex words in English, German and Spanish texts. The approach is based on the assumption that complex words are likely to contain different <a href=https://en.wikipedia.org/wiki/Character_(computing)>character sequences</a> than simple words. The <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>multinomial Naive Bayes classifier</a> was used with n-grams of different lengths as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, and the best results were obtained for the combination of 2-grams and 4-grams. This variant was submitted to the Complex Word Identification Shared Task 2018 for all texts and achieved F-scores between 70 % and 83 %. The system was ranked in the middle range for all English texts, as third of fourteen submissions for <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and as tenth of seventeen submissions for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is not very convenient for the cross-language task, achieving only 59 % on the <a href=https://en.wikipedia.org/wiki/French_language>French text</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0545 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-0545" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-0545/>Deep Factorization Machines for Knowledge Tracing</a></strong><br><a href=/people/j/jill-jenn-vie/>Jill-Jênn Vie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0545><div class="card-body p-3 small">This paper introduces our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We used deep factorization machines, a wide and deep learning model of pairwise relationships between users, items, skills, and other entities considered. Our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> (AUC 0.815) hopefully managed to beat the logistic regression baseline (AUC 0.774) but not the top performing <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> (AUC 0.861) and reveals interesting strategies to build upon <a href=https://en.wikipedia.org/wiki/Item_response_theory>item response theory models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0546.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0546 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0546 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0546/>CLUF : a Neural Model for Second Language Acquisition Modeling<span class=acl-fixed-case>CLUF</span>: a Neural Model for Second Language Acquisition Modeling</a></strong><br><a href=/people/s/shuyao-xu/>Shuyao Xu</a>
|
<a href=/people/j/jin-chen/>Jin Chen</a>
|
<a href=/people/l/long-qin/>Long Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0546><div class="card-body p-3 small">Second Language Acquisition Modeling is the task to predict whether a <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>second language learner</a> would respond correctly in future exercises based on their learning history. In this paper, we propose a neural network based system to utilize rich contextual, linguistic and user information. Our neural model consists of a Context encoder, a Linguistic feature encoder, a User information encoder and a Format information encoder (CLUF). Furthermore, a <a href=https://en.wikipedia.org/wiki/Code>decoder</a> is introduced to combine such encoded features and make final predictions. Our system ranked in first place in the English track and second place in the Spanish and French track with an AUROC score of 0.861, 0.835 and 0.854 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0547 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0547/>Neural sequence modelling for learner error prediction</a></strong><br><a href=/people/z/zheng-yuan/>Zheng Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0547><div class="card-body p-3 small">This paper describes our use of two recurrent neural network sequence models : sequence labelling and sequence-to-sequence models, for the prediction of future learner errors in our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We show that these two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> capture complementary information as combining them improves performance. Furthermore, the same network architecture and group of features can be used directly to build competitive prediction models in all three language tracks, demonstrating that our approach generalises well across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0548.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0548 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0548 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0548/>Automatic Distractor Suggestion for Multiple-Choice Tests Using Concept Embeddings and <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a></a></strong><br><a href=/people/l/le-an-ha/>Le An Ha</a>
|
<a href=/people/v/victoria-yaneva/>Victoria Yaneva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0548><div class="card-body p-3 small">Developing plausible distractors (wrong answer options) when writing multiple-choice questions has been described as one of the most challenging and time-consuming parts of the item-writing process. In this paper we propose a fully automatic method for generating distractor suggestions for <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple-choice questions</a> used in high-stakes medical exams. The system uses a question stem and the correct answer as an input and produces a list of suggested distractors ranked based on their similarity to the stem and the correct answer. To do this we use a novel approach of combining concept embeddings with <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval methods</a>. We frame the <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> as a <a href=https://en.wikipedia.org/wiki/Prediction>prediction task</a> where we aim to predict the human-produced distractors used in large sets of <a href=https://en.wikipedia.org/wiki/Medical_research>medical questions</a>, i.e. if a distractor generated by our system is good enough it is likely to feature among the list of distractors produced by the human item-writers. The results reveal that combining concept embeddings with information retrieval approaches significantly improves the generation of plausible distractors and enables us to match around 1 in 5 of the human-produced distractors. The approach proposed in this paper is generalisable to all scenarios where the distractors refer to concepts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0549 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0549/>Co-Attention Based Neural Network for Source-Dependent Essay Scoring</a></strong><br><a href=/people/h/haoran-zhang/>Haoran Zhang</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0549><div class="card-body p-3 small">This paper presents an investigation of using a co-attention based neural network for source-dependent essay scoring. We use a co-attention mechanism to help the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learn the importance of each part of the essay more accurately. Also, this paper shows that the co-attention based neural network model provides reliable score prediction of source-dependent responses. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two source-dependent response corpora. Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> on both <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a>. We also show that the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is similar to the <a href=https://en.wikipedia.org/wiki/Expert_witness>expert opinions</a> with examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0550.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0550 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0550 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0550/>Cross-Lingual Content Scoring</a></strong><br><a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/s/sebastian-stennmanns/>Sebastian Stennmanns</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0550><div class="card-body p-3 small">We investigate the feasibility of cross-lingual content scoring, a scenario where training and test data in an automatic scoring task are from two different languages. Cross-lingual scoring can contribute to <a href=https://en.wikipedia.org/wiki/Educational_equality>educational equality</a> by allowing answers in multiple languages. Training a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> in one language and applying it to another language might also help to overcome data sparsity issues by re-using trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> from other languages. As there is no suitable dataset available for this new task, we create a comparable bi-lingual corpus by extending the English ASAP dataset with German answers. Our experiments with cross-lingual scoring based on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine-translating</a> either training or test data show a considerable drop in scoring quality.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>