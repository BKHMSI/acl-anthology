<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 3rd Workshop on Neural Generation and Translation - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/D19-56.pdf>Proceedings of the 3rd Workshop on Neural Generation and Translation</a></h2><p class=lead><a href=/people/a/alexandra-birch/>Alexandra Birch</a>,
<a href=/people/a/andrew-finch/>Andrew Finch</a>,
<a href=/people/h/hiroaki-hayashi/>Hiroaki Hayashi</a>,
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>,
<a href=/people/m/minh-thang-luong/>Thang Luong</a>,
<a href=/people/g/graham-neubig/>Graham Neubig</a>,
<a href=/people/y/yusuke-oda/>Yusuke Oda</a>,
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>D19-56</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Hong Kong</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/ngt/>NGT</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/D19-56>https://aclanthology.org/D19-56</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/D19-56.pdf>https://aclanthology.org/D19-56.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/D19-56.pdf title="Open PDF of 'Proceedings of the 3rd Workshop on Neural Generation and Translation'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+3rd+Workshop+on+Neural+Generation+and+Translation" title="Search for 'Proceedings of the 3rd Workshop on Neural Generation and Translation' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5600/>Proceedings of the 3rd Workshop on Neural Generation and Translation</a></strong><br><a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a>
|
<a href=/people/h/hiroaki-hayashi/>Hiroaki Hayashi</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/m/minh-thang-luong/>Thang Luong</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5603 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5603/>Recycling a Pre-trained BERT Encoder for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a><span class=acl-fixed-case>BERT</span> Encoder for Neural Machine Translation</a></strong><br><a href=/people/k/kenji-imamura/>Kenji Imamura</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5603><div class="card-body p-3 small">In this paper, a pre-trained Bidirectional Encoder Representations from Transformers (BERT) model is applied to Transformer-based neural machine translation (NMT). In contrast to monolingual tasks, the number of unlearned model parameters in an NMT decoder is as huge as the number of learned parameters in the BERT model. To train all the models appropriately, we employ two-stage optimization, which first trains only the unlearned parameters by freezing the BERT model, and then fine-tunes all the sub-models. In our experiments, stable two-stage optimization was achieved, in contrast the BLEU scores of direct fine-tuning were extremely low. Consequently, the BLEU scores of the proposed method were better than those of the Transformer base model and the same <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> without pre-training. Additionally, we confirmed that <a href=https://en.wikipedia.org/wiki/Neurotransmitter>NMT</a> with the BERT encoder is more effective in low-resource settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5604 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5604/>Generating a Common Question from Multiple Documents using Multi-source Encoder-Decoder Models</a></strong><br><a href=/people/w/woon-sang-cho/>Woon Sang Cho</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5604><div class="card-body p-3 small">Ambiguous user queries in <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> result in the retrieval of documents that often span multiple topics. One potential solution is for the <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a> to generate multiple refined queries, each of which relates to a subset of the documents spanning the same topic. A preliminary step towards this goal is to generate a question that captures common concepts of multiple documents. We propose a new task of generating common question from multiple documents and present simple variant of an existing multi-source encoder-decoder framework, called the Multi-Source Question Generator (MSQG). We first train an RNN-based single encoder-decoder generator from (single document, question) pairs. At test time, given multiple documents, the Distribute step of our MSQG model predicts target word distributions for each document using the trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. The Aggregate step aggregates these <a href=https://en.wikipedia.org/wiki/Probability_distribution>distributions</a> to generate a common question. This simple yet effective strategy significantly outperforms several existing baseline models applied to the new task when evaluated using automated metrics and human judgments on the MS-MARCO-QA dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5607 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5607/>Transformer-based Model for Single Documents Neural Summarization</a></strong><br><a href=/people/e/elozino-egonmwan/>Elozino Egonmwan</a>
|
<a href=/people/y/yllias-chali/>Yllias Chali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5607><div class="card-body p-3 small">We propose a system that improves performance on single document summarization task using the CNN / DailyMail and Newsroom datasets. It follows the popular encoder-decoder paradigm, but with an extra focus on the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. The intuition is that the probability of correctly decoding an information significantly lies in the pattern and correctness of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. Hence we introduce, encode encode decode. A framework that encodes the source text first with a transformer, then a sequence-to-sequence (seq2seq) model. We find that the transformer and seq2seq model complement themselves adequately, making for a richer encoded vector representation. We also find that paying more attention to the vocabulary of target words during <a href=https://en.wikipedia.org/wiki/Abstraction>abstraction</a> improves performance. We experiment our hypothesis and framework on the task of extractive and abstractive single document summarization and evaluate using the standard CNN / DailyMail dataset and the recently released Newsroom dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5608 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5608/>Making Asynchronous Stochastic Gradient Descent Work for Transformers</a></strong><br><a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5608><div class="card-body p-3 small">Asynchronous stochastic gradient descent (SGD) converges poorly for Transformer models, so synchronous SGD has become the norm for Transformer training. This is unfortunate because asynchronous SGD is faster at raw training speed since it avoids waiting for <a href=https://en.wikipedia.org/wiki/Synchronization_(computer_science)>synchronization</a>. Moreover, the Transformer model is the basis for state-of-the-art models for several tasks, including <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, so training speed matters. To understand why asynchronous SGD under-performs, we blur the lines between asynchronous and synchronous methods. We find that summing several asynchronous updates, rather than applying them immediately, restores convergence behavior. With this <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, the Transformer attains the same BLEU score 1.36 times as fast.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5612.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5612 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5612" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5612/>On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation<span class=acl-fixed-case>K</span>ullback-<span class=acl-fixed-case>L</span>eibler Divergence Term in Variational Autoencoders for Text Generation</a></strong><br><a href=/people/v/victor-prokhorov/>Victor Prokhorov</a>
|
<a href=/people/e/ehsan-shareghi/>Ehsan Shareghi</a>
|
<a href=/people/y/yingzhen-li/>Yingzhen Li</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5612><div class="card-body p-3 small">Variational Autoencoders (VAEs) are known to suffer from learning uninformative latent representation of the input due to issues such as approximated posterior collapse, or entanglement of the latent space. We impose an explicit <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> on the Kullback-Leibler (KL) divergence term inside the VAE objective function. While the explicit <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> naturally avoids posterior collapse, we use it to further understand the significance of the KL term in controlling the information transmitted through the VAE channel. Within this framework, we explore different properties of the estimated posterior distribution, and highlight the trade-off between the amount of information encoded in a latent code during training, and the generative capacity of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5615 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5615/>Enhanced Transformer Model for Data-to-Text Generation</a></strong><br><a href=/people/l/li-gong/>Li Gong</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5615><div class="card-body p-3 small">Neural models have recently shown significant progress on data-to-text generation tasks in which descriptive texts are generated conditioned on <a href=https://en.wikipedia.org/wiki/Record_(computer_science)>database records</a>. In this work, we present a new Transformer-based data-to-text generation model which learns content selection and summary generation in an end-to-end fashion. We introduce two extensions to the baseline transformer model : First, we modify the latent representation of the input, which helps to significantly improve the content correctness of the output summary ; Second, we include an additional learning objective that accounts for content selection modelling. In addition, we propose two data augmentation methods that succeed to further improve performance of the resulting generation models. Evaluation experiments show that our final model outperforms current state-of-the-art systems as measured by different metrics : BLEU, content selection precision and content ordering. We made publicly available the transformer extension presented in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5616 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5616.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5616/>Generalization in Generation : A closer look at Exposure Bias</a></strong><br><a href=/people/f/florian-schmidt/>Florian Schmidt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5616><div class="card-body p-3 small">Exposure bias refers to the train-test discrepancy that seemingly arises when an <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive generative model</a> uses only ground-truth contexts at training time but generated ones at test time. We separate the contribution of the learning framework and the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to clarify the debate on consequences and review proposed counter-measures. In this light, we argue that <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> is the underlying property to address and propose unconditional generation as its fundamental benchmark. Finally, we combine <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable modeling</a> with a recent formulation of exploration in <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to obtain a rigorous handling of true and generated contexts. Results on <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and variational sentence auto-encoding confirm the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s generalization capability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5621 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5621/>A Margin-based Loss with Synthetic Negative Samples for Continuous-output Machine Translation</a></strong><br><a href=/people/g/gayatri-bhat/>Gayatri Bhat</a>
|
<a href=/people/s/sachin-kumar/>Sachin Kumar</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5621><div class="card-body p-3 small">Neural models that eliminate the softmax bottleneck by generating word embeddings (rather than <a href=https://en.wikipedia.org/wiki/Multinomial_distribution>multinomial distributions</a> over a vocabulary) attain faster training with fewer learnable parameters. These models are currently trained by maximizing densities of pretrained target embeddings under von Mises-Fisher distributions parameterized by corresponding model-predicted embeddings. This work explores the utility of margin-based loss functions in optimizing such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We present syn-margin loss, a novel margin-based loss that uses a synthetic negative sample constructed from only the predicted and target embeddings at every step. The <a href=https://en.wikipedia.org/wiki/Profit_(accounting)>loss</a> is efficient to compute, and we use a <a href=https://en.wikipedia.org/wiki/Geometric_analysis>geometric analysis</a> to argue that it is more consistent and interpretable than other margin-based losses. Empirically, we find that syn-margin provides small but significant improvements over both vMF and standard margin-based losses in continuous-output neural machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5622 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5622/>Mixed Multi-Head Self-Attention for Neural Machine Translation</a></strong><br><a href=/people/h/hongyi-cui/>Hongyi Cui</a>
|
<a href=/people/s/shohei-iida/>Shohei Iida</a>
|
<a href=/people/p/po-hsuan-hung/>Po-Hsuan Hung</a>
|
<a href=/people/t/takehito-utsuro/>Takehito Utsuro</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5622><div class="card-body p-3 small">Recently, the Transformer becomes a state-of-the-art architecture in the filed of neural machine translation (NMT). A key point of its high-performance is the multi-head self-attention which is supposed to allow the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to independently attend to information from different representation subspaces. However, there is no explicit mechanism to ensure that different attention heads indeed capture different features, and in practice, <a href=https://en.wikipedia.org/wiki/Redundancy_(engineering)>redundancy</a> has occurred in multiple heads. In this paper, we argue that using the same <a href=https://en.wikipedia.org/wiki/Attentional_control>global attention</a> in multiple heads limits multi-head self-attention&#8217;s capacity for learning distinct features. In order to improve the expressiveness of multi-head self-attention, we propose a novel Mixed Multi-Head Self-Attention (MMA) which models not only global and local attention but also forward and backward attention in different attention heads. This enables the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn distinct representations explicitly among multiple heads. In our experiments on both WAT17 English-Japanese as well as IWSLT14 German-English translation task, we show that, without increasing the number of parameters, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> yield consistent and significant improvements (0.9 BLEU scores on average) over the strong Transformer baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5624 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5624" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5624/>Interrogating the Explanatory Power of <a href=https://en.wikipedia.org/wiki/Attention>Attention</a> in Neural Machine Translation</a></strong><br><a href=/people/p/pooya-moradi/>Pooya Moradi</a>
|
<a href=/people/n/nishant-kambhatla/>Nishant Kambhatla</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5624><div class="card-body p-3 small">Attention models have become a crucial component in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a>. They are often implicitly or explicitly used to justify the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s decision in generating a specific token but it has not yet been rigorously established to what extent attention is a reliable source of information in <a href=https://en.wikipedia.org/wiki/Mathematical_model>NMT</a>. To evaluate the explanatory power of <a href=https://en.wikipedia.org/wiki/Attention>attention</a> for NMT, we examine the possibility of yielding the same prediction but with counterfactual attention models that modify crucial aspects of the trained <a href=https://en.wikipedia.org/wiki/Attention>attention model</a>. Using these counterfactual attention mechanisms we assess the extent to which they still preserve the generation of function and content words in the translation process. Compared to a state of the art attention model, our counterfactual attention models produce 68 % of <a href=https://en.wikipedia.org/wiki/Function_word>function words</a> and 21 % of <a href=https://en.wikipedia.org/wiki/Content_word>content words</a> in our German-English dataset. Our experiments demonstrate that attention models by themselves can not reliably explain the decisions made by a NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5625.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5625 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5625" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5625/>Auto-Sizing the Transformer Network : Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation</a></strong><br><a href=/people/k/kenton-murray/>Kenton Murray</a>
|
<a href=/people/j/jeffery-kinnison/>Jeffery Kinnison</a>
|
<a href=/people/t/toan-q-nguyen/>Toan Q. Nguyen</a>
|
<a href=/people/w/walter-scheirer/>Walter Scheirer</a>
|
<a href=/people/d/david-chiang/>David Chiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5625><div class="card-body p-3 small">Neural sequence-to-sequence models, particularly the Transformer, are the state of the art in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Yet these <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> are very sensitive to <a href=https://en.wikipedia.org/wiki/Network_architecture>architecture</a> and hyperparameter settings. Optimizing these settings by grid or random search is computationally expensive because it requires many training runs. In this paper, we incorporate <a href=https://en.wikipedia.org/wiki/Architecture_search>architecture search</a> into a single training run through auto-sizing, which uses <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> to delete <a href=https://en.wikipedia.org/wiki/Neuron>neurons</a> in a <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> over the course of training. On very low-resource language pairs, we show that auto-sizing can improve BLEU scores by up to 3.9 points while removing one-third of the <a href=https://en.wikipedia.org/wiki/Parameter_(computer_programming)>parameters</a> from the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5628.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5628 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5628 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5628/>Monash University’s Submissions to the WNGT 2019 Document Translation Task<span class=acl-fixed-case>WNGT</span> 2019 Document Translation Task</a></strong><br><a href=/people/s/sameen-maruf/>Sameen Maruf</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5628><div class="card-body p-3 small">We describe the work of Monash University for the shared task of Rotowire document translation organised by the 3rd Workshop on Neural Generation and Translation (WNGT 2019). We submitted systems for both directions of the English-German language pair. Our main focus is on employing an established document-level neural machine translation model for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We achieve a <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a> of 39.83 (41.46 BLEU per WNGT evaluation) for En-De and 45.06 (47.39 BLEU per WNGT evaluation) for De-En translation directions on the Rotowire test set. All experiments conducted in the process are also described.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5630.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5630 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5630 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5630" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5630/>University of Edinburgh’s submission to the Document-level Generation and Translation Shared Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s submission to the Document-level Generation and Translation Shared Task</a></strong><br><a href=/people/r/ratish-puduppully/>Ratish Puduppully</a>
|
<a href=/people/j/jonathan-mallinson/>Jonathan Mallinson</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5630><div class="card-body p-3 small">The University of Edinburgh participated in all six tracks : NLG, MT, and MT+NLG with both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a> as targeted languages. For the NLG track, we submitted a multilingual system based on the Content Selection and Planning model of Puduppully et al (2019). For the MT track, we submitted Transformer-based Neural Machine Translation models, where out-of-domain parallel data was augmented with in-domain data extracted from monolingual corpora. Our MT+NLG systems disregard the structured input data and instead rely exclusively on the source summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5631.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5631 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5631 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5631/>Naver Labs Europe’s Systems for the Document-Level Generation and Translation Task at WNGT 2019<span class=acl-fixed-case>E</span>urope’s Systems for the Document-Level Generation and Translation Task at <span class=acl-fixed-case>WNGT</span> 2019</a></strong><br><a href=/people/f/fahimeh-saleh/>Fahimeh Saleh</a>
|
<a href=/people/a/alexandre-berard/>Alexandre Berard</a>
|
<a href=/people/i/ioan-calapodescu/>Ioan Calapodescu</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5631><div class="card-body p-3 small">Recently, neural models led to significant improvements in both <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation tasks (NLG)</a>. However, generation of long descriptive summaries conditioned on <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> remains an open challenge. Likewise, <a href=https://en.wikipedia.org/wiki/Metadata>MT</a> that goes beyond <a href=https://en.wikipedia.org/wiki/Metadata>sentence-level context</a> is still an open issue (e.g., document-level MT or <a href=https://en.wikipedia.org/wiki/Metadata>MT</a> with metadata). To address these challenges, we propose to leverage data from both tasks and do <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> between MT, NLG, and MT with source-side metadata (MT+NLG). First, we train document-based MT systems with large amounts of <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>. Then, we adapt these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to pure NLG and MT+NLG tasks by <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> with smaller amounts of domain-specific data. This end-to-end NLG approach, without data selection and planning, outperforms the previous state of the art on the Rotowire NLG task. We participated to the Document Generation and Translation task at WNGT 2019, and ranked first in all tracks.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>