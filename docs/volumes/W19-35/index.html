<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Third Workshop on Abusive Language Online - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W19-35.pdf>Proceedings of the Third Workshop on Abusive Language Online</a></h2><p class=lead><a href=/people/s/sarah-t-roberts/>Sarah T. Roberts</a>,
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>,
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>,
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W19-35</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Florence, Italy</dd><dt>Venues:</dt><dd><a href=/venues/acl/>ACL</a>
| <a href=/venues/alw/>ALW</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W19-35>https://aclanthology.org/W19-35</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W19-35.pdf>https://aclanthology.org/W19-35.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W19-35.pdf title="Open PDF of 'Proceedings of the Third Workshop on Abusive Language Online'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Third+Workshop+on+Abusive+Language+Online" title="Search for 'Proceedings of the Third Workshop on Abusive Language Online' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3500/>Proceedings of the Third Workshop on Abusive Language Online</a></strong><br><a href=/people/s/sarah-t-roberts/>Sarah T. Roberts</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3501/>Subversive Toxicity Detection using Sentiment Information</a></strong><br><a href=/people/e/eloi-brassard-gourdeau/>Eloi Brassard-Gourdeau</a>
|
<a href=/people/r/richard-khoury/>Richard Khoury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3501><div class="card-body p-3 small">The presence of toxic content has become a major problem for many <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a>. Moderators try to limit this problem by implementing more and more refined comment filters, but toxic users are constantly finding new ways to circumvent them. Our hypothesis is that while modifying toxic content and <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> to fool filters can be easy, hiding sentiment is harder. In this paper, we explore various aspects of sentiment detection and their correlation to <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a>, and use our results to implement a <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity detection tool</a>. We then test how adding the sentiment information helps detect <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> in three different real-world datasets, and incorporate <a href=https://en.wikipedia.org/wiki/Subversion>subversion</a> to these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to simulate a user trying to circumvent the system. Our results show <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> has a positive impact on toxicity detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3504" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3504/>Racial Bias in <a href=https://en.wikipedia.org/wiki/Hate_speech>Hate Speech</a> and Abusive Language Detection Datasets</a></strong><br><a href=/people/t/thomas-davidson/>Thomas Davidson</a>
|
<a href=/people/d/debasmita-bhattacharya/>Debasmita Bhattacharya</a>
|
<a href=/people/i/ingmar-weber/>Ingmar Weber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3504><div class="card-body p-3 small">Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine <a href=https://en.wikipedia.org/wiki/Racism_in_the_United_States>racial bias</a> in five different sets of Twitter data annotated for hate speech and abusive language. We train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on these datasets and compare the predictions of these <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on tweets written in <a href=https://en.wikipedia.org/wiki/African-American_English>African-American English</a> with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on them tend to predict that tweets written in <a href=https://en.wikipedia.org/wiki/African-American_English>African-American English</a> are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these <a href=https://en.wikipedia.org/wiki/System>systems</a> may discriminate against the groups who are often the targets of the abuse we are trying to detect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3505/>Automated Identification of Verbally Abusive Behaviors in Online Discussions</a></strong><br><a href=/people/s/srecko-joksimovic/>Srecko Joksimovic</a>
|
<a href=/people/r/ryan-s-baker/>Ryan S. Baker</a>
|
<a href=/people/j/jaclyn-ocumpaugh/>Jaclyn Ocumpaugh</a>
|
<a href=/people/j/juan-miguel-l-andres/>Juan Miguel L. Andres</a>
|
<a href=/people/i/ivan-tot/>Ivan Tot</a>
|
<a href=/people/e/elle-yuan-wang/>Elle Yuan Wang</a>
|
<a href=/people/s/shane-dawson/>Shane Dawson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3505><div class="card-body p-3 small">Discussion forum participation represents one of the crucial factors for <a href=https://en.wikipedia.org/wiki/Learning>learning</a> and often the only way of supporting <a href=https://en.wikipedia.org/wiki/Social_relation>social interactions</a> in <a href=https://en.wikipedia.org/wiki/Online_and_offline>online settings</a>. However, as much as sharing new ideas or asking thoughtful questions contributes <a href=https://en.wikipedia.org/wiki/Learning>learning</a>, verbally abusive behaviors, such as expressing negative emotions in online discussions, could have disproportionate detrimental effects. To provide means for mitigating the potential negative effects on course participation and <a href=https://en.wikipedia.org/wiki/Learning>learning</a>, we developed an automated classifier for identifying <a href=https://en.wikipedia.org/wiki/Communication>communication</a> that show linguistic patterns associated with <a href=https://en.wikipedia.org/wiki/Hostility>hostility</a> in online forums. In so doing, we employ several well-established automated text analysis tools and build on the common practices for handling highly imbalanced datasets and reducing the sensitivity to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. Although still in its infancy, our approach shows promising results (ROC AUC.73) towards establishing a robust detector of abusive behaviors. We further provide an overview of the classification (linguistic and contextual) features most indicative of online aggression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3508/>Pay Attention to your Context when Classifying Abusive Language</a></strong><br><a href=/people/t/tuhin-chakrabarty/>Tuhin Chakrabarty</a>
|
<a href=/people/k/kilol-gupta/>Kilol Gupta</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3508><div class="card-body p-3 small">The goal of any <a href=https://en.wikipedia.org/wiki/Social_media>social media platform</a> is to facilitate healthy and meaningful interactions among its users. But more often than not, it has been found that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> becomes an avenue for wanton attacks. We propose an experimental study that has three aims : 1) to provide us with a deeper understanding of current data sets that focus on different types of abusive language, which are sometimes overlapping (racism, sexism, hate speech, offensive language, and personal attacks) ; 2) to investigate what type of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> (contextual vs. self-attention) is better for abusive language detection using deep learning architectures ; and 3) to investigate whether stacked architectures provide an advantage over simple architectures for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3509" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3509/>Challenges and frontiers in abusive content detection</a></strong><br><a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/a/alex-harris/>Alex Harris</a>
|
<a href=/people/d/dong-nguyen/>Dong Nguyen</a>
|
<a href=/people/r/rebekah-tromble/>Rebekah Tromble</a>
|
<a href=/people/s/scott-hale/>Scott Hale</a>
|
<a href=/people/h/helen-margetts/>Helen Margetts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3509><div class="card-body p-3 small">Online abusive content detection is an inherently difficult task. It has received considerable attention from academia, particularly within the computational linguistics community, and performance appears to have improved as the field has matured. However, considerable challenges and unaddressed frontiers remain, spanning technical, social and ethical dimensions. These issues constrain the performance, <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a> and generalizability of abusive content detection systems. In this article we delineate and clarify the main challenges and frontiers in the field, critically evaluate their implications and discuss potential solutions. We also highlight ways in which <a href=https://en.wikipedia.org/wiki/Social_science>social scientific insights</a> can advance research. We discuss the lack of support given to researchers working with abusive content and provide guidelines for ethical research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3511 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3511/>A System to Monitor Cyberbullying based on Message Classification and Social Network Analysis</a></strong><br><a href=/people/s/stefano-menini/>Stefano Menini</a>
|
<a href=/people/g/giovanni-moretti/>Giovanni Moretti</a>
|
<a href=/people/m/michele-corazza/>Michele Corazza</a>
|
<a href=/people/e/elena-cabrio/>Elena Cabrio</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/s/serena-villata/>Serena Villata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3511><div class="card-body p-3 small">Social media platforms like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Instagram>Instagram</a> face a surge in cyberbullying phenomena against young users and need to develop scalable computational methods to limit the negative consequences of this kind of abuse. Despite the number of approaches recently proposed in the Natural Language Processing (NLP) research area for detecting different forms of abusive language, the issue of identifying cyberbullying phenomena at scale is still an unsolved problem. This is because of the need to couple abusive language detection on textual message with network analysis, so that repeated attacks against the same person can be identified. In this paper, we present a system to monitor cyberbullying phenomena by combining message classification and <a href=https://en.wikipedia.org/wiki/Social_network_analysis>social network analysis</a>. We evaluate the classification module on a data set built on <a href=https://en.wikipedia.org/wiki/Instagram>Instagram messages</a>, and we describe the cyberbullying monitoring user interface.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3512/>L-HSAB : A Levantine Twitter Dataset for Hate Speech and Abusive Language<span class=acl-fixed-case>L</span>-<span class=acl-fixed-case>HSAB</span>: A <span class=acl-fixed-case>L</span>evantine <span class=acl-fixed-case>T</span>witter Dataset for Hate Speech and Abusive Language</a></strong><br><a href=/people/h/hala-mulki/>Hala Mulki</a>
|
<a href=/people/h/hatem-haddad/>Hatem Haddad</a>
|
<a href=/people/c/chedi-bechikh-ali/>Chedi Bechikh Ali</a>
|
<a href=/people/h/halima-alshabani/>Halima Alshabani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3512><div class="card-body p-3 small">Hate speech and abusive language have become a common phenomenon on Arabic social media. Automatic hate speech and abusive detection systems can facilitate the prohibition of toxic textual contents. The <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>, informality and ambiguity of the <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> hindered the provision of the needed resources for Arabic abusive / hate speech detection research. In this paper, we introduce the first publicly-available Levantine Hate Speech and Abusive (L-HSAB) Twitter dataset with the objective to be a benchmark dataset for automatic detection of online Levantine toxic contents. We, further, provide a detailed review of the data collection steps and how we design the annotation guidelines such that a reliable dataset annotation is guaranteed. This has been later emphasized through the comprehensive evaluation of the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> as the annotation agreement metrics of Cohen&#8217;s Kappa (k) and Krippendorff&#8217;s alpha () indicated the consistency of the annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3514/>Preemptive Toxic Language Detection in Wikipedia Comments Using Thread-Level Context<span class=acl-fixed-case>W</span>ikipedia Comments Using Thread-Level Context</a></strong><br><a href=/people/m/mladen-karan/>Mladen Karan</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3514><div class="card-body p-3 small">We address the task of automatically detecting toxic content in <a href=https://en.wikipedia.org/wiki/User-generated_content>user generated texts</a>. We fo cus on exploring the potential for preemptive moderation, i.e., predicting whether a particular conversation thread will, in the future, incite a toxic comment. Moreover, we perform preliminary investigation of whether a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that jointly considers all comments in a <a href=https://en.wikipedia.org/wiki/Conversation_threading>conversation thread</a> outperforms a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that considers only individual comments. Using an existing <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of conversations among <a href=https://en.wikipedia.org/wiki/Wikipedia_community>Wikipedia contributors</a> as a starting point, we compile a new large-scale dataset for this task consisting of labeled comments and comments from their conversation threads.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3516/>A Platform Agnostic Dual-Strand Hate Speech Detector</a></strong><br><a href=/people/j/johannes-skjeggestad-meyer/>Johannes Skjeggestad Meyer</a>
|
<a href=/people/b/bjorn-gamback/>Björn Gambäck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3516><div class="card-body p-3 small">Hate speech detectors must be applicable across a multitude of services and platforms, and there is hence a need for detection approaches that do not depend on any information specific to a given platform. For instance, the information stored about the text&#8217;s author may differ between services, and so using such data would reduce a <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s general applicability. The paper thus focuses on using exclusively <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text-based input</a> in the detection, in an optimised architecture combining <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> and Long Short-Term Memory-networks. The hate speech detector merges two strands with <a href=https://en.wikipedia.org/wiki/N-gram>character n-grams</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to produce the final classification, and is shown to outperform comparable previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3520 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3520/>Online aggression from a sociological perspective : An integrative view on determinants and possible countermeasures</a></strong><br><a href=/people/s/sebastian-weingartner/>Sebastian Weingartner</a>
|
<a href=/people/l/lea-stahel/>Lea Stahel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3520><div class="card-body p-3 small">The present paper introduces a <a href=https://en.wikipedia.org/wiki/Mathematical_model>theoretical model</a> for explaining aggressive online comments from a sociological perspective. It is innovative as <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> combines individual, situational, and social-structural determinants of online aggression and tries to theoretically derive their interplay. Moreover, the paper suggests an empirical strategy for testing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. The main contribution will be to match online commenting data with survey data containing rich background data of non- /aggressive online commentators.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>