<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2021.alta-1.pdf>Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association</a></h2><p class=lead><a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>,
<a href=/people/w/william-lane/>William Lane</a>,
<a href=/people/g/guido-zuccon/>Guido Zuccon</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.alta-1</dd><dt>Month:</dt><dd>December</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venue:</dt><dd><a href=/venues/alta/>ALTA</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Australasian Language Technology Association</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.alta-1>https://aclanthology.org/2021.alta-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.alta-1.pdf>https://aclanthology.org/2021.alta-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.alta-1.pdf title="Open PDF of 'Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+The+19th+Annual+Workshop+of+the+Australasian+Language+Technology+Association" title="Search for 'Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alta-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alta-1.0/>Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association</a></strong><br><a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/w/william-lane/>William Lane</a>
|
<a href=/people/g/guido-zuccon/>Guido Zuccon</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alta-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--alta-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.alta-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alta-1.2/>An Approach to the Frugal Use of Human Annotators to Scale up Auto-coding for Text Classification Tasks</a></strong><br><a href=/people/l/lian-chen/>Liâ€™An Chen</a>
|
<a href=/people/h/hanna-suominen/>Hanna Suominen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--alta-1--2><div class="card-body p-3 small">Human annotation for establishing the training data is often a very costly process in natural language processing (NLP) tasks, which has led to frugal NLP approaches becoming an important research topic. Many research teams struggle to complete projects with limited funding, labor, and computational resources. Driven by the Move-Step analytic framework theorized in the applied linguistics field, our study offers a rigorous approach to the frugal use of two human annotators to scale up auto-coding for text classification tasks. We applied the Linear Support Vector Machine algorithm to <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> of a job ad corpus. Our Cohens Kappa for inter-rater agreement and Area Under the Curve (AUC) values reached averages of 0.76 and 0.80, respectively. The calculated time consumption for our human training process was 36 days. The results indicated that even the strategic and frugal use of only two human annotators could enable the efficient training of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> with reasonably good performance. This study does not aim to provide generalizability of the results. Rather, we propose that the annotation strategies arising from this study be considered by our readers only if such <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> are fit for one&#8217;s specific research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alta-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--alta-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.alta-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alta-1.5/>Multi-modal Intent Classification for Assistive Robots with Large-scale Naturalistic Datasets</a></strong><br><a href=/people/k/karun-varghese-mathew/>Karun Varghese Mathew</a>
|
<a href=/people/v/venkata-s-aditya-tarigoppula/>Venkata S Aditya Tarigoppula</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--alta-1--5><div class="card-body p-3 small">Recent years have brought a tremendous growth in assistive robots / prosthetics for people with partial or complete loss of upper limb control. These technologies aim to help the users with various reaching and grasping tasks in their daily lives such as picking up an object and transporting it to a desired location ; and their utility critically depends on the ease and effectiveness of communication between the user and robot. One of the natural ways of communicating with <a href=https://en.wikipedia.org/wiki/Assistive_technology>assistive technologies</a> is through <a href=https://en.wikipedia.org/wiki/Linguistic_description>verbal instructions</a>. The meaning of natural language commands depends on the current configuration of the surrounding environment and needs to be interpreted in this multi-modal context, as accurate interpretation of the command is essential for a successful execution of the users intent by an <a href=https://en.wikipedia.org/wiki/Assistive_device>assistive device</a>. The research presented in this paper demonstrates how large-scale situated natural language datasets can support the development of robust assistive technologies. We leveraged a navigational dataset comprising 25k human-provided natural language commands covering diverse situations. We demonstrated a way to extend the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> in a task-informed way and use it to develop multi-modal intent classifiers for pick and place tasks. Our best <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> reached 98 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in a 16-way multi-modal intent classification task, suggesting high robustness and <a href=https://en.wikipedia.org/wiki/Stiffness>flexibility</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alta-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--alta-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.alta-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alta-1.7/>Combining Shallow and Deep Representations for Text-Pair Classification</a></strong><br><a href=/people/v/vincent-nguyen/>Vincent Nguyen</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/z/zhenchang-xing/>Zhenchang Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--alta-1--7><div class="card-body p-3 small">Text-pair classification is the task of determining the class relationship between two sentences. It is embedded in several tasks such as paraphrase identification and duplicate question detection. Contemporary methods use fine-tuned transformer encoder semantic representations of the classification token in the text-pair sequence from the transformer&#8217;s final layer for <a href=https://en.wikipedia.org/wiki/Statistical_classification>class prediction</a>. However, research has shown that earlier parts of the network learn shallow features, such as <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and <a href=https://en.wikipedia.org/wiki/Structure>structure</a>, which existing methods do not directly exploit. We propose a novel convolution-based decoder for transformer-based architecture that maximizes the use of encoder hidden features for text-pair classification. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exploits hidden representations within transformer-based architecture. It outperforms a transformer encoder baseline on average by 50 % (relative F1-score) on six datasets from the <a href=https://en.wikipedia.org/wiki/Medicine>medical</a>, software engineering, and open-domains. Our work shows that transformer-based models can improve text-pair classification by modifying the fine-tuning step to exploit shallow features while improving model generalization, with only a slight reduction in efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alta-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--alta-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.alta-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alta-1.9/>Evaluation of Review Summaries via <a href=https://en.wikipedia.org/wiki/Question_answering>Question-Answering</a></a></strong><br><a href=/people/n/nannan-huang/>Nannan Huang</a>
|
<a href=/people/x/xiuzhen-jenny-zhang/>Xiuzhen Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--alta-1--9><div class="card-body p-3 small">Summarisation of reviews aims at compressing opinions expressed in multiple review documents into a concise form while still covering the key opinions. Despite the advancement in summarisation models, evaluation metrics for opinionated text summaries lag behind and still rely on lexical-matching metrics such as ROUGE. In this paper, we propose to use the question-answering(QA) approach to evaluate summaries of opinions in reviews. We propose to identify opinion-bearing text spans in the reference summary to generate QA pairs so as to capture salient opinions. A QA model is then employed to probe the candidate summary to evaluate information overlap between candidate and reference summaries. We show that our metric RunQA, Review Summary Evaluation via Question Answering, correlates well with human judgments in terms of coverage and focus of information. Finally, we design an adversarial task and demonstrate that the proposed approach is more robust than <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> in the literature for ranking summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alta-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--alta-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.alta-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alta-1.13/>Document Level Hierarchical Transformer</a></strong><br><a href=/people/n/najam-zaidi/>Najam Zaidi</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--alta-1--13><div class="card-body p-3 small">Generating long and coherent text is an important and challenging task encompassing many application areas such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, document level machine translation and <a href=https://en.wikipedia.org/wiki/Storytelling>story generation</a>. Despite the success in modeling intra-sentence coherence, existing long text generation models (e.g., BART and GPT-3) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to revise, replace, revoke or delete any part that has been generated by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. In this paper, we present a novel semi-autoregressive document generation model capable of revising and editing the generated text. Building on recent models by (Gu et al., 2019 ; Xu and Carpuat, 2020) we propose document generation as a hierarchical Markov decision process with a two level hierarchy, where the high and low level editing programs. We train our model using imitation learning (Hussein et al., 2017) and introduce roll-in policy such that each <a href=https://en.wikipedia.org/wiki/Policy>policy</a> learns on the output of applying the previous action. Experiments applying the proposed approach sheds various insights on the problems of long text generation using our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. We suggest various remedies such as using distilled dataset, designing better attention mechanisms and using <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a> as a <a href=https://en.wikipedia.org/wiki/Low-level_programming_language>low level program</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alta-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--alta-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.alta-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alta-1.15/>Generating and Modifying Natural Language Explanations</a></strong><br><a href=/people/a/abdus-salam/>Abdus Salam</a>
|
<a href=/people/r/rolf-schwitter/>Rolf Schwitter</a>
|
<a href=/people/m/mehmet-orgun/>Mehmet Orgun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--alta-1--15><div class="card-body p-3 small">HESIP is a hybrid explanation system for image predictions that combines sub-symbolic and symbolic machine learning techniques to explain the predictions of image classification tasks. The sub-symbolic component makes a prediction for an image and the symbolic component learns probabilistic symbolic rules in order to explain that prediction. In HESIP, the explanations are generated in <a href=https://en.wikipedia.org/wiki/Controlled_natural_language>controlled natural language</a> from the learned probabilistic rules using a bi-directional logic grammar. In this paper, we present an explanation modification method where a <a href=https://en.wikipedia.org/wiki/Human-in-the-loop>human-in-the-loop</a> can modify an incorrect explanation generated by the <a href=https://en.wikipedia.org/wiki/Human-in-the-loop>HESIP system</a> and afterwards, the modified explanation is used by <a href=https://en.wikipedia.org/wiki/Human-in-the-loop>HESIP</a> to learn a better explanation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alta-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--alta-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.alta-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alta-1.25/>An Ensemble Model for Automatic Grading of Evidence</a></strong><br><a href=/people/y/yuting-guo/>Yuting Guo</a>
|
<a href=/people/y/yao-ge/>Yao Ge</a>
|
<a href=/people/r/ruqi-liao/>Ruqi Liao</a>
|
<a href=/people/a/abeed-sarker/>Abeed Sarker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--alta-1--25><div class="card-body p-3 small">This paper describes our approach for the automatic grading of evidence task from the Australasian Language Technology Association (ALTA) Shared Task 2021. We developed two classification models with SVM and RoBERTa and applied an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble technique</a> to combine the grades from different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. Our results showed that the <a href=https://en.wikipedia.org/wiki/Statistical_model>SVM model</a> achieved comparable results to the RoBERTa model, and the <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble system</a> outperformed the individual models on this task. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved the first place among five teams and obtained 3.3 % higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than the second place.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>