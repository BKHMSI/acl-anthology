<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2020.sigdial-1.pdf>Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></h2><p class=lead><a href=/people/o/olivier-pietquin/>Olivier Pietquin</a>,
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a>,
<a href=/people/v/vivian-chen/>Vivian Chen</a>,
<a href=/people/c/casey-kennington/>Casey Kennington</a>,
<a href=/people/d/david-vandyke/>David Vandyke</a>,
<a href=/people/n/nina-dethlefs/>Nina Dethlefs</a>,
<a href=/people/k/koji-inoue/>Koji Inoue</a>,
<a href=/people/e/erik-ekstedt/>Erik Ekstedt</a>,
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.sigdial-1</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>1st virtual meeting</dd><dt>Venue:</dt><dd><a href=/venues/sigdial/>SIGDIAL</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigdial/>SIGDIAL</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.sigdial-1>https://aclanthology.org/2020.sigdial-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.sigdial-1.pdf>https://aclanthology.org/2020.sigdial-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.sigdial-1.pdf title="Open PDF of 'Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+21th+Annual+Meeting+of+the+Special+Interest+Group+on+Discourse+and+Dialogue" title="Search for 'Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.0/>Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></strong><br><a href=/people/o/olivier-pietquin/>Olivier Pietquin</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a>
|
<a href=/people/v/vivian-chen/>Vivian Chen</a>
|
<a href=/people/c/casey-kennington/>Casey Kennington</a>
|
<a href=/people/d/david-vandyke/>David Vandyke</a>
|
<a href=/people/n/nina-dethlefs/>Nina Dethlefs</a>
|
<a href=/people/k/koji-inoue/>Koji Inoue</a>
|
<a href=/people/e/erik-ekstedt/>Erik Ekstedt</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=nNvH4co2qr0" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.1/>Semantic Guidance of Dialogue Generation with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/c/cheng-hsun-hsueh/>Cheng-Hsun Hsueh</a>
|
<a href=/people/w/wei-yun-ma/>Wei-Yun Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--1><div class="card-body p-3 small">Neural encoder-decoder models have shown promising performance for <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer dialogue systems</a> over the past few years. However, due to the maximum-likelihood objective for the decoder, the generated responses are often universal and safe to the point that they lack meaningful information and are no longer relevant to the post. To address this, in this paper, we propose semantic guidance using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to ensure that the generated responses indeed include the given or predicted semantics and that these <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> do not appear repeatedly in the response. Synsets, which comprise sets of manually defined synonyms, are used as the form of assigned semantics. For a given / assigned / predicted synset, only one of its <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> should appear in the generated response ; this constitutes a simple but effective semantic-control mechanism. We conduct both quantitative and qualitative evaluations, which show that the generated responses are not only higher-quality but also reflect the assigned semantic controls.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=qWLnp4tPbPM" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.4/>TripPy : A Triple Copy Strategy for Value Independent Neural Dialog State Tracking<span class=acl-fixed-case>T</span>rip<span class=acl-fixed-case>P</span>y: A Triple Copy Strategy for Value Independent Neural Dialog State Tracking</a></strong><br><a href=/people/m/michael-heck/>Michael Heck</a>
|
<a href=/people/c/carel-van-niekerk/>Carel van Niekerk</a>
|
<a href=/people/n/nurul-lubis/>Nurul Lubis</a>
|
<a href=/people/c/christian-geishauser/>Christian Geishauser</a>
|
<a href=/people/h/hsien-chin-lin/>Hsien-Chin Lin</a>
|
<a href=/people/m/marco-moresi/>Marco Moresi</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--4><div class="card-body p-3 small">Task-oriented dialog systems rely on dialog state tracking (DST) to monitor the user&#8217;s goal during the course of an interaction. Multi-domain and open-vocabulary settings complicate the task considerably and demand scalable solutions. In this paper we present a new approach to <a href=https://en.wikipedia.org/wiki/Discrete_cosine_transform>DST</a> which makes use of various copy mechanisms to fill slots with values. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has no need to maintain a list of candidate values. Instead, all values are extracted from the <a href=https://en.wikipedia.org/wiki/Context_(computing)>dialog context</a> on-the-fly. A slot is filled by one of three copy mechanisms : (1) Span prediction may extract values directly from the user input ; (2) a value may be copied from a system inform memory that keeps track of the system&#8217;s inform operations (3) a value may be copied over from a different slot that is already contained in the dialog state to resolve coreferences within and across domains. Our approach combines the advantages of span-based slot filling methods with memory methods to avoid the use of value picklists altogether. We argue that our <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> simplifies the DST task while at the same time achieving state of the art performance on various popular evaluation sets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=ipGAj_qLXz4" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.7/>MC-Saar-Instruct : a Platform for Minecraft Instruction Giving Agents<span class=acl-fixed-case>MC</span>-Saar-Instruct: a Platform for <span class=acl-fixed-case>M</span>inecraft Instruction Giving Agents</a></strong><br><a href=/people/a/arne-kohn/>Arne Köhn</a>
|
<a href=/people/j/julia-wichlacz/>Julia Wichlacz</a>
|
<a href=/people/c/christine-schafer/>Christine Schäfer</a>
|
<a href=/people/a/alvaro-torralba/>Álvaro Torralba</a>
|
<a href=/people/j/joerg-hoffmann/>Joerg Hoffmann</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--7><div class="card-body p-3 small">We present a comprehensive platform to run <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer experiments</a> where an <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> instructs a human in Minecraft, a 3D blocksworld environment. This <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> enables comparisons between different <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> by matching users to agents. It performs extensive logging and takes care of all boilerplate, allowing to easily incorporate new <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> to evaluate <a href=https://en.wikipedia.org/wiki/Intelligent_agent>them</a>. Our <a href=https://en.wikipedia.org/wiki/Environment_(systems)>environment</a> is prepared to evaluate any kind of instruction giving system, recording the <a href=https://en.wikipedia.org/wiki/Interaction>interaction</a> and all actions of the user. We provide example <a href=https://en.wikipedia.org/wiki/Architect>architects</a>, a Wizard-of-Oz architect and set-up scripts to automatically download, build and start the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.10/>Identifying Collaborative Conversations using Latent Discourse Behaviors</a></strong><br><a href=/people/a/ayush-jain/>Ayush Jain</a>
|
<a href=/people/m/maria-leonor-pacheco/>Maria Leonor Pacheco</a>
|
<a href=/people/s/steven-lancette/>Steven Lancette</a>
|
<a href=/people/m/mahak-goindani/>Mahak Goindani</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--10><div class="card-body p-3 small">In this work, we study collaborative online conversations. Such <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> are rich in content, constructive and motivated by a shared goal. Automatically identifying such conversations requires modeling complex discourse behaviors, which characterize the flow of information, <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> and <a href=https://en.wikipedia.org/wiki/Community_structure>community structure</a> within discussions. To help capture these <a href=https://en.wikipedia.org/wiki/Behavior>behaviors</a>, we define a hybrid relational model in which relevant discourse behaviors are formulated as discrete latent variables and scored using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. These <a href=https://en.wikipedia.org/wiki/Variable_(mathematics)>variables</a> provide the information needed for predicting the overall collaborative characterization of the entire conversational thread. We show that adding <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> in the form of <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> results in performance improvement, while providing a natural way to explain the decision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=KS4LPkdFyBU" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.14/>Learning and Reasoning for Robot Dialog and Navigation Tasks</a></strong><br><a href=/people/k/keting-lu/>Keting Lu</a>
|
<a href=/people/s/shiqi-zhang/>Shiqi Zhang</a>
|
<a href=/people/p/peter-stone/>Peter Stone</a>
|
<a href=/people/x/xiaoping-chen/>Xiaoping Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--14><div class="card-body p-3 small">Reinforcement learning and probabilistic reasoning algorithms aim at learning from interaction experiences and reasoning with probabilistic contextual knowledge respectively. In this research, we develop <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for robot task completions, while looking into the complementary strengths of <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> and probabilistic reasoning techniques. The robots learn from trial-and-error experiences to augment their declarative knowledge base, and the augmented knowledge can be used for speeding up the learning process in potentially different tasks. We have implemented and evaluated the developed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> using <a href=https://en.wikipedia.org/wiki/Mobile_robot>mobile robots</a> conducting dialog and navigation tasks. From the results, we see that our <a href=https://en.wikipedia.org/wiki/Robot>robot</a>&#8217;s performance can be improved by both reasoning with <a href=https://en.wikipedia.org/wiki/Knowledge>human knowledge</a> and learning from task-completion experience. More interestingly, the <a href=https://en.wikipedia.org/wiki/Robot>robot</a> was able to learn from navigation tasks to improve its dialog strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=Ds4LiqSh_EA" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.15/>An Attentive Listening System with Android ERICA : Comparison of Autonomous and WOZ Interactions<span class=acl-fixed-case>ERICA</span>: Comparison of Autonomous and <span class=acl-fixed-case>WOZ</span> Interactions</a></strong><br><a href=/people/k/koji-inoue/>Koji Inoue</a>
|
<a href=/people/d/divesh-lala/>Divesh Lala</a>
|
<a href=/people/k/kenta-yamamoto/>Kenta Yamamoto</a>
|
<a href=/people/s/shizuka-nakamura/>Shizuka Nakamura</a>
|
<a href=/people/k/katsuya-takanashi/>Katsuya Takanashi</a>
|
<a href=/people/t/tatsuya-kawahara/>Tatsuya Kawahara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--15><div class="card-body p-3 small">We describe an attentive listening system for the autonomous android robot ERICA. The proposed system generates several types of listener responses : backchannels, repeats, elaborating questions, assessments, generic sentimental responses, and generic responses. In this paper, we report a subjective experiment with 20 elderly people. First, we evaluated each system utterance excluding backchannels and generic responses, in an offline manner. It was found that most of the system utterances were linguistically appropriate, and they elicited positive reactions from the subjects. Furthermore, 58.2 % of the responses were acknowledged as being appropriate listener responses. We also compared the proposed <a href=https://en.wikipedia.org/wiki/System>system</a> with a WOZ system where a human operator was operating the robot. From the subjective evaluation, the proposed system achieved comparable scores in basic skills of attentive listening such as encouragement to talk, focused on the talk, and actively listening. It was also found that there is still a gap between the <a href=https://en.wikipedia.org/wiki/System>system</a> and the WOZ for more sophisticated skills such as dialogue understanding, showing interest, and empathy towards the user.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=OD_c-tim8JI" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.18/>Discovering Knowledge Graph Schema from Short Natural Language Text via Dialog</a></strong><br><a href=/people/s/subhasis-ghosh/>Subhasis Ghosh</a>
|
<a href=/people/a/arpita-kundu/>Arpita Kundu</a>
|
<a href=/people/a/aniket-pramanick/>Aniket Pramanick</a>
|
<a href=/people/i/indrajit-bhattacharya/>Indrajit Bhattacharya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--18><div class="card-body p-3 small">We study the problem of schema discovery for <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. We propose a <a href=https://en.wikipedia.org/wiki/Solution>solution</a> where an agent engages in multi-turn dialog with an expert for this purpose. Each mini-dialog focuses on a short natural language statement, and looks to elicit the expert&#8217;s desired schema-based interpretation of that statement, taking into account possible augmentations to the schema. The overall schema evolves by performing <a href=https://en.wikipedia.org/wiki/Dialogue>dialog</a> over a collection of such statements. We take into account the probability that the expert does not respond to a query, and model this probability as a function of the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> of the query. For such mini-dialogs with response uncertainty, we propose a dialog strategy that looks to elicit the schema over as short a dialog as possible. By combining the notion of uncertainty sampling from active learning with generalized binary search, the strategy asks the query with the highest expected reduction of entropy. We show that this significantly reduces dialog complexity while engaging the expert in meaningful dialog.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=-i9XnHcoIRc" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.19/>User Impressions of Questions to Acquire Lexical Knowledge</a></strong><br><a href=/people/k/kazunori-komatani/>Kazunori Komatani</a>
|
<a href=/people/m/mikio-nakano/>Mikio Nakano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--19><div class="card-body p-3 small">For the acquisition of knowledge through <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a>, it is crucial for systems to ask questions that do not diminish the user&#8217;s willingness to talk, i.e., that do not degrade the user&#8217;s impression. This paper reports the results of our analysis on how user impression changes depending on the types of questions to acquire lexical knowledge, that is, explicit and implicit questions, and the correctness of the content of the questions. We also analyzed how sequences of the same type of questions affect <a href=https://en.wikipedia.org/wiki/User_experience>user impression</a>. User impression scores were collected from 104 participants recruited via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> and then <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression analysis</a> was conducted. The results demonstrate that implicit questions give a good impression when their content is correct, but a bad impression otherwise. We also found that consecutive explicit questions are more annoying than implicit ones when the content of the questions is correct. Our findings reveal helpful insights for creating a <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> to avoid user impression deterioration during <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=9jerzgGw0pY" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.20/>Simulating Turn-Taking in Conversations with Delayed Transmission</a></strong><br><a href=/people/t/thilo-michael/>Thilo Michael</a>
|
<a href=/people/s/sebastian-moller/>Sebastian Möller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--20><div class="card-body p-3 small">Conversations over the telephone require timely turn-taking cues that signal the participants when to speak and when to listen. When a two-way transmission delay is introduced into such <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a>, the immediate feedback is delayed, and the interactivity of the conversation is impaired. With <a href=https://en.wikipedia.org/wiki/Delayed_speech>delayed speech</a> on each side of the transmission, different conversation realities emerge on both ends, which alters the way the participants interact with each other. Simulating conversations can give insights on turn-taking and spoken interactions between humans but can also used for analyzing and even predicting human behavior in conversations. In this paper, we simulate two types of <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> with distinct levels of <a href=https://en.wikipedia.org/wiki/Interactivity>interactivity</a>. We then introduce three levels of two-way transmission delay between the agents and compare the resulting interaction-patterns with human-to-human dialog from an empirical study. We show how the turn-taking mechanisms modeled for conversations without delay perform in scenarios with delay and identify to which extend the <a href=https://en.wikipedia.org/wiki/Simulation>simulation</a> is able to model the delayed turn-taking observed in human conversation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=IIcHVI9Kc0Y" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sigdial-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.21/>Is this Dialogue Coherent? Learning from Dialogue Acts and Entities</a></strong><br><a href=/people/a/alessandra-cervone/>Alessandra Cervone</a>
|
<a href=/people/g/giuseppe-riccardi/>Giuseppe Riccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--21><div class="card-body p-3 small">In this work, we investigate the <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>human perception of coherence</a> in open-domain dialogues. In particular, we address the problem of annotating and modeling the coherence of next-turn candidates while considering the entire history of the dialogue. First, we create the Switchboard Coherence (SWBD-Coh) corpus, a dataset of human-human spoken dialogues annotated with turn coherence ratings, where next-turn candidate utterances ratings are provided considering the full dialogue context. Our statistical analysis of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> indicates how turn coherence perception is affected by patterns of distribution of entities previously introduced and the Dialogue Acts used. Second, we experiment with different architectures to model entities, Dialogue Acts and their combination and evaluate their performance in predicting human coherence ratings on SWBD-Coh. We find that models combining both DA and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity information</a> yield the best performances both for response selection and turn coherence rating.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=1PH6JXbc3EI" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.23/>Contextualized Emotion Recognition in Conversation as Sequence Tagging</a></strong><br><a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/j/jiayu-zhang/>Jiayu Zhang</a>
|
<a href=/people/j/jun-ma/>Jun Ma</a>
|
<a href=/people/s/shaojun-wang/>Shaojun Wang</a>
|
<a href=/people/j/jing-xiao/>Jing Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--23><div class="card-body p-3 small">Emotion recognition in conversation (ERC) is an important topic for developing empathetic machines in a variety of areas including social opinion mining, <a href=https://en.wikipedia.org/wiki/Health_care>health-care</a> and so on. In this paper, we propose a method to model ERC task as sequence tagging where a Conditional Random Field (CRF) layer is leveraged to learn the emotional consistency in the conversation. We employ LSTM-based encoders that capture self and inter-speaker dependency of interlocutors to generate contextualized utterance representations which are fed into the CRF layer. For capturing long-range global context, we use a multi-layer Transformer encoder to enhance the LSTM-based encoder. Experiments show that our method benefits from modeling the emotional consistency and outperforms the current state-of-the-art methods on multiple emotion classification datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=3uC3ZJSL2Xc" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.31/>Agent-Based Dynamic Collaboration Support in a Smart Office Space</a></strong><br><a href=/people/y/yansen-wang/>Yansen Wang</a>
|
<a href=/people/r/r-charles-murray/>R. Charles Murray</a>
|
<a href=/people/h/haogang-bao/>Haogang Bao</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rose</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--31><div class="card-body p-3 small">For the past 15 years, in computer-supported collaborative learning applications, conversational agents have been used to structure group interactions in online chat-based environments. A series of experimental studies has provided an empirical foundation for the design of chat-based conversational agents that significantly improve learning over no-support control conditions and static-support control conditions. In this demo, we expand upon this foundation, bringing conversational agents to structure group interaction into physical spaces, with the specific goal of facilitating <a href=https://en.wikipedia.org/wiki/Collaboration>collaboration</a> and <a href=https://en.wikipedia.org/wiki/Learning>learning</a> in workplace scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=p8cvYEjct5g" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.34/>A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems</a></strong><br><a href=/people/h/hongjie-shi/>Hongjie Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--34><div class="card-body p-3 small">Dialog systems capable of filling slots with numerical values have wide applicability to many task-oriented applications. In this paper, we perform a particular case study on the number_of_guests slot-filling in hotel reservation domain, and propose two methods to improve current dialog system model on 1. numerical reasoning performance by training the model to predict arithmetic expressions, and 2. multi-turn question generation by introducing additional context slots. Furthermore, because the proposed methods are all based on an end-to-end trainable sequence-to-sequence (seq2seq) neural model, it is possible to achieve further performance improvement on increasing dialog logs in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=zs0yOpHWBf8" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.38/>Similarity Scoring for Dialogue Behaviour Comparison</a></strong><br><a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/w/wolfgang-maier/>Wolfgang Maier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--38><div class="card-body p-3 small">The differences in <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a> between behavioural models of voice interfaces are hard to capture using existing measures for the absolute performance of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. For instance, two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> may have a similar task success rate, but very different ways of getting there. In this paper, we propose a general <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to compute the <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> of two dialogue behaviour models and investigate different ways of computing scores on both the semantic and the textual level. Complementing absolute measures of performance, we test our scores on three different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and show the practical usability of the <a href=https://en.wikipedia.org/wiki/Measurement>measures</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigdial-1.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigdial-1--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigdial-1.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://youtube.com/watch?v=hFIHx-PqzDE" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigdial-1.39/>Collection and Analysis of Dialogues Provided by Two Speakers Acting as One</a></strong><br><a href=/people/t/tsunehiro-arimoto/>Tsunehiro Arimoto</a>
|
<a href=/people/r/ryuichiro-higashinaka/>Ryuichiro Higashinaka</a>
|
<a href=/people/k/kou-tanaka/>Kou Tanaka</a>
|
<a href=/people/t/takahito-kawanishi/>Takahito Kawanishi</a>
|
<a href=/people/h/hiroaki-sugiyama/>Hiroaki Sugiyama</a>
|
<a href=/people/h/hiroshi-sawada/>Hiroshi Sawada</a>
|
<a href=/people/h/hiroshi-ishiguro/>Hiroshi Ishiguro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigdial-1--39><div class="card-body p-3 small">We are studying a cooperation style where multiple speakers can provide both advanced dialogue services and operator education. We focus on a style in which two operators interact with a user by pretending to be a single operator. For two operators to effectively act as one, each must adjust his / her conversational content and timing to the other. In the process, we expect each operator to experience the conversational content of his / her partner as if it were his / her own, creating efficient and effective learning of the other&#8217;s skill. We analyzed this educational effect and examined whether dialogue services can be successfully provided by collecting travel guidance dialogue data from operators who give travel information to users. In this paper, we report our preliminary results on dialogue content and user satisfaction of operators and users.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>