<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W17-12.pdf>Proceedings of the Fourth Workshop on <span class=acl-fixed-case>NLP</span> for Similar Languages, Varieties and Dialects (<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial)</a></h2><p class=lead><a href=/people/p/preslav-nakov/>Preslav Nakov</a>,
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>,
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>,
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>,
<a href=/people/s/shervin-malmasi/>Shevin Malmasi</a>,
<a href=/people/a/ahmed-ali/>Ahmed Ali</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W17-12</dd><dt>Month:</dt><dd>April</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Valencia, Spain</dd><dt>Venues:</dt><dd><a href=/venues/vardial/>VarDial</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W17-12>https://aclanthology.org/W17-12</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/W17-12 title="To the current version of the paper by DOI">10.18653/v1/W17-12</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W17-12.pdf>https://aclanthology.org/W17-12.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W17-12.pdf title="Open PDF of 'Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Fourth+Workshop+on+NLP+for+Similar+Languages%2C+Varieties+and+Dialects+%28VarDial%29" title="Search for 'Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1200/>Proceedings of the Fourth Workshop on <span class=acl-fixed-case>NLP</span> for Similar Languages, Varieties and Dialects (<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial)</a></strong><br><a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/s/shervin-malmasi/>Shevin Malmasi</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1201/>Findings of the VarDial Evaluation Campaign 2017<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial Evaluation Campaign 2017</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/n/noemi-aepli/>Noëmi Aepli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1201><div class="card-body p-3 small">We present the results of the VarDial Evaluation Campaign on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects, which we organized as part of the fourth edition of the VarDial workshop at EACL&#8217;2017. This year, we included four shared tasks : Discriminating between Similar Languages (DSL), Arabic Dialect Identification (ADI), German Dialect Identification (GDI), and Cross-lingual Dependency Parsing (CLP). A total of 19 teams submitted runs across the four <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, and 15 of them wrote system description papers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1203/>Computational analysis of Gondi dialects<span class=acl-fixed-case>G</span>ondi dialects</a></strong><br><a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/p/pavel-sofroniev/>Pavel Sofroniev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1203><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational analysis</a> of <a href=https://en.wikipedia.org/wiki/Gondi_language>Gondi dialects</a> spoken in central India. We present a digitized data set of the dialect area, and analyze the <a href=https://en.wikipedia.org/wiki/Data>data</a> using different techniques from <a href=https://en.wikipedia.org/wiki/Dialectometry>dialectometry</a>, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, and <a href=https://en.wikipedia.org/wiki/Computational_biology>computational biology</a>. We show that the <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> largely agree with each other and with the earlier non-computational analyses of the language group.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1204/>Investigating Diatopic Variation in a Historical Corpus</a></strong><br><a href=/people/s/stefanie-dipper/>Stefanie Dipper</a>
|
<a href=/people/s/sandra-waldenberger/>Sandra Waldenberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1204><div class="card-body p-3 small">This paper investigates diatopic variation in a historical corpus of German. Based on equivalent word forms from different language areas, replacement rules and <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> are derived which describe the relations between these word forms. These rules and <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> are then interpreted as reflections of morphological, phonological or graphemic variation. Based on sample rules and <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a>, we show that our approach can replicate results from <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>. While previous studies were restricted to predefined word lists, or confined to single authors or texts, our approach uses a much wider range of data available in historical corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1206/>The similarity and Mutual Intelligibility between Amharic and Tigrigna Varieties<span class=acl-fixed-case>A</span>mharic and <span class=acl-fixed-case>T</span>igrigna Varieties</a></strong><br><a href=/people/t/tekabe-legesse-feleke/>Tekabe Legesse Feleke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1206><div class="card-body p-3 small">The present study has examined the similarity and the <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>mutual intelligibility</a> between <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> and Tigrigna using three tools namely <a href=https://en.wikipedia.org/wiki/Levenshtein_distance>Levenshtein distance</a>, intelligibility test and questionnaires. The study has shown that both <a href=https://en.wikipedia.org/wiki/Tigrigna_language>Tigrigna varieties</a> have almost equal phonetic and lexical distances from Amharic. The study also indicated that <a href=https://en.wikipedia.org/wiki/Amharic>Amharic speakers</a> understand less than 50 % of the two varieties. Furthermore, the study showed that Amharic speakers are more positive about the Ethiopian Tigrigna variety than the Eritrean Variety. However, their attitude towards the two <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>varieties</a> does not have an impact on their intelligibility. The <a href=https://en.wikipedia.org/wiki/Amharic>Amharic speakers</a>&#8217; familiarity to the <a href=https://en.wikipedia.org/wiki/Tigrinya_language>Tigrigna varieties</a> is largely dependent on the genealogical relation between <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> and the two <a href=https://en.wikipedia.org/wiki/Tigrinya_language>Tigrigna varieties</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1207/>Why Catalan-Spanish Neural Machine Translation? Analysis, comparison and combination with standard Rule and Phrase-based technologies<span class=acl-fixed-case>C</span>atalan-<span class=acl-fixed-case>S</span>panish Neural Machine Translation? Analysis, comparison and combination with standard Rule and Phrase-based technologies</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1207><div class="card-body p-3 small">Catalan and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> are two related languages given that both derive from <a href=https://en.wikipedia.org/wiki/Latin>Latin</a>. They share similarities in several linguistic levels including <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>, <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. This makes them particularly interesting for the MT task. Given the recent appearance and popularity of neural MT, this paper analyzes the performance of this new approach compared to the well-established rule-based and phrase-based MT systems. Experiments are reported on a large database of 180 million words. Results, in terms of standard automatic measures, show that neural MT clearly outperforms the rule-based and phrase-based MT system on in-domain test set, but it is worst in the out-of-domain test set. A naive system combination specially works for the latter. In-domain manual analysis shows that neural MT tends to improve both adequacy and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, for example, by being able to generate more natural translations instead of literal ones, choosing to the adequate target word when the source word has several translations and improving gender agreement. However, out-of-domain manual analysis shows how neural MT is more affected by unknown words or contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1208/>Kurdish Interdialect Machine Translation<span class=acl-fixed-case>K</span>urdish Interdialect Machine Translation</a></strong><br><a href=/people/h/hossein-hassani/>Hossein Hassani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1208><div class="card-body p-3 small">This research suggests a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> among two <a href=https://en.wikipedia.org/wiki/Kurdish_languages>Kurdish dialects</a>. We chose the two widely spoken dialects, <a href=https://en.wikipedia.org/wiki/Kurmanji>Kurmanji</a> and <a href=https://en.wikipedia.org/wiki/Sorani>Sorani</a>, which are considered to be mutually unintelligible. Also, despite being spoken by about 30 million people in different countries, <a href=https://en.wikipedia.org/wiki/Kurdish_languages>Kurdish</a> is among less-resourced languages. The research used bi-dialectal dictionaries and showed that the lack of <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> is not a major obstacle in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> between the two dialects. The experiments showed that the machine translated texts are comprehensible to those who do not speak the dialect. The research is the first attempt for inter-dialect machine translation in <a href=https://en.wikipedia.org/wiki/Kurdish_languages>Kurdish</a> and particularly could help in making online texts in one dialect comprehensible to those who only speak the target dialect. The results showed that the translated texts are in 71 % and 79 % cases rated as understandable for <a href=https://en.wikipedia.org/wiki/Kurmanji>Kurmanji</a> and <a href=https://en.wikipedia.org/wiki/Sorani>Sorani</a> respectively. They are rated as slightly-understandable in 29 % cases for <a href=https://en.wikipedia.org/wiki/Kurmanji>Kurmanji</a> and 21 % for <a href=https://en.wikipedia.org/wiki/Sorani>Sorani</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1209/>Twitter Language Identification Of Similar Languages And Dialects Without Ground Truth<span class=acl-fixed-case>T</span>witter Language Identification Of Similar Languages And Dialects Without Ground Truth</a></strong><br><a href=/people/j/jennifer-williams/>Jennifer Williams</a>
|
<a href=/people/c/charlie-dagli/>Charlie Dagli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1209><div class="card-body p-3 small">We present a new method to bootstrap filter Twitter language ID labels in our dataset for automatic language identification (LID). Our method combines geo-location, original Twitter LID labels, and <a href=https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk>Amazon Mechanical Turk</a> to resolve missing and unreliable labels. We are the first to compare LID classification performance using the MIRA algorithm and langid.py. We show <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> performance on different versions of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> using only Twitter data, without <a href=https://en.wikipedia.org/wiki/Ground_truth>ground truth</a>, and very few training examples. We also show how <a href=https://en.wikipedia.org/wiki/Platt_scaling>Platt Scaling</a> can be use to calibrate MIRA classifier output values into a <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> over candidate classes, making the output more intuitive. Our method allows for fine-grained distinctions between similar languages and dialects and allows us to rediscover the language composition of our Twitter dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1210/>Multi-source morphosyntactic tagging for spoken Rusyn<span class=acl-fixed-case>R</span>usyn</a></strong><br><a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/a/achim-rabus/>Achim Rabus</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1210><div class="card-body p-3 small">This paper deals with the development of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphosyntactic taggers</a> for <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>spoken varieties</a> of the Slavic minority language Rusyn. As neither annotated corpora nor parallel corpora are electronically available for <a href=https://en.wikipedia.org/wiki/Rusyn_language>Rusyn</a>, we propose to combine existing resources from the etymologically close Slavic languages Russian, <a href=https://en.wikipedia.org/wiki/Ukrainian_language>Ukrainian</a>, <a href=https://en.wikipedia.org/wiki/Slovak_language>Slovak</a>, and <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and adapt them to <a href=https://en.wikipedia.org/wiki/Rusyn_language>Rusyn</a>. Using MarMoT as tagging toolkit, we show that a tagger trained on a balanced set of the four source languages outperforms single language taggers by about 9 %, and that additional automatically induced morphosyntactic lexicons lead to further improvements. The best observed accuracies for Rusyn are 82.4 % for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and 75.5 % for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>full morphological tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1211/>Identifying dialects with textual and acoustic cues</a></strong><br><a href=/people/a/abualsoud-hanani/>Abualsoud Hanani</a>
|
<a href=/people/a/aziz-qaroush/>Aziz Qaroush</a>
|
<a href=/people/s/stephen-taylor/>Stephen Taylor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1211><div class="card-body p-3 small">We describe several systems for identifying short samples of Arabic or Swiss-German dialects, which were prepared for the shared task of the 2017 DSL Workshop (Zampieri et al., 2017). The Arabic data comprises both text and acoustic files, and our best run combined both. The Swiss-German data is text-only. Coincidently, our best runs achieved a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of nearly 63 % on both the Swiss-German and Arabic dialects tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1212/>Evaluating HeLI with Non-Linear Mappings<span class=acl-fixed-case>H</span>e<span class=acl-fixed-case>LI</span> with Non-Linear Mappings</a></strong><br><a href=/people/t/tommi-jauhiainen/>Tommi Jauhiainen</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a>
|
<a href=/people/h/heidi-jauhiainen/>Heidi Jauhiainen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1212><div class="card-body p-3 small">In this paper we describe the non-linear mappings we used with the Helsinki language identification method, HeLI, in the 4th edition of the Discriminating between Similar Languages (DSL) shared task, which was organized as part of the VarDial 2017 workshop. Our SUKI team participated on the closed track together with 10 other teams. Our <a href=https://en.wikipedia.org/wiki/System>system</a> reached the 7th position in the track. We describe the HeLI method and the non-linear mappings in <a href=https://en.wikipedia.org/wiki/Mathematical_notation>mathematical notation</a>. The HeLI method uses a <a href=https://en.wikipedia.org/wiki/Statistical_model>probabilistic model</a> with character n-grams and word-based backoff. We also describe our trials using the non-linear mappings instead of relative frequencies and we present statistics about the back-off function of the HeLI method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1213/>A Perplexity-Based Method for Similar Languages Discrimination</a></strong><br><a href=/people/p/pablo-gamallo/>Pablo Gamallo</a>
|
<a href=/people/j/jose-ramom-pichel-campos/>Jose Ramom Pichel</a>
|
<a href=/people/i/inaki-alegria/>Iñaki Alegria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1213><div class="card-body p-3 small">This article describes the system submitted by the Citius_Ixa_Imaxin team to the VarDial 2017 (DSL and GDI tasks). The strategy underlying our <a href=https://en.wikipedia.org/wiki/System>system</a> is based on a <a href=https://en.wikipedia.org/wiki/Language_distance>language distance</a> computed by means of model perplexity. The best model configuration we have tested is a <a href=https://en.wikipedia.org/wiki/Electoral_system>voting system</a> making use of several n-grams models of both words and characters, even if word unigrams turned out to be a very competitive <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with reasonable results in the tasks we have participated. An <a href=https://en.wikipedia.org/wiki/Error_analysis_(linguistics)>error analysis</a> has been performed in which we identified many test examples with no linguistic evidences to distinguish among the variants.<tex-math>n</tex-math>-grams models of both words and characters, even if word unigrams turned out to be a very competitive model with reasonable results in the tasks we have participated. An error analysis has been performed in which we identified many test examples with no linguistic evidences to distinguish among the variants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1214/>Improving the Character Ngram Model for the DSL Task with BM25 Weighting and Less Frequently Used Feature Sets<span class=acl-fixed-case>DSL</span> Task with <span class=acl-fixed-case>BM</span>25 Weighting and Less Frequently Used Feature Sets</a></strong><br><a href=/people/y/yves-bestgen/>Yves Bestgen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1214><div class="card-body p-3 small">This paper describes the system developed by the Centre for English Corpus Linguistics (CECL) to discriminating similar languages, <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language varieties</a> and dialects. Based on a <a href=https://en.wikipedia.org/wiki/Symmetric_multiprocessing>SVM</a> with character and POStag n-grams as features and the BM25 weighting scheme, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> achieved 92.7 % accuracy in the Discriminating between Similar Languages (DSL) task, ranking first among eleven systems but with a lead over the next three teams of only 0.2 %. A simpler version of the <a href=https://en.wikipedia.org/wiki/System>system</a> ranked second in the German Dialect Identification (GDI) task thanks to several ad hoc postprocessing steps. Complementary analyses carried out by a <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>cross-validation procedure</a> suggest that the BM25 weighting scheme could be competitive in this type of tasks, at least in comparison with the sublinear TF-IDF. POStag n-grams also improved the <a href=https://en.wikipedia.org/wiki/System>system</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1215/>Discriminating between Similar Languages with Word-level Convolutional Neural Networks</a></strong><br><a href=/people/m/marcelo-criscuolo/>Marcelo Criscuolo</a>
|
<a href=/people/s/sandra-aluisio/>Sandra Maria Aluísio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1215><div class="card-body p-3 small">Discriminating between Similar Languages (DSL) is a challenging task addressed at the VarDial Workshop series. We report on our participation in the DSL shared task with a <a href=https://en.wikipedia.org/wiki/Multithreading_(computer_architecture)>two-stage system</a>. In the first stage, character n-grams are used to separate <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language groups</a>, then specialized classifiers distinguish similar language varieties. We have conducted experiments with three system configurations and submitted one run for each. Our main approach is a word-level convolutional neural network (CNN) that learns task-specific vectors with minimal text preprocessing. We also experiment with multi-layer perceptron (MLP) networks and another hybrid configuration. Our best run achieved an accuracy of 90.76 %, ranking 8th among 11 participants and getting very close to the system that ranked first (less than 2 points). Even though the CNN model could not achieve the best results, it still makes a viable approach to discriminating between similar languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1216/>Cross-lingual dependency parsing for closely related languages-Helsinki’s submission to VarDial 2017<span class=acl-fixed-case>H</span>elsinki’s submission to <span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2017</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1216><div class="card-body p-3 small">This paper describes the submission from the University of Helsinki to the shared task on cross-lingual dependency parsing at VarDial 2017. We present work on annotation projection and treebank translation that gave good results for all three target languages in the test set. In particular, <a href=https://en.wikipedia.org/wiki/Slovak_language>Slovak</a> seems to work well with information coming from the Czech treebank, which is in line with related work. The attachment scores for cross-lingual models even surpass the <a href=https://en.wikipedia.org/wiki/Supervised_learning>fully supervised models</a> trained on the target language treebank. Croatian is the most difficult language in the test set and the improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> are rather modest. Norwegian works best with information coming from <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> whereas <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> contributes surprisingly little.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1217/>Discriminating between Similar Languages Using a Combination of Typed and Untyped Character N-grams and Words</a></strong><br><a href=/people/h/helena-gomez/>Helena Gomez</a>
|
<a href=/people/i/ilia-markov/>Ilia Markov</a>
|
<a href=/people/j/jorge-baptista/>Jorge Baptista</a>
|
<a href=/people/g/grigori-sidorov/>Grigori Sidorov</a>
|
<a href=/people/d/david-pinto/>David Pinto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1217><div class="card-body p-3 small">This paper presents the cic_ualg&#8217;s system that took part in the Discriminating between Similar Languages (DSL) shared task, held at the VarDial 2017 Workshop. This year&#8217;s task aims at identifying 14 languages across 6 language groups using a corpus of excerpts of journalistic texts. Two classification approaches were compared : a single-step (all languages) approach and a two-step (language group and then languages within the group) approach. Features exploited include lexical features (unigrams of words) and character n-grams. Besides traditional (untyped) character n-grams, we introduce typed character n-grams in the DSL task. Experiments were carried out with different feature representation methods (binary and raw term frequency), frequency threshold values, and machine-learning algorithms Support Vector Machines (SVM) and Multinomial Naive Bayes (MNB). Our best run in the DSL task achieved 91.46 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1218 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1218/>Tbingen system in VarDial 2017 shared task : experiments with <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a> and cross-lingual parsing<span class=acl-fixed-case>T</span>übingen system in <span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2017 shared task: experiments with language identification and cross-lingual parsing</a></strong><br><a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/t/taraka-rama/>Taraka Rama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1218><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>systems</a> and results on VarDial 2017 shared tasks. Besides three language / dialect discrimination tasks, we also participated in the cross-lingual dependency parsing (CLP) task using a simple methodology which we also briefly describe in this paper. For all the discrimination tasks, we used linear SVMs with character and word features. The <a href=https://en.wikipedia.org/wiki/System>system</a> achieves competitive results among other <a href=https://en.wikipedia.org/wiki/System>systems</a> in the shared <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We also report additional experiments with neural network models. The performance of neural network models was close but always below the corresponding SVM classifiers in the discrimination tasks. For the cross-lingual parsing task, we experimented with an approach based on automatically translating the source treebank to the target language, and training a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> on the translated treebank. We used off-the-shelf tools for both <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Despite achieving better-than-baseline results, our scores in CLP tasks were substantially lower than the scores of the other participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1219 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1219/>When Sparse Traditional Models Outperform Dense Neural Networks : the Curious Case of Discriminating between Similar Languages</a></strong><br><a href=/people/m/maria-medvedeva/>Maria Medvedeva</a>
|
<a href=/people/m/martin-kroon/>Martin Kroon</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1219><div class="card-body p-3 small">We present the results of our participation in the VarDial 4 shared task on discriminating closely related languages. Our submission includes simple traditional models using linear support vector machines (SVMs) and a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network (NN)</a>. The main idea was to leverage language group information. We did so with a two-layer approach in the traditional <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and a multi-task objective in the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network case</a>. Our results confirm earlier findings : simple traditional models outperform <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> consistently for this task, at least given the amount of systems we could examine in the available time. Our two-layer linear SVM ranked 2nd in the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1220 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1220/>German Dialect Identification in Interview Transcriptions<span class=acl-fixed-case>G</span>erman Dialect Identification in Interview Transcriptions</a></strong><br><a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1220><div class="card-body p-3 small">This paper presents three systems submitted to the German Dialect Identification (GDI) task at the VarDial Evaluation Campaign 2017. The task consists of training <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to identify the dialect of Swiss-German speech transcripts. The dialects included in the GDI dataset are <a href=https://en.wikipedia.org/wiki/Canton_of_Basel-Stadt>Basel</a>, <a href=https://en.wikipedia.org/wiki/Canton_of_Bern>Bern</a>, Lucerne, and <a href=https://en.wikipedia.org/wiki/Canton_of_Z&#252;rich>Zurich</a>. The three systems we submitted are based on : a plurality ensemble, a mean probability ensemble, and a meta-classifier trained on character and word n-grams. The best results were obtained by the <a href=https://en.wikipedia.org/wiki/Meta-analysis>meta-classifier</a> achieving 68.1 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and 66.2 % <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a>, ranking first among the 10 teams which participated in the GDI shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1221 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1221/>CLUZH at VarDial GDI 2017 : Testing a Variety of Machine Learning Tools for the Classification of Swiss German Dialects<span class=acl-fixed-case>CLUZH</span> at <span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial <span class=acl-fixed-case>GDI</span> 2017: Testing a Variety of Machine Learning Tools for the Classification of <span class=acl-fixed-case>S</span>wiss <span class=acl-fixed-case>G</span>erman Dialects</a></strong><br><a href=/people/s/simon-clematide/>Simon Clematide</a>
|
<a href=/people/p/peter-makarov/>Peter Makarov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1221><div class="card-body p-3 small">Our submissions for the GDI 2017 Shared Task are the results from three different types of classifiers : Nave Bayes, Conditional Random Fields (CRF), and Support Vector Machine (SVM). Our CRF-based run achieves a weighted F1 score of 65 % (third rank) being beaten by the best system by 0.9 %. Measured by classification accuracy, our ensemble run (Nave Bayes, CRF, SVM) reaches 67 % (second rank) being 1 % lower than the best system. We also describe our experiments with Recurrent Neural Network (RNN) architectures. Since they performed worse than our non-neural approaches we did not include them in the submission.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1222 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1222/>Arabic Dialect Identification Using iVectors and ASR Transcripts<span class=acl-fixed-case>A</span>rabic Dialect Identification Using i<span class=acl-fixed-case>V</span>ectors and <span class=acl-fixed-case>ASR</span> Transcripts</a></strong><br><a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1222><div class="card-body p-3 small">This paper presents the systems submitted by the MAZA team to the Arabic Dialect Identification (ADI) shared task at the VarDial Evaluation Campaign 2017. The goal of the task is to evaluate <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational models</a> to identify the <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>dialect of Arabic utterances</a> using both <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>audio and text transcriptions</a>. The ADI shared task dataset included Modern Standard Arabic (MSA) and four Arabic dialects : <a href=https://en.wikipedia.org/wiki/Egyptian_Arabic>Egyptian</a>, <a href=https://en.wikipedia.org/wiki/Gulf_Arabic>Gulf</a>, <a href=https://en.wikipedia.org/wiki/Levantine_Arabic>Levantine</a>, and <a href=https://en.wikipedia.org/wiki/North_African_Arabic>North-African</a>. The three systems submitted by MAZA are based on combinations of multiple machine learning classifiers arranged as (1) voting ensemble ; (2) mean probability ensemble ; (3) meta-classifier. The best results were obtained by the <a href=https://en.wikipedia.org/wiki/Meta-analysis>meta-classifier</a> achieving 71.7 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, ranking second among the six teams which participated in the ADI shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1224 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1224/>Exploring Lexical and Syntactic Features for Language Variety Identification</a></strong><br><a href=/people/c/chris-van-der-lee/>Chris van der Lee</a>
|
<a href=/people/a/antal-van-den-bosch/>Antal van den Bosch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1224><div class="card-body p-3 small">We present a method to discriminate between texts written in either the Netherlandic or the Flemish variant of the <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch language</a>. The method draws on a feature bundle representing text statistics, <a href=https://en.wikipedia.org/wiki/Syntax>syntactic features</a>, and <a href=https://en.wikipedia.org/wiki/N-gram>word n-grams</a>. Text statistics include average word length and <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>, while syntactic features include ratios of function words and part-of-speech n-grams. The effectiveness of the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> was measured by classifying <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch subtitles</a> developed for either <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> or <a href=https://en.wikipedia.org/wiki/Vlaamse_Radio-_en_Televisieomroeporganisatie>Flemish television</a>. Several <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> were compared as well as feature combination methods in order to find the optimal generalization performance. A machine-learning meta classifier based on <a href=https://en.wikipedia.org/wiki/AdaBoost>AdaBoost</a> attained the best <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.92.<tex-math>n</tex-math>-grams. Text statistics include average word length and sentence length, while syntactic features include ratios of function words and part-of-speech <tex-math>n</tex-math>-grams. The effectiveness of the classifier was measured by classifying Dutch subtitles developed for either Dutch or Flemish television. Several machine learning algorithms were compared as well as feature combination methods in order to find the optimal generalization performance. A machine-learning meta classifier based on AdaBoost attained the best F-score of 0.92.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>