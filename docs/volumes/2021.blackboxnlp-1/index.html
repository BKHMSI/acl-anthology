<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</h2><p class=lead><a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>,
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>,
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a>,
<a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>,
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>,
<a href=/people/y/yuval-pinter/>Yuval Pinter</a>,
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.blackboxnlp-1</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Punta Cana, Dominican Republic</dd><dt>Venues:</dt><dd><a href=/venues/blackboxnlp/>BlackboxNLP</a>
| <a href=/venues/emnlp/>EMNLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.blackboxnlp-1>https://aclanthology.org/2021.blackboxnlp-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Fourth+BlackboxNLP+Workshop+on+Analyzing+and+Interpreting+Neural+Networks+for+NLP" title="Search for 'Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.0/>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></strong><br><a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a>
|
<a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.3/>Does <a href=https://en.wikipedia.org/wiki/Knowledge>External Knowledge</a> Help Explainable <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a>? Automatic Evaluation vs. Human Ratings</a></strong><br><a href=/people/h/hendrik-schuff/>Hendrik Schuff</a>
|
<a href=/people/h/hsiu-yu-yang/>Hsiu-Yu Yang</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--3><div class="card-body p-3 small">Natural language inference (NLI) requires models to learn and apply commonsense knowledge. These reasoning abilities are particularly important for explainable NLI systems that generate a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language explanation</a> in addition to their label prediction. The integration of external knowledge has been shown to improve NLI systems, here we investigate whether it can also improve their explanation capabilities. For this, we investigate different sources of external knowledge and evaluate the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on in-domain data as well as on special transfer datasets that are designed to assess fine-grained reasoning capabilities. We find that different sources of knowledge have a different effect on reasoning abilities, for example, <a href=https://en.wikipedia.org/wiki/Implicit_knowledge>implicit knowledge</a> stored in language models can hinder reasoning on numbers and <a href=https://en.wikipedia.org/wiki/Negation>negations</a>. Finally, we conduct the largest and most fine-grained explainable NLI crowdsourcing study to date. It reveals that even large differences in automatic performance scores do neither reflect in human ratings of label, explanation, commonsense nor <a href=https://en.wikipedia.org/wiki/Grammar>grammar correctness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.5/>On the Limits of <a href=https://en.wikipedia.org/wiki/Minimal_pairs>Minimal Pairs</a> in Contrastive Evaluation</a></strong><br><a href=/people/j/jannis-vamvas/>Jannis Vamvas</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--5><div class="card-body p-3 small">Minimal sentence pairs are frequently used to analyze the behavior of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. It is often assumed that model behavior on contrastive pairs is predictive of <a href=https://en.wikipedia.org/wiki/Behavioral_model>model behavior</a> at large. We argue that two conditions are necessary for this assumption to hold : First, a tested hypothesis should be well-motivated, since experiments show that contrastive evaluation can lead to false positives. Secondly, test data should be chosen such as to minimize distributional discrepancy between evaluation time and deployment time. For a good approximation of deployment-time decoding, we recommend that <a href=https://en.wikipedia.org/wiki/Minimal_pairs>minimal pairs</a> are created based on machine-generated text, as opposed to human-written references. We present a contrastive evaluation suite for EnglishGerman MT that implements this recommendation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.6/>What Models Know About Their Attackers : Deriving Attacker Information From Latent Representations</a></strong><br><a href=/people/z/zhouhang-xie/>Zhouhang Xie</a>
|
<a href=/people/j/jonathan-brophy/>Jonathan Brophy</a>
|
<a href=/people/a/adam-noack/>Adam Noack</a>
|
<a href=/people/w/wencong-you/>Wencong You</a>
|
<a href=/people/k/kalyani-asthana/>Kalyani Asthana</a>
|
<a href=/people/c/carter-perkins/>Carter Perkins</a>
|
<a href=/people/s/sabrina-reis/>Sabrina Reis</a>
|
<a href=/people/z/zayd-hammoudeh/>Zayd Hammoudeh</a>
|
<a href=/people/d/daniel-lowd/>Daniel Lowd</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--6><div class="card-body p-3 small">Adversarial attacks curated against NLP models are increasingly becoming practical threats. Although various methods have been developed to detect adversarial attacks, securing learning-based NLP systems in practice would require more than identifying and evading perturbed instances. To address these issues, we propose a new set of adversary identification tasks, Attacker Attribute Classification via Textual Analysis (AACTA), that attempts to obtain more detailed information about the attackers from adversarial texts. Specifically, given a piece of adversarial text, we hope to accomplish tasks such as localizing perturbed tokens, identifying the attacker&#8217;s access level to the target model, determining the evasion mechanism imposed, and specifying the perturbation type employed by the attacking algorithm. Our contributions are as follows : we formalize the task of classifying attacker attributes, and create a benchmark on various target models from sentiment classification and abuse detection domains. We show that signals from BERT models and target models can be used to train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> that reveal the properties of the attacking algorithms. We demonstrate that adversarial attacks leave interpretable traces in both feature spaces of pre-trained language models and target models, making AACTA a promising direction towards more trustworthy NLP systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.8/>ProSPer : Probing Human and Neural Network Language Model Understanding of Spatial Perspective<span class=acl-fixed-case>P</span>ro<span class=acl-fixed-case>SP</span>er: Probing Human and Neural Network Language Model Understanding of Spatial Perspective</a></strong><br><a href=/people/t/tessa-masis/>Tessa Masis</a>
|
<a href=/people/c/carolyn-anderson/>Carolyn Anderson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--8><div class="card-body p-3 small">Understanding perspectival language is important for applications like <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> and <a href=https://en.wikipedia.org/wiki/Human&#8211;robot_interaction>human-robot interaction</a>. We propose a probe task that explores how well <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> understand <a href=https://en.wikipedia.org/wiki/Perspective_(graphical)>spatial perspective</a>. We present a dataset for evaluating perspective inference in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, ProSPer, and use it to explore how humans and Transformer-based language models infer perspective. Although the best bidirectional model performs similarly to humans, they display different strengths : <a href=https://en.wikipedia.org/wiki/Human>humans</a> outperform <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> in conversational contexts, while RoBERTa excels at written genres.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.10/>Transferring Knowledge from Vision to Language : How to Achieve it and how to Measure it?</a></strong><br><a href=/people/t/tobias-norlund/>Tobias Norlund</a>
|
<a href=/people/l/lovisa-hagstrom/>Lovisa Hagström</a>
|
<a href=/people/r/richard-johansson/>Richard Johansson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--10><div class="card-body p-3 small">Large language models are known to suffer from the hallucination problem in that they are prone to output statements that are false or inconsistent, indicating a lack of knowledge. A proposed solution to this is to provide the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with additional data modalities that complements the knowledge obtained through text. We investigate the use of visual data to complement the knowledge of large language models by proposing a method for evaluating visual knowledge transfer to text for uni- or multimodal language models. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is based on two steps, 1) a novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> querying for <a href=https://en.wikipedia.org/wiki/Memory_color>knowledge of memory colors</a>, i.e. typical colors of well-known objects, and 2) filtering of model training data to clearly separate knowledge contributions. Additionally, we introduce a <a href=https://en.wikipedia.org/wiki/Modeling_language>model architecture</a> that involves a visual imagination step and evaluate it with our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. We find that our method can successfully be used to measure visual knowledge transfer capabilities in models and that our novel model architecture shows promising results for leveraging multimodal knowledge in a unimodal setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.13/>A howling success or a working sea? Testing what BERT knows about metaphors<span class=acl-fixed-case>BERT</span> knows about metaphors</a></strong><br><a href=/people/p/paolo-pedinotti/>Paolo Pedinotti</a>
|
<a href=/people/e/eliana-di-palma/>Eliana Di Palma</a>
|
<a href=/people/l/ludovica-cerini/>Ludovica Cerini</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--13><div class="card-body p-3 small">Metaphor is a widespread linguistic and cognitive phenomenon that is ruled by mechanisms which have received attention in the literature. Transformer Language Models such as BERT have brought improvements in metaphor-related tasks. However, they have been used only in application contexts, while their knowledge of the phenomenon has not been analyzed. To test what BERT knows about <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a>, we challenge it on a new dataset that we designed to test various aspects of this phenomenon such as variations in linguistic structure, variations in <a href=https://en.wikipedia.org/wiki/Convention_(norm)>conventionality</a>, the boundaries of the plausibility of a metaphor and the interpretations that we attribute to metaphoric expressions. Results bring out some tendencies that suggest that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can reproduce some human intuitions about <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.14/>How Length Prediction Influence the Performance of Non-Autoregressive Translation?</a></strong><br><a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/g/guo-jiaxin/>Guo Jiaxin</a>
|
<a href=/people/y/yuxia-wang/>Yuxia Wang</a>
|
<a href=/people/y/yimeng-chen/>Yimeng Chen</a>
|
<a href=/people/s/su-chang/>Su Chang</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/s/shimin-tao/>Shimin Tao</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--14><div class="card-body p-3 small">Length prediction is a special task in a series of NAT models where target length has to be determined before generation. However, the performance of length prediction and its influence on translation quality has seldom been discussed. In this paper, we present comprehensive analyses on length prediction task of NAT, aiming to find the factors that influence performance, as well as how it associates with translation quality. We mainly perform experiments based on Conditional Masked Language Model (CMLM) (Ghazvininejad et al., 2019), a representative NAT model, and evaluate it on two language pairs, En-De and En-Ro. We draw two conclusions : 1) The performance of length prediction is mainly influenced by properties of language pairs such as alignment pattern, <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> or intrinsic length ratio, and is also affected by the usage of knowledge distilled data. 2) There is a positive correlation between the performance of the length prediction and the BLEU score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.15/>On the Language-specificity of Multilingual BERT and the Impact of <a href=https://en.wikipedia.org/wiki/Fine-tuning>Fine-tuning</a><span class=acl-fixed-case>BERT</span> and the Impact of Fine-tuning</a></strong><br><a href=/people/m/marc-tanti/>Marc Tanti</a>
|
<a href=/people/l/lonneke-van-der-plas/>Lonneke van der Plas</a>
|
<a href=/people/c/claudia-borg/>Claudia Borg</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--15><div class="card-body p-3 small">Recent work has shown evidence that the <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> acquired by multilingual BERT (mBERT) has two components : a language-specific and a language-neutral one. This paper analyses the relationship between them, in the context of fine-tuning on two tasks POS tagging and natural language inference which require the model to bring to bear different degrees of language-specific knowledge. Visualisations reveal that mBERT loses the ability to cluster representations by language after <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, a result that is supported by evidence from <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a> experiments. However, further experiments on &#8216;unlearning&#8217; language-specific representations using gradient reversal and iterative adversarial learning are shown not to add further improvement to the language-independent component over and above the effect of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. The results presented here suggest that the process of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> causes a reorganisation of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s limited representational capacity, enhancing language-independent representations at the expense of language-specific ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.18/>Variation and generality in encoding of syntactic anomaly information in sentence embeddings</a></strong><br><a href=/people/q/qinxuan-wu/>Qinxuan Wu</a>
|
<a href=/people/a/allyson-ettinger/>Allyson Ettinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--18><div class="card-body p-3 small">While sentence anomalies have been applied periodically for testing in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, we have yet to establish a picture of the precise status of anomaly information in representations from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a>. In this paper we aim to fill two primary gaps, focusing on the domain of syntactic anomalies. First, we explore fine-grained differences in anomaly encoding by designing probing tasks that vary the hierarchical level at which anomalies occur in a sentence. Second, we test not only models&#8217; ability to detect a given <a href=https://en.wikipedia.org/wiki/Anomaly_(physics)>anomaly</a>, but also the generality of the detected anomaly signal, by examining transfer between distinct anomaly types. Results suggest that all <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> encode some information supporting <a href=https://en.wikipedia.org/wiki/Anomaly_detection>anomaly detection</a>, but detection performance varies between anomalies, and only representations from more re- cent transformer models show signs of generalized knowledge of anomalies. Follow-up analyses support the notion that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> pick up on a legitimate, general notion of sentence oddity, while coarser-grained word position information is likely also a contributor to the observed <a href=https://en.wikipedia.org/wiki/Anomaly_detection>anomaly detection</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.19/>Enhancing Interpretable Clauses Semantically using Pretrained Word Representation</a></strong><br><a href=/people/r/rohan-kumar-yadav/>Rohan Kumar Yadav</a>
|
<a href=/people/l/lei-jiao/>Lei Jiao</a>
|
<a href=/people/o/ole-christoffer-granmo/>Ole-Christoffer Granmo</a>
|
<a href=/people/m/morten-goodwin/>Morten Goodwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--19><div class="card-body p-3 small">Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based on <a href=https://en.wikipedia.org/wiki/Propositional_calculus>propositional logic</a>, which has demonstrated competitive performance in many Natural Language Processing (NLP) tasks, including <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, text classification, and <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a>. To obtain human-level interpretability, legacy TM employs Boolean input features such as bag-of-words (BOW). However, the BOW representation makes it difficult to use any pre-trained information, for instance, <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and GloVe word representations. This restriction has constrained the performance of TM compared to deep neural networks (DNNs) in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. To reduce the performance gap, in this paper, we propose a novel way of using pre-trained word representations for TM. The approach significantly enhances the performance and interpretability of TM. We achieve this by extracting semantically related words from pre-trained word representations as input features to the TM. Our experiments show that the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the proposed approach is significantly higher than the previous BOW-based TM, reaching the level of DNN-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.21/>An in-depth look at Euclidean disk embeddings for structure preserving parsing<span class=acl-fixed-case>E</span>uclidean disk embeddings for structure preserving parsing</a></strong><br><a href=/people/f/federico-fancellu/>Federico Fancellu</a>
|
<a href=/people/l/lan-xiao/>Lan Xiao</a>
|
<a href=/people/a/allan-jepson/>Allan Jepson</a>
|
<a href=/people/a/afsaneh-fazly/>Afsaneh Fazly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--21><div class="card-body p-3 small">Preserving the structural properties of trees or graphs when embedding them into a <a href=https://en.wikipedia.org/wiki/Metric_space>metric space</a> allows for a high degree of <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>, and has been shown beneficial for <a href=https://en.wikipedia.org/wiki/Downstream_(computer_science)>downstream tasks</a> (e.g., hypernym detection, natural language inference, multimodal retrieval). However, whereas the majority of prior work looks at using structure-preserving embeddings when encoding a <a href=https://en.wikipedia.org/wiki/Structure>structure</a> given as input, e.g., <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> (Fellbaum, 1998), there is little exploration on how to use such <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> when predicting one. We address this gap for two structure generation tasks, namely dependency and semantic parsing. We test the applicability of disk embeddings (Suzuki et al., 2019) that has been proposed for embedding Directed Acyclic Graphs (DAGs) but has not been tested on tasks that generate such structures. Our experimental results show that for both tasks the original disk embedding formulation leads to much worse performance when compared to non-structure-preserving baselines. We propose enhancements to this formulation and show that they almost close the performance gap for dependency parsing. However, the gap still remains notable for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> due to the complexity of meaning representation graphs, suggesting a challenge for generating interpretable semantic parse representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.26/>Assessing the Generalization Capacity of Pre-trained Language Models through Japanese Adversarial Natural Language Inference<span class=acl-fixed-case>J</span>apanese Adversarial Natural Language Inference</a></strong><br><a href=/people/h/hitomi-yanaka/>Hitomi Yanaka</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--26><div class="card-body p-3 small">Despite the success of multilingual pre-trained language models, it remains unclear to what extent these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> have human-like generalization capacity across languages. The aim of this study is to investigate the out-of-distribution generalization of pre-trained language models through Natural Language Inference (NLI) in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, the typological properties of which are different from those of <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We introduce a synthetically generated Japanese NLI dataset, called the Japanese Adversarial NLI (JaNLI) dataset, which is inspired by the English HANS dataset and is designed to require understanding of Japanese linguistic phenomena and illuminate the vulnerabilities of models. Through a series of experiments to evaluate the <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> performance of both Japanese and multilingual BERT models, we demonstrate that there is much room to improve current <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on Japanese NLI tasks. Furthermore, a comparison of human performance and model performance on the different types of garden-path sentences in the JaNLI dataset shows that structural phenomena that ease interpretation of garden-path sentences for human readers do not help <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> in the same way, highlighting a difference between human readers and the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.27/>Investigating Negation in Pre-trained Vision-and-language Models</a></strong><br><a href=/people/r/radina-dobreva/>Radina Dobreva</a>
|
<a href=/people/f/frank-keller/>Frank Keller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--27><div class="card-body p-3 small">Pre-trained vision-and-language models have achieved impressive results on a variety of tasks, including ones that require complex reasoning beyond <a href=https://en.wikipedia.org/wiki/Outline_of_object_recognition>object recognition</a>. However, little is known about how they achieve these results or what their limitations are. In this paper, we focus on a particular linguistic capability, namely the understanding of negation. We borrow techniques from the analysis of language models to investigate the ability of pre-trained vision-and-language models to handle <a href=https://en.wikipedia.org/wiki/Negation>negation</a>. We find that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> severely underperform in the presence of <a href=https://en.wikipedia.org/wiki/Negation>negation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.30/>Learning Mathematical Properties of Integers</a></strong><br><a href=/people/m/maria-ryskina/>Maria Ryskina</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--30><div class="card-body p-3 small">Embedding words in <a href=https://en.wikipedia.org/wiki/High-dimensional_space>high-dimensional vector spaces</a> has proven valuable in many <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language applications</a>. In this work, we investigate whether similarly-trained embeddings of integers can capture concepts that are useful for <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>mathematical applications</a>. We probe the integer embeddings for mathematical knowledge, apply them to a set of numerical reasoning tasks, and show that by learning the representations from mathematical sequence data, we can substantially improve over number embeddings learned from English text corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.34" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.34/>An Investigation of Language Model Interpretability via Sentence Editing</a></strong><br><a href=/people/s/samuel-stevens/>Samuel Stevens</a>
|
<a href=/people/y/yu-su/>Yu Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--34><div class="card-body p-3 small">Pre-trained language models (PLMs) like BERT are being used for almost all language-related tasks, but interpreting their behavior still remains a significant challenge and many important questions remain largely unanswered. In this work, we re-purpose a sentence editing dataset, where faithful high-quality human rationales can be automatically extracted and compared with extracted model rationales, as a new testbed for interpretability. This enables us to conduct a systematic investigation on an array of questions regarding PLMs&#8217; interpretability, including the role of pre-training procedure, comparison of rationale extraction methods, and different layers in the PLM. The investigation generates new insights, for example, contrary to the common understanding, we find that attention weights correlate well with <a href=https://en.wikipedia.org/wiki/Rationality>human rationales</a> and work better than gradient-based saliency in extracting model rationales. Both the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and code will be released to facilitate future interpretability research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.blackboxnlp-1.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.37/>Controlled tasks for model analysis : Retrieving discrete information from sequences</a></strong><br><a href=/people/i/ionut-sorodoc/>Ionut-Teodor Sorodoc</a>
|
<a href=/people/g/gemma-boleda/>Gemma Boleda</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--37><div class="card-body p-3 small">In recent years, the NLP community has shown increasing interest in analysing how <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> work. Given that large models trained on complex <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are difficult to inspect, some of this work has focused on controlled tasks that emulate specific aspects of <a href=https://en.wikipedia.org/wiki/Language>language</a>. We propose a new set of such controlled tasks to explore a crucial aspect of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> that has not received enough attention : the need to retrieve discrete information from sequences. We also study model behavior on the tasks with simple instantiations of <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformers</a> and <a href=https://en.wikipedia.org/wiki/Light-emitting_diode>LSTMs</a>. Our results highlight the beneficial role of decoder attention and its sometimes unexpected interaction with other components. Moreover, we show that, for most of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, these simple models still show significant difficulties. We hope that the community will take up the analysis possibilities that our tasks afford, and that a clearer understanding of model behavior on the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> will lead to better and more transparent models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--blackboxnlp-1--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.blackboxnlp-1.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.40/>Do <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> Know the Way to Rome?<span class=acl-fixed-case>R</span>ome?</a></strong><br><a href=/people/b/bastien-lietard/>Bastien Liétard</a>
|
<a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--blackboxnlp-1--40><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Global_geometry>global geometry</a> of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> is important for a range of applications, but language model probes tend to evaluate rather local relations, for which ground truths are easily obtained. In this paper we exploit the fact that in <a href=https://en.wikipedia.org/wiki/Geography>geography</a>, ground truths are available beyond local relations. In a series of experiments, we evaluate the extent to which <a href=https://en.wikipedia.org/wiki/Language_model>language model representations</a> of city and country names are isomorphic to real-world geography, e.g., if you tell a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> where <a href=https://en.wikipedia.org/wiki/Paris>Paris</a> and <a href=https://en.wikipedia.org/wiki/Berlin>Berlin</a> are, does it know the way to Rome? We find that language models generally encode limited geographic information, but with larger <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> performing the best, suggesting that geographic knowledge can be induced from higher-order co-occurrence statistics.<i>can</i> be induced from higher-order co-occurrence statistics.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>