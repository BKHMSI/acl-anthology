<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 14th International Conference on Spoken Language Translation - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2017.iwslt-1.pdf>Proceedings of the 14th International Conference on Spoken Language Translation</a></h2><p class=lead><a href=/people/s/sakriani-sakti/>Sakriani Sakti</a>,
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2017.iwslt-1</dd><dt>Month:</dt><dd>December 14-15</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Tokyo, Japan</dd><dt>Venue:</dt><dd><a href=/venues/iwslt/>IWSLT</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigslt/>SIGSLT</a></dd><dt>Publisher:</dt><dd>International Workshop on Spoken Language Translation</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2017.iwslt-1>https://aclanthology.org/2017.iwslt-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2017.iwslt-1.pdf>https://aclanthology.org/2017.iwslt-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2017.iwslt-1.pdf title="Open PDF of 'Proceedings of the 14th International Conference on Spoken Language Translation'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+14th+International+Conference+on+Spoken+Language+Translation" title="Search for 'Proceedings of the 14th International Conference on Spoken Language Translation' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.0/>Proceedings of the 14th International Conference on Spoken Language Translation</a></strong><br><a href=/people/s/sakriani-sakti/>Sakriani Sakti</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.1/>Overview of the IWSLT 2017 Evaluation Campaign<span class=acl-fixed-case>IWSLT</span> 2017 Evaluation Campaign</a></strong><br><a href=/people/m/mauro-cettolo/>Mauro Cettolo</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/l/luisa-bentivogli/>Luisa Bentivogli</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--1><div class="card-body p-3 small">The IWSLT 2017 evaluation campaign has organised three tasks. The Multilingual task, which is about training <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> handling many-to-many language directions, including so-called zero-shot directions. The Dialogue task, which calls for the integration of context information in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, in order to resolve <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphoric references</a> that typically occur in human-human dialogue turns. And, finally, the Lecture task, which offers the challenge of automatically transcribing and translating real-life university lectures. Following the tradition of these reports, we will described all tasks in detail and present the results of all runs submitted by their participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.2/>Going beyond zero-shot MT : combining phonological, morphological and semantic factors. The UdS-DFKI System at IWSLT 2017<span class=acl-fixed-case>MT</span>: combining phonological, morphological and semantic factors. The <span class=acl-fixed-case>U</span>d<span class=acl-fixed-case>S</span>-<span class=acl-fixed-case>DFKI</span> System at <span class=acl-fixed-case>IWSLT</span> 2017</a></strong><br><a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--2><div class="card-body p-3 small">This paper describes the UdS-DFKI participation to the multilingual task of the IWSLT Evaluation 2017. Our approach is based on factored multilingual neural translation systems following the small data and zero-shot training conditions. Our systems are designed to fully exploit multilinguality by including factors that increase the number of common elements among languages such as phonetic coarse encodings and synsets, besides shallow part-of-speech tags, <a href=https://en.wikipedia.org/wiki/Word_stem>stems</a> and <a href=https://en.wikipedia.org/wiki/Lemma_(morphology)>lemmas</a>. Document level information is also considered by including the topic of every document. This approach improves a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> without any additional factor for all the language pairs and even allows beyond-zero-shot translation. That is, the translation from unseen languages is possible thanks to the common elements especially <a href=https://en.wikipedia.org/wiki/Synonym>synsets</a> in our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> among languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.3/>The Samsung and University of Edinburgh’s submission to IWSLT17<span class=acl-fixed-case>S</span>amsung and <span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s submission to <span class=acl-fixed-case>IWSLT</span>17</a></strong><br><a href=/people/p/pawel-przybysz/>Pawel Przybysz</a>
|
<a href=/people/m/marcin-chochowski/>Marcin Chochowski</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--3><div class="card-body p-3 small">This paper describes the joint submission of Samsung Research and Development, Warsaw, Poland and the University of Edinburgh team to the IWSLT MT task for TED talks. We took part in two translation directions, en-de and de-en. We also participated in the en-de and de-en lectures SLT task. The models have been trained with an attentional encoder-decoder model using the BiDeep model in <a href=https://en.wikipedia.org/wiki/Nematus>Nematus</a>. We filtered the training data to reduce the problem of <a href=https://en.wikipedia.org/wiki/Noisy_data>noisy data</a>, and we use back-translated monolingual data for domain-adaptation. We demonstrate the effectiveness of the different techniques that we applied via <a href=https://en.wikipedia.org/wiki/Ablation>ablation studies</a>. Our submission system outperforms our <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, and last year&#8217;s University of Edinburgh submission to IWSLT, by more than 5 <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>BLEU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.4/>The RWTH Aachen Machine Translation Systems for IWSLT 2017<span class=acl-fixed-case>RWTH</span> <span class=acl-fixed-case>A</span>achen Machine Translation Systems for <span class=acl-fixed-case>IWSLT</span> 2017</a></strong><br><a href=/people/p/parnia-bahar/>Parnia Bahar</a>
|
<a href=/people/j/jan-rosendahl/>Jan Rosendahl</a>
|
<a href=/people/n/nick-rossenbach/>Nick Rossenbach</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--4><div class="card-body p-3 small">This work describes the Neural Machine Translation (NMT) system of the RWTH Aachen University developed for the English$German tracks of the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2017. We use NMT systems which are augmented by state-of-the-art <a href=https://en.wikipedia.org/wiki/Plug-in_(computing)>extensions</a>. Furthermore, we experiment with techniques that include data filtering, a larger vocabulary, two extensions to the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> and <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>. Using these methods, we can show considerable improvements over the respective baseline systems and our IWSLT 2016 submission.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.5/>FBK’s Multilingual Neural Machine Translation System for IWSLT 2017<span class=acl-fixed-case>FBK</span>’s Multilingual Neural Machine Translation System for <span class=acl-fixed-case>IWSLT</span> 2017</a></strong><br><a href=/people/s/surafel-m-lakew/>Surafel M. Lakew</a>
|
<a href=/people/q/quintino-f-lotito/>Quintino F. Lotito</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--5><div class="card-body p-3 small">Neural Machine Translation has been shown to enable <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and cross-lingual knowledge transfer across multiple language directions using a single multilingual model. Focusing on this multilingual translation scenario, this work summarizes FBK&#8217;s participation in the IWSLT 2017 shared task. Our submissions rely on two multilingual systems trained on five languages (English, <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, and Romanian). The first one is a 20 language direction model, which handles all possible combinations of the five languages. The second multilingual system is trained only on 16 directions, leaving the others as zero-shot translation directions (i.e representing a more complex inference task on language pairs not seen at training time). More specifically, our zero-shot directions are <a href=https://en.wikipedia.org/wiki/German_language>Dutch$German</a> and <a href=https://en.wikipedia.org/wiki/Romanian_language>Italian$Romanian</a> (resulting in four language combinations). Despite the small amount of parallel data used for training these systems, the resulting multilingual models are effective, even in comparison with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained separately for every language pair (i.e. in more favorable conditions). We compare and show the results of the two multilingual models against a baseline single language pair systems. Particularly, we focus on the four zero-shot directions and show how a multilingual model trained with small data can provide reasonable results. Furthermore, we investigate how pivoting (i.e using a bridge / pivot language for <a href=https://en.wikipedia.org/wiki/Inference>inference</a> in a source!pivot!target translations) using a multilingual model can be an alternative to enable zero-shot translation in a low resource setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.6/>KIT’s Multilingual Neural Machine Translation systems for IWSLT 2017<span class=acl-fixed-case>KIT</span>’s Multilingual Neural Machine Translation systems for <span class=acl-fixed-case>IWSLT</span> 2017</a></strong><br><a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--6><div class="card-body p-3 small">In this paper, we present KIT&#8217;s multilingual neural machine translation (NMT) systems for the IWSLT 2017 evaluation campaign machine translation (MT) and spoken language translation (SLT) tasks. For our MT task submissions, we used our multi-task system, modified from a standard attentional neural machine translation framework, instead of building 20 individual NMT systems. We investigated different <a href=https://en.wikipedia.org/wiki/Software_architecture>architectures</a> as well as different <a href=https://en.wikipedia.org/wiki/Data_corpus>data corpora</a> in training such a multilingual system. We also suggested an effective adaptation scheme for <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual systems</a> which brings great improvements compared to <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual systems</a>. For the SLT track, in addition to a monolingual neural translation system used to generate correct punctuations and true cases of the data prior to training our multilingual system, we introduced a noise model in order to make our system more robust. Results show that our novel modifications improved our <a href=https://en.wikipedia.org/wiki/System>systems</a> considerably on all <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.7/>Towards better translation performance on <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a></a></strong><br><a href=/people/c/chao-bei/>Chao Bei</a>
|
<a href=/people/h/hao-zong/>Hao Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--7><div class="card-body p-3 small">In this paper, we describe GTCOM&#8217;s neural machine translation(NMT) systems for the International Workshop on Spoken Language Translation(IWSLT) 2017. We participated in the English-to-Chinese and Chinese-to-English tracks in the small data condition of the <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual task</a> and the zero-shot condition of the <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual task</a>. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> are based on the encoder-decoder architecture with attention mechanism. We build byte pair encoding (BPE) models in parallel data and back-translated monolingual training data provided in the small data condition. Other techniques we explored in our system include two deep architectures, layer nomalization, weight normalization and training models with annealing Adam, etc. The official scores of <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-to-Chinese</a>, Chinese-to-English are 28.13 and 21.35 on test set 2016 and 28.30 and 22.16 on test set 2017. The official scores on German-to-Dutch, Dutch-to-German, Italian-to-Romanian and Romanian-to-Italian are 19.59, 17.95, 18.62 and 20.39 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.8/>Kyoto University MT System Description for IWSLT 2017<span class=acl-fixed-case>K</span>yoto <span class=acl-fixed-case>U</span>niversity <span class=acl-fixed-case>MT</span> System Description for <span class=acl-fixed-case>IWSLT</span> 2017</a></strong><br><a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/f/fabien-cromieres/>Fabien Cromieres</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--8><div class="card-body p-3 small">We describe here our Machine Translation (MT) model and the results we obtained for the IWSLT 2017 Multilingual Shared Task. Motivated by Zero Shot NMT [ 1 ] we trained a Multilingual Neural Machine Translation by combining all the training data into one single collection by appending the tokens to the source sentences in order to indicate the target language they should be translated to. We observed that even in a low resource situation we were able to get translations whose quality surpass the quality of those obtained by Phrase Based Statistical Machine Translation by several BLEU points. The most surprising result we obtained was in the zero shot setting for Dutch-German and Italian-Romanian where we observed that despite using no parallel corpora between these language pairs, the NMT model was able to translate between these languages and the translations were either as good as or better (in terms of BLEU) than the non zero resource setting. We also verify that the NMT models that use feed forward layers and self attention instead of recurrent layers are extremely fast in terms of training which is useful in a NMT experimental setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.10/>Neural Machine Translation Training in a Multi-Domain Scenario</a></strong><br><a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/s/stephan-vogel/>Stephan Vogel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--10><div class="card-body p-3 small">In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and multi-model ensemble. Our findings show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted ensemble</a> of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning an already trained model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.12/>Synthetic Data for Neural Machine Translation of Spoken-Dialects</a></strong><br><a href=/people/h/hany-hassan-awadalla/>Hany Hassan</a>
|
<a href=/people/m/mostafa-elaraby/>Mostafa Elaraby</a>
|
<a href=/people/a/ahmed-y-tawfik/>Ahmed Y. Tawfik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--12><div class="card-body p-3 small">In this paper, we introduce a novel approach to generate synthetic data for training Neural Machine Translation systems. The proposed approach supports <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>language variants</a> and <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a> with very limited parallel training data. This is achieved using a seed data to project words from a closely-related resource-rich language to an under-resourced language variant via word embedding representations. The proposed approach is based on localized embedding projection of distributed representations which utilizes monolingual embeddings and approximate nearest neighbors queries to transform parallel data across language variants. Our approach is language independent and can be used to generate data for any variant of the source language such as <a href=https://en.wikipedia.org/wiki/Slang>slang</a> or spoken dialect or even for a different language that is related to the source language. We report experimental results on Levantine to English translation using <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>. We show that the synthetic data can provide significant improvements over a very large scale system by more than 2.8 Bleu points and it can be used to provide a reliable translation system for a spoken dialect which does not have sufficient parallel data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2017.iwslt-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.14/>Monolingual Embeddings for Low Resourced Neural Machine Translation</a></strong><br><a href=/people/m/mattia-antonino-di-gangi/>Mattia Antonino Di Gangi</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--14><div class="card-body p-3 small">Neural machine translation (NMT) is the state of the art for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, and it shows the best performance when there is a considerable amount of data available. When only little data exist for a language pair, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can not produce good representations for <a href=https://en.wikipedia.org/wiki/Word_(group_theory)>words</a>, particularly for rare words. One common solution consists in reducing data sparsity by segmenting words into sub-words, in order to allow rare words to have shared representations with other words. Taking a different approach, in this paper we present a method to feed an NMT network with word embeddings trained on monolingual data, which are combined with the task-specific embeddings learned at training time. This method can leverage an embedding matrix with a huge number of words, which can therefore extend the word-level vocabulary. Our experiments on two language pairs show good results for the typical low-resourced data scenario (IWSLT in-domain dataset). Our consistent improvements over the baselines represent a positive proof about the possibility to leverage <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> pre-trained on monolingual data in NMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.16/>Improving Zero-Shot Translation of Low-Resource Languages</a></strong><br><a href=/people/s/surafel-m-lakew/>Surafel M. Lakew</a>
|
<a href=/people/q/quintino-f-lotito/>Quintino F. Lotito</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--16><div class="card-body p-3 small">Recent work on multilingual neural machine translation reported competitive performance with respect to bilingual models and surprisingly good performance even on (zero-shot) translation directions not observed at training time. We investigate here a zero-shot translation in a particularly low-resource multilingual setting. We propose a simple iterative training procedure that leverages a duality of translations directly generated by the <a href=https://en.wikipedia.org/wiki/System>system</a> for the zero-shot directions. The translations produced by the system (sub-optimal since they contain mixed language from the shared vocabulary), are then used together with the original parallel data to feed and iteratively re-train the multilingual network. Over time, this allows the <a href=https://en.wikipedia.org/wiki/System>system</a> to learn from its own generated and increasingly better output. Our approach shows to be effective in improving the two zero-shot directions of our multilingual model. In particular, we observed gains of about 9 BLEU points over a baseline multilingual model and up to 2.08 BLEU over a pivoting mechanism using two bilingual models. Further analysis shows that there is also a slight improvement in the non-zero-shot language directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.17/>Evolution Strategy Based Automatic Tuning of Neural Machine Translation Systems</a></strong><br><a href=/people/h/hao-qin/>Hao Qin</a>
|
<a href=/people/t/takahiro-shinozaki/>Takahiro Shinozaki</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--17><div class="card-body p-3 small">Neural machine translation (NMT) systems have demonstrated promising results in recent years. However, non-trivial amounts of manual effort are required for tuning <a href=https://en.wikipedia.org/wiki/Network_architecture>network architectures</a>, <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training configurations</a>, and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-processing settings</a> such as byte pair encoding (BPE). In this study, we propose an evolution strategy based automatic tuning method for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>. In particular, we apply the covariance matrix adaptation-evolution strategy (CMA-ES), and investigate a Pareto-based multi-objective CMA-ES to optimize the translation performance and computational time jointly. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> automatically finds NMT systems that outperform the initial manual setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.18/>Continuous Space Reordering Models for Phrase-based MT<span class=acl-fixed-case>MT</span></a></strong><br><a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/f/fahim-dalvi/>Fahim Dalvi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--18><div class="card-body p-3 small">Bilingual sequence models improve phrase-based translation and reordering by overcoming phrasal independence assumption and handling long range reordering. However, due to data sparsity, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> often fall back to very small context sizes. This problem has been previously addressed by learning sequences over generalized representations such as <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tags</a> or word clusters. In this paper, we explore an alternative based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a>. More concretely we train neuralized versions of lexicalized reordering [ 1 ] and the operation sequence models [ 2 ] using feed-forward neural network. Our results show improvements of up to 0.6 and 0.5 BLEU points on top of the baseline German!English and English!German systems. We also observed improvements compared to the <a href=https://en.wikipedia.org/wiki/System>systems</a> that used POS tags and word clusters to train these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Because we modify the bilingual corpus to integrate reordering operations, this allows us to also train a sequence-to-sequence neural MT model having explicit reordering triggers. Our motivation was to directly enable reordering information in the encoder-decoder framework, which otherwise relies solely on the attention model to handle long range reordering. We tried both coarser and fine-grained reordering operations. However, these experiments did not yield any improvements over the baseline Neural MT systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2017.iwslt-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2017--iwslt-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2017.iwslt-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2017.iwslt-1.19/>Data Selection with Cluster-Based Language Difference Models and Cynical Selection</a></strong><br><a href=/people/l/lucia-santamaria/>Lucía Santamaría</a>
|
<a href=/people/a/amittai-axelrod/>Amittai Axelrod</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2017--iwslt-1--19><div class="card-body p-3 small">We present and apply two methods for addressing the problem of selecting relevant training data out of a general pool for use in tasks such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Building on existing work on class-based language difference models [ 1 ], we first introduce a cluster-based method that uses Brown clusters to condense the vocabulary of the corpora. Secondly, we implement the cynical data selection method [ 2 ], which incrementally constructs a training corpus to efficiently model the task corpus. Both the cluster-based and the cynical data selection approaches are used for the first time within a machine translation system, and we perform a head-to-head comparison. Our intrinsic evaluations show that both new methods outperform the standard Moore-Lewis approach (cross-entropy difference), in terms of better perplexity and OOV rates on in-domain data. The cynical approach converges much quicker, covering nearly all of the in-domain vocabulary with 84 % less data than the other methods. Furthermore, the new approaches can be used to select machine translation training data for training better <a href=https://en.wikipedia.org/wiki/System>systems</a>. Our results confirm that class-based selection using Brown clusters is a viable alternative to POS-based class-based methods, and removes the reliance on a <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagger</a>. Additionally, we are able to validate the recently proposed cynical data selection method, showing that its performance in SMT models surpasses that of traditional cross-entropy difference methods and more closely matches the sentence length of the task corpus.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>