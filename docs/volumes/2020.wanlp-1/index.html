<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Fifth Arabic Natural Language Processing Workshop - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the Fifth Arabic Natural Language Processing Workshop</h2><p class=lead><a href=/people/i/imed-zitouni/>Imed Zitouni</a>,
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>,
<a href=/people/h/houda-bouamor/>Houda Bouamor</a>,
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>,
<a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>,
<a href=/people/n/nadi-tomeh/>Nadi Tomeh</a>,
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.wanlp-1</dd><dt>Month:</dt><dd>December</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Barcelona, Spain (Online)</dd><dt>Venues:</dt><dd><a href=/venues/coling/>COLING</a>
| <a href=/venues/wanlp/>WANLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.wanlp-1>https://aclanthology.org/2020.wanlp-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Fifth+Arabic+Natural+Language+Processing+Workshop" title="Search for 'Proceedings of the Fifth Arabic Natural Language Processing Workshop' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.0/>Proceedings of the Fifth Arabic Natural Language Processing Workshop</a></strong><br><a href=/people/i/imed-zitouni/>Imed Zitouni</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>
|
<a href=/people/n/nadi-tomeh/>Nadi Tomeh</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.5/>A Semi-Supervised BERT Approach for Arabic Named Entity Recognition<span class=acl-fixed-case>BERT</span> Approach for <span class=acl-fixed-case>A</span>rabic Named Entity Recognition</a></strong><br><a href=/people/c/chadi-helwe/>Chadi Helwe</a>
|
<a href=/people/g/ghassan-dib/>Ghassan Dib</a>
|
<a href=/people/m/mohsen-shamas/>Mohsen Shamas</a>
|
<a href=/people/s/shady-elbassuoni/>Shady Elbassuoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--5><div class="card-body p-3 small">Named entity recognition (NER) plays a significant role in many applications such as <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and even <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Most of the work on NER using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> was done for non-Arabic languages like <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and only few studies focused on <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. This paper proposes a semi-supervised learning approach to train a BERT-based NER model using labeled and semi-labeled datasets. We compared our approach against various baselines, and state-of-the-art Arabic NER tools on three datasets : AQMAR, NEWS, and TWEETS. We report a significant improvement in <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> for the AQMAR and the NEWS datasets, which are written in Modern Standard Arabic (MSA), and competitive results for the TWEETS dataset, which contains tweets that are mostly in the Egyptian dialect and contain many mistakes or misspellings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.14/>MANorm : A Normalization Dictionary for Moroccan Arabic Dialect Written in Latin Script<span class=acl-fixed-case>MAN</span>orm: A Normalization Dictionary for <span class=acl-fixed-case>M</span>oroccan <span class=acl-fixed-case>A</span>rabic Dialect Written in <span class=acl-fixed-case>L</span>atin Script</a></strong><br><a href=/people/r/randa-zarnoufi/>Randa Zarnoufi</a>
|
<a href=/people/h/hamid-jaafar/>Hamid Jaafar</a>
|
<a href=/people/w/walid-bachri/>Walid Bachri</a>
|
<a href=/people/m/mounia-abik/>Mounia Abik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--14><div class="card-body p-3 small">Social media user generated text is actually the main resource for many NLP tasks. This text, however, does not follow the standard <a href=https://en.wikipedia.org/wiki/Writing_system>rules of writing</a>. Moreover, the use of <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> such as <a href=https://en.wikipedia.org/wiki/Moroccan_Arabic>Moroccan Arabic</a> in <a href=https://en.wikipedia.org/wiki/Writing>written communications</a> increases further NLP tasks complexity. A <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> is a <a href=https://en.wikipedia.org/wiki/Spoken_language>verbal language</a> that does not have a standard <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a>. The <a href=https://en.wikipedia.org/wiki/Dialect>written dialect</a> is based on the phonetic transliteration of spoken words which leads users to improvise spelling while writing. Thus, for the same word we can find multiple forms of <a href=https://en.wikipedia.org/wiki/Transliteration>transliterations</a>. Subsequently, it is mandatory to normalize these different <a href=https://en.wikipedia.org/wiki/Transliteration>transliterations</a> to one canonical word form. To reach this goal, we have exploited the powerfulness of word embedding models generated with a corpus of YouTube comments. Besides, using a Moroccan Arabic dialect dictionary that provides the canonical forms, we have built a normalization dictionary that we refer to as MANorm. We have conducted several experiments to demonstrate the efficiency of MANorm, which have shown its usefulness in dialect normalization. We made MANorm freely available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.17/>AraWEAT : Multidimensional Analysis of Biases in Arabic Word Embeddings<span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>WEAT</span>: Multidimensional Analysis of Biases in <span class=acl-fixed-case>A</span>rabic Word Embeddings</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/r/rafik-takieddin/>Rafik Takieddin</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--17><div class="card-body p-3 small">Recent work has shown that distributional word vector spaces often encode <a href=https://en.wikipedia.org/wiki/Bias>human biases</a> like <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> or <a href=https://en.wikipedia.org/wiki/Racism>racism</a>. In this work, we conduct an extensive analysis of biases in Arabic word embeddings by applying a range of recently introduced bias tests on a variety of embedding spaces induced from corpora in Arabic. We measure the presence of biases across several dimensions, namely : embedding models (Skip-Gram, CBOW, and FastText) and vector sizes, types of text (encyclopedic text, and news vs. user-generated content), dialects (Egyptian Arabic vs. Modern Standard Arabic), and time (diachronic analyses over corpora from different time periods). Our analysis yields several interesting findings, e.g., that implicit gender bias in <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> trained on Arabic news corpora steadily increases over time (between 2007 and 2017). We make the Arabic bias specifications (AraWEAT) publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.18/>Parallel resources for Tunisian Arabic Dialect Translation<span class=acl-fixed-case>T</span>unisian <span class=acl-fixed-case>A</span>rabic Dialect Translation</a></strong><br><a href=/people/s/sameh-kchaou/>Saméh Kchaou</a>
|
<a href=/people/r/rahma-boujelbane/>Rahma Boujelbane</a>
|
<a href=/people/l/lamia-hadrich-belguith/>Lamia Hadrich-Belguith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--18><div class="card-body p-3 small">The difficulty of processing <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a> is clearly observed in the high cost of building <a href=https://en.wikipedia.org/wiki/Text_corpus>representative corpus</a>, in particular for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Indeed, all <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> require a huge amount and good management of training data, which represents a challenge in a low-resource setting such as the <a href=https://en.wikipedia.org/wiki/Tunisian_Arabic>Tunisian Arabic dialect</a>. In this paper, we present a data augmentation technique to create a parallel corpus for Tunisian Arabic dialect written in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and standard <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> in order to build a Machine Translation (MT) model. The created <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> was used to build a sentence-based translation model. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> reached a BLEU score of 15.03 % on a test set, while it was limited to 13.27 % utilizing the corpus without augmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wanlp-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.21/>Improving Arabic Text Categorization Using Transformer Training Diversification<span class=acl-fixed-case>A</span>rabic Text Categorization Using Transformer Training Diversification</a></strong><br><a href=/people/s/shammur-absar-chowdhury/>Shammur Absar Chowdhury</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a>
|
<a href=/people/j/jung-soon-gyo/>Jung Soon-Gyo</a>
|
<a href=/people/j/joni-salminen/>Joni Salminen</a>
|
<a href=/people/b/bernard-j-jansen/>Bernard J. Jansen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--21><div class="card-body p-3 small">Automatic categorization of short texts, such as <a href=https://en.wikipedia.org/wiki/Headline>news headlines</a> and <a href=https://en.wikipedia.org/wiki/Social_media>social media posts</a>, has many applications ranging from <a href=https://en.wikipedia.org/wiki/Content_analysis>content analysis</a> to <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation systems</a>. In this paper, we use such <a href=https://en.wikipedia.org/wiki/Categorization>text categorization</a> i.e., labeling the social media posts to categories like &#8216;sports&#8217;, &#8216;politics&#8217;, &#8216;human-rights&#8217; among others, to showcase the efficacy of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> across different sources and varieties of <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. In doing so, we show that diversifying the training data, whether by using diverse training data for the specific task (an increase of 21 % macro F1) or using diverse data to pre-train a BERT model (26 % macro F1), leads to overall improvements in classification effectiveness. In our work, we also introduce two new Arabic text categorization datasets, where the first is composed of social media posts from a popular Arabic news channel that cover <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a>, and <a href=https://en.wikipedia.org/wiki/YouTube>YouTube</a>, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>dialectal Arabic</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.22/>Team Alexa at NADI Shared Task<span class=acl-fixed-case>A</span>lexa at <span class=acl-fixed-case>NADI</span> Shared Task</a></strong><br><a href=/people/m/mutaz-younes/>Mutaz Younes</a>
|
<a href=/people/n/nour-al-khdour/>Nour Al-khdour</a>
|
<a href=/people/m/mohammad-al-smadi/>Mohammad AL-Smadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--22><div class="card-body p-3 small">In this paper, we discuss our team&#8217;s work on the NADI Shared Task. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> requires classifying Arabic tweets among 21 dialects. We tested out different <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a>, and the best <a href=https://en.wikipedia.org/wiki/One_(pronoun)>one</a> was the simplest one. Our best submission was using Multinational Naive Bayes (MNB) classifier (Small and Hsiao, 1985) with <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> as <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a>. Despite its simplicity, this <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> shows better results than complicated <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> such as BERT. Our best submitted score was 17 % <a href=https://en.wikipedia.org/wiki/F1_score>F1-score</a> and 35 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.27/>Weighted combination of BERT and N-GRAM features for Nuanced Arabic Dialect Identification<span class=acl-fixed-case>BERT</span> and N-<span class=acl-fixed-case>GRAM</span> features for Nuanced <span class=acl-fixed-case>A</span>rabic Dialect Identification</a></strong><br><a href=/people/a/abdellah-el-mekki/>Abdellah El Mekki</a>
|
<a href=/people/a/ahmed-alami/>Ahmed Alami</a>
|
<a href=/people/h/hamza-alami/>Hamza Alami</a>
|
<a href=/people/a/ahmed-khoumsi/>Ahmed Khoumsi</a>
|
<a href=/people/i/ismail-berrada/>Ismail Berrada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--27><div class="card-body p-3 small">Around the Arab world, different <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> are spoken by more than 300 M persons, and are increasingly popular in social media texts. However, <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> are considered to be low-resource languages, limiting the development of machine-learning based systems for these <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a>. In this paper, we investigate the Arabic dialect identification task, from two perspectives : country-level dialect identification from 21 Arab countries, and province-level dialect identification from 100 provinces. We introduce an unified pipeline of state-of-the-art models, that can handle the two subtasks. Our experimental studies applied to the NADI shared task, show promising results both at the country-level (F1-score of 25.99 %) and the province-level (F1-score of 6.39 %), and thus allow us to be ranked 2nd for the country-level subtask, and 1st in the province-level subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.29/>Faheem at NADI shared task : Identifying the dialect of Arabic tweet<span class=acl-fixed-case>NADI</span> shared task: Identifying the dialect of <span class=acl-fixed-case>A</span>rabic tweet</a></strong><br><a href=/people/n/nouf-alshenaifi/>Nouf AlShenaifi</a>
|
<a href=/people/a/aqil-azmi/>Aqil Azmi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--29><div class="card-body p-3 small">This paper describes Faheem (adj. of understand), our submission to NADI (Nuanced Arabic Dialect Identification) shared task. With so many <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> being under-studied due to the scarcity of the resources, the objective is to identify the <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialect</a> used in the tweet, country wise. We propose a machine learning approach where we utilize word-level n-gram (n = 1 to 3) and tf-idf features and feed them to six different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. We train the <a href=https://en.wikipedia.org/wiki/System>system</a> using a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> of 21,000 tweetsprovided by the organizerscovering twenty-one Arab countries. Our top performing classifiers are : <a href=https://en.wikipedia.org/wiki/Logistic_regression>Logistic Regression</a>, <a href=https://en.wikipedia.org/wiki/Support_vector_machine>Support Vector Machines</a>, and Multinomial Na ve Bayes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wanlp-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wanlp-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wanlp-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wanlp-1.31/>The QMUL / HRBDT contribution to the NADI Arabic Dialect Identification Shared Task<span class=acl-fixed-case>QMUL</span>/<span class=acl-fixed-case>HRBDT</span> contribution to the <span class=acl-fixed-case>NADI</span> <span class=acl-fixed-case>A</span>rabic Dialect Identification Shared Task</a></strong><br><a href=/people/a/abdulrahman-aloraini/>Abdulrahman Aloraini</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a>
|
<a href=/people/a/ayman-alhelbawy/>Ayman Alhelbawy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wanlp-1--31><div class="card-body p-3 small">We present the Arabic dialect identification system that we used for the country-level subtask of the NADI challenge. Our model consists of three components : BiLSTM-CNN, character-level TF-IDF, and topic modeling features. We represent each tweet using these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and feed them into a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a>. We then add an effective <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic</a> that improves the overall performance. We achieved an F1-Macro score of 20.77 % and an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 34.32 % on the test set. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was also evaluated on the Arabic Online Commentary dataset, achieving results better than the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>