<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</h2><p class=lead><a href=/people/s/stefania-degaetano-ortlieb/>Stefania Degaetano-Ortlieb</a>,
<a href=/people/a/anna-kazantseva/>Anna Kazantseva</a>,
<a href=/people/n/nils-reiter/>Nils Reiter</a>,
<a href=/people/s/stan-szpakowicz/>Stan Szpakowicz</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.latechclfl-1</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Punta Cana, Dominican Republic (online)</dd><dt>Venues:</dt><dd><a href=/venues/clfl/>CLFL</a>
| <a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/latech/>LaTeCH</a>
| <a href=/venues/latechclfl/>LaTeCHCLfL</a></dd><dt>SIG:</dt><dd><a href=/sigs/sighum/>SIGHUM</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.latechclfl-1>https://aclanthology.org/2021.latechclfl-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+5th+Joint+SIGHUM+Workshop+on+Computational+Linguistics+for+Cultural+Heritage%2C+Social+Sciences%2C+Humanities+and+Literature" title="Search for 'Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.0/>Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></strong><br><a href=/people/s/stefania-degaetano-ortlieb/>Stefania Degaetano-Ortlieb</a>
|
<a href=/people/a/anna-kazantseva/>Anna Kazantseva</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a>
|
<a href=/people/s/stan-szpakowicz/>Stan Szpakowicz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.2/>FrameNet-like Annotation of Olfactory Information in Texts<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et-like Annotation of Olfactory Information in Texts</a></strong><br><a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/s/stefano-menini/>Stefano Menini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--2><div class="card-body p-3 small">Although olfactory references play a crucial role in our <a href=https://en.wikipedia.org/wiki/Cultural_memory>cultural memory</a>, only few works in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> have tried to capture them from a computational perspective. Currently, the main challenge is not much the development of technological components for olfactory information extraction, given recent advances in <a href=https://en.wikipedia.org/wiki/Semantic_processing>semantic processing</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, but rather the lack of a theoretical framework to capture this <a href=https://en.wikipedia.org/wiki/Information>information</a> from a linguistic point of view, as a preliminary step towards the development of automated systems. Therefore, in this work we present the annotation guidelines, developed with the help of history scholars and domain experts, aimed at capturing all the relevant elements involved in olfactory situations or events described in texts. These guidelines have been inspired by FrameNet annotation, but underwent some adaptations, which are detailed in this paper. Furthermore, we present a case study concerning the annotation of olfactory situations in English historical travel writings describing trips to <a href=https://en.wikipedia.org/wiki/Italy>Italy</a>. An analysis of the most frequent role fillers show that olfactory descriptions pertain to some typical domains such as <a href=https://en.wikipedia.org/wiki/Religion>religion</a>, <a href=https://en.wikipedia.org/wiki/Food>food</a>, <a href=https://en.wikipedia.org/wiki/Nature>nature</a>, ancient past, poor sanitation, all supporting the creation of a stereotypical imagery related to <a href=https://en.wikipedia.org/wiki/Italy>Italy</a>. On the other hand, positive feelings triggered by smells are prevalent, and contribute to framing travels to Italy as an exciting experience involving all senses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.3/>Batavia asked for advice. Pretrained language models for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> in historical texts.</a></strong><br><a href=/people/s/sophie-i-arnoult/>Sophie I. Arnoult</a>
|
<a href=/people/l/lodewijk-petram/>Lodewijk Petram</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--3><div class="card-body p-3 small">Pretrained language models like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> have advanced the state of the art for many NLP tasks. For resource-rich languages, one has the choice between a number of language-specific models, while multilingual models are also worth considering. These <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are well known for their crosslingual performance, but have also shown competitive in-language performance on some tasks. We consider monolingual and multilingual models from the perspective of historical texts, and in particular for <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> enriched with editorial notes : how do language models deal with the historical and editorial content in these texts? We present a new Named Entity Recognition dataset for <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> based on 17th and 18th century United East India Company (VOC) reports extended with modern editorial notes. Our experiments with multilingual and Dutch pretrained language models confirm the crosslingual abilities of multilingual models while showing that all <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> can leverage mixed-variant data. In particular, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> successfully incorporate <a href=https://en.wikipedia.org/wiki/Note_(typography)>notes</a> for the prediction of entities in <a href=https://en.wikipedia.org/wiki/History>historical texts</a>. We also find that multilingual models outperform monolingual models on our data, but that this superiority is linked to the task at hand : multilingual models lose their advantage when confronted with more semantical tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.8/>Emotion Classification in <a href=https://en.wikipedia.org/wiki/German_language>German</a> Plays with Transformer-based Language Models Pretrained on Historical and Contemporary Language<span class=acl-fixed-case>G</span>erman Plays with Transformer-based Language Models Pretrained on Historical and Contemporary Language</a></strong><br><a href=/people/t/thomas-schmidt/>Thomas Schmidt</a>
|
<a href=/people/k/katrin-dennerlein/>Katrin Dennerlein</a>
|
<a href=/people/c/christian-wolff/>Christian Wolff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--8><div class="card-body p-3 small">We present results of a project on emotion classification on historical German plays of <a href=https://en.wikipedia.org/wiki/Age_of_Enlightenment>Enlightenment</a>, <a href=https://en.wikipedia.org/wiki/Storm_and_Stress>Storm and Stress</a>, and <a href=https://en.wikipedia.org/wiki/German_Classicism>German Classicism</a>. We have developed a hierarchical annotation scheme consisting of 13 sub-emotions like <a href=https://en.wikipedia.org/wiki/Suffering>suffering</a>, love and joy that sum up to 6 main and 2 polarity classes (positive / negative). We have conducted <a href=https://en.wikipedia.org/wiki/Annotation>textual annotations</a> on 11 German plays and have acquired over 13,000 <a href=https://en.wikipedia.org/wiki/Annotation>emotion annotations</a> by two annotators per play. We have evaluated multiple traditional machine learning approaches as well as transformer-based models pretrained on historical and contemporary language for a single-label text sequence emotion classification for the different <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion categories</a>. The evaluation is carried out on three different instances of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> : (1) taking all annotations, (2) filtering overlapping annotations by annotators, (3) applying a <a href=https://en.wikipedia.org/wiki/Heuristic>heuristic</a> for speech-based analysis. Best results are achieved on the filtered corpus with the best <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> being large transformer-based models pretrained on contemporary German language. For the polarity classification accuracies of up to 90 % are achieved. The accuracies become lower for settings with a higher number of classes, achieving 66 % for 13 sub-emotions. Further pretraining of a historical model with a corpus of dramatic texts led to no improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.9/>Automating the Detection of Poetic Features : The Limerick as Model Organism</a></strong><br><a href=/people/a/almas-abdibayev/>Almas Abdibayev</a>
|
<a href=/people/y/yohei-igarashi/>Yohei Igarashi</a>
|
<a href=/people/a/allen-riddell/>Allen Riddell</a>
|
<a href=/people/d/daniel-rockmore/>Daniel Rockmore</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--9><div class="card-body p-3 small">In this paper we take up the problem of limerick detection and describe a <a href=https://en.wikipedia.org/wiki/System>system</a> to identify five-line poems as <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limericks</a> or not. This turns out to be a surprisingly difficult challenge with many subtleties. More precisely, we produce an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> which focuses on the structural aspects of the <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limerick rhyme scheme</a> and <a href=https://en.wikipedia.org/wiki/Rhythm>rhythm</a> (i.e., stress patterns) and when tested on a a culled data set of 98,454 publicly available <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limericks</a>, our limerick filter accepts 67 % as <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limericks</a>. The primary failure of our <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filter</a> is on the detection of non-standard rhymes, which we highlight as an outstanding challenge in computational poetics. Our accent detection algorithm proves to be very robust. Our main contributions are (1) a novel rhyme detection algorithm that works on English words including rare proper nouns and made-up words (and thus, words not in the widely used CMUDict database) ; (2) a novel rhythm-identifying heuristic that is robust to language noise at moderate levels and comparable in accuracy to state-of-the-art scansion algorithms. As a third significant contribution (3) we make publicly available a large corpus of <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limericks</a> that includes tags of limerick or not-limerick as determined by our identification software, thereby providing a benchmark for the community. The poetic tasks that we have identified as challenges for machines suggest that the <a href=https://en.wikipedia.org/wiki/Limerick_(poetry)>limerick</a> is a useful model organism for the study of machine capabilities in <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a> and more broadly literature and language. We include a list of open challenges as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.12/>Translationese in Russian Literary Texts<span class=acl-fixed-case>R</span>ussian Literary Texts</a></strong><br><a href=/people/m/maria-kunilovskaya/>Maria Kunilovskaya</a>
|
<a href=/people/e/ekaterina-lapshinova-koltunski/>Ekaterina Lapshinova-Koltunski</a>
|
<a href=/people/r/ruslan-mitkov/>Ruslan Mitkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--12><div class="card-body p-3 small">The paper reports the results of a translationese study of <a href=https://en.wikipedia.org/wiki/Literature>literary texts</a> based on translated and non-translated Russian. We aim to find out if <a href=https://en.wikipedia.org/wiki/Translation>translations</a> deviate from non-translated literary texts, and if the established differences can be attributed to <a href=https://en.wikipedia.org/wiki/Linguistic_typology>typological relations</a> between source and target languages. We expect that literary translations from typologically distant languages should exhibit more translationese, and the fingerprints of individual source languages (and their families) are traceable in translations. We explore linguistic properties that distinguish non-translated Russian literature from translations into <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. Our results show that non-translated fiction is different from <a href=https://en.wikipedia.org/wiki/Translation>translations</a> to the degree that these two <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language varieties</a> can be automatically classified. As expected, <a href=https://en.wikipedia.org/wiki/Linguistic_typology>language typology</a> is reflected in translations of literary texts. We identified features that point to linguistic specificity of Russian non-translated literature and to shining-through effects. Some of translationese features cut across all language pairs, while others are characteristic of literary translations from languages belonging to specific language families.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.latechclfl-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.15/>A Pilot Study for BERT Language Modelling and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphological Analysis</a> for Ancient and Medieval Greek<span class=acl-fixed-case>BERT</span> Language Modelling and Morphological Analysis for Ancient and Medieval <span class=acl-fixed-case>G</span>reek</a></strong><br><a href=/people/p/pranaydeep-singh/>Pranaydeep Singh</a>
|
<a href=/people/g/gorik-rutten/>Gorik Rutten</a>
|
<a href=/people/e/els-lefever/>Els Lefever</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--15><div class="card-body p-3 small">This paper presents a pilot study to automatic linguistic preprocessing of <a href=https://en.wikipedia.org/wiki/Ancient_Greek>Ancient and Byzantine Greek</a>, and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a> more specifically. To this end, a novel subword-based BERT language model was trained on the basis of a varied corpus of Modern, Ancient and Post-classical Greek texts. Consequently, the obtained BERT embeddings were incorporated to train a fine-grained Part-of-Speech tagger for Ancient and Byzantine Greek. In addition, a corpus of Greek Epigrams was manually annotated and the resulting gold standard was used to evaluate the performance of the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyser</a> on <a href=https://en.wikipedia.org/wiki/Medieval_Greek>Byzantine Greek</a>. The experimental results show very good perplexity scores (4.9) for the BERT language model and state-of-the-art performance for the fine-grained Part-of-Speech tagger for in-domain data (treebanks containing a mixture of Classical and Medieval Greek), as well as for the newly created Byzantine Greek gold standard data set. The language models and associated code are made available for use at https://github.com/pranaydeeps/Ancient-Greek-BERT</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.latechclfl-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.16/>Zero-Shot Information Extraction to Enhance a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>Knowledge Graph</a> Describing Silk Textiles</a></strong><br><a href=/people/t/thomas-schleider/>Thomas Schleider</a>
|
<a href=/people/r/raphael-troncy/>Raphael Troncy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--16><div class="card-body p-3 small">The knowledge of the European silk textile production is a typical case for which the information collected is heterogeneous, spread across many museums and sparse since rarely complete. Knowledge Graphs for this cultural heritage domain, when being developed with appropriate <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontologies</a> and vocabularies, enable to integrate and reconcile this diverse information. However, many of these original <a href=https://en.wikipedia.org/wiki/Collection_(artwork)>museum records</a> still have some metadata gaps. In this paper, we present a zero-shot learning approach that leverages the ConceptNet common sense knowledge graph to predict categorical metadata informing about the silk objects production. We compared the performance of our approach with traditional supervised deep learning-based methods that do require <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>. We demonstrate promising and competitive performance for similar datasets and circumstances and the ability to predict sometimes more fine-grained information. Our results can be reproduced using the code and datasets published at https://github.com/silknow/ZSL-KG-silk.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.19/>Period Classification in Chinese Historical Texts<span class=acl-fixed-case>C</span>hinese Historical Texts</a></strong><br><a href=/people/z/zuoyu-tian/>Zuoyu Tian</a>
|
<a href=/people/s/sandra-kubler/>Sandra Kübler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--19><div class="card-body p-3 small">In this study, we study <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> in Chinese Biji by using a classification task : classifying Ancient Chinese texts by time periods. Specifically, we focus on a unique genre in <a href=https://en.wikipedia.org/wiki/Classical_Chinese_literature>classical Chinese literature</a> : <a href=https://en.wikipedia.org/wiki/Biji_(Chinese_literature)>Biji</a> (literally notebook or brush notes), i.e., collections of anecdotes, quotations, etc., anything authors consider noteworthy, <a href=https://en.wikipedia.org/wiki/Biji_(Chinese_literature)>Biji</a> span hundreds of years across many dynasties and conserve informal language in written form. For these reasons, they are regarded as a good resource for investigating <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> (Fang, 2010). In this paper, we create a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 108 Biji across four dynasties. Based on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we first introduce a time period classification task for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Then we investigate different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature representation methods</a> for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. The results show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using contextualized embeddings perform best. An analysis of the top <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> chosen by the word n-gram model (after bleaching proper nouns) confirms that these <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> are informative and correspond to observations and assumptions made by historical linguists.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.latechclfl-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--latechclfl-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.latechclfl-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.latechclfl-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.latechclfl-1.21/>Stylometric Literariness Classification : the Case of Stephen King</a></strong><br><a href=/people/a/andreas-van-cranenburgh/>Andreas van Cranenburgh</a>
|
<a href=/people/e/erik-ketzan/>Erik Ketzan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--latechclfl-1--21><div class="card-body p-3 small">This paper applies <a href=https://en.wikipedia.org/wiki/Stylometry>stylometry</a> to quantify the literariness of 73 novels and novellas by American author Stephen King, chosen as an extraordinary case of a writer who has been dubbed both high and low in literariness in critical reception. We operationalize <a href=https://en.wikipedia.org/wiki/Literariness>literariness</a> using a measure of stylistic distance (Cosine Delta) based on the 1000 most frequent words in two bespoke comparison corpora used as proxies for <a href=https://en.wikipedia.org/wiki/Literariness>literariness</a> : one of popular genre fiction, another of National Book Award-winning authors. We report that a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> is highly effective in distinguishing the two categories, with 94.6 % macro average in a <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a>. We define two subsets of texts by Kinghigh and low literariness works as suggested by critics and ourselvesand find that a predictive model does identify King&#8217;s Dark Tower series and novels such as Dolores Claiborne as among his most literary texts, consistent with critical reception, which has also ascribed postmodern qualities to the Dark Tower novels. Our results demonstrate the efficacy of Cosine Delta-based stylometry in quantifying the literariness of texts, while also highlighting the methodological challenges of <a href=https://en.wikipedia.org/wiki/Literariness>literariness</a>, especially in the case of <a href=https://en.wikipedia.org/wiki/Stephen_King>Stephen King</a>. The code and data to reproduce our results are available at https://github.com/andreasvc/kinglit</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>