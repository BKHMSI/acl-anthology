<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2020.nl4xai-1.pdf>2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence</a></h2><p class=lead><a href=/people/j/jose-m-alonso/>Jose M. Alonso</a>,
<a href=/people/a/alejandro-catala/>Alejandro Catala</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.nl4xai-1</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Dublin, Ireland</dd><dt>Venues:</dt><dd><a href=/venues/inlg/>INLG</a>
| <a href=/venues/nl4xai/>NL4XAI</a></dd><dt>SIG:</dt><dd><a href=/sigs/siggen/>SIGGEN</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.nl4xai-1>https://aclanthology.org/2020.nl4xai-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.nl4xai-1.pdf>https://aclanthology.org/2020.nl4xai-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.nl4xai-1.pdf title="Open PDF of '2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=2nd+Workshop+on+Interactive+Natural+Language+Technology+for+Explainable+Artificial+Intelligence" title="Search for '2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.0/>2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence</a></strong><br><a href=/people/j/jose-m-alonso/>Jose M. Alonso</a>
|
<a href=/people/a/alejandro-catala/>Alejandro Catala</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.2/>Bias in <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI-systems</a> : A multi-step approach<span class=acl-fixed-case>AI</span>-systems: A multi-step approach</a></strong><br><a href=/people/e/eirini-ntoutsi/>Eirini Ntoutsi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--2><div class="card-body p-3 small">Algorithmic-based decision making powered via AI and (big) data has already penetrated into almost all spheres of human life, from <a href=https://en.wikipedia.org/wiki/Recommender_system>content recommendation</a> and <a href=https://en.wikipedia.org/wiki/Health_care>healthcare</a> to <a href=https://en.wikipedia.org/wiki/Predictive_policing>predictive policing</a> and <a href=https://en.wikipedia.org/wiki/Self-driving_car>autonomous driving</a>, deeply affecting everyone, anywhere, anytime. While <a href=https://en.wikipedia.org/wiki/Technology>technology</a> allows previously unthinkable optimizations in the automation of expensive human decision making, the risks that the <a href=https://en.wikipedia.org/wiki/Technology>technology</a> can pose are also high, leading to an ever increasing public concern about the impact of the <a href=https://en.wikipedia.org/wiki/Technology>technology</a> in our lives. The area of responsible AI has recently emerged in an attempt to put humans at the center of AI-based systems by considering aspects, such as <a href=https://en.wikipedia.org/wiki/Social_justice>fairness</a>, <a href=https://en.wikipedia.org/wiki/Reliability_engineering>reliability</a> and <a href=https://en.wikipedia.org/wiki/Privacy>privacy of decision-making systems</a>. In this talk, we will focus on the fairness aspect. We will start with understanding the many sources of <a href=https://en.wikipedia.org/wiki/Bias>bias</a> and how <a href=https://en.wikipedia.org/wiki/Bias>biases</a> can enter at each step of the learning process and even get propagated / amplified from previous steps. We will continue with methods for mitigating <a href=https://en.wikipedia.org/wiki/Bias>bias</a> which typically focus on some step of the pipeline (data, algorithms or results) and why it is important to target bias in each step and collectively, in the whole (machine) learning pipeline. We will conclude this talk by discussing accountability issues in connection to <a href=https://en.wikipedia.org/wiki/Bias>bias</a> and in particular, proactive consideration via bias-aware data collection, processing and algorithmic selection and retroactive consideration via explanations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.3/>Content Selection for Explanation Requests in Customer-Care Domain</a></strong><br><a href=/people/l/luca-anselma/>Luca Anselma</a>
|
<a href=/people/m/mirko-di-lascio/>Mirko Di Lascio</a>
|
<a href=/people/d/dario-mana/>Dario Mana</a>
|
<a href=/people/a/alessandro-mazzei/>Alessandro Mazzei</a>
|
<a href=/people/m/manuela-sanguinetti/>Manuela Sanguinetti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--3><div class="card-body p-3 small">This paper describes a content selection module for the generation of explanations in a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> designed for customer care domain. First we describe the construction of a corpus of a dialogues containing explanation requests from customers to a virtual agent of a telco, and second we study and formalize the importance of a specific information content for the generated message. In particular, we adapt the notions of <a href=https://en.wikipedia.org/wiki/Importance>importance</a> and <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> in the case of schematic knowledge bases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.5/>The Natural Language Pipeline, Neural Text Generation and Explainability</a></strong><br><a href=/people/j/juliette-faille/>Juliette Faille</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--5><div class="card-body p-3 small">End-to-end encoder-decoder approaches to data-to-text generation are often black boxes whose predictions are difficult to explain. Breaking up the end-to-end model into sub-modules is a natural way to address this problem. The traditional pre-neural Natural Language Generation (NLG) pipeline provides a framework for breaking up the end-to-end encoder-decoder. We survey recent papers that integrate traditional NLG submodules in neural approaches and analyse their explainability. Our survey is a first step towards building explainable neural NLG models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.11/>Toward Natural Language Mitigation Strategies for Cognitive Biases in Recommender Systems</a></strong><br><a href=/people/a/alisa-rieger/>Alisa Rieger</a>
|
<a href=/people/m/mariet-theune/>Mariët Theune</a>
|
<a href=/people/n/nava-tintarev/>Nava Tintarev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--11><div class="card-body p-3 small">Cognitive biases in the context of consuming online information filtered by <a href=https://en.wikipedia.org/wiki/Recommender_system>recommender systems</a> may lead to sub-optimal choices. One approach to mitigate such <a href=https://en.wikipedia.org/wiki/Bias>biases</a> is through interface and interaction design. This survey reviews studies focused on cognitive bias mitigation of recommender system users during two processes : 1) item selection and 2) preference elicitation. It highlights a number of promising directions for Natural Language Generation research for mitigating <a href=https://en.wikipedia.org/wiki/Cognitive_bias>cognitive bias</a> including : the need for <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a>, as well as for transparency and control.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.13/>Learning from Explanations and Demonstrations : A Pilot Study</a></strong><br><a href=/people/s/silvia-tulli/>Silvia Tulli</a>
|
<a href=/people/s/sebastian-wallkotter/>Sebastian Wallkötter</a>
|
<a href=/people/a/ana-paiva/>Ana Paiva</a>
|
<a href=/people/f/francisco-s-melo/>Francisco S. Melo</a>
|
<a href=/people/m/mohamed-chetouani/>Mohamed Chetouani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--13><div class="card-body p-3 small">AI has become prominent in a growing number of <a href=https://en.wikipedia.org/wiki/System>systems</a>, and, as a direct consequence, the desire for explainability in such <a href=https://en.wikipedia.org/wiki/System>systems</a> has become prominent as well. To build explainable systems, a large portion of existing research uses various kinds of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language technologies</a>, e.g., <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech mechanisms</a>, or string visualizations. Here, we provide an overview of the challenges associated with <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language explanations</a> by reviewing existing literature. Additionally, we discuss the relationship between explainability and <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> in <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. We argue that explainability methods, in particular <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> that model the recipient of an explanation, might help increasing sample efficiency. For this, we present a computational approach to optimize the learner&#8217;s performance using explanations of another agent and discuss our results in light of effective natural language explanations for humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.14/>Generating Explanations of Action Failures in a <a href=https://en.wikipedia.org/wiki/Cognitive_robotics>Cognitive Robotic Architecture</a></a></strong><br><a href=/people/r/ravenna-thielstrom/>Ravenna Thielstrom</a>
|
<a href=/people/a/antonio-roque/>Antonio Roque</a>
|
<a href=/people/m/meia-chita-tegmark/>Meia Chita-Tegmark</a>
|
<a href=/people/m/matthias-scheutz/>Matthias Scheutz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--14><div class="card-body p-3 small">We describe an approach to generating explanations about why robot actions fail, focusing on the considerations of robots that are run by cognitive robotic architectures. We define a set of Failure Types and Explanation Templates, motivating them by the needs and constraints of cognitive architectures that use action scripts and interpretable belief states, and describe content realization and surface realization in this context. We then describe an <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> that can be extended to further study the effects of varying the explanation templates.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>