<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task" name=citation_title><meta content="Nikolay Bogoychev" name=citation_author><meta content="Roman Grundkiewicz" name=citation_author><meta content="Alham Fikri Aji" name=citation_author><meta content="Maximiliana Behnke" name=citation_author><meta content="Kenneth Heafield" name=citation_author><meta content="Sidharth Kashyap" name=citation_author><meta content="Emmanouil-Ioannis Farsarakis" name=citation_author><meta content="Mateusz Chudyk" name=citation_author><meta content="Proceedings of the Fourth Workshop on Neural Generation and Translation" name=citation_conference_title><meta content="2020/7" name=citation_publication_date><meta content="https://aclanthology.org/2020.ngt-1.26.pdf" name=citation_pdf_url><meta content="218" name=citation_firstpage><meta content="224" name=citation_lastpage><meta content="10.18653/v1/2020.ngt-1.26" name=citation_doi><meta property="og:title" content="Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task"><meta property="og:image" content="https://aclanthology.org/thumb/2020.ngt-1.26.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2020.ngt-1.26"><meta property="og:description" content="Nikolay Bogoychev, Roman Grundkiewicz, Alham Fikri Aji, Maximiliana Behnke, Kenneth Heafield, Sidharth Kashyap, Emmanouil-Ioannis Farsarakis, Mateusz Chudyk. Proceedings of the Fourth Workshop on Neural Generation and Translation. 2020."><link rel=canonical href=https://aclanthology.org/2020.ngt-1.26></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task<span class=acl-fixed-case>E</span>dinburgh’s Submissions to the 2020 Machine Translation Efficiency Task</a>
<a id=af_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgh se Submissions na die 2020 Masjien Vertaling Efficiency Task</a>
<a id=am_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgh's Submissions to the 2020 machine Translation Effective Task</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>تقديمات إدنبرة لمهمة كفاءة الترجمة الآلية لعام 2020</a>
<a id=az_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Докладите на Единбург за задачата за ефективност на машинния превод през 2020 г.</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>২০২০০ মেশিন অনুবাদের কাজে এডিনবার্গের সাবমিশন</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgovi podaci na zadatak za praćenje mašine 2020.</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Les Submissions d'Edimburgue a la tasca d'eficiència en la traducció màquina del 2020</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburghské příspěvky k úkolu efektivity strojového překladu 2020</a>
<a id=da_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburghs indlæg til opgaven om effektiv maskinoversættelse i 2020</a>
<a id=de_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburghs Einreichungen zur 2020-Aufgabe zur Effizienz maschineller Übersetzung</a>
<a id=el_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Υποβολές του Εδιμβούργου στο έργο αποδοτικότητας μηχανικής μετάφρασης 2020</a>
<a id=es_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Presentaciones de Edimburgo para la tarea de eficiencia de la traducción automática de 2020</a>
<a id=et_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburghi ettepanekud 2020. aasta masintõlke tõhususe ülesandele</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>مسئله‌های ادینبورگ برای تاثیر تغییرات ماشین ۲۰۰۲</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburghin ehdotukset vuoden 2020 konekäännöksen tehokkuustehtävään</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Soumissions d'Edimbourg pour la tâche d'efficacité de la traduction automatique 2020</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Aighneachtaí Dhún Éideann don Tasc Éifeachtúlachta um Aistriú Meaisín 2020</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>KCharselect unicode block name</a>
<a id=he_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>הגישות של אדינבורג למשימה של יעילות התרגום של מכונות 2020</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>2020 मशीन अनुवाद दक्षता कार्य के लिए एडिनबर्ग की प्रस्तुतियाँ</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgovi podaci na zadatak za praćenje strojeva 2020.</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgh beadványai a 2020-as gépi fordítás hatékonysági feladathoz</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Էդինբուրգի ներկայացումները 2020 թվականի մեքենային թարգմանման արդյունավետության խնդիրը</a>
<a id=id_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</a>
<a id=is_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Contributi di Edimburgo al compito 2020 di efficienza della traduzione automatica</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>2020年の機械翻訳効率化タスクへのエディンバラの提出</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Sub-misi nang Edgar 2020 Majin Terjamahan Efeffisisi Job</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>ექინდონის მისამართები 2020-ის მაქინის გადაწყვეტილების ეფექტურება</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Эдинбург 2020 жылы машинаны аудару мүмкіндігінің тапсырмасы</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>에든버러 2020년 기계번역 효율 임무 제출</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgo pasiūlymai dėl 2020 m. mašinų vertimo efektyvumo užduoties</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Предлозите на Единбург на задачата за ефикасност на преведувањето на машините во 2020 година</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>എഡിന്‍ബര്‍ഗിന്‍റെ അബ്രിമിഷന്‍ 2020 മെഷീന്‍ പരിഭാഷപ്പെടുത്തുന്നതിന്‍റെ പ്രവര്‍ത്തനങ്ങള്‍</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Эдинбургийн 2020 оны Машин хөрөнгө оруулах чадварын даалгавар</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburghs inzendingen aan de 2020 Machine Translation Efficiency Task</a>
<a id=no_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Zgłoszenia Edynburga do zadania związanego z efektywnością tłumaczenia maszynowego 2020</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Submissões de Edimburgo para a Tarefa de Eficiência da Tradução Automática de 2020</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Transmiterile de la Edinburgh la sarcina 2020 privind eficiența traducerii automate</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Материалы Эдинбурга по задаче «Эффективность машинного перевода 2020»</a>
<a id=si_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>ඩිවන්බින්ග්රින්ග් සාමිෂ්ණයෝ 2020වාර්ථාන පරිවර්තනය සක්‍රියාත්මක කාර්ය</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgovi prispevki k nalogi učinkovitosti strojnega prevajanja za leto 2020</a>
<a id=so_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgh Submissions to the 2020 machine Translation Effective Task</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Paraqitjet e Edinburgut në detyrën 2020 për efektshmërinë e përkthimit të makinave</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburgovi podaci na zadatak za prevod mašine 2020.</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburghs bidrag till 2020-uppgiften för effektivitet i maskinöversättning</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Mipango ya Edinburgh kwenye kazi ya Tafsiri ya Mashiniki 2020</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>2020 இயந்திரத்தின் மொழிபெயர்ப்பு விளைவுகள்</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Edinburg's Submissions to the 2020 Machine Translation Efficiency Task</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>۲۰۰۲ ماشین ترجمہ فعالیت ٹاکس کے لئے ادینڈینبور کے سرماشین</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>Sự đệ trình của Edinburgh vào Nhiệm vụ Độ khẩn máy 2020</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2020.ngt-1.26.pdf>爱丁堡提交 2020 年机器翻译效率任</a></h2><p class=lead><a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>,
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>,
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>,
<a href=/people/m/maximiliana-behnke/>Maximiliana Behnke</a>,
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>,
<a href=/people/s/sidharth-kashyap/>Sidharth Kashyap</a>,
<a href=/people/e/emmanouil-ioannis-farsarakis/>Emmanouil-Ioannis Farsarakis</a>,
<a href=/people/m/mateusz-chudyk/>Mateusz Chudyk</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task : <a href=https://en.wikipedia.org/wiki/Single-core>single-core CPU</a>, <a href=https://en.wikipedia.org/wiki/Multi-core_processor>multi-core CPU</a>, and <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a>. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. On <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPUs</a>, we used 16-bit floating-point tensor cores. On <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPUs</a>, we customized 8-bit quantization and <a href=https://en.wikipedia.org/wiki/Multiprocessing>multiple processes</a> with affinity for the <a href=https://en.wikipedia.org/wiki/Multi-core_processor>multi-core setting</a>. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ons het gedeel in alle snitte van die Werkshop op Neurale Generasie en Vertaling 2020 Gedeelde Opdrag: enkelcore CPU, multi core CPU en GPU. Op die model vlak gebruik ons onderwysers-studente onderwyseling met 'n verskillende studente grootte, tie inbêding en soms laag, gebruik ons die Eenvoudige Herhaalde Eenheid en introduseer hoof aandrukking. Op GPUs gebruik ons 16- bit floating- point tensor cores. Op CPUs, ons pasmaak 8- bit quantisasie en veelvuldige prosesse met affinity vir die multi core instelling. Om model grootte te verminder, het ons eksperimenteer met 4- bit log quantiseering, maar gebruik vliewe op looptyd. In die gedeelde taak was die meeste van ons ondersoek Pareto optimal met respek van die handel tussen tyd en kwaliteit.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>በኔural Generation እና ትርጓሜ 2020 ተርጓሚዎች የስራ ትርጓሜ ሰርቨርስቲ ሁሉ ተጋርተናል: አንድ-core CPU፣ ብዙ-core CPU እና GPU. በሞዴል ደረጃው፣ አስተማሪ-ተማሪ ትምህርት ትምህርት በተለይ መጠን፣ እየቆረጥ እና አንዳንድ ጊዜ ደረጃዎች፣ ቀላል የቀረበውን ተማሪ እኩል እናስታውቃለን፡፡ በGPU ላይ 16 ቢትር እየነጥፍ ቆንጆር እየቆረጥን ነበር፡፡ በCPU ላይ 8 ቢትር መጠን እና በብዙ አካባቢ ማህበረሰብ ጋር አብዛኛዎችን ፍጥረቶች አስቀምጠን፡፡ የሞዴል መጠን ለማሳነስ፣ 4 ቢትር የሎግ ማሰናከል ፈተናል ግን በሮን ጊዜ መፍጠርን ተጠቀምን፡፡ In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>شاركنا في جميع مسارات ورشة العمل الخاصة بالجيل العصبي والترجمة 2020 Efficiency Shared Task: وحدة المعالجة المركزية أحادية النواة ، ووحدة المعالجة المركزية متعددة النواة ، ووحدة معالجة الرسومات. على مستوى النموذج ، نستخدم تدريب المعلم والطالب مع مجموعة متنوعة من أحجام الطلاب ، وربطة عنق وأحيانًا طبقات ، ونستخدم وحدة أبسط بسيطة متكررة ، ونقدم تقليم الرأس. في وحدات معالجة الرسومات ، استخدمنا نوى موتر ذات فاصلة عائمة 16 بت. على وحدات المعالجة المركزية (CPU) ، قمنا بتخصيص 8 بت تكميم وعمليات متعددة مع تقارب لإعداد متعدد النواة. لتقليل حجم النموذج ، جربنا تكميمًا لسجل 4 بت ولكننا استخدمنا عوامات في وقت التشغيل. في المهمة المشتركة ، كانت معظم عمليات الإرسال لدينا مثالية باريتو فيما يتعلق بالمفاضلة بين الوقت والجودة.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Biz Nural Generation and Translation 2020 İşçin in Bütün İşçinin İşçilərinə daxil olduq: single-core CPU, multi-core CPU və GPU. Model səviyyəsində, müəllimlər-öğrencilər təhsilini müxtəlif ölçülərlə istifadə edirik, körpüsü və bəzi səviyyələr istifadə edirik, sadəcə olaraq təhsil təhsilini istifadə edirik və başlığı təhsil edirik. GPUlar üzerində 16-bit yüksək nöqtələr tenzeri kullandıq. CPUlar barəsində, çoxlu-core ayarlarının bağlılığı ilə 8-bit kvantifikasyonu və çoxlu prosesləri müəyyən etdik. Model böyüklüyünü azaltmaq üçün 4-bit log kvantifikasiyası ilə imtahana çəkdik, amma hərəkət vaxtında floats kullandıq. Bu paylaşdığımız işdə, müsəlmanlarımızın çoxu zaman və keyfiyyəti arasındakı ticarət haqqında Pareto optimal idi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Участвахме във всички песни на семинара за генериране и превод на неврони 2020 Споделена задача за ефективност: едноядрен процесор, многоядрен процесор и графичен процесор. На ниво модел използваме обучение учител-студент с различни размери на учениците, вграждания на вратовръзки и понякога слоеве, използваме по-простата обикновена повтаряща се единица и въвеждаме подрязване на главата. На GPU използвахме 16-битови тензорни ядра с плаваща точка. На процесорите персонализирахме 8-битова квантизация и множество процеси с афинитет към многоядрената настройка. За да намалим размера на модела, експериментирахме с 4-битова логова квантизация, но използвахме плувки по време на изпълнение. В споделената задача повечето от нашите предложения бяха оптимални по отношение на компромиса между време и качество.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>আমরা নিউরাল জেনারেশন এবং অনুবাদের ২০২০ টি প্রভাব শেয়ার করার কার্যকর কর্মশালায় অংশগ্রহণ করেছি: একক মূল সিপিউ, বহুমূল সিপিউ এবং জিপিউ। মডেল পর্যায়ে আমরা শিক্ষক-ছাত্রের প্রশিক্ষণ ব্যবহার করি বিভিন্ন ধরনের ছাত্রের আকার, বাড়ি বাঁধা এবং মাঝে মাঝে মাঝে মাঝে মাঝে সাধারণ পুনরাবার ইউনিট জিপিউসে আমরা ১৬ বিট ফ্লানিং পয়েন্ট ট টেনসার কোর ব্যবহার করেছিলাম। সিপিউসে আমরা ৮ বিটের পরিমাণ ব্যবহার করেছি এবং অনেক প্রক্রিয়ার সাথে মাল্টিক মূল সেটের সাথে সাথে যোগাযোগ করেছি। মডেলের আকার কমানোর জন্য আমরা ৪ বিট লগের পরীক্ষা করেছি কিন্তু রান্টাইমে ফ্লোটগুলো ব্যবহার করেছি। শেয়ার কর্মসূচীতে আমাদের বেশীরভাগ উপস্থাপন ছিল প্যারেটো অপেক্ষায়, সময় এবং মানের মধ্যে ব্যবসায়িক সম্মানের</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. མ་དབུལ་གྱི་ཚད་ལྟར། ང་ཚོས་སློབ་ཆེན་པོ་སྣ་ཚོགས་ལས་སྦྱོར་བྱེད་སྐབས་ཡིག་ཆ་རྐྱེན་བྱས་ནས། ང་ཚོས་GPUདུ་16-bit floating-point tensor cores་སྤྱོད་པ་ཡིན། On CPUs, we customized 8-bit quantization and multiple processes with affinity for the multi-core setting. རྣམ་གྲངས་ཚད་དམའ་དགོས་བྱས་ན། ང་ཚོས་དྲན་ཐིག་གི་ཚད་ལྡན་བཞིན་པའི་བརྡ་སྟོན་བྱས་མིན་འདུག དབྱེ་སྤྱོད་མཁན་གྱི་ལས་འགུལ་དུ་མང་ཆེ་བ་ཡིན་ན་ང་ཚོའི་བསམ་འཆར་པ་ནི་དུས་ཚོད་དང་ཁྱད་པར་བརྗོད་སྤེལ་གྱི་</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Učestvovali smo u svim tragovima radionice o Neuralnoj generaciji i prevodi 2020. zajedničkom zadatku učinkovitosti: jedinstvenog procesora, multi core CPU i GPU. Na razini model a, koristimo trening učitelja i učitelja sa raznim veličinama učenika, ugrađenjem vezama i ponekad slojevima, koristimo Jednostavnu Jednostavnu povratnu jedinicu i predstavljamo glavnu pružanje. Na GPU, koristili smo 16-bit plivajućih tenzora. Na procesorima smo prilagodili kvantizaciju od 8 bita i višestruke procese sa afinitetom za višecore postavku. Da bismo smanjili veličinu modela, eksperimentirali smo sa kvantizacijom 4-bit dnevnika, ali koristili smo plove u provozu. U zajedničkom zadatku, većina naših podataka je bila Pareto optimalna s poštovanjem trgovine između vremena i kvalitete.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Vam participar en totes les pistes de la Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU i GPU. A nivell model, fem servir entrenament professor-estudiant amb una varietat de mida d'estudiants, enganxats i de vegades capes, fem servir l'Unitat Simple Recurrent, i introduïm pruning del cap. En GPU, vam utilitzar núcles de tensor flutuants de 16 bits. En els CPU, vam personalitzar la quantificació de 8 bits i múltiples processos amb afinitat a l'ajustament multinucli. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. En la tasca compartida, la majoria de les nostres proposicionseren Pareto óptims en relació amb el compromís entre temps i qualitat.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Účastnili jsme se všech stop workshopu o neuronové generaci a překladu 2020 Efficiency Shared Task: jednojádrový CPU, vícejádrový CPU a GPU. Na úrovni modelu používáme školení učitelů-studentů s různými velikostmi studentů, vkládáním vazeb a někdy vrstvami, používáme jednodušší jednoduché opakované jednotky a zavádíme prořezávání hlavy. Na GPU jsme použili 16bitová tenzorová jádra s plovoucím bodem. U procesorů jsme přizpůsobili 8bitovou kvantizaci a více procesů s afinitou pro vícejádrové nastavení. Pro snížení velikosti modelu jsme experimentovali s 4-bitovou kvantizací log, ale používali jsme plováky za runtime. Ve společném úkolu byla většina našich podání Pareto optimální s ohledem na kompromis mezi časem a kvalitou.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Vi deltog i alle spor af Workshoppen om Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU og GPU. På modelniveau bruger vi lærer-studerende træning med en række forskellige studerende størrelser, bindeindlejringer og undertiden lag, bruger Simpler Simple Recurrent Unit og introducerer hovedbeskæring. På GPU'er brugte vi 16-bit floating-point tensor kerner. På CPU'er tilpassede vi 8-bit kvantisering og flere processer med affinitet til multi-core indstillingen. For at reducere modelstørrelsen eksperimenterede vi med 4-bit logkvantisering, men bruger floats ved kørselstid. I den fælles opgave var de fleste af vores indlæg Pareto optimale med hensyn til afstemningen mellem tid og kvalitet.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Wir haben an allen Tracks des Workshops zur Neuralen Generation und Übersetzung 2020 Efficiency Shared Task teilgenommen: Single-Core CPU, Multi-Core CPU und GPU. Auf Modellebene verwenden wir Lehrer-Schüler-Training mit einer Vielzahl von Schülergrößen, Bindeeinbettungen und manchmal Schichten, verwenden die Simpler Simple Recurrent Unit und führen Kopfschnitt ein. Auf GPUs verwendeten wir 16-Bit Gleitkomma-Tensorkerne. Auf CPUs haben wir 8-Bit-Quantisierung und mehrere Prozesse mit Affinität für die Multi-Core-Einstellung angepasst. Um die Modellgröße zu reduzieren, experimentierten wir mit 4-Bit Log Quantisierung, verwenden aber Floats zur Laufzeit. In der gemeinsamen Aufgabe waren die meisten unserer Einreichungen Pareto optimal hinsichtlich des Kompromisses zwischen Zeit und Qualität.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Συμμετείχαμε σε όλα τα κομμάτια του Εργαστηρίου Νευρικής Παραγωγής και Μετάφρασης 2020 Κοινή Εργασία: μονοπυρήνων CPU, πολυπυρήνων CPU και GPU. Στο επίπεδο μοντέλου, χρησιμοποιούμε εκπαίδευση δασκάλου-μαθητή με ποικίλα μεγέθη μαθητών, ενσωμάτωση δεσμών και μερικές φορές στρώματα, χρησιμοποιούμε την απλούστερη απλή επαναλαμβανόμενη μονάδα και εισάγουμε κλάδεμα κεφαλής. Σε GPU, χρησιμοποιήσαμε 16bits κυμαινόμενου σημείου πυρήνες Tensor. Στους επεξεργαστές, προσαρμόσαμε 8-κβαντισμό και πολλαπλάσιες διαδικασίες με συγγένεια για τη ρύθμιση πολλαπλών πυρήνων. Για να μειώσουμε το μέγεθος του μοντέλου, πειραματιστήκαμε με 4-bit κβαντισμό καταγραφής, αλλά χρησιμοποιούμε επιπλέες κατά τη διάρκεια εκτέλεσης. Στο κοινό έργο, οι περισσότερες από τις υποβολές μας ήταν βέλτιστες όσον αφορά το συμβιβασμό μεταξύ χρόνου και ποιότητας.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Participamos en todos los temas del Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: CPU de un solo núcleo, CPU multinúcleo y GPU. A nivel de modelo, utilizamos la capacitación de maestros y estudiantes con una variedad de tamaños de estudiantes, incrustaciones de ataduras y, a veces, capas, utilizamos la Unidad Recurrente Simple Más Simple e introducimos la poda de cabezas. En las GPU, utilizamos núcleos tensoriales de punto flotante de 16 bits. En las CPU, personalizamos la cuantificación de 8 bits y los procesos múltiples con afinidad por la configuración multinúcleo. Para reducir el tamaño del modelo, experimentamos con la cuantificación logarítmica de 4 bits, pero utilizamos flotantes en tiempo de ejecución. En la tarea compartida, la mayoría de nuestras presentaciones fueron óptimas en términos de Pareto con respecto a la compensación entre tiempo y calidad.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Osalesime 2020. aasta neurogeneratsiooni ja tõlkimise töötoa kõikidel radadel: ühetuumaline protsessor, mitmetuumaline protsessor ja GPU. Mudeli tasandil kasutame õpetaja-õpilase koolitust erinevate õpilaste suuruste, lipsude manustamise ja mõnikord kihtidega, kasutame lihtsamat lihtsat korduvat üksust ja tutvustame pea lõikamist. GPU-del kasutasime 16-bitist ujuvpunktiga tensorsüdamikku. Protsessoritel kohandasime 8-bitist kvantiseerimist ja mitut protsessi mitme tuumaga seadistusega. Mudeli suuruse vähendamiseks katsetasime 4-bitist logikvantiseerimist, kuid kasutasime käivitusajal ujuvuid. Ühise ülesande puhul olid enamik meie ettepanekuid Pareto optimaalsed aja ja kvaliteedi vahelise kompromisse osas.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ما در تمام رده‌های کارگاه روی نسل‌های عصبی و ترجمه‌های فعالیت‌های مشترک در سال ۲۰۰۲ شرکت کردیم: CPU single-core, CPU multicore, and GPU. در سطح مدل، ما از آموزش آموزش آموزش دانش آموزش استفاده می کنیم با اندازه های مختلف دانش آموزشی، وسیله‌های قالب و گاهی طبقه‌ها، از واحد ساده‌ترین دوباره استفاده می‌کنیم، و سر را معرفی می‌کنیم. روی جی پی یوس، ما از ۱۶ بیت تنسور نقطه شناورن استفاده کردیم. در CPUs، ما 8 بیت کوانتیزی و فرایند چندین را با تعادل برای تنظیم مجموعه هسته‌های زیادی تنظیم کردیم. برای کاهش اندازه مدل، ما با کوانتیزی چهار بیت لیگ آزمایش کردیم ولی از پرواز در زمان چرخش استفاده کردیم. در وظیفه مشترک، بیشتر تسلیم‌های ما پارتو بهترین بود با احترام تجارت بین زمان و کیفیت.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Osallistuimme kaikkiin hermojen tuottamista ja kﾃ､ﾃ､ntﾃ､mistﾃ､ kﾃ､sittelevﾃ､n tyﾃｶpajan kappaleisiin 2020 Efficiency Shared Task: yksiytiminen suoritin, moniytiminen suoritin ja grafiikkasuoritin. Mallitasolla kﾃ､ytﾃ､mme opettaja-opiskelijakoulutusta, jossa on erilaisia opiskelijakokoja, solmioita ja joskus kerroksia, kﾃ､ytﾃ､mme Simpler Simple Toistuva yksikkﾃｶ ja esittelemme pﾃ､ﾃ､n karsimista. Grafiikkanﾃ､ytﾃｶnohjaimilla kﾃ､ytimme 16-bittisiﾃ､ kelluvapistetensoriytimiﾃ､. Prosessoreilla rﾃ､ﾃ､tﾃ､lﾃｶimme 8-bittistﾃ､ kvantisointia ja useita prosesseja moniytimisiin asetuksiin. Mallin koon pienentﾃ､miseksi kokeilimme 4-bittistﾃ､ lokikivantointia, mutta kﾃ､ytﾃ､mme floatseja ajoaikana. Yhteisessﾃ､ tehtﾃ､vﾃ､ssﾃ､ suurin osa ehdotuksistamme oli Pareto-optimaalista ajan ja laadun vﾃ､lisen kompromissin suhteen.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nous avons participé à toutes les pistes de l'atelier sur la tâche partagée d'efficacité de génération et de traduction neuronales 2020 : processeur monocœur, processeur multicœur et processeur graphique. Au niveau du modèle, nous utilisons la formation enseignant-étudiant avec différentes tailles d'étudiants, des incrustations de cravate et parfois des couches, nous utilisons l'unité récurrente simple plus simple et nous introduisons l'élagage de la tête. Sur les GPU, nous avons utilisé des cœurs tenseurs en virgule flottante 16 bits. Sur les processeurs, nous avons personnalisé la quantification 8 bits et plusieurs processus avec une affinité pour le réglage multicœur. Pour réduire la taille du modèle, nous avons expérimenté la quantification logarithmique sur 4 bits, mais nous avons utilisé des nombres flottants lors de l'exécution. Dans la tâche partagée, la plupart de nos soumissions étaient optimales au sens de Pareto en termes de compromis entre le temps et la qualité.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ghlacamar páirt i ngach rian den Cheardlann ar Thasc Comhroinnte Éifeachtúlachta Giniúint Néar agus Aistriúcháin 2020: LAP aon-lárnach, LAP il-lárnach, agus GPU. Ag leibhéal an mhúnla, bainimid úsáid as oiliúint múinteoirí-mac léinn le méideanna éagsúla mac léinn, leabaithe ceangail agus uaireanta sraitheanna, úsáidimid an tAonad Athfhillteach Níos Simplí, agus tugtar isteach bearradh cinn. Ar GPUanna, d’úsáideamar croíleacáin tensor snámhphointe 16-giotán. Ar LAPanna, rinneamar cainníochtú 8-giotán agus próisis iolracha a shaincheapadh le cleamhnas don suíomh il-lárnach. Chun méid an mhúnla a laghdú, rinneamar turgnamh le cainníochtú loga 4-giotán ach úsáidimid snámháin ag am rite. Sa tasc roinnte, bhí an chuid is mó dár n-aighneachtaí Pareto optamach maidir leis an gcomhbhabhtáil idir am agus cáilíocht.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>@ info: whatsthis At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. Ga GPU, mun yi amfani da karatun tsumarni na guda na guda. Ga CPU, mun ƙayyade lissafar 8-bitan sami masu yawa da masu hushi wa daidaita mulki-nufi. Ga mu ƙara girmar shirin ayuka, sai muka jarraba tsarin logogin 4-bits kuma amma, mun yi amfani da floatsu idan na yi tafiya. Kuma a cikin aikin da aka raba, mafi yawanku da musuluntu sun kasance Parato mafificiya game da cinikin fara tsakanin lokaci da sifa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>השתתפנו בכל העקבות של הלימודים על דנרציה נוירולית ותרגום 2020 יעילות משימה משותפת: ברמה של המודל, אנו משתמשים באימונים מורים-סטודנטים עם מגוון של גודלים סטודנטים, קישורים קשרים ולפעמים שכבות, להשתמש ביחידה חדשה פשוטה יותר, ולהציג שיקוי ראש. על GPUs, השתמשנו בגרעני טנסור נקודת צף 16-ביט. על CPUs, אנחנו מתאימים 8-ביט קוונטיזציה ומספר תהליכים עם affinity לסטה multi-core. כדי להפחית את גודל המודל, ניסונו עם קיוונטיזציה של לוג 4-ביטים אבל להשתמש בציפורים בזמן הריצה. במשימה המשותפת, רוב ההצעות שלנו היו Pareto אופטימליים בכבוד הסחר בין זמן לאיכות.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>हमने न्यूरल जनरेशन और अनुवाद 2020 दक्षता साझा कार्य पर कार्यशाला के सभी पटरियों में भाग लिया: एकल-कोर सीपीयू, मल्टी-कोर सीपीयू और जीपीयू। मॉडल स्तर पर, हम विभिन्न प्रकार के छात्र आकारों के साथ शिक्षक-छात्र प्रशिक्षण का उपयोग करते हैं, एम्बेडिंग और कभी-कभी परतों को बांधते हैं, सरल सरल आवर्तक इकाई का उपयोग करते हैं, और सिर की छंटाई का परिचय देते हैं। जीपीयू पर, हमने 16-बिट फ्लोटिंग-पॉइंट टेंसर कोर का उपयोग किया। सीपीयू पर, हमने मल्टी-कोर सेटिंग के लिए आत्मीयता के साथ 8-बिट परिमाणीकरण और कई प्रक्रियाओं को अनुकूलित किया। मॉडल आकार को कम करने के लिए, हमने 4-बिट लॉग परिमाणीकरण के साथ प्रयोग किया लेकिन रनटाइम पर फ्लोट का उपयोग करें। साझा कार्य में, हमारे अधिकांश सबमिशन समय और गुणवत्ता के बीच व्यापार-बंद के संबंध में पारेटो इष्टतम थे।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Učinili smo se u svim tragovima radionice o Neuralnoj generaciji i prevodi 2020. zajedničkom zadatku učinkovitosti: jedinstvenog procesora, višecore CPU i GPU. Na razini model a, koristimo trening učitelja i učitelja s raznim veličinama učenika, ugrađenjem vezama i ponekad slojevima, koristimo jednostavniju jednostavnu povratnu jedinicu i predstavljamo glavnu pružanje. Na GPU, koristili smo 16-bit plivajućih tenzorskih kabla. Na procesorima smo prilagodili kvantizaciju od 8 bita i višestruke procese s afinitetom za višecore postavku. Da bismo smanjili veličinu modela, eksperimentirali smo s kvantizacijom 4-bit dnevnika, ali koristili smo plove u provozu. U zajedničkom zadatku, većina naših podataka je bila Pareto optimalna s poštovanjem trgovine između vremena i kvalitete.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Részt vettünk a 2020-as Neural Generation and Translation Efficiency Shared Task Workshop összes pályáján: egymagos processzor, többmagos processzor és GPU. Modellszinten tanár-diák képzést használunk különböző diákméretekkel, kötőbeágyazásokkal és néha rétegekkel, az Egyszerűbb Egyszerű Ismétlődő Egységet használjuk, és bevezetjük a fejmetszést. GPU-kon 16 bites lebegőpontos tenzormagokat használtunk. A processzorokon testre szabtuk a 8 bites kvantizálást és a több folyamatot, amelyek a többmagos beállításhoz hasonlóak. A modell méretének csökkentése érdekében 4 bites log kvantizálással kísérleteztünk, de futási időben float-okat használtunk. A közös feladat során a beadványok többsége Pareto optimális volt az idő és a minőség közötti kompromisszum tekintetében.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Մենք մասնակցեցինք նյարդային սերունդի և 2020-ի թարգմանման արդյունավետության բոլոր գործընթացներին' մեկ հիմնական պրոցեբույսը, բազմահիմնական պրոցեբույսը և GPU-ը: Մոդելի մակարդակում մենք օգտագործում ենք ուսուցիչներ-ուսանողներ ուսուցիչներ ուսանողների տարբեր չափսերով, կապերով և երբեմն շերտերով, օգտագործում ենք պարզ կրկնվող միավորը և ներկայացնում գլխի կտրտումը: GPU-ում մենք օգտագործեցինք 16-բիթ լողացող կետի տենսորի հիմքեր: Համակարգչային համակարգերի վրա մենք պատրաստեցինք 8-բիտի քանակությամբ և բազմաթիվ գործընթացներով, որոնք բազմահիմնական սահմանափակում են աֆինիտիվություն: Մոդելի չափսի կրճատելու համար մենք փորձեցինք 4-բիտ լոգ քվանտիզացիայի միջոցով, բայց օգտագործեցինք լոգակներ ընթացքում: Մեր ընդհանուր խնդրի մեծամասնությունը Pareto-ն օպտիմալ էր ժամանակի և որակի միջև հակամարտության հարցում:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kami berpartisipasi dalam semua jejak Workshop tentang Generasi Neural dan Translation 2020 Efisiensi Bersama Tugas: CPU satu-inti, CPU multi-inti, dan GPU. Pada tingkat model, kami menggunakan pelatihan guru-siswa dengan berbagai ukuran siswa, dasi embedding dan kadang-kadang lapisan, menggunakan Simpler Simple Recurrent Unit, dan memperkenalkan pemotong kepala. Pada GPU, kami menggunakan inti tensor titik berenang 16 bit. Pada CPU, kami menyesuaikan kuantisasi 8 bit dan proses berbilang dengan afinitas untuk seting multi-core. Untuk mengurangi ukuran model, kami eksperimen dengan kuantisasi log 4-bit tetapi menggunakan float pada waktu berjalan. Dalam tugas bersama, kebanyakan pengiriman kami adalah Pareto optimal dengan menghormati perdagangan antara waktu dan kualitas.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Abbiamo partecipato a tutte le tracce del Workshop sulla Generazione Neurale e Translation 2020 Efficiency Shared Task: CPU single-core, CPU multi-core e GPU. A livello di modello, utilizziamo la formazione insegnante-studente con una varietà di dimensioni studentesche, incorporazioni di cravatte e talvolta strati, usiamo l'unità ricorrente semplice più semplice e introduciamo la potatura della testa. Sulle GPU, abbiamo utilizzato core tensori a 16 bit a virgola mobile. Sulle CPU, abbiamo personalizzato la quantizzazione a 8 bit e più processi con affinità per l'impostazione multi-core. Per ridurre le dimensioni del modello, abbiamo sperimentato la quantizzazione di log a 4 bit ma abbiamo utilizzato float al runtime. Nel compito condiviso, la maggior parte delle nostre proposte sono state Pareto ottimali nel rispetto del compromesso tra tempo e qualità.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>私たちは、シングルコアCPU、マルチコアCPU、およびGPUという、ニューラルジェネレーションと翻訳に関するワークショップ2020効率共有タスクのすべてのトラックに参加しました。モデルレベルでは、さまざまな生徒のサイズの教師と生徒のトレーニング、タイの埋め込み、時にはレイヤー、シンプルなリカレントユニットを使用し、頭の刈り込みを導入します。GPUでは、16ビットの浮動小数点テンソルコアを使用しました。CPUでは、マルチコア設定に親和性のある8ビット量子化と複数のプロセスをカスタマイズしました。モデルサイズを小さくするために、4ビットのログ量子化を実験しましたが、実行時にフロートを使用します。共有タスクでは、パレートの提出物のほとんどは、時間と品質のトレードオフに関して最適でした。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Awak dhéwé wis ngubah barêng-barêng ning arep Workspace nang Generation Neral lan translation 2020 Efeffectness Joined tasks: single-coral Nang kudu model, kita gambar nggawe aturan guru kelas karo akeh pisan umut, ditambah barang nggawe layang lan sampek Layer, kuwi iso nggawe Simple Jejaring section Laptop" and "Desktop Nang ngomongke karo hal-hal nganggo kesempatan, akeh sing paling awak dhéwé Peretono sing paling nggawe ngupakan negoro kejahatan ning terakhir lan kaliwat</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ჩვენ მივიღეთ ყველა სამუშაო სამუშაო ნეიროლური განვითარება და განვითარება 2020 ეფექციენტის სამუშაო სამუშაო პარამეტრებში: ერთ-core CPU, მრავალ-core CPU და GPU. მოდელური დონეში, ჩვენ სტუდენტის სტუდენტის სტუდენტის სტუდენტის განმავლობას გამოყენებთ სტუდენტის განმავლობას, დაკავშირებას და ზოგჯერ სტუდენტის განმავლობას, გამოყ GPUs-ში ჩვენ გამოყენეთ 16-ბიტური კონტაქტის ტენსორის კონტაქტი. პროცესების შესახებ, ჩვენ 8- ბიტის კვანტიზაციას და მრავალ პროცესების განმავლობაში, რამდენიმე კვანტიზაციას დავაკეთებდით. მოდელური ზომის შემცირებისთვის, ჩვენ 4- ბიტური ლოგური კვანტიზაციით ექსპერიმენტირებდით, მაგრამ გამოყენეთ წარმოდგენების დროს. ოჲგვფვრჲ ჲრ ნაქთრვ ოპჲეყლზვნთწ ბვქვ ოაპვრჲ ჲორთმალნთ ჟ გყჱდლვზეანვრჲ ნა რპყდგარა между გპვმვრჲ თ კალთრვრჲ.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Біз 2020 жылы нейралық жасау және аудару және аудару жұмысының барлық жолдарына қатысушы болдық: бір негізгі процессор, бірнеше негізгі процессор және GPU. Үлгі деңгейінде мұғалім- студенттердің оқытуын көптеген студенттердің өлшемі, көптеген ендіру және кейбірде қабаттарды қолданып, Қарапайым қайталану бірлігін қолдану және басының көптеген GPУ үшін 16- биттік жылжымалы нүктелген тензер тензерін қолдандық. Процессорларда біз 8- бит квантизациясын және бірнеше процестерді бірнеше негізгі параметрлердің көпшілігімен өзгертдік. Үлгі өлшемін азайту үшін 4- биттік журналды квантизациялау арқылы тәжірибедік, бірақ жегу уақытында жылжымыз. Ортақ тапсырманың көпшілігіміз уақыт мен сапа арасындағы тәжірибесіне қатынасыз керек болды.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>우리는 신경 생성 및 번역 2020 효율 공유 작업 세미나의 모든 궤도인 단핵 CPU, 다핵 CPU와 GPU에 참가했다.모델 차원에서 우리는 교사-학생 교육을 사용하는데 각종 학생 규모, 넥타이를 포함하고 때로는 차원도 있다. 더욱 간단한 귀속 단원을 사용하고 머리 커팅을 도입한다.GPU에서 우리는 16비트 부동 소수점 장량 핵을 사용한다.CPU에서는 멀티 코어 설정을 위한 8비트 계량화 및 멀티 프로세스를 맞춤형으로 구성했습니다.모델의 크기를 줄이기 위해서, 우리는 4개의 로그 양화를 시도했지만, 실행할 때 부동점을 사용했다.공유 작업 중 시간과 품질 사이의 균형을 보면, 우리의 대다수 제출은 파르토리코가 가장 좋다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dalyvavome visuose seminaro „Neural in ės generacijos ir 2020 m. vertimo efektyvumo klausimais“ etapuose: vienas pagrindinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis Modelio lygmeniu mes naudojame mokytojų ir student ų mokymą su įvairiais studentų dydžiais, sąsajų įdėjimais ir kartais sluoksniais, naudojame paprastesnį paprastesnį pakartotinį vienetą ir įdiegiame galvos smulkinimą. GPU naudojome 16 bit ų svyravimo taško tempimo jėgas. CPU, mes pritaikėme 8 bit ų kiekybinį nustatymą ir kelis procesus su afinitetu daugiašaliam nustatymui. Siekdami sumažinti modelio dydį, eksperimentavome su 4 bit ų log kiekybiškumu, bet naudojame plaukiojimo metu. Bendroje užduotyje dauguma mūsų pareiškimų buvo Pareto optimalus laikas ir kokybė.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Учествувавме на сите траги од работилницата за неурална генерација и превод 2020 ефикасност споделена задача: еднојадрен процесор, мултијадрен процесор и GPU. На моделно ниво, користиме обука на учител-студент со различни студентски големини, врзувања и понекогаш слоеви, користиме едноставна единица за повторно повторно, и воведуваме кревање на главата. На GPU, користевме 16-битни тензорски јадра. На процесорите, ја прилагодивме квантизацијата од 8 бити и многуте процеси со афинитет за поставувањето на многуте јадра. За да ја намалиме големината на моделот, експериментиравме со квантизација на логот од 4 бити, но употребувавме пливачки на време на бегство. Во заедничката задача, повеќето од нашите предлози беа Парето оптимални во однос на разликата помеѓу времето и квалитетот.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ഞങ്ങള്‍ നെയുറല്‍ ജനറനും പരിഭാഷവും 2020 സാധാരണ പങ്കാളികള്‍ പങ്കുചേര്‍ക്കുന്ന പണിയില്‍ പങ്കാളികളില്‍ പങ്കാളികളായി പങ്കുചേര്‍ത്തിരിക്കുന്നു. ഒര മോഡല്‍ നിലയില്‍, ഞങ്ങള്‍ വിദ്യാര്‍ത്ഥികളുടെ വലിപ്പം ഉപയോഗിക്കുന്നു, വിദ്യാര്‍ത്ഥികളുടെ വലിപ്പമുള്ള പരിശീലനം ഉപയോഗിക്കുന്നു, ചിലപ്പോള്‍ തട്ട ജിപിയുസില്‍ ഞങ്ങള്‍ 16- ബിറ്റ് നീലുന്ന ടെന്‍സര്‍ കോര്‍ ഉപയോഗിച്ചു. സിപിയുസില്‍ ഞങ്ങള്‍ 8 ബിറ്റ് വ്യവസ്ഥയും പല പ്രക്രിയകളും കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ സംവിധാ മോഡലിന്റെ വലിപ്പം കുറയ്ക്കാന്‍ ഞങ്ങള്‍ 4- ബിറ്റ് ലോഗിന്റെ പരീക്ഷണത്തില്‍ പരീക്ഷിച്ചിരിക്കുന്നു. പക്ഷെ റ പങ്കാളിയുള്ള ജോലിയില്‍, നമ്മുടെ കീഴ്പ്പെടുത്തുന്നവരില്‍ മിക്കവാറും സമയവും ഗുണവും തമ്മിലുള്ള വ്യാപാര്‍ത്</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Бид 2020 оны мэдрэлийн бүтээлч, хөгжлийн үр дүнтэй хуваалцах үйл ажиллагаанд оролцсон. Нэг төвөгтэй CPU, олон төвөгтэй CPU, GPU. Загварын түвшинд бид багш нарын оюутнуудын сургалтын сургалтыг олон төрлийн хэмжээтэй хэрэглэдэг. Заримдаа холбоотой холбоотой холбоотой, заримдаа давхар холбоотой холбоотой холбоотой. Энгийн энгийн давхар GPUs дээр бид 16-бит floating point tensor cores ашигласан. Процессорын хувьд бид 8-бит хэмжээст болон олон төвөгтэй олон төвөгтэй процессүүдийг хувьд зөвшөөрсөн. Загварын хэмжээг багасгахын тулд бид 4-бит лог квантизацийг туршиж, гэхдээ ажиллах цаг хугацаанд хөдөлгөөн ашиглаж байна. Бидний хуваалцах ажлын ихэнх нь хугацаа болон сайн чанарын хоорондох худалдааны тухай Парето зөвхөн эерэг байсан.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kami berpartisipasi dalam semua trek Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. Pada GPU, kami menggunakan inti tensor titik-terapung 16-bit. Pada CPU, kami menyesuaikan kuantisasi 8-bit dan proses berbilang dengan afini untuk tetapan berbilang-inti. Untuk mengurangi saiz model, kami eksperimen dengan kuantisasi log 4-bit tetapi guna float pada masa berjalan. Dalam tugas berkongsi, kebanyakan penghantaran kami adalah Pareto optimal dengan menghormati perdagangan antara masa dan kualiti.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Parteċipajna fil-binarji kollha tal-Workshop dwar il-Ġenerazzjoni Newrali u t-Traduzzjoni 2020 Kompitu Konġunt dwar l-Effiċjenza: CPU uniku, CPU multi ċentrali, u GPU. Fil-livell tal-mudell, a ħna nużaw taħriġ bejn l-għalliema u l-istudenti b’varjetà ta’ daqsijiet tal-istudenti, inkorporazzjonijiet tal-irbit u xi kultant saffi, nużaw l-Unit à Simpli u Rikorrenti, u nintroduċu l-pruning tar-ras. Fuq GPUs, użajna ċentri tat-tensur b’punt floating ta’ 16-il bit. Fuq is-CPUs, aħna addattajna kwantifikazzjoni ta’ 8 bits u proċessi multipli b’affinità għall-issettjar multi-core. Biex tnaqqas id-daqs tal-mudell, esperimentajna b’kwantifikazzjoni ta’ logaritmu ta’ 4 bits iżda użajna floats waqt ir-runtime. Fil-kompitu kondiviż, il-biċċa l-kbira tas-sottomissjonijiet tagħna kienu Pareto ottimali fir-rigward tal-kompromess bejn iż-żmien u l-kwalità.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>We hebben deelgenomen aan alle tracks van de Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU en GPU. Op modelniveau maken we gebruik van leraar-student training met een verscheidenheid aan studentengrootten, bindbindingen en soms lagen, gebruiken we de Simpler Simple Recurrent Unit en introduceren we hoofdsnijden. Op GPU's gebruikten we 16-bit floating-point tensor cores. Op CPU's hebben we 8-bits kwantisatie en meerdere processen aangepast met affiniteit voor de multi-core instelling. Om de modelgrootte te verkleinen, experimenteerden we met 4-bits logquantisatie, maar gebruikten floats tijdens runtime. In de gedeelde taak waren de meeste van onze inzendingen Pareto optimaal met respect voor de afweging tussen tijd en kwaliteit.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Vi delta i alle spor av arbeidsområdet på Neuralgenerasjon og omsetjing 2020 delt delt effektivt delt oppgåve: enkelkjerne CPU, fleire kjerneCPU og GPU. På modellnivået bruker vi lærer-studenttrening med forskjellige studentstorleik, knyttingar og noen ganger lag, bruker den enklare gjentakingseininga og introduserer hodeprinning. På GPUs brukte vi 16- bits flytande- punkttensorer. På prosessarar har vi tilpassa 8- bit kvantisering og fleire prosessar med affinitet for multikjerneinnstillingane. For å redusera modellstorleiken, eksperimenterte vi med 4- bit loggkvantisering, men bruk flyttar ved køyringsdato. I den delte oppgåva var dei fleste av våre søknader Pareto optimalt med respekt av utviklinga mellom tid og kvalitet.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Uczestniczyliśmy we wszystkich utworach Warsztatu Generacji Neuralnej i Translation 2020 Efektywność Wspólnego Zadania: procesor jednorzeniowy, procesor wielorzeniowy i GPU. Na poziomie modelu stosujemy szkolenia nauczyciela-ucznia z różnymi rozmiarami uczniów, osadzeniami wiązań, a czasem warstwami, używamy prostszej jednostki powtarzającej i wprowadzamy przycinanie głowy. Na procesorach graficznych używaliśmy 16-bitowych rdzeni tensorów zmiennoprzecinkowych. Na procesorach dostosowaliśmy 8-bitową kwantyzację i wiele procesów z powinowactwem do ustawienia wielorzeniowego. Aby zmniejszyć rozmiar modelu, eksperymentowaliśmy z 4-bitową kwantyzacją logów, ale używaliśmy floatów w czasie uruchomienia. W ramach wspólnego zadania większość naszych zgłoszeń była optymalna w Pareto pod względem kompromisu między czasem a jakością.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Participamos de todas as trilhas do Workshop sobre Geração Neural e Tarefa Compartilhada de Eficiência de Tradução 2020: CPU single-core, CPU multi-core e GPU. No nível do modelo, usamos o treinamento professor-aluno com uma variedade de tamanhos de alunos, incorporações de gravata e, às vezes, camadas, usamos a Unidade Recorrente Simples Mais Simples e introduzimos a poda de cabeça. Em GPUs, usamos núcleos tensores de ponto flutuante de 16 bits. Em CPUs, personalizamos a quantização de 8 bits e vários processos com afinidade para a configuração multi-core. Para reduzir o tamanho do modelo, experimentamos a quantização de log de 4 bits, mas usamos floats em tempo de execução. Na tarefa compartilhada, a maioria de nossas submissões foram ótimas de Pareto em relação ao compromisso entre tempo e qualidade.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Am participat la toate piesele Workshop-ului privind generarea neurală și traducerea eficienței 2020 Sarcină partajată: procesor single-core, procesor multi-core și GPU. La nivelul modelului, folosim instruirea profesor-elev cu o varietate de dimensiuni de elev, încorporări de cravată și uneori straturi, folosim Unitatea Recurentă Simplă și introducem tăierea capului. Pe GPU-uri, am folosit nuclee tensoare cu punct plutitor pe 16 biți. Pe procesoare, am personalizat cuantificarea pe 8 biți și procese multiple cu afinitate pentru setarea multi-core. Pentru a reduce dimensiunea modelului, am experimentat cuantificarea logului pe 4 biți, dar am folosit flotoare la rulare. În sarcina partajată, majoritatea depunerilor noastre au fost Pareto optime în ceea ce privește compromisul dintre timp și calitate.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Мы участвовали во всех направлениях Семинара по нейрогенерации и совместному решению задачи «Эффективность 2020»: одноядерный ЦП, многоядерный ЦП и графический процессор. На уровне модели мы используем обучение учителя-студента с различными размерами учащихся, вставками и иногда слоями, используем упрощенную простую рекуррентную единицу и вводим обрезку головы. На графических процессорах мы использовали 16-битные тензорные ядра с плавающей запятой. На процессорах мы настроили 8-битное квантование и несколько процессов с аффинностью к многоядерным настройкам. Чтобы уменьшить размер модели, мы экспериментировали с 4-битным логарифмическим квантованием, но использовали плавающие значения во время выполнения. В совместной задаче большинство наших представлений были оптимальными по Парето с точки зрения компромисса между временем и качеством.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>අපි සියළුම ප්‍රවේශනය සහ පරිවර්තනය සම්ප්‍රවේශනයේ සියළුම ප්‍රවේශකයේ සියළුම ප්‍රවේශකයේ සම්පූර්ණය කරලා: එක-කෝර් CPU මොඩල් ස්ථානයේදී, අපි ගුරුවර්ති-විද්‍යාණික ප්‍රශ්නයක් පාවිච්චි කරනවා, විද්‍යාණික ප්‍රශ්නයක් සමග විද්‍යාණික ප්‍රශ GPUSට, අපි 16-බිට් ප්‍රවාහනය ප්‍රවාහනය කරලා තියෙන්නේ. CPU වලින්, අපි 8- බිට් ක්වාන්තිකරණය සහ ගොඩක් පරීක්ෂණය සඳහා ගොඩක් ක්‍රියාත්මක සැකසුම් වෙනුවෙන්. මොඩල් ප්‍රමාණය අඩු කරන්න, අපි 4- බිට් ලොග් ක්වාන්ටිස් එක්ක පරීක්ෂණය කරලා තියෙන්නේ නමුත් රන්ටිම් ව සමාගත වැඩේ ඉන්නේ, අපේ ගොඩක් පිළිගන්නේ පැරෙටෝ විශේෂය සහ කුළුතිය අතර ව්‍යාපාරයක් ගැන ගෞරවයෙන</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Sodelovali smo na vseh skladbah delavnice o nevralni generaciji in prevajanju 2020 Efficiency Shared Task: enojedrni procesor, večjedrni procesor in GPU. Na ravni modela uporabljamo usposabljanje učiteljev-učencev z različnimi velikostmi študentov, vgradnjo kravat in včasih plastmi, uporabljamo enoto enostavnejše ponavljajoče se enote in uvajamo obrezovanje glave. Na GPU smo uporabili 16-bitna tenzorska jedra s plavajočo točko. Na procesih smo prilagodili 8-bitno kvantizacijo in več procesov z afiniteto za večjedrno nastavitev. Za zmanjšanje velikosti modela smo eksperimentirali s 4-bitno kvantizacijo log, vendar uporabljali plovce v času delovanja. V skupni nalogi je bila večina naših prispevkov Pareto optimalna glede kompromisa med časom in kakovostjo.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. Iskuulka tusaale ahaan waxaynu isticmaalnaa waxbarashada waxbarashada ardayda oo kala duduwan, qashinka xidhan iyo qasnada qaarkood, waxaynu isticmaalnaa qaybta soo deganaanshaha ee fudud, waxaana soo bandhigaynaa caqliga madaxa. GPUs, waxaynu isticmaalnay 16-bit oo ku socota barta tensor. CPUs, waxaynu u isticmaalnay qiyaastii 8 bit oo kala duduwan, waxaana la xiriirnay kooxaha kala duduwan. Si aan u hooseeyo tirada modellka, waxaan ku tijaabiyey qiyaastii qoriga 4-bit, laakiin waxaynu isticmaalnaa goobta lagu soconayo. Shaqada la qaybsan, inta badan ka soo dhiibeyno waxay ahaayeen Pareto mid aad u wanaagsan inay ka heshiiyaan ganacsiga u dhexeeya wakhtiga iyo qiimo.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ne morëm pjesë në të gjitha gjurmët e Workshop mbi Gjenerimin Neural dhe Efikasitetin e Përkthimit 2020 Detyrën e Përbashkët: CPU me një qendër, CPU me shumë qendër dhe GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. Në GPU, kemi përdorur 16-bit pikë-fluturimi në bazë të tensorit. Në CPU, ne personalizuam kuantizimin 8-bit dhe proceset e shumëfishtë me afinitet për përcaktimin e shumëbërthamës. Për të reduktuar madhësinë e modelit, ne eksperimentuam me kuantizim logaritmi 4-bit por përdorim fluturime në kohën e funksionimit. Në detyrën e përbashkët, shumica e paraqitjeve tona ishin Pareto optimale me respektin e kompromisit midis kohës dhe cilësisë.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Učestvovali smo u svim tragovima radionice o Neuralnoj generaciji i prevodi 2020. zajedničkom zadatku učinkovitosti: jedinstvenog procesora, multi core CPU i GPU. Na nivou model a, koristimo trening učitelja i učitelja sa raznim veličinama studenata, vezama i ponekad slojevima, koristimo jednostavniju jednostavnu povratnu jedinicu i predstavljamo glavnu pružanje. Na GPU, koristili smo 16 bitnih tenzorskih koraka. Na procesorima smo prilagodili kvantizaciju od 8 bita i višestruke procese sa afinitetom za višecore postavljanje. Da bismo smanjili veličinu modela, eksperimentirali smo sa kvantizacijom 4-bitnih dnevnika, ali koristili smo plove u provoznom vremenu. U zajedničkom zadatku, većina naših podataka je bila Pareto optimalna s poštovanjem trgovine između vremena i kvaliteta.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Vi deltog i alla spår av Workshopen om Neural Generation and Translation 2020 Efficiency Shared Task: enkärnig processor, flerkärnig processor och GPU. På modellnivå använder vi lärar-elevutbildning med olika elevstorlekar, slipsbäddning och ibland lager, använder Simpler Simple Recurrent Unit och introducerar huvudbeskärning. På GPU:er använde vi 16-bitars flytande tensorkärnor. På processorer anpassade vi 8-bitars kvantisering och flera processer med affinitet för flerkärnig inställning. För att minska modellstorleken experimenterade vi med 4-bitars loggkvantisering men använde floats vid körning. I den delade uppgiften var de flesta av våra bidrag Pareto optimala med hänsyn till avvägningen mellan tid och kvalitet.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tumeshiriki katika tafiti zote za Warsha ya Uzalishaji na Tafsiri ya Neurali 2020 Kushirikishwa na kazi: CPU moja kwa moja, CPU na GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. Kwenye GPUs, tulitumia viungo vya viungo vya sekta 16 vya ndege. Kwenye CPU, tulitumia kiasi cha takwimu 8 na michakato mengi yenye uhusiano wa mazingira mengi. Ili kupunguza ukubwa wa mifano, tulijaribu kwa kiasi kikubwa cha log 4-bit lakini tunatumia mafua wakati wa runtime. Katika jukumu lililoshirikishwa, maoni mengi yetu yalikuwa bora zaidi ya Pareto kwa kuheshimu biashara kati ya muda na kiwango.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>2020 மொழிபெயர்ப்புகள் பங்கிடப்பட்ட பணியில் நாங்கள் நெயுரல் உருவாக்குதல் மற்றும் மொழிபெயர்ப்புகளின் அனைத்து தடங்களிலும் பங்கிடப்பட்டோம்: ஒ At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. ஜிபியூஸில், நாங்கள் 16 பிட்டு மிதவை புள்ளி டென்சார் கோர்களை பயன்படுத்தினோம். சிபியூஸ் மீது, நாம் 8 பிட் அளவு மற்றும் பல உறுப்பு அமைப்புகளுக்கு தொடர்புடன் பல செயல்களை தனிப்பயனாக்கினோம். மாதிரி அளவை குறைக்க, நாம் 4- பிட் பதிவு மதிப்புடன் பரிசோதித்தோம் ஆனால் ஓடும் நேரத்தில் மிதவைகளை பயன்படுத்து. பங்கிடப்பட்ட பணியில், எங்கள் பெரும்பாலானவர்கள் பார்டோ விருப்பத்தேர்வாக இருந்தனர் நேரம் மற்றும் தரம் இடையே வணி</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Biz Nural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, we GPU'yň ähli hatlaryna goşuldyk. Model derejesinde mugallymlary okuwçylar üçin birnäçe topar ölçüsi, baglaýyşlar we käwagt katlar bilen ulanýarys, Kiçiräk ýeterlik Birlikden ullan we kellämizi süýtgetmek üçin ullanýarys. GPU'da 16 bit ýüzlik noktalar esnesörlerini ulandyk CPU-lerde, multi-core düzenlemek üçin 8-bit küntatiýasyny we köp prosesleri bejerdik. Model ölçüsini azaltmak üçin 4-bit küçümseme ile denedik ama eserdeki çizgileri kullandık. Paýlaşan zadyň köpüsi wagt we kwalitet arasyndaky syýahat alyp Pareto optimaldyr.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ہم نے نورول پیدائش اور ترجمہ 2020 کے تمام ٹراکیوں میں شامل ہوا: single-core CPU, multicore CPU اور GPU. نمڈل سطح میں، ہم استاد-استاد کی تعلیم کے مطابق مختلف استاد کے ساتھ استعمال کرتے ہیں، ٹائی ابڈینگ اور کبھی لہروں کے مطابق، ساده ساده دوبارہ یونیٹ کے مطابق استعمال کرتے ہیں، اور سر پرینگ کو معلوم کرتے ہیں. جی پی یوس پر ہم 16 بیٹ فلانٹ پوینٹ ٹینسٹر کور استعمال کرتے تھے۔ CPUs پر ہم نے 8-bit quantization اور بہت سی پروسسوں کو مثبت کے ساتھ مطابق کیا۔ ماڈل کی اندازہ کم کرنے کے لئے، ہم نے 4-بیٹ لاگ کوانتیزی کے ساتھ آزمائش کی لیکن رونٹ زمانہ میں فلاٹ استعمال کریں۔ مشترک کام میں، ہماری اکثریت مسلمانوں کو وقت اور کیفیت کے درمیان تجارت کے معاملہ سے پاریٹا بہترین تھا.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>2020 Tafsilotlar bilan bir necha CPU, bir nechta CPU, va GPU va bir nechta tarjima qilgan Workshopining hamma yo'plalariga ega bo'lgan. Model darajada, biz o'qituvchi o'qituvchi o'quvchi o'quvchi o'quvchi o'quvchi o'quvchi o'rganishdan foydalanamiz, o'zlarining ko'plab o'quvchilari sizlari bilan boshlanamiz, va ba'zida qachon qachon qatlamlar bilan boshlanamiz, od GPUs'da, biz 16-bit floating-point tensorning tugmalaridan foydalanamiz. CPU bilan 8- bit qiymatni moslash va bir nechta vazifalarni bir nechta boshqarish. Modelning oʻlchamini kamaytirish uchun biz 4- bitta logning tizimini aniqlashni istadik lekin ishga tushganda floatsiyalarni ishlatish. Bizning ko'pchiligimizning qismlarimiz vaqt va сифат орасидаги гуруҳларни муҳим қилиш учун Pareto.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Chúng tôi đã tham gia vào mọi dấu vết của "Workshop in Neural generation and translation 2020 efficity shared Task: single-core CPU, đa-core CPU, và GPU. Ở mức mô hình, chúng tôi dùng khóa giáo viên-sinh viên với nhiều kích thước sinh viên, sự ghép thắt cà vạt và đôi khi nhiều lớp, dùng đơn giản đơn vị phục hồi, và giới thiệu việc tỉa đầu. Trên GPU, chúng tôi dùng lõi tensor ở tư khoản 16-cắn. Trên CPU, chúng tôi đã tùy chỉnh lượng 8-cắn và nhiều tiến trình có sự đồng thuận với thiết lập đa lõi. Để giảm cỡ mô hình, chúng tôi thí nghiệm với phân lượng 4-cắn bản ghi, nhưng sử dụng nổi ở thời gian chạy. Trong công việc chia sẻ, hầu hết tài liệu của chúng tôi là "Pareto" tối ưu với tôn trọng sự trao đổi giữa thời gian và chất lượng.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>2020年神经成转研讨会效率共事分会场单核CPU多核CPUGPU。 凡模形等级,用诸生规模师生培训,绑扎嵌之,或为层层,用更简循环单元,引入头剪。 于 GPU ,我用 16 位浮点张量心。 于 CPU 之上,自定义 8 位量化多进,与多核设有关联性。 为小大,试 4 位日志量化,浮数于行。 凡所共任,多为帕累托最,时质之权衡。</span></div></div><dl><dt>Anthology ID:</dt><dd>2020.ngt-1.26</dd><dt>Volume:</dt><dd><a href=/volumes/2020.ngt-1/>Proceedings of the Fourth Workshop on Neural Generation and Translation</a></dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/acl/>ACL</a>
| <a href=/venues/ngt/>NGT</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>218–224</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.ngt-1.26>https://aclanthology.org/2020.ngt-1.26</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2020.ngt-1.26 title="To the current version of the paper by DOI">10.18653/v1/2020.ngt-1.26</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">bogoychev-etal-2020-edinburghs</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Nikolay Bogoychev, Roman Grundkiewicz, Alham Fikri Aji, Maximiliana Behnke, Kenneth Heafield, Sidharth Kashyap, Emmanouil-Ioannis Farsarakis, and Mateusz Chudyk. 2020. <a href=https://aclanthology.org/2020.ngt-1.26>Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task</a>. In <i>Proceedings of the Fourth Workshop on Neural Generation and Translation</i>, pages 218–224, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2020.ngt-1.26>Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task</a> (Bogoychev et al., NGT 2020)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.ngt-1.26.pdf>https://aclanthology.org/2020.ngt-1.26.pdf</a></dd><dt class=acl-button-row>Dataset:</dt><dd class=acl-button-row><a href=https://aclanthology.org/attachments/2020.ngt-1.26.Dataset.txt class="btn btn-attachment btn-sm"><i class="fas fa-file"></i>
&nbsp;2020.ngt-1.26.Dataset.txt</a></dd><dt class=acl-button-row>Video:</dt><dd class=acl-button-row><a href=http://slideslive.com/38929840 class="btn btn-attachment btn-sm"><i class="fas fa-video"></i>&nbsp;http://slideslive.com/38929840</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.ngt-1.26.pdf title="Open PDF of 'Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Edinburgh%E2%80%99s+Submissions+to+the+2020+Machine+Translation+Efficiency+TaskEdinburgh%E2%80%99s+Submissions+to+the+2020+Machine+Translation+Efficiency+Task" title="Search for 'Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a>
<a class="btn btn-attachment d-flex flex-wrap justify-content-center" href=https://aclanthology.org/attachments/2020.ngt-1.26.Dataset.txt title="Open dataset for 'Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task'"><span class="align-self-center px-1"><i class="fas fa-file"></i></span>
<span class=px-1>Dataset</span></a>
<a class="btn btn-attachment d-flex flex-wrap justify-content-center" href=http://slideslive.com/38929840 title="Open video for 'Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task'"><span class="align-self-center px-1"><i class="fas fa-video"></i></span>
<span class=px-1>Video</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task](https://aclanthology.org/2020.ngt-1.26) (Bogoychev et al., NGT 2020)</p><ul class=mt-2><li><a href=https://aclanthology.org/2020.ngt-1.26>Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task</a> (Bogoychev et al., NGT 2020)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Nikolay Bogoychev, Roman Grundkiewicz, Alham Fikri Aji, Maximiliana Behnke, Kenneth Heafield, Sidharth Kashyap, Emmanouil-Ioannis Farsarakis, and Mateusz Chudyk. 2020. <a href=https://aclanthology.org/2020.ngt-1.26>Edinburgh’s Submissions to the 2020 Machine Translation Efficiency TaskEdinburgh’s Submissions to the 2020 Machine Translation Efficiency Task</a>. In <i>Proceedings of the Fourth Workshop on Neural Generation and Translation</i>, pages 218–224, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>