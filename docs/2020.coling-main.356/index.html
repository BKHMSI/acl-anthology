<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural TextLSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural TextLSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text" name=citation_title><meta content="Chihiro Shibata" name=citation_author><meta content="Kei Uchiumi" name=citation_author><meta content="Daichi Mochihashi" name=citation_author><meta content="Proceedings of the 28th International Conference on Computational Linguistics" name=citation_conference_title><meta content="2020/12" name=citation_publication_date><meta content="https://aclanthology.org/2020.coling-main.356.pdf" name=citation_pdf_url><meta content="4033" name=citation_firstpage><meta content="4043" name=citation_lastpage><meta content="10.18653/v1/2020.coling-main.356" name=citation_doi><meta property="og:title" content="How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural TextLSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text"><meta property="og:image" content="https://aclanthology.org/thumb/2020.coling-main.356.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2020.coling-main.356"><meta property="og:description" content="Chihiro Shibata, Kei Uchiumi, Daichi Mochihashi. Proceedings of the 28th International Conference on Computational Linguistics. 2020."><link rel=canonical href=https://aclanthology.org/2020.coling-main.356></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2020.coling-main.356.pdf>How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural Text<span class=acl-fixed-case>LSTM</span> Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text</a>
<a id=af_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Hoe LSTM enkodeer sintaks: Ondersoek Konteks Vektors en Semi- Quantisasie op Natuurlike Teks</a>
<a id=am_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>كيف يشفر LSTM النحو: استكشاف متجهات السياق والتكميم شبه الكمي على النص الطبيعي</a>
<a id=az_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>LSTM Sintaksi nec톛 kodlay캼r: T톛bi톛tli Metin 칲zerind톛 Kontekst Vekt칬rl톛rini v톛 Yar캼-Quantizat캼n캼 Exploring</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Как кодира синтаксиса: проучване на контекстни вектори и полуквантоване на естествен текст</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>কিভাবে এলস্টিএম সিন্ট্যাক্স এনকোড করে: বিষয়বস্তু ভেক্টর এবং স্বাভাবিক টেক্সটের বিষয়বস্তু বিশ্লেষণ করা</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>LSTM སྦྲེལ་མཐུད་སྟངས་ལག་སྟར་འདྲ་བྱེད་སྟངས། རང་བཞིན་ཡི་གེའི་ཁྱད་ཚད་ལྟ་བཤེར་བྱེད་བཞིན་པ</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Kako LSTM kodira sintaks: istraživanje kontekstskih vektora i polu kvantizacije na prirodnom tekstu</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Com LSTM codifica la sintaxi: Explorar els vectors contextuals i la semiquantificació del text natural</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Jak LSTM kóduje syntaxi: zkoumání kontextových vektorů a polokvantizace na přírodním textu</a>
<a id=da_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Hvordan LSTM koder syntaks: Udforskning af kontekstvektorer og halvkvantisering på naturlig tekst</a>
<a id=de_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Wie LSTM Syntax kodiert: Untersuchung von Kontextvektoren und Halbquantifizierung auf natürlichem Text</a>
<a id=el_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Πώς κωδικοποιεί τη σύνταξη: Εξερεύνηση διανυσμάτων περιβάλλοντος και ημι-ποσοτικοποίηση σε φυσικό κείμενο</a>
<a id=es_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Cómo codifica LSTM la sintaxis: exploración de vectores de contexto y semicuantificación en texto natural</a>
<a id=et_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Kuidas LSTM kodeerib süntaksit: konteksti vektorite uurimine ja loodusliku teksti poolkvantiseerimine</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>چگونه سینتکس LSTM رمزبندی می‌کند: تحقیق ویکتورهای متصل و نصف Quantization بر متن طبیعی</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Kuinka LSTM koodaa syntaksia: Kontekstivektorien tutkiminen ja luonnontekstin puolikvantitointi</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Comment LSTM encode la syntaxe : exploration des vecteurs contextuels et de la semi-quantification sur du texte naturel</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Conas a Ionchódaíonn LSTM Comhréir: Veicteoirí Comhthéacs a Iniúchadh agus Leathchainníochtú ar Théacs Nádúrtha</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>@ item Text character set</a>
<a id=he_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Comment</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>कैसे LSTM वाक्यविन्यास encodes: प्राकृतिक पाठ पर संदर्भ वैक्टर और अर्ध परिमाणीकरण की खोज</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Kako LSTM kodira sintaks: istraživanje kontekstskih vektora i polu kvantizacije prirodnog teksta</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Hogyan kódolja az LSTM a szintaxist: kontextusvektorok és félkvantizáció feltárása a természetes szövegen</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Comment</a>
<a id=id_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Bagaimana LSTM Mengenkod Sintaks: Menjelajah Vektor Konteks dan Semi-Quantisasi pada Teks Alami</a>
<a id=is_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Come LSTM codifica la sintassi: esplorazione di vettori di contesto e semi-quantizzazione sul testo naturale</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>LSTMが構文をエンコードする方法:コンテキストベクトルの探索と自然テキストの半定量化</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Text</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>როგორ LSTM სინტექსტის კონტექსტის სინტექსტი: კონტექსტის გვეკტორები და პირველი კვანტიზაცია</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>LSTM синтаксисін қалай кодтамасы: Контексті векторларды және түсінікті мәтіннің жарты көлемін зерттеу</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>LSTM의 문법 인코딩 방법: 자연 텍스트의 상하문 벡터와 반량화 탐색</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Kaip LSTM koduoja sintaksą: konteksto vektorių tyrimas ir puskiekybinis natūralaus teksto tyrimas</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Како LSTM го кодира синтаксот: Истражување на контекстни вектори и полуквантизација на природниот текст</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>എംഎസ്റ്റിം എങ്ങനെയാണ് സിന്റാക്സ് എന്‍കോഡ് ചെയ്യുന്നത്: ഉള്ളിലുള്ള വെക്റ്ററുകളും സ്വാഭാവിക വാചകത്തില്‍</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>LSTM шинжлэх ухаан хэрхэн шинжлэх ухаан: Контекст векторуудыг, байгалийн текст дээр хагас хэмжээний тоо шинжлэх ухаан</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>How LSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Kif LSTM tikkodifika s-Sintassa: L-Esplorazzjoni tal-Vetturi tal-Kuntest u s-Semi-Kwantizzazzjoni fuq it-Test Naturali</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Hoe LSTM syntaxis codeert: Verken contextvectoren en semi-kwantificatie op natuurlijke tekst</a>
<a id=no_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Korleis LSTM kodar syntaks: Utforskar kontekstvektorar og halvkvantisering på naturtekst</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Jak LSTM koduje składnię: badanie wektorów kontekstowych i półkwantyzacja tekstu naturalnego</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Como o LSTM codifica a sintaxe: explorando vetores de contexto e semiquantização em texto natural</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Cum LSTM codează sintaxa: explorarea vectorilor de context și semi-cuantificarea pe text natural</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Как LSTM Кодирует Синтаксис: Изучая Векторы Контекста и Semi-Quantization на Естественном Тексте</a>
<a id=si_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>LSTM සංකේතය කොහොමද සංකේතය: සංකේතය වෙක්ටර්ස් සහ සාමාන්‍ය පාළුවට ප්‍රවේශනය කරන්න</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Kako LSTM kodira sintakso: raziskovanje kontekstnih vektorjev in polkvantizacija naravnega besedila</a>
<a id=so_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Sida LSTM uu u kooban yahay kaalmada la-Syntada: Baadayida wadanka hoose iyo Semi-Quantification ku qoran qoraalka asalka ah</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Si LSTM kodon sintaksën: Shqyrtimi i vektorëve të kontekstit dhe gjysmë-kuantizimit në tekstin natyror</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Kako LSTM kodira sintaks: istraživanje kontekstskih vektora i polu kvantizacije na prirodnom tekstu</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Hur LSTM kodar syntax: utforska kontextvektorer och halvkvantisering på naturlig text</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Namna LSTM inavyoorodhesha kodi: Kuchunguza Vectors of Context and Section Quantification on Natural Text</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>எப்படி LSTM ஒத்திசைவை குறியிடுகிறது: சாதாரண உரையில் உள்ள வெக்டார் மற்றும் பாதி குறியீடு</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>LSTM Sintaks Nasıl Kodlayır: Kontekst vektörleri ve doğal Metin yarısını Tarama</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>LSTM کیسے سینٹکس کوڈ کرتا ہے: کانٹکس ویکتروں اور نصف- کوانتیزی طبیعی متن پر</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Comment</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>Cách Mật mã LSD: Thăm dò sự lây nhiễm ngữ cảnh và phân loại văn bản tự nhiên</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2020.coling-main.356.pdf>LSTM 何以编码语法:原自然之上下文向量半量化</a></h2><p class=lead><a href=/people/c/chihiro-shibata/>Chihiro Shibata</a>,
<a href=/people/k/kei-uchiumi/>Kei Uchiumi</a>,
<a href=/people/d/daichi-mochihashi/>Daichi Mochihashi</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Long Short-Term Memory recurrent neural network (LSTM) is widely used and known to capture informative long-term syntactic dependencies. However, how such <a href=https://en.wikipedia.org/wiki/Information>information</a> are reflected in its internal vectors for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural text</a> has not yet been sufficiently investigated. We analyze them by learning a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> where <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structures</a> are implicitly given. We empirically show that the <a href=https://en.wikipedia.org/wiki/Context_(computing)>context update vectors</a>, i.e. outputs of internal gates, are approximately quantized to binary or ternary values to help the language model to count the depth of nesting accurately, as Suzgun et al. (2019) recently show for synthetic Dyck languages. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as <a href=https://en.wikipedia.org/wiki/Phrase_structure>VP</a> and <a href=https://en.wikipedia.org/wiki/Phrase_structure>NP</a>. Moreover, with an L1 regularization, we also found that it can accurately predict whether a word is inside a <a href=https://en.wikipedia.org/wiki/Phrase_structure>phrase structure</a> or not from a small number of components of the context vector. Even for the case of <a href=https://en.wikipedia.org/wiki/Learning>learning</a> from raw text, context vectors are shown to still correlate well with the <a href=https://en.wikipedia.org/wiki/Phrase_structure>phrase structures</a>. Finally, we show that natural clusters of the functional words and the part of speeches that trigger phrases are represented in a small but principal subspace of the context-update vector of LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Lang- Termine Geheue herhaalde neuralnetwerk (LSTM) is vaste gebruik en bekend om informatiewe lang- term sintaktiewe afhanklikhede te vang. Maar hoe sodanige inligting in sy interne vektore vir natuurlike teks nog nie genoeg is ondersoek nie. Ons analyseer hulle deur 'n taal model te leer waar sintaktieke strukture inplisite gegee word. Ons wys empiriese dat die konteks opdateer vektores, t.d. uitvoerdes van interne poorte, is omtrent quantiseer na binêre of ternary waardes om die taal model te help om die diepte van presies te tel, as Suzgun et al. (2019) onlangs vertoon vir sintetiese diek tale. Vir sommige dimensies in die konteksvektor, wys ons dat hulle aktiwiteite baie verbind is met die diepte van frase strukture, soos VP en NP. Ook, met 'n L1 regularisasie, ons het ook gevind dat dit kan presies voorskou of 'n woord binne 'n frase struktuur is of nie van' n klein aantal komponente van die konteksvektor nie. Selfs vir die geval van leer van raai teks, word konteksvektore vertoon om nog goed te korrelaat met die frase strukture. Eindelik, ons wys dat natuurlike klasters van die funksionele woorde en die deel van speletjies wat uittrek frases word in 'n klein maar hoofspasie van die konteks-opdateer vektor van LSTM verteenwoordig word.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>የረጅም የረጅም ጊዜ ማስታወስ recurrent ኔural network (LSTM) በተስፋው ተጠቃሚ እና የመረጃ መረጃ የረጅም ዘመን Syntactic ተሟጋቾች ለመያዝ የታወቀ ነው። ነገር ግን እንደዚህ ያሉ መረጃ በአውስጣዊ የጽሑፍ አካባቢ ውስጥ እንዴት እንደተመለከቱ ነው፡፡ የቋንቋን ምሳሌ በማስተማር እናስተምርላቸዋለን፡፡ በአሳማሚ እናሳየዋለን፣ የውይይት ደጆች የውጤት ውጤቶች፣ የቋንቋ ሞዴል ጥልቁን እንደ ሱዙን እና አል የሚቆጥር ጥልቅ ጥልቅ እናስረዳለን፡፡ በአካባቢው ጠቅላላ ውስጥ ያሉ አካባቢዎች፣ ሥራቸው እንደ VP እና የNP ጥልቅ በጽሑፍ ግንኙነት እንዲያያሰራሉ እናሳያቸዋለን፡፡ ከዚህም ጋር የL1 ሥርዓት ጋር ቃላት በጽሑፍ አካባቢ ውስጥ መሆኑን ወይም ከታናሹ ክፍሎች ትንሽ ጉዳይ መሆኑን በመግለጥ እናውቃለን፡፡ ለጥሩ ጽሑፍ መማር እንኳ፣ የጽሑፉ መሠረት መሳሪያዎች ደግሞ በጽሑፍ መሠረቶች ደጋፍ እንዲታሰሩ ይገልጣሉ፡፡ በመጨረሻም፣ የሥርዓት ቃላት እና የንግግሮችን መፍጠር የሚያሳየው የፍጥረት ቃላት እና የንግግር ክፍል የLSTM ማቀናጃ መሠረት በትንሽ ነገር ግን የዋነኛ ክፍል ነው፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>تُستخدم الشبكة العصبية المتكررة للذاكرة طويلة المدى (LSTM) على نطاق واسع ومعروفة بالتقاط التبعيات النحوية طويلة المدى بالمعلومات. ومع ذلك ، فإن كيفية انعكاس هذه المعلومات في المتجهات الداخلية للنص الطبيعي لم يتم التحقيق فيها بشكل كافٍ حتى الآن. نقوم بتحليلها من خلال تعلم نموذج لغوي حيث يتم إعطاء التراكيب النحوية ضمنيًا. نظهر بشكل تجريبي أن متجهات تحديث السياق ، أي مخرجات البوابات الداخلية ، يتم قياسها تقريبًا إلى قيم ثنائية أو ثلاثية لمساعدة نموذج اللغة على حساب عمق التداخل بدقة ، مثل Suzgun et al. (2019) عرض مؤخرًا للغات Dyck الاصطناعية. بالنسبة لبعض الأبعاد في متجه السياق ، نظهر أن عمليات تنشيطها مرتبطة ارتباطًا وثيقًا بعمق تراكيب العبارة ، مثل VP و NP. علاوة على ذلك ، مع تنظيم L1 ، وجدنا أيضًا أنه يمكن أن يتنبأ بدقة بما إذا كانت الكلمة موجودة داخل بنية عبارة أم لا من عدد صغير من مكونات متجه السياق. حتى في حالة التعلم من النص الخام ، يظهر أن متجهات السياق لا تزال مرتبطة جيدًا بتراكيب العبارة. أخيرًا ، نوضح أن المجموعات الطبيعية للكلمات الوظيفية وجزء الخطابات التي تؤدي إلى تشغيل العبارات يتم تمثيلها في مساحة فرعية صغيرة ولكنها رئيسية لمتجه تحديث السياق لـ LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Uzun-Uzun Uzun-Term Yadın Yenidən Yenidən Nəyral Ağ (LSTM) geniş işlədilir və informativ uzun-müddət sintaktik bağlılıqları almaq üçün tanınar. Ancaq bu məlumat, təbiətli məlumat üçün iç vektorlarında necə təsirlənəcəkdir? Biz onları sintaktik quruların müəyyən edildiyi dil modelini öyrənib analizə edirik. İçki qapıların sonuçları, suzgun et al kimi, dil modelinin dəyişikliyini doğru saymaq üçün kömək etmək üçün binar və ternar qiymətlərinə kvantifikat edilmişdir. Bazı ölçülərə görə, biz onların aktivasiyaları VP və NP kimi fraz qurularının derinlikləri ilə çox bağlı olduğunu göstəririk. Daha sonra, L1 düzgünlüyü ilə, biz də gördük ki, bu sözün bir fraz quruluşunun içində olmadığını və ya müxtəlif vektorun kiçik bir növ komponentlərin içində olmadığını təmin edə bilər. Sıfır metindən öyrənmək məqsədilə də, məlumat vektörləri hələ də fraza yapılarıyla yaxşı bağlanmaq üçün göstəriləcəklər. Sonunda, fərqli sözlərin təbiətli clusterlərin və sözlərin parçasını LSTM kontekst-güncelləndirmə vektorunun kiçik, ancaq əsl alt alanında göstəririk.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Дългосрочната краткосрочна памет рецидивираща невронна мрежа (ЛСТМ) е широко използвана и известна с улавянето на информативни дългосрочни синтактични зависимости. Въпреки това, как тази информация се отразява в нейните вътрешни вектори за естествен текст все още не е достатъчно проучена. Анализираме ги чрез изучаване на езиков модел, в който синтактичните структури са имплицитно дадени. Емпирично показваме, че векторите за актуализация на контекста, т.е. изходите на вътрешните портали, са приблизително количествени до двоични или тристранни стойности, за да помогнат на езиковия модел да преброи точно дълбочината на гнездене, както показват наскоро за синтетичните езици на Дик. За някои измерения в контекстния вектор показваме, че техните активирания са силно корелирани с дълбочината на фразовите структури като VP и NP. Освен това, с регулировка открихме, че тя може точно да предскаже дали дадена дума е в структурата на фразата или не от малък брой компоненти на контекстния вектор. Дори в случая на учене от суров текст, контекстните вектори все още корелират добре със структурите на фразите. Накрая, показваме, че естествените клъстери от функционалните думи и частта от речите, която задейства фразите, са представени в малко, но основно подпространство на вектора за актуализация на контекста на ЛСТМ.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>দীর্ঘ সংক্ষিপ্ত-টার্ম মেমোরি পুনরাবর্তন নিউরেল নেটওয়ার্ক (LSTM) ব্যবহার করা হয় এবং তথ্য দীর্ঘমেয়াদ সিন্ট্যাকটিক নির্ভরের জন্ তবে প্রাকৃতিক টেক্সটের জন্য এই ধরনের তথ্য কিভাবে প্রতিফলিত হয়েছে তার অভ্যন্তরীণ ভেক্টরে এখনও যথেষ্ট তদন্ত করা হয় ন আমরা তাদের বিশ্লেষণ করি ভাষার মডেল শিখার মাধ্যমে যেখানে সিন্ট্যাক্টিক কাঠামোগুলো ব্যর্থতা প্রদা আমরা ক্ষমতাশীল ভাবে দেখাচ্ছি যে ভেক্টর, যেমন ইন্টারনেট গেটের আপটপুট, বাইনারি অথবা টার্নারি মানের প্রায় পরিমাণ বাইনারি বা টার্নারি মূল্য, যাতে ভাষার মডেল সঠিকভাবে প্রতিষ্ঠানে For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP. এছাড়াও, এল১ নিয়ন্ত্রণের সাথে আমরা আবিষ্কার করেছি যে এটি সঠিকভাবে ভবিষ্যদ্বাণী করতে পারে যে একটি শব্দ কাঠামোর ভেতরে আছে কিনা কিংবা প্রেক্ষি এমনকি ক্ষুদ্র লেখা থেকে শিখার ক্ষেত্রেও প্রেক্ষিত ভেক্টর এখনো ভালো কাঠামোর সাথে সংশ্লিষ্ট। অবশেষে, আমরা দেখাচ্ছি যে কার্যকলাপ শব্দের প্রাকৃতিক ক্লাস্টার এবং ভাষণের অংশ যে বাক্ষতিগুলোর প্রতিনিধিত্ব করা হচ্ছে এলএসএমএর কন্টেক্সটেক্ট</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Long-Term Memory recurrent neural network (LSTM) is widely used and known to capture informative long-term syntactic dependencies. འོན་ཀྱང་། རང་རྒྱུས་ཡིག་གི་གནས་ཚུལ་འདི་ག་དེ་རྟོགས་པའི་སྣ་ཚོགས་ནང་ལ་མཐོང་ན་ཕལ་མེད་པ་རེད། ང་ཚོས་དེ་དག་ཚོར་སྐད་ཡིག་གི་མ་དབྱིབས་ཞིབ་འཇུག་བྱེད་ཀྱི་ཡོད་པ་ལས་དབྱེ་ཞིབ་བྱེད་ཀྱི་ཡོད། We empirically show that the context update vectors, i.e. outputs of internal gates, are approximately quantized to binary or ternary values to help the language model to count the depth of nesting accurately, as Suzgun et al. (2019) recently show for synthetic Dyck languages. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP. ད་དུང་། L1 རྒྱུན་ལྡན་བཟོ་བྱས་པ་དེ་ང་ཚོས་དྲན་ཐུབ་པའི་ཐ་སྙད་ཅིག་རྟོགས་པ་ཅིན་ཡིན་ནམ། ཚིག་ཡི་གེ་ནས་སྐད་ཡིག་གི་ནང་དུ་བསླབ་པའི་ལྟ་བུའི་རྣམ་པ་དེ་ལས་སྦྲེལ་མཐུད་དང་མཉམ་དུ་མཐུན་པ་ཡིན། Finally, we show that natural clusters of the functional words and the part of speeches that trigger phrases are represented in a small but principal subspace of the context-update vector of LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dugo kratkoročna memorija se široko koristi i poznaje kako bi uhvatili informativne dugoročne sintaktične zavisnosti. Međutim, kako se takve informacije odražavaju u svojim unutrašnjim vektorima prirodnog teksta još nisu dovoljno istražene. Analiziramo ih učeći jezički model gdje se sintaktičke strukture implicitno daju. Mi empirički pokazujemo da su vektori za aktualiziranje konteksta, tj. ishod unutrašnjih vrata, približno kvantizirani na binarne ili ternarne vrijednosti kako bi pomogli jezičkom modelu da računa dubinu gnijezda tačno, kao Suzgun et al. (2019) nedavno pokazuju za sintetičke jezike. Za neke dimenzije u kontekstskom vektoru pokazujemo da su njihove aktivacije visoko povezani sa dubinom frazu strukture, poput VP i NP. Osim toga, s regularizacijom L1, također smo otkrili da može precizno predvidjeti da li je riječ unutar strukture fraze ili ne iz malog broja komponenta kontekstnog vektora. Čak i za slučaj učenja sa sirovog teksta, kontekstski vektori se pokazuju da se i dalje dobro povezuju sa strukturama fraza. Napokon, pokazujemo da su prirodni skupini funkcionalnih riječi i dio govora koje okidaju fraze predstavljeni u malom ali glavnom podprostoru vektora LSTM-a za aktualizaciju konteksta.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>La xarxa neural recurrent de memòria a curt termini (LSTM) s'utilitza ampliament i es coneix per capturar dependencies sinàctiques informatives a llarg termini. No obstant això, la manera en què aquesta informació s'reflecteix en els seus vectors interns del text natural encara no ha estat investigada suficientment. We analyze them by learning a language model where syntactic structures are implicitly given. Mostrem empíricament que els vectors d'actualització del context e, és a dir, les sortides de portas internas, es quantifiquen aproximadament a valors binaris o ternaris per ajudar al model de llenguatge a comptar la profunditat del ninjat amb precisió, com Suzgun et al. (2019) mostren recentment per a les llengües sintètiques Dyck. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP. A més, amb una regularització L1, també vam descobrir que pot predir exactament si una paraula està dins una estructura de frases o no d'un petit nombre de components del vector contextual. Fins i tot en el cas d'aprendre amb text brut, es demostra que els vectors de context encara correlacionen bé amb les estructures de frases. Finalment, demostram que agrupaments naturals de paraules funcionals i la part de discursos que desencadena frases són representats en un petit, però principal, subsespai del vector d'actualització del context de LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Rekurentní neuronová síť s dlouhodobou krátkodobou pamětí (LSTM) je široce používána a známa pro zachycování informativních dlouhodobých syntaktických závislostí. Způsob, jakým se tyto informace odrážejí v jejich vnitřních vektorech pro přirozený text, však dosud nebyl dostatečně zkoumán. Analyzujeme je pomocí jazykového modelu, kde jsou implicitně dány syntaktické struktury. Empiricky ukazujeme, že vektory aktualizace kontextu, tj. výstupy interních bran, jsou přibližně kvantizovány na binární nebo ternární hodnoty, aby jazykovému modelu pomohly přesně spočítat hloubku vnoření, jak to nedávno ukazují Suzgun et al. (2019) u syntetických jazyků Dycka. U některých dimenzí kontextového vektoru ukazujeme, že jejich aktivace jsou vysoce korelovány s hloubkou frázových struktur, jako jsou VP a NP. Díky regularizaci L1 jsme také zjistili, že může přesně předpovědět, zda je slovo uvnitř frázové struktury nebo ne z malého počtu komponent kontextového vektoru. Dokonce i v případě učení se ze surového textu jsou ukázány, že kontextové vektory stále dobře korelují s frázovými strukturami. Nakonec ukazujeme, že přirozené shluky funkčních slov a části projevů, které spouštějí fráze, jsou reprezentovány v malém, ale hlavním subprostoru kontextového aktualizačního vektoru LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Langtidshukommelse tilbagevendende neurale netværk (LSTM) er meget udbredt og kendt til at fange informative langsigtede syntaktiske afhængigheder. Men hvordan sådanne oplysninger afspejles i dens interne vektorer for naturlig tekst er endnu ikke blevet tilstrækkeligt undersøgt. Vi analyserer dem ved at lære en sprogmodel, hvor syntaktiske strukturer implicit gives. Vi viser empirisk, at kontekst opdateringsvektorer, dvs. udgange af interne porte, er omtrent kvantiseret til binære eller ternære værdier for at hjælpe sprogmodellen til at tælle dybden af indlejring nøjagtigt, som Suzgun et al. (2019) for nylig viser for syntetiske Dyck sprog. For nogle dimensioner i kontekstvektoren viser vi, at deres aktiveringer er stærkt korreleret med dybden af sætningsstrukturer, såsom VP og NP. Desuden, med en L1 regulering, fandt vi også, at det nøjagtigt kan forudsige, om et ord er inde i en sætningsstruktur eller ej fra et lille antal komponenter i kontekstvektoren. Selv i tilfælde af at lære af rå tekst, vises kontekstvektorer stadig at korrelere godt med sætningsstrukturerne. Endelig viser vi, at naturlige klynger af de funktionelle ord og den del af taler, der udløser sætninger, er repræsenteret i et lille, men vigtigste underrum af kontekst-opdatering vektor LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>LSTM (Long-Short-Term Memory Recidiviert Neuronal Network) ist weit verbreitet und dafür bekannt, langfristige syntaktische Abhängigkeiten zu erfassen. Wie sich solche Informationen in ihren internen Vektoren für Naturtext widerspiegeln, ist jedoch noch nicht ausreichend untersucht worden. Wir analysieren sie, indem wir ein Sprachmodell lernen, in dem syntaktische Strukturen implizit gegeben sind. Wir zeigen empirisch, dass die Kontextaktualisierungsvektoren, d.h. Ausgänge interner Gates, ungefähr auf binäre oder ternäre Werte quantisiert sind, um dem Sprachmodell zu helfen, die Tiefe der Verschachtelung genau zu zählen, wie Suzgun et al. (2019) kürzlich für synthetische Dyck-Sprachen zeigen. Für einige Dimensionen im Kontextvektor zeigen wir, dass ihre Aktivierungen stark mit der Tiefe von Phrasenstrukturen korrelieren, wie VP und NP. Darüber hinaus fanden wir mit einer L1-Regularisierung auch heraus, dass es aus einer kleinen Anzahl von Komponenten des Kontextvektors genau vorhersagen kann, ob sich ein Wort innerhalb einer Phrasenstruktur befindet oder nicht. Selbst beim Lernen aus Rohtext korrelieren Kontextvektoren noch gut mit den Phrasenstrukturen. Schließlich zeigen wir, dass natürliche Cluster der funktionalen Wörter und des Teils der Reden, die Phrasen auslösen, in einem kleinen, aber prinzipiellen Subraum des Kontext-Update-Vektors von LSTM dargestellt werden.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Το επαναλαμβανόμενο νευρωνικό δίκτυο μακράς βραχυπρόθεσμης μνήμης (χρησιμοποιείται ευρέως και είναι γνωστό για να συλλάβει ενημερωτικές μακροπρόθεσμες συντακτικές εξαρτήσεις. Ωστόσο, ο τρόπος με τον οποίο οι πληροφορίες αυτές αντικατοπτρίζονται στα εσωτερικά διανύσματά τους για φυσικό κείμενο δεν έχει ακόμη διερευνηθεί επαρκώς. Τους αναλύουμε μαθαίνοντας ένα γλωσσικό μοντέλο όπου οι συντακτικές δομές δίνονται έμμεσα. Δείχνουμε εμπειρικά ότι τα διανύσματα ενημέρωσης περιβάλλοντος, δηλαδή οι εξόδοι των εσωτερικών πυλών, είναι περίπου κβαντισμένα σε δυαδικές ή τριμηνιαίες τιμές για να βοηθήσουν το γλωσσικό μοντέλο να μετρήσει με ακρίβεια το βάθος της φωλιάσματος, όπως δείχνουν πρόσφατα για τις συνθετικές γλώσσες του Ντάικ. Για ορισμένες διαστάσεις στο διάνυσμα περιβάλλοντος, δείχνουμε ότι οι ενεργοποίησές τους συσχετίζονται ιδιαίτερα με το βάθος των δομών φράσεων, όπως η VP και η NP. Επιπλέον, με μια ρύθμιση διαπιστώσαμε επίσης ότι μπορεί να προβλέψει με ακρίβεια αν μια λέξη βρίσκεται μέσα σε μια δομή φράσεων ή όχι από έναν μικρό αριθμό συστατικών του διανυσματικού πλαισίου. Ακόμη και στην περίπτωση της εκμάθησης από ακατέργαστο κείμενο, τα διανύσματα περιβάλλοντος δείχνουν ότι εξακολουθούν να συσχετίζονται καλά με τις δομές φράσεων. Τέλος, δείχνουμε ότι οι φυσικές συστάδες των λειτουργικών λέξεων και του τμήματος των ομιλιών που προκαλούν φράσεις αντιπροσωπεύονται σε ένα μικρό αλλά κύριο υποδιαστημικό διάνυσμα της ενημέρωσης περιβάλλοντος του LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>La red neuronal recurrente de memoria a corto plazo (LSTM) se usa ampliamente y se sabe que captura dependencias sintácticas informativas a largo plazo. Sin embargo, aún no se ha investigado suficientemente cómo se refleja esa información en sus vectores internos para el texto natural. Los analizamos aprendiendo un modelo de lenguaje en el que se dan implícitamente estructuras sintácticas. Demostramos empíricamente que los vectores de actualización de contexto, es decir, las salidas de las puertas internas, se cuantifican aproximadamente a valores binarios o ternarios para ayudar al modelo de lenguaje a contar la profundidad del anidamiento con precisión, como lo demuestran recientemente Suzgun et al. (2019) para los lenguajes Dyck sintéticos. Para algunas dimensiones en el vector de contexto, mostramos que sus activaciones están altamente correlacionadas con la profundidad de las estructuras de frases, como VP y NP. Además, con una regularización L1, también encontramos que puede predecir con precisión si una palabra está dentro de una estructura de frase o no a partir de un pequeño número de componentes del vector de contexto. Incluso para el caso de aprender del texto sin procesar, se muestra que los vectores de contexto se correlacionan bien con las estructuras de las frases. Finalmente, mostramos que los grupos naturales de las palabras funcionales y la parte de los discursos que activan las frases se representan en un subespacio pequeño pero principal del vector de actualización de contexto de LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Pikaajalist lühiajalist mälu korduvat närvivõrku (LSTM) kasutatakse laialdaselt ja teadaolevalt jäädvustatakse informatiivseid pikaajalisi süntaktilisi sõltuvusi. Kuid seda, kuidas selline teave kajastub loodusliku teksti sisemises vektoris, ei ole veel piisavalt uuritud. Analüüsime neid keelemudeli õppides, kus süntaktilised struktuurid on kaudselt esitatud. Empiiriliselt näitame, et konteksti värskendusvektorid, st siseväravate väljundid, on ligikaudu kvantiseeritud kahe- või kolmeväärtusteks, et aidata keelemudelil pesitsemise sügavust täpselt lugeda, nagu Suzgun jt. (2019) hiljuti näitasid sünteetiliste Dyckide keelte puhul. Mõnede kontekstivektori dimensioonide puhul näitame, et nende aktiveerimine on tugevalt korrelatsioonis fraasistruktuuride, näiteks VP ja NP sügavusega. Lisaks leidsime L1 regulariseerimisega, et see suudab täpselt ennustada, kas sõna on fraasistruktuuri sees või mitte kontekstivektori väikesest arvust komponentidest. Isegi toortekstist õppimise puhul näidatakse, et konteksti vektorid on endiselt hästi korrelatsioonis fraasistruktuuridega. Lõpuks näitame, et funktsionaalsete sõnade loomulikud klastrid ja kõnede osa, mis käivitavad fraase, on esindatud LSTM konteksti-uuenduse vektori väikeses, kuid põhilises alamruumis.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>شبکه عصبی (LSTM) دوباره حافظه کوتاه مدت طولانی استفاده می‌شود و برای گرفتن وابستگی‌های سنتاکتیک طولانی اطلاعات شناخته می‌شود. با این حال، چگونه این اطلاعات در ویکتورهای داخلی آن برای متن طبیعی به اندازه کافی تحقیق نشده است. ما آنها را با یاد گرفتن مدل زبانی تحلیل می‌کنیم که ساختارهای سنتاکتیک به طور معنی داده می‌شوند. ما به صورت عمومی نشان می دهیم که ویکتورهای آگاهی محیط، یعنی نتیجه دروازه داخلی، تقریباً به ارزش دویینی یا ترنری کمک به مدل زبان برای شماره عمیق آگاهی دقیقا، به عنوان سوزگان et al. (2019) اخیرا برای زبانهای سینتاتیک دیک نشان می دهند. برای بعضی اندازه‌ها در ویکتور محیط، نشان می‌دهیم که فعالیت‌های آنها با عمیق ساختارهای عبارت، مثل VP و NP بسیار ارتباط دارند. علاوه بر این، با یک تنظیم L1، همچنین فهمیدیم که می تواند دقیقا پیش بینی کند که آیا یک کلمه داخل یک ساختار عبارت است یا نه از تعداد اندکی از بخش‌های ویکتور محیط است. حتی برای یادگیری از متن خالی، ویکتورهای محیط هنوز با ساختارهای عبارت به خوبی ارتباط دارند. بالاخره، ما نشان می دهیم که کلاس طبیعی از کلمات عملکرد و بخشی از سخنرانی که کلمات ماجرا می کنند در یک فضای کوچک ولی اصلی از ویکتور آغاز کنترل LSTM نشان می دهند.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Pitkän lyhyen aikavälin muistin toistuvia hermoverkkoja (LSTM) käytetään laajalti ja tiedetään keräävän informatiivisia pitkän aikavälin syntaktisia riippuvuuksia. Kuitenkin sitä, miten tällainen tieto heijastuu sen sisäisiin vektoreihin luonnolliselle tekstille, ei ole vielä riittävästi tutkittu. Analysoimme niitä oppimalla kielimallia, jossa syntaktiset rakenteet annetaan implisiittisesti. Empiirisesti osoitamme, että kontekstipäivityksen vektorit eli sisäisten porttien tuotokset kvantifioidaan likimäärin binaari- tai kolmikantaarvoiksi, jotta kielimalli pystyy laskemaan pesimisen syvyyden tarkasti, kuten Suzgun et al. (2019) äskettäin osoittivat synteettisille Dyck-kielille. Joidenkin kontekstivektorin ulottuvuuksien osalta osoitamme, että niiden aktivaatiot korreloivat voimakkaasti fraasirakenteiden syvyyteen, kuten VP ja NP. Lisäksi L1-säännöstelyn avulla huomasimme myös, että se pystyy ennustamaan tarkasti, onko sana fraasirakenteen sisällä vai ei pienestä määrästä kontekstivektorin komponentteja. Raakatekstistä opittaessa kontekstivektorit korreloivat edelleen hyvin fraasirakenteisiin. Lopuksi osoitamme, että funktionaalisten sanojen luonnolliset klusterit ja lauseita laukaisevat puheet ovat edustettuina LSTM:n kontekstipäivityksen vektorin pienessä mutta pääasiallisessa aliavaruudessa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Le réseau neuronal récurrent de mémoire à long terme (LSTM) est largement utilisé et connu pour capturer les dépendances syntaxiques informatives à long terme. Cependant, la façon dont ces informations sont reflétées dans ses vecteurs internes pour le texte naturel n'a pas encore été suffisamment étudiée. Nous les analysons en apprenant un modèle de langage où les structures syntaxiques sont implicitement données. Nous montrons empiriquement que les vecteurs de mise à jour du contexte, c'est-à-dire les sorties des portes internes, sont approximativement quantifiés en valeurs binaires ou ternaires pour aider le modèle de langage à compter avec précision la profondeur de l'imbrication, comme Suzgun et al. (2019) l'ont récemment montré pour les langages synthétiques Dyck. Pour certaines dimensions du vecteur de contexte, nous montrons que leurs activations sont fortement corrélées avec la profondeur des structures de phrases, telles que VP et NP. De plus, avec une régularisation L1, nous avons également constaté qu'elle peut prédire avec précision si un mot se trouve dans une structure de phrase ou non à partir d'un petit nombre de composantes du vecteur de contexte. Même dans le cas de l'apprentissage à partir de texte brut, les vecteurs contextuels sont toujours en bonne corrélation avec les structures de phrases. Enfin, nous montrons que les groupes naturels des mots fonctionnels et la partie des discours qui déclenchent des phrases sont représentés dans un sous-espace restreint mais principal du vecteur de mise à jour contextuelle de LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Úsáidtear go forleathan líonra néaraíoch athfhillteach Cuimhne Gearrthéarmach (LSTM) go forleathan agus is eol é chun spleáchais chomhréire fadtéarmacha faisnéiseacha a ghabháil. Mar sin féin, níl imscrúdú leordhóthanach déanta go fóill ar an gcaoi a léirítear an fhaisnéis sin ina veicteoirí inmheánacha do théacs nádúrtha. Déanaimid anailís orthu trí mhúnla teanga a fhoghlaim ina dtugtar struchtúir chomhréire go hintuigthe. Léirímid go heimpíreach go bhfuil na veicteoirí nuashonraithe comhthéacs, i.e. aschuir na ngeataí inmheánacha, cainníochtaithe a bheag nó a mhór go luachanna dénártha nó trínártha chun cabhrú leis an tsamhail teanga doimhneacht an neadaithe a chomhaireamh go cruinn, mar a dúirt Suzgun et al. (2019) seó le déanaí do theangacha sintéiseacha Dyck. I gcás roinnt toisí sa veicteoir comhthéacs, léirímid go bhfuil a gcuid gníomhachtaí comhghaolaithe go mór le doimhneacht struchtúir frása, mar shampla VP agus NP. Ina theannta sin, le gnáthú L1, fuaireamar amach freisin gur féidir leis a thuar go cruinn an bhfuil focal laistigh de struchtúr frása nó nach bhfuil ó líon beag comhpháirteanna den veicteoir comhthéacs. Fiú i gcás na foghlama ó théacs amh, taispeántar go bhfuil comhghaol maith fós idir veicteoirí comhthéacs agus struchtúir na bhfrása. Ar deireadh, léirímid go ndéantar braislí nádúrtha de na focail fheidhmiúla agus an chuid d’óráidí a spreagann frásaí a léiriú i bhfospás beag ach príomhúil de veicteoir comhthéacs-nuashonraithe LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>@ action: button A lokacin da, ko da ake yi wa wannan information a cikin hanyarsa na guda wa matsayin natsuwa ba'a iya ƙidãya ba. Ana anayya su da za'a sanar da wani misalin harshe a inda an bã su da tsaro masu haɗi. Tuna nuna kwamfyutan kodi ɗin agogo, misali, masu fitarwa na ƙõfõfin guda, za'a ƙayyade kima guda ko masu ƙari ko kuma za'a yi taimako da misalin harshen da za'a lissafa muhimmin sali da gaske, kamar Suzgun et al. (2019) na nuna wa harshen Dyck na ƙarami. Ga wasu tsohon cikin masu cikin masu sakan, Munã nuna cewa aikin su yana da giraffiyar da girgije na tsarin rasmi, kamar misali, vP da NP. Za kuma, da wani L1 ya ƙayyade shi, za mu gane cewa yana iya ƙayyade magana a cikin wani salo na rubutu ko kuma ba daga ƙarami ƙarami masu cikin sakan aikin muhalli. Ko dai da za'a sanar da daga matsayin raw, za'a nuna shiryoyi cikin muhimman da za'a yi daidai da tsarin rasmi. Haƙĩƙa, Munã nũna masu natsuwa na maganar aiki da rabon magana wanda ke fara magana za'a yi tasgaro a cikin ƙarami kuma mainli subpace of the context-update of LTRM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>רשת עצבית חוזרת לזיכרון ארוך לטווח קצר (LSTM) משתמשת באופן רחב וידועה לכלוף תלויות סינטקטיות מידעיות לטווח ארוך. עם זאת, איך מידע כזה משקף בוקטורים הפנימיים שלו לטקסט טבעי עדיין לא נחקר מספיק. We analyze them by learning a language model where syntactic structures are implicitly given. We empirically show that the context update vectors, i.e. outputs of internal gates, are approximately quantized to binary or ternary values to help the language model to count the depth of nesting accurately, as Suzgun et al. (2019) recently show for synthetic Dyck languages. למימדים מסוימים בוקטור הקשר, אנו מראים שהפעילות שלהם קשורות מאוד לעומק של מבנים ביטויים, כמו VP ו NP. חוץ מזה, עם ניתוח L1, מצאנו גם שהוא יכול לחזות בדיוק אם מילה נמצאת בתוך מבנה ביטוי או לא ממספר קטן של רכיבים של ווקטור הקשר. אפילו במקרה של לימוד מתוך טקסט חום, הוראים ויקטורי הקשר עדיין מתחברים היטב עם מבנים המשפטים. סוף סוף, אנו מראים כי קבוצות טבעיות של המילים המפעילות והחלק של נאומים שמפעילים ביטויים מייצגים בתא-חלל קטן אבל ראשי של ווקטור הקונקסט-עדכון של LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Long Short-Term Memory recurrent neural network (LSTM) व्यापक रूप से उपयोग किया जाता है और जानकारीपूर्ण दीर्घकालिक वाक्यात्मक निर्भरताओं को कैप्चर करने के लिए जाना जाता है। हालांकि, प्राकृतिक पाठ के लिए इस तरह की जानकारी अपने आंतरिक वैक्टर में कैसे परिलक्षित होती है, इसकी अभी तक पर्याप्त जांच नहीं की गई है। हम एक भाषा मॉडल सीखकर उनका विश्लेषण करते हैं जहां वाक्यात्मक संरचनाएं निहित रूप से दी जाती हैं। हम अनुभवजन्य रूप से दिखाते हैं कि संदर्भ अद्यतन वैक्टर, यानी आंतरिक गेट्स के आउटपुट, भाषा मॉडल को सटीक रूप से घोंसले के शिकार की गहराई की गणना करने में मदद करने के लिए लगभग बाइनरी या टर्नरी मूल्यों के लिए क्वांटाइज्ड होते हैं, जैसा कि सुज़गुन एट अल (2019) हाल ही में सिंथेटिक डाइक भाषाओं के लिए दिखाते हैं। संदर्भ वेक्टर में कुछ आयामों के लिए, हम दिखाते हैं कि उनके सक्रियण वीपी और एनपी जैसे वाक्यांश संरचनाओं की गहराई के साथ अत्यधिक सहसंबद्ध हैं। इसके अलावा, एक एल 1 नियमितीकरण के साथ, हमने यह भी पाया कि यह सटीक रूप से भविष्यवाणी कर सकता है कि कोई शब्द एक वाक्यांश संरचना के अंदर है या नहीं संदर्भ वेक्टर के घटकों की एक छोटी संख्या से। यहां तक कि कच्चे पाठ से सीखने के मामले के लिए, संदर्भ वैक्टर को अभी भी वाक्यांश संरचनाओं के साथ अच्छी तरह से सहसंबंधित दिखाया गया है। अंत में, हम दिखाते हैं कि कार्यात्मक शब्दों के प्राकृतिक समूहों और भाषणों के हिस्से को ट्रिगर करने वाले भाषणों का हिस्सा जो वाक्यांशों को ट्रिगर करता है, एलएसटीएम के संदर्भ-अपडेट वेक्टर के एक छोटे लेकिन प्रमुख सबस्पेस में दर्शाया जाता है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kratkoročna sjećanja ponovne neuralne mreže (LSTM) se široko koristi i poznaje kako bi uhvatili informativne dugoročne sintaktične zavisnosti. Međutim, kako se takve informacije odražavaju u svojim unutrašnjim vektorima prirodnog teksta još nisu dovoljno istražene. Analiziramo ih učeći jezički model gdje se sintaktičke strukture implicitno daju. Mi empirički pokazujemo da su vektori za aktualiziranje konteksta, tj. ishod unutrašnjih vrata, približno kvantizirani na binarne ili ternarne vrijednosti kako bi pomogli jezičkom modelu da precizno broji dubinu gnijezda, kao što je Suzgun et al. (2019) nedavno pokazuju za sintetičke jezike. Za neke dimenzije u kontekstskom vektoru pokazujemo da su njihove aktivacije visoko povezani s dubinom frazu strukture, poput VP i NP. Osim toga, s regularizacijom L1, također smo otkrili da može precizno predvidjeti da li je riječ unutar strukture fraze ili ne iz malog broja komponenta kontekstnog vektora. Čak i za slučaj učenja sa sirovog teksta, kontekstski vektori se pokazuju da se i dalje dobro povezuju sa strukturama fraza. Napokon, pokazujemo da su prirodni skupini funkcionalnih riječi i dio govora koje okidaju fraze predstavljeni u malom, ali glavnom podprostoru kontekstnog aktualnog vektora LSTM-a.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A hosszú rövid távú memória visszatérő neurális hálózatot (LSTM) széles körben használják és ismerik az információs hosszú távú szintaktikus függőségek rögzítésére. Azonban még nem vizsgálták megfelelően, hogy az ilyen információk hogyan tükröződnek a természetes szöveg belső vektoraiban. Elemezzük őket olyan nyelvi modell tanulásával, ahol a szintaktikus struktúrákat implicit módon megadjuk. empirikusan megmutatjuk, hogy a kontextusfrissítési vektorok, azaz a belső kapuk kimenetei, megközelítőleg bináris vagy ternáris értékekre vannak kvantizálva, hogy segítsen a nyelvmodell pontosan megszámolni a fészkelés mélységét, amint azt a Suzgun és mások (2019) mutatják a szintetikus Dyck nyelvek esetében. A kontextusvektorban lévő néhány dimenzió esetében megmutatjuk, hogy aktiválásuk nagymértékben korrelálódik a kifejezési struktúrák mélységével, mint például a VP és az NP. Ezenkívül az L1 szabályozással azt is találtuk, hogy pontosan megjósolhatja, hogy egy szó egy kifejezési struktúrában van-e vagy sem a kontextusvektor kis számú összetevőjéből. Még a nyers szövegből való tanulás esetén is kimutatható, hogy a kontextusvektorok még mindig jól korrelálnak a kifejezési struktúrákkal. Végül megmutatjuk, hogy a funkcionális szavak természetes klaszterei és a beszédek kiváltó része az LSTM kontextusfrissítő vektorának kis, de fő szubtérében jelenik meg.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Լանգ կարճ ժամանակահատվածի հիշողության կրկնվող նյարդային ցանցը (LSMT) լայնորեն օգտագործվում է և հայտնի է ինֆորմատիվ երկար ժամանակահատվածի սինտակտիկ կախվածությունների ձայնագրման համար: Այնուամենայնիվ, ինչպե՞ս է այդ տեղեկատվությունը արտացոլում բնական տեքստի ներքին վեկտորներում, դեռևս բավարար չափով չի ուսումնասիրել: We analyze them by learning a language model where syntactic structures are implicitly given. Մենք էմպրիկապես ցույց ենք տալիս, որ կոնտեքստի վերականգնման վեկտորները, այսինքն ներքին դարպասների արտադրումները, մոտավորապես քվանտավորված են երկու կամ երկրորդ արժեքների վրա, որպեսզի օգնեն լեզվի մոդելը ճշգրիտ հաշվարկել խմբավարման խորությունը, ինչպես Suzգուն և այլն (2019 Կոնտեքստի վեկտորի որոշ չափումների համար մենք ցույց ենք տալիս, որ նրանց ակտիվացվածքները շատ կապված են արտահայտության կառուցվածքների խորության հետ, ինչպիսիք են VP և NP: Ավելին, L1-ի կարգավորման դեպքում մենք նաև հայտնաբերեցինք, որ այն կարող է ճշգրիտ կանխատեսել, թե բառը գտնվում է արտահայտության կառուցվածքի մեջ, թե ոչ կոնտեքստի վեկտորի մի փոքր քանակից: Նույնիսկ ոչ մշակված տեքստից սովորելու դեպքում ցույց է տալիս, որ կոնտեքստի վեկտորները դեռևս լավ կապված են արտահայտության կառուցվածքների հետ: Վերջապես, մենք ցույց ենք տալիս, որ ֆունկցիոնալ բառերի բնական խմբերը և խոսքերի մասը, որոնք արտահայտում են արտահայտություններ, ներկայացված են LSMT-ի կոնտեքստի վերականգնման վեկտորի փոքր, բայց հիմնական ենթատարածքում:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Jaringan saraf recurrent Memori Panjang Term (LSTM) sangat digunakan dan dikenal untuk menangkap dependensi sintaksi informatif jangka panjang. Namun, bagaimana informasi tersebut terrefleksi dalam vektor dalamnya untuk teks alami belum cukup diselidiki. Kami menganalisis mereka dengan mempelajari model bahasa di mana struktur sintaksi secara implicit diberikan. Kami empiris menunjukkan bahwa vektor pembaruan konteks, i.e. output dari gerbang interna, sekitar kuantisasi ke nilai binar atau ternar untuk membantu model bahasa menghitung kedalaman sarang dengan akurat, seperti Suzgun et al. (2019) baru-baru ini menunjukkan untuk bahasa Dyck sintetik. Untuk beberapa dimensi dalam vektor konteks, kita menunjukkan bahwa aktivasi mereka sangat terkait dengan kedalaman struktur frasa, seperti VP dan NP. Selain itu, dengan regularisasi L1, kami juga menemukan bahwa dapat memprediksi dengan akurat apakah kata berada dalam struktur frasa atau tidak dari sejumlah kecil komponen vektor konteks. Even for the case of learning from raw text, context vectors are shown to still correlate well with the phrase structures. Akhirnya, kami menunjukkan bahwa kumpulan alami dari kata-kata fungsional dan bagian dari pidato yang memicu frasa adalah mewakili dalam subspace kecil tetapi utama vektor konteks-update LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>La rete neurale ricorrente di memoria a breve termine (LSTM) è ampiamente usata e conosciuta per catturare dipendenze sintattiche informative a lungo termine. Tuttavia, il modo in cui tali informazioni si riflettono nei suoi vettori interni per il testo naturale non è ancora stato sufficientemente studiato. Li analizziamo imparando un modello linguistico in cui sono implicitamente date strutture sintattiche. Mostriamo empiricamente che i vettori di aggiornamento del contesto, cioè le uscite di porte interne, sono approssimativamente quantizzati a valori binari o ternari per aiutare il modello linguistico a contare con precisione la profondità del nesting, come dimostrano recentemente Suzgun et al. (2019) per i linguaggi sintetici Dyck. Per alcune dimensioni nel vettore di contesto, mostriamo che le loro attivazioni sono altamente correlate con la profondità delle strutture di frase, come VP e NP. Inoltre, con una regolarizzazione L1, abbiamo anche scoperto che può prevedere con precisione se una parola è all'interno di una struttura di frase o meno da un piccolo numero di componenti del vettore di contesto. Anche nel caso di apprendimento dal testo grezzo, i vettori di contesto sono ancora ben correlati con le strutture delle frasi. Infine, mostriamo che i cluster naturali delle parole funzionali e la parte dei discorsi che innescano le frasi sono rappresentati in un piccolo ma principale sottospazio del vettore di aggiornamento del contesto di LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>長期記憶再帰ニューラルネットワーク（ ＬＳＴＭ ）は、情報的な長期構文依存性を捕捉するために広く使用され、知られている。 しかしながら、そのような情報が自然なテキストの内部ベクトルにどのように反映されるかは、まだ十分に調査されていない。 構文構造が暗黙的に与えられている言語モデルを学習することで、それらを分析します。 文脈更新ベクトル、すなわち内部ゲートの出力は、Suzgun et al .( 2019)が最近合成Dyck言語について示したように、言語モデルが入れ子の深さを正確にカウントするのに役立つように、二進数または三進数値に近い量子化されていることを経験的に示している。 コンテキストベクトルのいくつかの次元について、我々は、それらの活性化が、ＶＰ及びＮＰなどの語句構造の深さと高度に相関していることを示す。 さらに、L 1規則化では、単語がフレーズ構造内にあるかどうかをコンテキストベクトルの少数のコンポーネントから正確に予測できることもわかりました。 生テキストから学習する場合でも、コンテキストベクトルは依然としてフレーズ構造と良好に相関することが示されている。 最後に、機能単語の自然なクラスタと、フレーズを引き起こすスピーチの一部が、LSTMのコンテキスト更新ベクトルの小さいが主なサブスペースで表されることを示します。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>AllProgressBarUpdates politenessoffpolite"), and when there is a change ("assertivepoliteness Genjer-Genjer Awak dhéwé empirecally show that the context Updates vectors, i.e. output of intermediate gateway string" in "context_BAR_stringLink politenessoffpolite"), and when there is a change ("assertivepoliteness Text FindOK</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ძირითადი სიმხმარების რეკურენტი ნეირალური ქსელი (LSTM) widely used and known to capture informative long-term syntactic dependencies. მაგრამ, როგორ ეს ინფორმაცია განსაზღვრებულია ჩემი ინტერქსტური გვექტორებში, როგორ ჩემი ინტერქსტური ტექსტისთვის უკვე არ იყო ჩვენ ისინი ანალიზებთ ენის მოდელის შესწავლებით, სადაც სინტაქტიკური სტრუქტურები უნდა იყოს. ჩვენ ემპერიკურად ჩვენ აჩვენებთ, რომ კონტექსტური განახლებელი გვეკტენტები, მაგალითად ინტერესტური გეკტების გადასვლები, უფრო კვანტიზებულია ბიუნარი ან ტერნარიური მნიშვნელობებისთვის, როგორც სინტეტიკური დასვლების ზოგიერთი განზომილებებისთვის კონტექსტურის გვეკტორიში, ჩვენ ჩვენ ჩვენ ჩვენ ჩვენ ჩვენ ჩვენ ჩვენ ჩვენებთ, რომ მათი აქტივაციები ძალიან კონტექსტურაციების სიმაღლ დამატებით, L1 რეგილარიზაციით, ჩვენ დავიწყეთ, რომ ის შეუძლია წარმოდგინოთ თუ სიტყვა ფრაზის სტრუქტურაციაში არის ან არა კონტექსტურის პატარა კომპონენტების კონ კონტექსტური ტექსტიდან მესწავლის შემთხვევაში, კონტექსტური გვექტორები ჩვენებულია, რომ ფრაზების სტრუქტურებისთვის კონტექსტურა საბოლოოდ, ჩვენ ჩვენ აჩვენებთ, რომ ფუნქციონალური სიტყვების ნაირთი კლასტერი და სიტყვების ნაწილი, რომელიც გამოწყენებს ფრაზების ნაირთი, მაგრამ პირველური სამყაროში LSTM-ის კონ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Қысқа уақыт жады қайталанатын невралдық желі (LSTM) көп қолданылады және мәліметтік ұзын уақыт синтактикалық тәуелдіктерді алу үшін беймәлім. Бірақ бұл мәлімет тәуелді мәтін үшін ішкі векторларында қалай көрсетіліп тұрған жоқ. Біз оларды синтактикалық құрылғылар келтірілген тіл үлгісін оқып анализ. Біз импирикалық түрде контексті жаңарту векторлары, т.е. ішкі қапшықтардың шығысы, тіл үлгісін дұрыс рет есептеу үшін, Suzgun et al. (2019) синтетикалық дик тілдер үшін көрсетеді. Контексті векторының кейбір өлшемдері үшін, олардың белсенділіктері, VP мен NP секілді сөздер құрылымының тереңілігімен байланысты деп көрсетеді. Сонымен қатар, L1 үлгіліліктемесімен де ол сөздің құрылымының ішінде немесе контексті векторының кішкентай компоненттерінен емес екенін дұрыс түсіндіре алады. Мәтіннен оқыту үшін де, контексті векторлар сөздер құрылымына әлі жақсы сәйкес келеді. Соңында, біз функциялық сөздердің табиғи кластері және сөздерді бастайтын сөздердің бөлігін LSTM контексті жаңарту векторының кішкентай, бірақ негізгі ішкі жерінде көрсетіледі.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>장-단기기억순환신경망(LSTM)은 장기 문법 의존 정보를 얻는 데 광범위하게 응용된다.그러나 이러한 정보가 자연 텍스트의 내부 벡터에 어떻게 반영되는지는 아직 충분한 연구를 받지 못했다.우리는 은식으로 문법 구조를 제시하는 언어 모델을 배워서 그것들을 분석한다.우리의 경험에 의하면 상하문 업데이트 벡터, 즉 내부 부서의 출력은 근사량화되어 2원 또는 3원 값으로 양화되어 언어 모델이 삽입 깊이를 정확하게 계산하는 데 도움을 준다. Suzgun 등(2019)이 최근에 합성Dyck 언어에 한 것처럼.언어 환경 벡터의 일부 차원에 대해 우리는 그것들의 활성화가 단어 구조의 깊이와 관련이 있다는 것을 발견했다. 예를 들어 VP와 NP이다.또한 L1의 정규화를 통해 우리는 상하문에서 양의 소량의 분량으로 단어가 단어 구조에 있는지 정확하게 예측할 수 있음을 발견했다.비록 원시 텍스트에서 배운 경우에도 상하문 벡터는 여전히 단어 구조와 매우 좋은 관련성을 가진다.마지막으로 우리는 기능어의 자연 집합과 촉발 단어의 음성 부분이 LSTM 상하문 업데이트 벡터의 작지만 주요한 하위 공간에서 표시된다는 것을 증명했다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Long Short-Term Memory recurrent neural network (LSTM) is widely used and known to capture informative long-term syntactic dependencies. Vis dėlto, kaip tokia informacija atsispindi jos vidiniuose natūralaus teksto vektoriuose, dar nebuvo pakankamai ištirta. Analizuojame juos mokydami kalbos model į, kuriame numatomai pateikiamos sintaksinės struktūros. Empiriškai rodome, kad konteksto atnaujinimo vektoriai, t. y. vidinių vartų i šėjimai, yra maždaug kiekybiškai išmatuoti į dvišales arba trišales vertes, siekiant padėti kalbos modeliui tiksliai apskaičiuoti nesting gylį, kaip neseniai rodo Suzgun et al. (2019) sintetinių Dyck kalbų atžvilgiu. Kai kuriems konteksto vektoriaus matmenys rodo, kad jų aktyvinimas labai susijęs su frazių struktūrų gyliu, pvz., VP ir NP. Be to, nustatydami L1 reguliavimą, mes taip pat nustatėme, kad jis gali tiksliai nuspėti, ar žodis yra frazės struktūroje, ar ne iš nedidelio kontekstinio vektoriaus komponentų. Net mokymosi iš žaliavinio teksto atveju įrodoma, kad konteksto vektoriai vis dar gerai koreliuoja su frazės struktūromis. Finally, we show that natural clusters of the functional words and the part of speeches that trigger phrases are represented in a small but principal subspace of the context-update vector of LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Долга краткорочна меморија рецидентната нервна мрежа (LSTM) е широко употребена и позната за заземање информативни долгорочни синтактички зависности. Сепак, како ваквите информации се рефлектираат во своите внатрешни вектори за природен текст сé уште не е доволно истражено. Ние ги анализираме со учење на јазички модел каде што инплицитно се дадени синтактички структури. Емпирички покажуваме дека векторите за контекстно ажурирање, т.е. излезите од внатрешните порти, се околу квантизирани на бинарни или тринарни вредности за да му помогнат на јазичкиот модел да ја брои длабочината на гнездото точно, како што Suzgun et al. (2019) неодамна покажуваат за синтетичките јазици Dyck. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP. Покрај тоа, со регуларизација на L1, исто така откривме дека може точно да предвиде дали зборот е внатре во фразата структура или не од мал број компоненти на контекстниот вектор. Дури и за случајот на учење од суров текст, се покажува дека контекстните вектори сé уште се корелираат добро со фразните структури. Конечно, покажуваме дека природните групи на функционалните зборови и делот од говоровите кои предизвикуваат фрази се претставени во мал, но главен подпростор на контекстниот вектор на LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>നീണ്ട മെമ്മറി ആവര്‍ത്തിച്ചുകൊണ്ടിരിക്കുന്ന നെയൂറല്‍ ശൃംഖല (LSTM) വിശാലമായി ഉപയോഗിക്കുന്നു. വിവരങ്ങള്‍ വിവരങ്ങള്‍ നീണ്ട നീ എങ്കിലും സ്വാഭാവിക വാചകത്തിനായി ഇത്തരം വിവരങ്ങള്‍ എങ്ങനെയാണ് പ്രത്യേകിക്കുന്നതെന്ന് നോക്കിയിട്ടുള്ളത്. ഒരു ഭാഷ മോഡല്‍ പഠിപ്പിക്കുന്നത് കൊണ്ട് നമ്മള്‍ അവരെ അന്വേഷിക്കുന്നു. അവിടെ സിനിട്ടാക്റ്റിക്ക് കൂ നമ്മള്‍ ശാസ്ത്രികമായി കാണിക്കുന്നു സുസ്സുന്‍ എറ്റ് അല്‍. (2019) അടുത്തുതന്നെ സിന്തെറ്റിക്ക് ഭാഷയുടെ ആഴത്തിന്റെ ആഴത്തെ എണ്ണുന്നതിന് സഹായിക്കാന്‍ വെക്സ്റ്റര്‍ വെക്റ്റര്‍ ആഴ വെക്സ്റ്റെക്റ്റരിലെ ചില ഭാഗങ്ങള്‍ക്ക്, നമ്മള്‍ കാണിക്കുന്നു, അവരുടെ പ്രവര്‍ത്തനങ്ങള്‍ വാക്കുകളുടെ ആഴത്തിന്റെ ആഴത്തില്‍ വളരെ ബന ഒരു L1 ന്റെ നിയന്ത്രണം കൊണ്ട്, ഒരു വാക്ക് ഒരു വാക്കിന്റെ അടിസ്ഥാനത്തിലുണ്ടോ അല്ലെങ്കില്‍ വെക്സ്റ്റോറിന്റെ ചെറിയ എണ്ണം ഭാഗങ്ങളില്‍ ന ചുരുക്കുന്ന പദാവലിയില്‍ നിന്നും പഠിക്കുന്നതിനായി പോലും വെക്സ്റ്റെന്റ് വെക്റ്ററുകള്‍ വാക്കുകളുടെ അടിസ് അവസാനം, നമ്മള്‍ കാണിച്ചുകൊടുക്കുന്നത് പ്രകൃതിക വാക്കുകളുടെ സ്വാഭാവികമായ ക്ലസ്റ്ററുകളും, വാക്കുകള്‍ തുടങ്ങുന്ന വാക്കുകളുടെ ഭാഗവും എള്‍സ്റ്</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Урт богино-Term Memory recurrent neural network (LSTM) нь мэдээллийн урт хугацааны синтактик хамааралтай хамааралтай байдлыг авах болон мэддэг. Гэхдээ энэ мэдээлэл байгалийн текст дээр хэрхэн доторх векторууд харагдаж байгааг харуулж чадахгүй. Бид тэднийг хэл загвар суралцаж шинжлэх ухаан өгдөг. Бид дотоод нутгийн газрын шинэчлэлүүд нь хэлний загварын гүн гүнзгийг тодорхойлох тулд ойролцоогоор хоёр эсвэл үеийн утгыг тооцоолж сузгуун et al. (2019) саяхан Синтетик Дик хэлний тухай харуулж байна. Төвчтөн векторын зарим хэмжээсүүдийн тулд бид тэдний үйлдвэрлэлүүдийг VP болон NP зэрэг хэлбэрийн гүн гүнзгий холбоотой гэдгийг харуулж байна. Түүнчлэн, L1-н шугам хэлбэрээр бид мөн тодорхой хэлбэрээр үг хэлбэрийн бүтэц дотор байгааг эсвэл тодорхой хэлбэрээс биш гэдгийг ойлгосон. Хөгжин текстээс суралцах тохиолдолд хүртэл контекст векторууд хэлэлцээний бүтэцтэй холбоотой байдаг. Эцэст нь бид функцийн үгийн байгалийн кластерууд болон өгүүлбэрүүдийн нэг хэсэг нь LSTM-ын контекст жагсаалттай векторын жижиг гэхдээ үндсэн суурь зайд илэрхийлэгддэг.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Rangkaian saraf berkurang ingatan jangka pendek panjang (LSTM) digunakan secara luas dan diketahui untuk menangkap dependensi sintaktik jangka panjang maklumat. Bagaimanapun, bagaimana maklumat tersebut diselarang dalam vektor dalamnya untuk teks semulajadi belum diselesaikan cukup. Kami menganalisis mereka dengan mempelajari model bahasa di mana struktur sintaktik secara implicit diberikan. Kami empirik menunjukkan bahawa vektor kemaskini konteks, iaitu output gerbang dalaman, sekitar dikwantifikasikan kepada nilai binari atau ternary untuk membantu model bahasa menghitung kedalaman sarang dengan tepat, seperti Suzgun et al. (2019) baru-baru ini menunjukkan untuk bahasa Dyck sintetik. Untuk beberapa dimensi dalam vektor konteks, kita menunjukkan bahawa aktivasi mereka sangat berkorelaci dengan kedalaman struktur frasa, seperti VP dan NP. Moreover, with an L1 regularization, we also found that it can accurately predict whether a word is inside a phrase structure or not from a small number of components of the context vector. Walaupun untuk kes belajar dari teks mentah, vektor konteks dipaparkan masih berkorelasi dengan struktur frasa. Akhirnya, kita tunjukkan bahawa kumpulan alami perkataan berfungsi dan bahagian ucapan yang memicu frasa adalah mewakili dalam subspace kecil tetapi utama vektor kemaskini-konteks LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In-netwerk newrali rikorrenti tal-Memorja fuq medda qasira ta’ żmien twil (LSTM) jintuża b’mod wiesa’ u huwa magħruf li jaqbad dipendenzi sintattiċi informativi fuq medda twila ta’ żmien. Madankollu, kif tali informazzjoni tiġi riflessa fil-vetturi interni tagħha għat-test naturali għadha ma ġietx investigata biżżejjed. Aħna nianalizzawhom billi nitgħallmu mudell lingwistiku fejn jiġu impliċitament mogħtija strutturi sintattiċi. B’mod empiriku nuru li l-vetturi ta’ aġġornament tal-kuntest, jiġifieri l-ħruġ ta’ gates interni, huma kwantifikati bejn wieħed u i e ħor għal valuri binarji jew ternarji biex jgħinu lill-mudell tal-lingwa jgħodd il-fond tal-nidd b’mod preċi ż, kif juru Suzgun et al. (2019) reċentement għal-lingwi sintetiċi Dyck. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP. Barra minn hekk, b’regolarizzazzjoni L1, sabna wkoll li tista’ tbassar b’mod preċiż jekk kelma tkunx ġewwa struttura ta’ frażi jew le minn numru żgħir ta’ komponenti tal-vettur tal-kuntest. Anki fil-każ tat-tagħlim mit-test mhux ipproċessat, jidher li l-vetturi tal-kuntest għadhom jikkorrelaw tajjeb mal-istrutturi tal-frażi. Fl-a ħħar nett, nuru li raggruppamenti naturali tal-kliem funzjonali u l-parti tad-diskorsi li jiskattaw il-frażijiet huma rappreżentati f’sottospazju żgħir iżda prinċipali tal-vettur ta’ aġġornament tal-kuntest tal-LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Long Short-Term Memory Recidiviering Neural Network (LSTM) wordt veel gebruikt en staat erom bekend informatieve syntactische afhankelijkheden op lange termijn vast te leggen. Hoe dergelijke informatie wordt weerspiegeld in de interne vectoren voor natuurlijke tekst is echter nog niet voldoende onderzocht. We analyseren ze door een taalmodel te leren waarbij syntactische structuren impliciet gegeven zijn. We tonen empirisch aan dat de context update vectoren, d.w.z. outputs van interne gates, ongeveer gekwantiseerd zijn tot binaire of ternaire waarden om het taalmodel te helpen de diepte van nesting nauwkeurig te tellen, zoals Suzgun et al. (2019) onlangs laten zien voor synthetische Dyck talen. Voor sommige dimensies in de contextvector laten we zien dat hun activeringen sterk gecorreleerd zijn met de diepte van frasestructuren, zoals VP en NP. Bovendien hebben we met een L1 regularisatie ook ontdekt dat het nauwkeurig kan voorspellen of een woord binnen een frasestructuur zit of niet van een klein aantal componenten van de contextvector. Zelfs in het geval van leren van ruwe tekst, wordt aangetoond dat contextvectoren nog steeds goed correleren met de frasestructuren. Tot slot laten we zien dat natuurlijke clusters van de functionele woorden en het deel van toespraken die zinnen triggeren vertegenwoordigd zijn in een kleine maar belangrijkste subruimte van de context-update vector van LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Lang kortmann minne gjentakaste neuralnettverk (LSTM) er breidd brukt og kjent for å henta informativ langsiktige syntaksiske avhengighet. Men korleis slike informasjon er reflektert i sine interne vektorar for naturtekst enno ikkje er nok undersøkt. Vi analyserer dei ved å lære eit språk-modell der syntaktiske strukturar er implisitt gitt. Vi viser empirisk at kontekstoppdateringsvectorane, dvs. utdata av interne portar, er omtrent kvantiserte til binære eller ternare verdiar for å hjelpa språk-modellen til å telja dybde på nestinga nøyaktig, som Suzgun et al. (2019) nyleg viser for syntetiske dykkspråk. For nokre dimensjonar i kontekstvektoren viser vi at aktivasjonane sine er svært korrelaterte med dybde på frasestrukturar, som VP og NP. I tillegg har vi også funne at det kan nøyaktig foregå om eit ord er inne i eit fråstruktur eller ikkje frå ein liten tal komponentar i kontekstvektoren. I tillegg til å lære frå råtekst, vert kontekstvektorane viste til å fortsatt korrelasjona godt med frasenstrukturene. I slutt viser vi at naturlige grupper av funksjonelle ord og delen av språk som utløyser frasar er representert i ein liten, men hovudplass av den kontekstoppdaterte vektoren av LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Rekrutująca sieć neuronowa z pamięci krótkoterminowej (LSTM) jest szeroko stosowana i znana z przechwytywania informacyjnych długoterminowych zależności składni. Nie zostało jednak jeszcze wystarczająco zbadane, w jaki sposób informacje te znajdują odzwierciedlenie w wewnętrznych wektorach tekstu naturalnego. Analizujemy je poprzez uczenie się modelu językowego, w którym struktury składni są domyślnie podane. Empirycznie pokazujemy, że wektory aktualizacji kontekstu, tj. wyjścia bram wewnętrznych, są w przybliżeniu kwantyzowane do wartości binarnych lub trójstronnych, aby pomóc modelowi językowi dokładnie liczyć głębokość zagnieżdżenia, jak pokazują ostatnio Suzgun i wszyscy. (2019) dla syntetycznych języków Dycka. Dla niektórych wymiarów wektora kontekstowego pokazujemy, że ich aktywacje są wysoce skorelowane z głębokością struktur fraz, takich jak VP i NP. Ponadto, przy regularyzacji L1, stwierdziliśmy również, że może ona dokładnie przewidzieć, czy słowo znajduje się wewnątrz struktury fraz, czy nie z niewielkiej liczby składników wektora kontekstu. Nawet w przypadku uczenia się z tekstu surowego wektory kontekstowe nadal dobrze korelują ze strukturami fraz. Na koniec pokazujemy, że naturalne gromady słów funkcjonalnych i części wypowiedzi, które wywołują frazy, są reprezentowane w małej, ale głównej podprzestrzeni wektora aktualizacji kontekstu LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Long Short-Term Memory Recurrent Network (LSTM) é amplamente utilizado e conhecido por capturar dependências sintáticas informativas de longo prazo. No entanto, como tais informações são refletidas em seus vetores internos para texto natural ainda não foi suficientemente investigado. Nós os analisamos aprendendo um modelo de linguagem onde as estruturas sintáticas são dadas implicitamente. Mostramos empiricamente que os vetores de atualização de contexto, ou seja, saídas de portas internas, são aproximadamente quantizados para valores binários ou ternários para ajudar o modelo de linguagem a contar a profundidade de aninhamento com precisão, como Suzgun et al. (2019) mostram recentemente para linguagens sintéticas Dyck. Para algumas dimensões no vetor de contexto, mostramos que suas ativações estão altamente correlacionadas com a profundidade das estruturas de frases, como VP e NP. Além disso, com uma regularização L1, também descobrimos que ela pode prever com precisão se uma palavra está dentro de uma estrutura frasal ou não a partir de um pequeno número de componentes do vetor de contexto. Mesmo para o caso de aprendizado com texto bruto, os vetores de contexto ainda se correlacionam bem com as estruturas de frases. Por fim, mostramos que os agrupamentos naturais das palavras funcionais e a parte das falas que acionam as frases são representados em um pequeno mas principal subespaço do vetor de atualização de contexto do LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Rețeaua neurală recurentă de memorie pe termen scurt (LSTM) este utilizată pe scară largă și cunoscută pentru a capta dependențe sintactice informative pe termen lung. Cu toate acestea, modul în care aceste informații sunt reflectate în vectorii interni ai textului natural nu a fost încă investigat suficient. Le analizăm prin învățarea unui model lingvistic în care structurile sintactice sunt date implicit. Aratăm empiric că vectorii actualizării contextului, adică ieșirile porților interne, sunt aproximativ cuantificați la valori binare sau ternare pentru a ajuta modelul limbajului să numere cu precizie adâncimea cuibării, așa cum arată Suzgun et al. (2019) recent pentru limbile Dyck sintetice. Pentru unele dimensiuni din vectorul contextual, arătăm că activările lor sunt foarte corelate cu adâncimea structurilor de fraze, cum ar fi VP și NP. Mai mult decât atât, cu o regularizare L1, am descoperit, de asemenea, că poate prezice cu exactitate dacă un cuvânt se află în interiorul unei structuri de frază sau nu dintr-un număr mic de componente ale vectorului contextual. Chiar și în cazul învățării din text brut, vectorii contextului sunt încă corelați bine cu structurile frazelor. În cele din urmă, arătăm că grupurile naturale de cuvinte funcționale și partea de discursuri care declanșează fraze sunt reprezentate într-un subspațiu mic, dar principal al vectorului de actualizare a contextului LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Рекуррентная нейронная сеть с длительной краткосрочной памятью (LSTM) широко используется и известна для захвата информативных долгосрочных синтаксических зависимостей. Однако вопрос о том, каким образом такая информация отражается в ее внутренних векторах для естественного текста, еще не изучен в достаточной степени. Мы анализируем их, изучая языковую модель, где синтаксические структуры задаются неявно. Мы эмпирически показываем, что векторы обновления контекста, то есть выходы внутренних ворот, приблизительно квантуются в двоичные или тернарные значения, чтобы помочь языковой модели точно подсчитать глубину вложенности, как недавно показали Suzgun et al. (2019) для синтетических языков Dyck. Для некоторых измерений в векторе контекста показываем, что их активации сильно коррелируют с глубиной фразных структур, таких как VP и NP. Кроме того, при регуляризации L1 мы также обнаружили, что он может точно предсказать, находится ли слово внутри структуры фразы или нет из небольшого количества компонентов контекстного вектора. Даже в случае обучения на основе необработанного текста контекстные векторы по-прежнему хорошо коррелируют со структурами фраз. Наконец, мы показываем, что естественные кластеры функциональных слов и часть речей, которые запускают фразы, представлены в небольшом, но главном подпространстве контекстно-обновленного вектора LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ලොකු කොටි වාර්තාවක් මතකය ආපහු ප්‍රතික්‍රීය න්‍යූරල් ජාලය (LSTM) භාවිත වෙනවා සහ තොරතුරු ලොකු වාර්තාවක් සංවි නමුත්, මේ තොරතුරු කොහොමද ස්වභාවික පාළුවක් සඳහා ඇතුළු වෙක්ටර් වලට ප්‍රතිකෘති වෙන්නේ. අපි ඔවුන්ව විශ්ලේෂ කරනවා භාෂාවක් මොඩල් ඉගෙන ගන්න කියලා, කියලා සංවිධානය සංවිධාන අපි පෙන්වන්නේ සාමාන්‍ය වෙක්ටර් අවස්ථාව, ඉතින් ඇතුළු ගේට්ටුවන් අවස්ථාව, බායිනාරි නැත්තර් අවස්ථාවක් සඳහා භාෂාව මදුල්යට උදව් කරන්න, සුස්ගුන් ට් ල් සම්බන්ධ වෙක්ටර් වලින් කිසිම පරීක්ෂණයක් වෙනුවෙන්, අපි පෙන්වන්නේ ඔවුන්ගේ සක්‍රියාවන් ගොඩක් සම්බන්ධ වෙනවා ක එතකොට, L1 සාමාන්‍ය විස්තරයක් සමග, අපි හොයාගත්තා ඒක හරියට පුළුවන් වචනයක් ප්‍රශ්නයක් සංවිධානයක් ඇතුලේ තියෙන්නේ නැ පිළිබඳින් ඉගෙන ඉගෙන ගන්න ප්‍රතිකාරය සමහර වෙක්ටර්ස් පෙන්වන්න පුළුවන්. අන්තිමේදි, අපි පෙන්වන්නේ වැඩ වචනයේ ස්වභාවික කොටස් සහ ප්‍රතිකාරිය වචනයේ කොටස් සහ ප්‍රතිකාරිය වචනයේ ප්‍රතිකාරිත වෙක්ටර් LSTM ග</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dolgoročno kratkoročno ponovno nevronsko omrežje (LSTM) se široko uporablja in zna, da zajema informativne dolgoročne sintaktične odvisnosti. Vendar pa, kako se takšne informacije odražajo v notranjih vektorjih naravnega besedila, še ni bilo dovolj raziskano. Analiziramo jih z učenjem jezikovnega modela, v katerem so implicitno navedene sintaktične strukture. Empirično kažemo, da so vektorji kontekstne posodobitve, tj. izhodi notranjih vrat, približno kvantizirani na binarne ali tristranske vrednosti, da bi jezikovni model pomagal natančno šteti globino gnezdenja, kot so pred kratkim pokazali Suzgun et al. (2019) za sintetične Dyckove jezike. Za nekatere dimenzije kontekstnega vektorja pokažemo, da so njihove aktivacije močno korelacirane z globino fraznih struktur, kot sta VP in NP. Poleg tega smo z ureditvijo L1 ugotovili tudi, da lahko natančno predvidi, ali je beseda znotraj frazne strukture ali ne iz majhnega števila komponent kontekstnega vektorja. Tudi v primeru učenja iz surovega besedila je dokazano, da kontekstni vektorji še vedno dobro korelacirajo s strukturami fraz. Na koncu pokažemo, da so naravni grozdi funkcionalnih besed in del govorov, ki sprožajo fraze, predstavljeni v majhnem, a glavnem podprostoru vektorja kontekstne posodobitve LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Shabakadda neurada ee soo socda ee xasuusta waqtiga dheer (LSTM) waxaa loo isticmaalaa oo loo yaqaan inuu qabsado macluumaad ku saabsan xirfadaha la xiriira waqtiga dheer. Si kastaba ha ahaatee sida macluumaadkaas looga fiiriyo wadooyinkooda gudaha ah ee qoraalka dabiicadda ah weli looma baahan yahay si kugu filan. Waxaannu ku baaraynaa barashada model luuqada, taasoo lagu siiyo dhismaha muusikada si aan waxtarla’aan ah. Waxaynu si fudud ugu muujinnaa in wadooyinka cusboonaysiinta irdaha gudaha ah ay qiyaastii qiimaha labaad ama beeraha dhexe lagu qiimeeyaa in lagu caawiyo modelka afka si saxda ah u tiriyo moolka guriga, sida Suzgun et al. (2019) ugu dhowaad u muujiyo luqada syntettika Dykk. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP. Sidoo kale waxaynu aragnay in uu si sax ah u sii sheegi karo in ereygu uu ku jiro dhismaha hadalka ama in uu ka yimaado qeybo yar oo ka mid ah vectorka hoose. Xataa haddii aad wax ka baraneyso qoraalka saxda ah, waxaa sidoo kale la muujiyaa wado qalabka kooxaha ah oo ay ku xiriiraan dhismaha afka. Ugu dambaysta, waxaynu muujinnaa kooxo asalka ah ee hadallada waxqabadka iyo qayb ka mid ah hadallada ay ka soo baxaan waxay ka muuqan yihiin mid yar ee hoos-hoose ee ka mid ah vector-updates of LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kujtesa e gjatë afat-shkurtër rrjeti neural recurrent (LSTM) përdoret gjerësisht dhe është i njohur për të kapur varësitë sintaktike afat-gjatë informative. Megjithatë, si informacioni i tillë pasqyrohet në vektorët e tij të brendshëm për tekstin natyror nuk është hetuar ende në mënyrë të mjaftueshme. Ne i analizojmë ato duke mësuar një model gjuhësh ku strukturat sintaktike janë dhënë implicitisht. Ne empirikisht tregojmë se vektorët e përditësimit të kontekstit, i.e. daljet e portave të brendshme, janë përafërsisht kuantizuar në vlera binare apo ternare për të ndihmuar modelin gjuhësor të numërojë thellësinë e foljes me saktësi, siç tregojnë Suzgun et al. (2019) kohët e fundit për gjuhët sintetike Dyck. Për disa dimensione në vektorin e kontekstit, ne tregojmë se aktivitetet e tyre janë shumë të lidhura me thellësinë e strukturave të frazëve, të tilla si VP dhe NP. Përveç kësaj, me një rregullalizim L1, gjetëm gjithashtu se mund të parashikojë saktësisht nëse një fjalë është brenda një strukture fraze apo jo nga një numër i vogël komponentesh të vektorit të kontekstit. Edhe për rastin e mësimit nga teksti i papërpunuar, vektorët e kontekstit shfaqen të korrelohen ende mirë me strukturat e frazëve. Më në fund, ne tregojmë se grupet natyrore të fjalëve funksionale dhe pjesa e fjalimeve që shkaktojnë frazat janë përfaqësuar në një nënhapësirë të vogël por kryesore të vektorit të përditësimit të kontekstit të LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dugo kratkoročna memorija ponovna neuralna mreža (LSTM) se široko koristi i poznaje kako bi uhvatili informativne dugoročne sintaktične zavisnosti. Međutim, kako se takve informacije odražavaju u svojim unutrašnjim vektorima za prirodni tekst još nisu dovoljno istražene. Analiziramo ih učeći jezički model gde se sintaktičke strukture implicitno daju. Mi empirički pokazujemo da su vektori za aktualizaciju konteksta, tj. izlazi unutrašnjih vrata, približno kvantizirani na binarne ili ternarne vrijednosti da pomognu jezičkom modelu da računa dubinu gnijezda tačno, kao što je Suzgun et al. (2019) nedavno pokazuju za sintetičke jezike Dycka. Za neke dimenzije u kontekstskom vektoru pokazujemo da su njihove aktivacije veoma povezani sa dubinom frazu strukture, poput VP i NP. Osim toga, sa regularizacijom L1, takođe smo otkrili da može precizno predvidjeti da li je reč unutar strukture fraze ili ne iz malih broja komponenata kontekstnog vektora. Čak i za slučaj učenja sa sirovog teksta, kontekstski vektori se pokazuju da se i dalje dobro povezuju sa strukturama fraze. Konačno, pokazujemo da su prirodni skupini funkcionalnih reči i deo govora koje okidaju rečenice predstavljeni u malom, ali glavnom podprostoru vektora LSTM-a za aktualizaciju konteksta.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Long Short-Term Memory återkommande neuralt nätverk (LSTM) används ofta och är känt för att fånga informativa långsiktiga syntaktiska beroenden. Hur sådan information återspeglas i dess interna vektorer för naturtext har dock ännu inte undersökts tillräckligt. Vi analyserar dem genom att lära oss en språkmodell där syntaktiska strukturer ges underförstått. Vi visar empiriskt att kontextuppdateringsvektorerna, dvs utgångar av interna portar, är ungefär kvantiserade till binära eller ternära värden för att hjälpa språkmodellen att räkna djupet i häckningen korrekt, vilket Suzgun et al. (2019) nyligen visar för syntetiska Dyck språk. För vissa dimensioner i kontextvektorn visar vi att deras aktiveringar är starkt korrelerade med djupet i frasstrukturer, såsom VP och NP. Dessutom, med en L1 regularisering, fann vi också att det exakt kan förutsäga om ett ord är inuti en frasstruktur eller inte från ett litet antal komponenter i sammanhangsvektorn. Även vid inlärning av obehandlad text, har kontextvektorer visat sig fortfarande korrelera bra med frasstrukturerna. Slutligen visar vi att naturliga kluster av de funktionella orden och den del av tal som utlöser fraser representeras i ett litet men huvudområde av kontextuppdateringsvektorn LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mtandao wa neura unaoendelea kukumbuka muda mrefu wa muda mrefu (LSTM) unatumiwa sana na unajulikana kukutana na kutegemea matumaini ya muda mrefu ya kukutana taarifa. Hata hivyo, namna taarifa hizi zinavyoonekana katika vectors za ndani kwa ajili ya maandishi ya asili bado haijachunguzwa vizuri. Tunawachambua kwa kujifunza muundo wa lugha ambapo muundo wa ushirikiano usio na maana. Tunaonyesha kwa makini kuwa vectors mpya za muktadha, yaani matokeo ya milango ya ndani, zinakadiriwa kwa kiasi cha thamani za binarie au za mwisho ili kusaidia muonekano wa lugha kuhesabu kina kizuizi cha makazi, kama Suzgun et al. (2019) hivi karibuni unaonyesha lugha za Dyck. Kwa baadhi ya utofauti katika vector za muktadha, tunaonyesha kwamba vitendo vyao vimeunganishwa sana na kina mfumo wa msemo, kama vile VP na NP. Moreover, with an L1 regularization, we also found that it can accurately predict whether a word is inside a phrase structure or not from a small number of components of the context vector. Hata kwa kesi ya kujifunza kutoka kwa ujumbe mfupi, vectors za mazingira bado zinaonyesha kuwa wanaunganisha vizuri na miundombinu ya msemo. Mwisho, tunaonyesha kuwa mabadiliko ya asili ya maneno ya kazi na sehemu ya hotuba ambazo zinaibua maneno yanawakilishwa katika upande mdogo lakini wa msingi wa vector mpya wa LSTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>நீண்ட குறுக்க- முறை நினைவகம் திரும்ப நெருக்கிய வலைப்பின்னல் (LSTM) விரிவாக பயன்படுத்தப்பட்டுள்ளது மற்றும் அறியப்பட்டுள்ளது தக ஆயினும், அதன் உள்ளார்ந்த நெறிகளில் எவ்வாறு இவ்வாறு தகவல் பிரதிபலிக்கப்படுகிறது என்றாலும் இயல்பான உரைக்கு போதுமா நாம் ஒரு மொழி மாதிரியை கற்றுக் கொண்டு அவர்களை ஆராய்ச்சி செய்கிறோம் அதில் ஒத்திசைவு அமைப்பு நாம் விருப்பமாக காட்டுகிறோம் சூசன் மற்றும் அல்லது உள் வாதில்களின் புதுப்பித்தல் வெளியீடுகள் சுக்சுன் டிக் மொழிகளின் ஆழத்தை சரியாக எண்ண உதவும் மொழி மாதிரி மதிப்புகளுக்கு சரிய சில பரிமாணங்களுக்கு, விபி மற்றும் NP போன்ற சொற்றொடர் அமைப்புகளின் ஆழத்தில் அவர்கள் செயல்பாடுகள் மிகவும் சார்ந்திருக்கிறது என மேலும், ஒரு L1 கட்டுப்பாடுடன், அது ஒரு சொற்றொடர் கட்டுப்பாட்டிற்கு உள்ளே இருக்கிறதா அல்லது சூழல் நெறியின் சிறிய எண்ணிக்கையில் இருந்து அல் குறைந்த உரையிலிருந்து கற்றுக் கூட, சூழல் நெறிகள் இன்னும் சொற்றொடர் அமைப்புகளுடன் நன்றாக இணைக்க காட்டும். இறுதியில், செயல்பாடுகளின் இயல்பான வார்த்தைகள் மற்றும் வார்த்தைகளின் பகுதி என்பதை நாம் காட்டுகிறோம் LSTM-ன் சூழல் புதுப்பித்தல் நெறியின்</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Uzun zamandyr Bu şekilde, täbiçi metin üçin bu maglumat içeri vektörlerinde nädip gözetlenmedi. Biz olary bir dil nusgasyny öwrenip sintaktik düzümleri diýip analyzýarys. Biz görkezilýän görkezilişimiz, diýmek bolsa daşary çubuklaryň netijesi, dil nusgasyny dogry ýagdaýda hasaplamak üçin bilim nusgasyna kömek etmek üçin 2-nji ýa-da üçin 3-nji mykdarlaryň derejesi, Suzgun et al. (2019) i ň soňra syntetik dyk diller üçin görkezilýär Bazı ölçüler için, VP ve NP gibi ifade yapılarının derinlikleri ile bağlantıldığını gösteriyoruz. L1 düzenlemesi ile de bu sözün bir fraz yapısının içinde olup olmadığını doğru tahmin edebileceğini fark etdik. - Hat metinden öwrenmek üçin hem, kontekst vektörleri fraz struktörleri bilen has gowy bir şekilde görkezilýär. Sonunda, funksyonal sözlerin doğal toparlarını ve sözlerin tetikleyen bir küçük fakat LSTM kontekst güncelleştirme vektörünün en önemli alt alanını gösteriyoruz.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>لنگ- ٹریم میموروی دوبارہ نیورل نیٹورک (LSTM) widely used and known to capture informative long-term syntactic dependencies. لیکن، یہ معلومات اس کے اندر کی ویکتروں میں کس طرح تحقیق نہیں کی گئی ہے کہ طبیعی متن کے لئے۔ ہم ان کو ایک زبان مدل سکھاتے ہیں جہاں سینٹاکٹیک ساختاریں معلوم ہوتی ہیں۔ ہم عمدہ طور پر دکھاتے ہیں کہ کنٹکس اوڈیٹ ویکتور، یعنی داخلی دروازے کے نتائج، تقریباً دوئناری یا ترنری ارزش کے مطابق دوئناری یا ترنری ارزش کی مدد کرنے کے لئے زبان موڈل کے مطابق مضبوط طریقہ کے مطابق، سوزگن et al. (2019) اچھے وقت سینٹیٹیک ڈیک زبان ہم ان کی فعالیت کو دکھاتے ہیں کہ ان کی فعالیت بالکل تعلق کی جڑ کی عمیق بنائی جاتی ہے جیسے وی پی اور ان پی۔ اور ہم نے ایک ل1 قانونی کے ساتھ بھی پایا کہ یہ ٹھیک طور پر پیش بینی کر سکتا ہے کہ ایک لفظ ساختار کے اندر ہے یا ایک چھوٹی تعداد سے نہیں ہے۔ Even for the case of learning from raw text, context vectors are shown to still be well correlated with the phrase structures. آخر میں، ہم دکھاتے ہیں کہ فعال کلمات کی طبیعی کلسٹر اور کلمات کی حصہ جو ٹریگر کلمات کو ایک چھوٹی لیکن اصلی زیر فضا میں LSTM کے کنٹکس اوڈڈیٹ ویکتور کی بنیادی جاتی ہیں.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>@ info: whatsthis Lekin, bu maʼlumotning ichki vectorlarida qanday ko'rsatilgan narsa asl matn uchun juda yetarli qidirilmaydi. Biz ularni tillar modelini o'rganish bilan analyzeriz, bu yerda syntactik tuzuvlari muvaffaqiyatli emas. Biz oddiy ko'rsatganimiz, ichki portlarning qiymatlari ikki yoki ternariy qiymatlariga qiymatlar qiymatni aniqlash mumkin, Suzgun et al (2019) yaqinda birinchi darajada bir qiymatni ko'rsatish mumkin. Ko'pchilik vektordagi bir necha shakllar uchun biz ularning amallari VP va NP kabi imkoniyatlarining yuqoriga juda bog'liq. Keyin, L1 boshqaruvi bilan biz bu so'zning bir so'z tuzuvning ichida yoki context vektorining kichkina qismlaridan emas deb hisoblash mumkin. Even for the case of learning from raw text, context vectors are shown to still correlate well with the phrase structures. Oxirgi, biz ishlayotgan so'zlarning tabiiy soʻzlar va so'zlarni ishga tushirishga ega bo'lgan so'zlarning qismi LSTM'ning context-update vektorlarining kichkina, balki asosiy subspati.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mạng thần kinh tái tạo của Ký Ức Dài Hạn (LSTM) được sử dụng rộng và được biết đến để nắm giữ các quan hệ pháp thuật lâu dài thông tin. Tuy nhiên, cách mà thông tin đó được phản ánh trong cơ quan nội bộ của nó về văn bản tự nhiên vẫn chưa được nghiên cứu đủ. Chúng tôi phân tích chúng bằng cách học một mô hình ngôn ngữ nơi cấu trúc cú pháp ẩn chứa. Chúng tôi có kinh nghiệm cho thấy các véc- tơ cập nhật ngữ cảnh, ví dụ các xuất bản của cổng nội tạng, có độ lượng xấp xỉ đến các giá trị nhị phân hoặc tế bào để giúp mô hình ngôn ngữ đo độ sâu của ấp giữ chính xác, như Suzgun et al. (209) gần đây cho thấy về ngôn ngữ nhuộm tổng hợp. Đối với một số kích thước trong véc- tơ ngữ cảnh, chúng tôi cho thấy kích hoạt của chúng có mối quan hệ chặt chẽ với độ sâu của cấu trúc cụm từ, như Phó Tổng thống và NPR. Hơn nữa, với một chỉnh sửa L1, chúng tôi cũng tìm ra rằng nó có thể dự đoán chính xác nếu một từ nằm trong một cấu trúc cụm từ hay không từ một số nhỏ các thành phần của véc- tơ ngữ cảnh. Thậm chí trong trường hợp học từ văn bản nguyên bản, các véc- tơ trường hợp vẫn liên kết tốt với cấu trúc từ điển thành ngữ. Cuối cùng, chúng tôi cho thấy các cụm số tự nhiên của các từ chức năng và phần của các bài phát biểu được đại diện cho dạng chữ kích hoạt nhỏ nhưng chính của véc- tơ cập nhật ngữ cảnh của HTTM.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>长短期记递归神经网络(LSTM)为博用,且已知用于获信息性之长句法依赖性。 然此等信息,在自然文本之内载体未尽究也。 吾以一言析之,其句法结构隐式也。 臣等以经验言之,上下文更新向量(即内司所输)近量化为二进制或三元直,以助言语准量嵌套深度,如Suzgun等(2019)近为合成Dyck语所示。 其于上下文向量之维度,明其激活与短语构(如VP与NP)之深相关也。 此外因L1正则化,犹见其从上下文向量少量组件中准测单词在短语结构否。 虽学于始,上下文向量犹关短语构。 最后,我明白聚类发短语词性在 LSTM 上下文更新向量一小而要子空中。</span></div></div><dl><dt>Anthology ID:</dt><dd>2020.coling-main.356</dd><dt>Volume:</dt><dd><a href=/volumes/2020.coling-main/>Proceedings of the 28th International Conference on Computational Linguistics</a></dd><dt>Month:</dt><dd>December</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Barcelona, Spain (Online)</dd><dt>Venue:</dt><dd><a href=/venues/coling/>COLING</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>International Committee on Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>4033–4043</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.coling-main.356>https://aclanthology.org/2020.coling-main.356</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2020.coling-main.356 title="To the current version of the paper by DOI">10.18653/v1/2020.coling-main.356</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">shibata-etal-2020-lstm</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Chihiro Shibata, Kei Uchiumi, and Daichi Mochihashi. 2020. <a href=https://aclanthology.org/2020.coling-main.356>How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural TextLSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text</a>. In <i>Proceedings of the 28th International Conference on Computational Linguistics</i>, pages 4033–4043, Barcelona, Spain (Online). International Committee on Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2020.coling-main.356>How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural TextLSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text</a> (Shibata et al., COLING 2020)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.coling-main.356.pdf>https://aclanthology.org/2020.coling-main.356.pdf</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.coling-main.356.pdf title="Open PDF of 'How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural TextLSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=How+LSTM+Encodes+Syntax+%3A+Exploring+Context+Vectors+and+Semi-Quantization+on+Natural+TextLSTM+Encodes+Syntax%3A+Exploring+Context+Vectors+and+Semi-Quantization+on+Natural+Text" title="Search for 'How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural TextLSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural TextLSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural TextLSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text](https://aclanthology.org/2020.coling-main.356) (Shibata et al., COLING 2020)</p><ul class=mt-2><li><a href=https://aclanthology.org/2020.coling-main.356>How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural TextLSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text</a> (Shibata et al., COLING 2020)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Chihiro Shibata, Kei Uchiumi, and Daichi Mochihashi. 2020. <a href=https://aclanthology.org/2020.coling-main.356>How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural TextLSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text</a>. In <i>Proceedings of the 28th International Conference on Computational Linguistics</i>, pages 4033–4043, Barcelona, Spain (Online). International Committee on Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>