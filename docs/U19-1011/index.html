<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLPLSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLPLSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP" name=citation_title><meta content="Gaurav Arora" name=citation_author><meta content="Afshin Rahimi" name=citation_author><meta content="Timothy Baldwin" name=citation_author><meta content="Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association" name=citation_conference_title><meta content="2019" name=citation_publication_date><meta content="https://aclanthology.org/U19-1011.pdf" name=citation_pdf_url><meta content="77" name=citation_firstpage><meta content="86" name=citation_lastpage><meta property="og:title" content="Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLPLSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP"><meta property="og:image" content="https://aclanthology.org/thumb/U19-1011.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/U19-1011"><meta property="og:description" content="Gaurav Arora, Afshin Rahimi, Timothy Baldwin. Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association. 2019."><link rel=canonical href=https://aclanthology.org/U19-1011></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/U19-1011.pdf>Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a><span class=acl-fixed-case>LSTM</span> forget more than a <span class=acl-fixed-case>CNN</span>? An empirical study of catastrophic forgetting in <span class=acl-fixed-case>NLP</span></a>
<a id=af_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Vergeet 'n LSTM meer as 'n CNN? Name</a>
<a id=am_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM ከCNN ይልቅ ይረሳልን? በNLP ውስጥ የረሳው የግጭት ጉዳይ ትምህርት</a>
<a id=ar_title style=display:none href=https://aclanthology.org/U19-1011.pdf>هل تنسى LSTM أكثر من CNN؟ دراسة تجريبية للنسيان الكارثي في البرمجة اللغوية العصبية</a>
<a id=az_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM CNN-d…ôn daha √ßox unutdu mu? NLP'd…ô unutduƒüu katastrofi t…ôhsil</a>
<a id=bg_title style=display:none href=https://aclanthology.org/U19-1011.pdf>ЛСМ забравя ли нещо повече от CNN? Емпирично изследване на катастрофалното забравяне в НЛП</a>
<a id=bn_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Does an LSTM forget more than a CNN? এনএলপিতে বিপর্যয় ভুলে যাওয়ার একটি প্রাকৃতিক গবেষণা</a>
<a id=bo_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM་ཡིས་CNN་ལས་བརྗེད་དགོས་སམ། NLP ནང་དུ་སྣང་བརྗེད་པའི་ཡུལ་གླེང་བརྗེད་སྐབས་སུ་གཏོང་ཁང་ཞིག་</a>
<a id=bs_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Da li LSTM zaboravi više od CNN-a? Empirička studija katastrofe zaboravljanja na NLP-u</a>
<a id=ca_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Un LSTM oblida més que un CNN? Un estudi empíric d'oblidar catastròficament en NLP</a>
<a id=cs_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Zapomíná LSTM víc než CNN? Empirická studie katastrofického zapomenutí v NLP</a>
<a id=da_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Glemmer en LSTM mere end et CNN? En empirisk undersøgelse af katastrofal glemme i NLP</a>
<a id=de_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Vergisst ein LSTM mehr als ein CNN? Eine empirische Studie zum katastrophalen Vergessen in NLP</a>
<a id=el_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Ξεχνάει ένα LSTM περισσότερα από ένα CNN; Μια εμπειρική μελέτη της καταστροφικής λησμονής στο NLP</a>
<a id=es_title style=display:none href=https://aclanthology.org/U19-1011.pdf>¿Olvida un LSTM más que una CNN? Un estudio empírico del olvido catastrófico en la PNL</a>
<a id=et_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Kas LSTM unustab rohkem kui CNN? Empiiriline uuring katastroofilise unustamise kohta NLP-s</a>
<a id=fa_title style=display:none href=https://aclanthology.org/U19-1011.pdf>آيا LSTM بيشتر از CNN فراموش ميکنه؟ یک مطالعه امپراطوری از فاجعه‌ای که در NLP فراموش می‌کند</a>
<a id=fi_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Unohtaako LSTM muutakin kuin CNN? Empiirinen tutkimus katastrofaalisesta unohduksesta NLP:ssä</a>
<a id=fl_title style=display:none href=https://aclanthology.org/U19-1011.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Est-ce qu'un LSTM oublie plus qu'un CNN ? Une étude empirique sur l'oubli catastrophique en PNL</a>
<a id=ga_title style=display:none href=https://aclanthology.org/U19-1011.pdf>An ndéanann LSTM dearmad ar níos mó ná CNN? Staidéar eimpíreach ar dearmad tubaisteach i NLP</a>
<a id=ha_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Shin an manta wani LSSM ko wani CNN? An jarraba karatun astroikin da aka manta cikin NLP</a>
<a id=he_title style=display:none href=https://aclanthology.org/U19-1011.pdf>האם LSTM שוכח יותר מCNN? מחקר אמפרי של שכחות קטסטרופיות ב-NLP</a>
<a id=hi_title style=display:none href=https://aclanthology.org/U19-1011.pdf>क्या एक एलएसटीएम सीएनएन से अधिक भूल जाता है? एनएलपी में विनाशकारी भूलने का एक अनुभवजन्य अध्ययन</a>
<a id=hr_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Da li LSTM zaboravi više od CNN-a? Empiričko ispitivanje katastrofe zaboravljanja na NLP-u</a>
<a id=hu_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Egy LSTM többet felejt el, mint egy CNN? A katasztrofális felejtés empirikus tanulmánya az NLP-ben</a>
<a id=hy_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Արդյո՞ք LSMT-ը մոռանում է CNN-ից ավելին: ՆԼՊ-ում մոռանալու ճակատագրական ուսումնասիրությունը</a>
<a id=id_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Does an LSTM forget more than a CNN? Sebuah studi empirik tentang melupakan bencana di NLP</a>
<a id=is_title style=display:none href=https://aclanthology.org/U19-1011.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Un LSTM dimentica più di una CNN? Uno studio empirico sull'oblio catastrofico nella PNL</a>
<a id=ja_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTMはCNNよりも多くを忘れますか？ NLPにおける壊滅的な忘却の実証的研究</a>
<a id=jv_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Apa kok LTT M dikandani layang sandi, tho ? Inji empir sing paling kelas karo kelas kuwi alih di NLP</a>
<a id=ka_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM დახსოვს უფრო მეტი CNN-ზე? კატარსტროფიკაში დავიბრუნეთ იმპერიკალური კვლევა</a>
<a id=kk_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM CNN- ден артық ұмытты ма? NLP- де ұмытты катастрофиялық зерттеу</a>
<a id=ko_title style=display:none href=https://aclanthology.org/U19-1011.pdf>CNN보다 LSTM이 더 건망증이 심해요?자연 언어 처리 중 재난적 망각에 대한 실증 연구</a>
<a id=lt_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP</a>
<a id=mk_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Дали LSTM заборава повеќе од CNN? Емпирична студија за катастрофално заборавање во НЛП</a>
<a id=ml_title style=display:none href=https://aclanthology.org/U19-1011.pdf>ഒരു എല്‍സ്റ്റം സിന്‍എനിനെക്കാള്‍ മറക്കുന്നുണ്ടോ? NLP-ല്‍ മറന്നുപോകുന്ന വിപത്തുകളുടെ ഒരു ശാസ്ത്രീയ പഠനം</a>
<a id=mn_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM CNN-ээс илүү их мартах уу? НЛП-д алдсан гамшгийн судалгаа</a>
<a id=ms_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Adakah LSTM melupakan lebih dari CNN? Sebuah kajian empirik tentang melupakan bencana di NLP</a>
<a id=mt_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM tinsa aktar minn CNN? Studju empiriku ta’ tinsa katastrofika f’NLP</a>
<a id=nl_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Vergeet een LSTM meer dan een CNN? Een empirische studie van catastrofal vergeten in NLP</a>
<a id=no_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Glommer ein LSTM meir enn ein CNN? Name</a>
<a id=pl_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Czy LSTM zapomina więcej niż CNN? Badanie empiryczne katastrofalnego zapomnienia w NLP</a>
<a id=pt_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Um LSTM esquece mais do que uma CNN? Um estudo empírico do esquecimento catastrófico em PNL</a>
<a id=ro_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Un LSTM uită mai mult decât un CNN? Un studiu empiric al uitarii catastrofale în PNL</a>
<a id=ru_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Неужели LSTM забывает больше, чем CNN? Эмпирическое исследование катастрофического забывания в NLP</a>
<a id=si_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM එකක් CNN වඩා වඩා අමතක වෙනවද? Name</a>
<a id=sk_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Ali LSTM pozabi več kot CNN? Empirična študija katastrofalnega pozabljanja v NLP</a>
<a id=so_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM ma halmaamay wax ka badan CNN? Baaritaan jimicsi ah oo dhibaato ku saabsan halmaanshada NLP</a>
<a id=sq_title style=display:none href=https://aclanthology.org/U19-1011.pdf>A harron një LSTM më shumë se një CNN? Një studim empirik për harrimin katastrofik në NLP</a>
<a id=sr_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Da li LSTM zaboravi više od CNN? Impirička studija katastrofe zaboravljanja na NLP-u</a>
<a id=sv_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Glömmer en LSTM mer än en CNN? En empirisk studie av katastrofal glömska i NLP</a>
<a id=sw_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Je, LSTM anasahau zaidi ya CNN? Utafiti wa msisimko wa ajabu unaosahau NLP</a>
<a id=ta_title style=display:none href=https://aclanthology.org/U19-1011.pdf>ஒரு LSTM CNN ஐ விட அதிகமாக மறந்து விடுகிறதா? NLP யில் மறந்து விட்ட பிரச்சனையின் ஒரு முக்கியமான ஆராய்ச்சி</a>
<a id=tr_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM CNN'den köp ýatdan çykarýarmy? NLP'da ýatdan çykan katastrofiýanyň empiriýaly okuwy</a>
<a id=uk_title style=display:none href=https://aclanthology.org/U19-1011.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/U19-1011.pdf>کیا LSTM ایک سی ان سے زیادہ بھول جاتا ہے؟ NLP میں بھول جانے والی مصیبت کی مطالعہ</a>
<a id=uz_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM CNN'dan ko'proq yoʻqolishini saqlaymi? Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/U19-1011.pdf>Một băng cử viên quên nhiều hơn cả CNN sao? Nghiên cứu kinh nghiệm về sự lãng quên thảm họa ở Njala.</a>
<a id=zh_title style=display:none href=https://aclanthology.org/U19-1011.pdf>LSTM忘于CNN乎? NLP中灾难性遗忘之实</a></h2><p class=lead><a href=/people/g/gaurav-arora/>Gaurav Arora</a>,
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>,
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Catastrophic forgetting whereby a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> trained on one <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is fine-tuned on a second, and in doing so, suffers a catastrophic drop in performance over the first <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is a hurdle in the development of better transfer learning techniques. Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different <a href=https://en.wikipedia.org/wiki/Network_architecture>architectures</a> and hyper-parameters affect <a href=https://en.wikipedia.org/wiki/Forgetting>forgetting</a> in a <a href=https://en.wikipedia.org/wiki/Computer_network>network</a>. With this study, we aim to understand factors which cause <a href=https://en.wikipedia.org/wiki/Forgetting>forgetting</a> during sequential training. Our primary finding is that CNNs forget less than LSTMs. We show that max-pooling is the underlying operation which helps CNNs alleviate forgetting compared to LSTMs. We also found that curriculum learning, placing a hard task towards the end of task sequence, reduces <a href=https://en.wikipedia.org/wiki/Forgetting>forgetting</a>. We analysed the effect of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning contextual embeddings</a> on catastrophic forgetting and found that using embeddings as feature extractor is preferable to <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> in continual learning setup.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofiske vergeet - waarmee 'n model wat op een taak opgelei is, is fyn-tuned op 'n sekonde, en in dit te doen, lyf 'n 'katastrofiske' afval in prestasie oor die eerste taak - is 'n hurdle in die ontwikkeling van beter oordrag onderwerp teknike. Ons het beperk verstanding van hoe verskillende arkitektuure en hiper-parameters effekteer die vergeet in 'n netwerk. Met hierdie studie is ons doel om faktore te verstaan wat vergeet het tydens sekwensiele onderwerp. Ons primêre vinding is dat CNN vergeet minder as LSTMs. Ons wys dat max- pooling is die onderstelling operasie wat help CNN alleviate vergeet vergelyking met LSTMs. Ons het ook gevind dat die onderwerp van die program leer, die plaasïng van 'n moeilike taak na die einde van die taak sekvensie, verduur vergeet. Ons het die effek van fine-tuning contextual inbêdings gekansleer op katastrofiske vergeet en gevind dat gebruik van inbêdings as funksie uittrekker is voorkeur na fin-tuning in voortdurende leerinstelling.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>የጭንቀት ግጭት፣ በአንድ ስራ ላይ የተማረ ሞዴል በሁለተኛው ሁለተኛ የተጠቃሚ ነው፣ እንዲሁም በመጀመሪያ ስራ ላይ የጭንቀት 'ጉዳይ' የሚያስጨንቀው ነው - የተሻለ ትምህርት ተማርኮት መፍጠር ነው፡፡ Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different architectures and hyper-parameters affect forgetting in a network. በዚህ ትምህርት፣ በኋላው ትምህርት ውስጥ የሚረሳውን ውርደትን ለማስተዋል እናስፈልጋለን፡፡ የመጀመሪያው ፍለጋችን የCNN ሰዎች ከLST ይልቅ ትንሹን ይረሳሉ፡፡ እናሳያቸዋለን የክፍለ ጉዳይ የCNNs መሳሳት ከLSTMs ጋር ለመረሳት ማቅረብ የሚችል የውጤት ሥራ ነው፡፡ የትምህርት ትምህርት እና ትክክለኛውን ስራ ወደ ፍጻሜው መጨረሻ የሚያደርገውን አግኝተናል፡፡ የአሁኑን ግንኙነት በመረሳት ላይ የመጠቀምን ውጤት አስተያየን እና የግንኙነት አካባቢዎች በጥቅምት መቀናቀል በዘወትር ትምህርት ግንኙነት በመጠቀም ይሻላል ብለን አግኝተናል፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>النسيان الكارثي - حيث يتم ضبط النموذج الذي يتم تدريبه على مهمة واحدة بدقة في مهمة ثانية ، ويعاني بذلك انخفاضًا "كارثيًا" في الأداء مقارنة بالمهمة الأولى - يمثل عقبة في طريق تطوير تقنيات تعلم نقل أفضل. على الرغم من التقدم المثير للإعجاب في الحد من النسيان الكارثي ، لدينا فهم محدود لكيفية تأثير البنى المختلفة والمعلمات المفرطة على النسيان في الشبكة. من خلال هذه الدراسة ، نهدف إلى فهم العوامل التي تسبب النسيان أثناء التدريب المتسلسل. النتيجة الأساسية التي توصلنا إليها هي أن شبكات CNN تنسى أقل من LSTMs. نوضح أن max-pooling هي العملية الأساسية التي تساعد شبكات CNN على التخفيف من النسيان مقارنةً بـ LSTM. وجدنا أيضًا أن تعلم المنهج ، بوضع مهمة صعبة في نهاية تسلسل المهام ، يقلل من النسيان. قمنا بتحليل تأثير الضبط الدقيق لحفلات الزفاف السياقية على النسيان الكارثي ووجدنا أن استخدام الزخارف كمستخرج ميزة أفضل من الضبط الدقيق في إعداد التعلم المستمر.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofik unutmaq - bir işdə təhsil edilmiş modellər bir saniyədə təhsil edilir. Bunu etmək üçün ilk işin üstündə 'katastrofik' düşüşünü çəkir - daha yaxşı təhsil öyrənmə tekniklərinin təhsil edilməsi. Katastrofik unutmaq üçün təsirli ilerleme baxmayaraq, fərqli arhitektür və hiper-parametrlərin a ğda unutmaq üçün nə təsirlərini təsir edir? Bu təcrübə ilə, sonrakı təcrübə sırasında unutduğu faktorları anlamaq istəyirik. Bizim ilk tapımız, CNN-lər LSTMs-dən daha az unutduğu şeydir. Biz max-pooling, LSTMs ilə qarşılaşdığı unutmağa kömək edir. Biz də öyrəndik ki, öyrənmək, işlərin sonuna çətin bir işi yerləşdirər, unutmağı azaldırır. Biz katastrofi unutduğu təsirlərin müxtəlif təsirlərin etkisini analiz etdik və həmişəlik öyrənmə qurğularının istifadə etməsinin müəyyən edilməsi daha yaxşıdır.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Катастрофното забравяне - при което модел, обучен за една задача, е фино настроен на втора и по този начин претърпява "катастрофален" спад в изпълнението на първата задача - е пречка за разработването на по-добри техники за трансферно обучение. Въпреки впечатляващия напредък в намаляването на катастрофалното забравяне, ние имаме ограничено разбиране за това как различните архитектури и хиперпараметри влияят на забравянето в мрежа. С това изследване се стремим да разберем факторите, които причиняват забравяне по време на последователно обучение. Основното ни откритие е, че CNN забравят по-малко от LSTMs. Показваме, че максималното обединяване е основната операция, която помага на CNN да облекчат забравянето в сравнение с LSTMs. Също така открихме, че учебното обучение, поставянето на трудна задача към края на поредицата от задачи, намалява забравянето. Анализирахме ефекта от фината настройка на контекстуалните вграждания върху катастрофалното забравяне и установихме, че използването на вграждания като екстрактор на функции е за предпочитане пред фината настройка при непрекъснато обучение.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>বিপর্যয় ভুলে যাচ্ছে - যেখানে একটি কাজে একটি মডেল প্রশিক্ষণ প্রদান করা হয়েছে সেকেন্ডের মধ্যে ভালোভাবে, আর এর ফলে প্রথম কাজে বিপর্যয়ের ক্ষেত্রে একটি 'বিপর্যয়' প্রদর্শনের ব বিপর্যয় ভুলে যাওয়ার ক্ষেত্রে চমৎকার অগ্রগতি সত্ত্বেও আমরা সীমিত বুঝতে পারি কিভাবে বিভিন্ন প্রতিষ্ঠান এবং হাইপার-প্যারামিটার এই গবেষণা দিয়ে আমরা বুঝতে চাই যে কারণগুলো বুঝতে পারে যার ফলে পরবর্তী প্রশিক্ষণের সময় ভুলে যায়। আমাদের প্রাথমিক খুঁজে পাওয়া যাচ্ছে যে সিএনএন এলএসএমএস এর চেয়ে কম ভুলে যাচ্ছে। আমরা দেখাচ্ছি যে ম্যাক্স-পুলিং হচ্ছে অন্তর্ভুক্ত কার্যক্রম যা সিএনএন-এর সাহায্য করে এলএসএমএস-এর তুলনায় ভুলে যাওয়ার আমরা একই সাথে পেয়েছি যে কার্কুল শিক্ষা, কাজের শেষের দিকে কঠিন কাজ রাখা, ভুলে যাচ্ছে। বিপর্যয় ভুলে যাওয়ার প্রভাব আমরা বিশ্লেষণ করেছিলাম এবং দেখেছিলাম যে বৈশিষ্ট্যের ব্যবহার ব্যবহার করে বিশেষ ক্ষেত্রে শিক্ষা ব্যবস্থা ব্যবস্থার সু</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>མི་མང་གི་སྐོར་ལས་བརྗེད་དགོས་བྱུང་། དེ་ལྟ་བུའི་རྣམ་པ་ཞིག་གིས་ལས་ཀྱང་བརྟན་དཔྱད་སྒྲུབ་གཅིག་གི་ནང་དུ་བཏུབ་པ་ཞིག་ཡིན། Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different architectures and hyper-parameters affect forgetting in a network. འདི་ལྟ་བུའི་ནང་དུ་ང་ཚོས་རྐྱེན་ངག་ཅན་གྱིས་རྐྱེན་གྱིས་འཛིན་བྱེད་པའི་ཆ་རྐྱེན་དུ་རྟོགས་འདོད་ཡོད། ང་ཚོའི་རྩ་བའི་མཐོང་སྣང་ནི་CNNs ཡིས་LSTMs ལས་ཉུང་བའི་བརྗེད་ཟིན་ཡིན་པ་རེད། འུ་ཅག་གིས་ཆེ་ཤོས་མཉམ་དུ་བསྡུས་ནི་རྨང་གཞིའི་བཀོལ འོན་ཀྱང་། Curriculum learning་དེ་ལ་གནད་དོན་ལྡན་གྱི་ལས་འགུལ་རྗེས་སུ་འཇོག་བྱས་པ་ཡིན་ན། We analyzed the effect of fine-tuning contextual embeddings on catastrophic forgetting and found that using embeddings as feature extractor is preferable to fine-tuning in continual learning setup.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofski zaboravljanje - s kojim se model obučen na jednom zadatku dobro određuje na sekundu, a u tom slučaju pati "katastrofski" pad ekspluatacije na prvom zadatku - je prepreka u razvoju boljih tehnika učenja transfera. Uprkos impresivnom napretku u smanjenju katastrofskog zaboravljanja, imamo ograničeno razumijevanje kako različite arhitekture i hiper-parametre utječu na zaboravljanje u mreži. Sa ovom studijom, ciljamo da razumijemo faktore koji uzrokuju zaboravljanje tokom sekvencijskog treninga. Naš primarni nalaz je da CNN zaboravlja manje od LSTMs. Pokazujemo da je maksimalno okupljanje temeljna operacija koja pomaže CNN-ima da ublaži zaboravljanje u usporedbi s LSTMs-om. Također smo otkrili da učenje nastavnog programa, stavljanje težak zadatak ka kraju poslovne sekvence, smanjuje zaboravljanje. Analizirali smo učinak određenog kontekstnog ugrađenja na katastrofsko zaboravljanje i saznali da je koristiti ugrađenje kao ekstraktor karakteristike bolje da se ispravi u nastavku učenja.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>L'oblidança catastròfica - en la qual un model entrenat en una tasca es ajusta en un segon, i en fer-ho, pateix una 'catastròfica' baixa en el rendiment durant la primera tasca - és un obstacle al desenvolupament de millors tècniques d'aprenentatge de transfer ència. Malgrat el progrés impressionant en la reducció de l'oblide catàstrofic, tenim una comprensió limitada de com diferents arquitectures i hiperparamètres afecten l'oblide en una xarxa. Amb aquest estudi, volem entendre els factors que provoquen l'oblide durant l'entrenament seqüencial. La nostra descoberta primària és que els CNN obliden menys que els LSTM. Ens demostram que la max-pooling és l'operació subjacente que ajuda als CNN a alleviar l'oblide comparat amb els LSTM. També vam descobrir que l'aprenentatge del currículum, posant una tasca difícil cap al final de la seqüència de tasques, redueix l'oblid. Vam analitzar l'efecte d'ajustar les incorporacions contextuals en l'oblidança catastròfica i vam descobrir que utilitzar les incorporacions com a extractor de característiques és preferible que ajustar en la configuració continua d'aprenentatge.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofické zapomenutí, kdy model trénovaný na jednom úkolu je na druhém doladěn a přitom trpí "katastrofickým" poklesem výkonu nad prvním úkolem, je překážkou ve vývoji lepších technik transferového učení. Navzdory působivému pokroku v redukci katastrofického zapomenutí máme omezené pochopení toho, jak různé architektury a hyperparametry ovlivňují zapomenutí v síti. Cílem této studie je porozumět faktorům, které způsobují zapomenutí během sekvenčního tréninku. Naším primárním zjištěním je, že CNN zapomínají méně než LSTMs. Ukazujeme, že max-pooling je základní operací, která pomáhá CNN zmírnit zapomenutí ve srovnání s LSTMs. Zjistili jsme také, že učení osnov, které umístí těžký úkol na konec sekvence úkolů, snižuje zapomenutí. Analyzovali jsme vliv jemného ladění kontextových vložení na katastrofické zapomenutí a zjistili jsme, že použití vložení jako extraktor funkcí je vhodnější než jemné ladění v nastavení kontinuálního učení.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofisk glemmelse - hvor en model, der er uddannet til en opgave, finjusteres på en anden og dermed lider et "katastrofalt" fald i ydeevnen i forhold til den første opgave - er en hindring for udviklingen af bedre overførselsindlæringsteknikker. På trods af imponerende fremskridt med at reducere katastrofal glemme, har vi begrænset forståelse af, hvordan forskellige arkitekturer og hyperparametre påvirker glemme i et netværk. Med denne undersøgelse forsøger vi at forstå faktorer, der forårsager glemmelse under sekventiel træning. Vores primære opdagelse er, at CNN glemmer mindre end LSTMs. Vi viser, at max-pooling er den underliggende operation, som hjælper CNN'er med at lindre glemme sammenlignet med LSTM'er. Vi fandt også ud af, at læring af læseplaner, der placerer en hård opgave mod slutningen af opgavesekvensen, reducerer glemmelse. Vi analyserede effekten af finjustering af kontekstuelle indlejringer på katastrofal glemning og fandt ud af, at brug af indlejringer som feature extractor er at foretrække frem for finjustering i kontinuerlig læring setup.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrophisches Vergessen, bei dem ein Modell, das auf einer Aufgabe trainiert wird, auf einer Sekunde feinjustiert wird und dabei einen "katastrophalen" Leistungsabfall gegenüber der ersten Aufgabe erleidet, ist eine Hürde bei der Entwicklung besserer Transferlerntechniken. Trotz beeindruckender Fortschritte bei der Verringerung des katastrophalen Vergessens haben wir nur ein begrenztes Verständnis dafür, wie verschiedene Architekturen und Hyperparameter das Vergessen in einem Netzwerk beeinflussen. Mit dieser Studie wollen wir Faktoren verstehen, die das Vergessen während des sequentiellen Trainings verursachen. Unsere wichtigste Erkenntnis ist, dass CNNs weniger vergessen als LSTMs. Wir zeigen, dass Max-Pooling die zugrunde liegende Operation ist, die CNNs hilft, Vergessen im Vergleich zu LSTMs zu lindern. Wir haben auch festgestellt, dass das Lernen im Curriculum, das eine schwierige Aufgabe gegen Ende der Aufgabenfolge platziert, das Vergessen reduziert. Wir analysierten den Effekt der Feinabstimmung kontextueller Einbettungen auf katastrophales Vergessen und fanden heraus, dass die Verwendung von Einbettungen als Feature Extractor der Feinabstimmung im kontinuierlichen Lernaufbau vorzuziehen ist.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Η καταστροφική λησμονή, με την οποία ένα μοντέλο εκπαιδευμένο σε μια εργασία προσαρμόζεται σε ένα δευτερόλεπτο, και με αυτόν τον τρόπο, υφίσταται μια "καταστροφική" πτώση στην απόδοση έναντι της πρώτης εργασίας αποτελεί εμπόδιο στην ανάπτυξη καλύτερων τεχνικών μάθησης μεταφοράς. Παρά την εντυπωσιακή πρόοδο στη μείωση της καταστροφικής λησμονής, έχουμε περιορισμένη κατανόηση του πώς διαφορετικές αρχιτεκτονικές και υπερ-παράμετροι επηρεάζουν τη λησμονή σε ένα δίκτυο. Με αυτή τη μελέτη, στοχεύουμε στην κατανόηση παραγόντων που προκαλούν τη λήθη κατά τη διάρκεια της διαδοχικής εκπαίδευσης. Το βασικό μας εύρημα είναι ότι τα CNN ξεχνούν λιγότερα από τα LSTMs. Δείχνουμε ότι η μέγιστη συγκέντρωση είναι η υποκείμενη λειτουργία που βοηθά τα CNN να ανακουφίσουν τη λησμονή σε σύγκριση με τα LSTMs. Διαπιστώσαμε επίσης ότι η εκμάθηση του προγράμματος σπουδών, τοποθετώντας μια δύσκολη εργασία προς το τέλος της ακολουθίας εργασιών, μειώνει τη λήθη. Αναλύσαμε την επίδραση του λεπτού συντονισμού των περιεχομένων ενσωμάτωσης στην καταστροφική λησμονή και διαπιστώσαμε ότι η χρήση ενσωμάτωσης ως εξαγωγέα χαρακτηριστικών είναι προτιμότερη από τον συντονισμό στη συνεχή ρύθμιση μάθησης.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>El olvido catastrófico, en el que un modelo entrenado en una tarea se ajusta en una segunda y, al hacerlo, sufre una caída «catastrófica» del rendimiento con respecto a la primera tarea, es un obstáculo en el desarrollo de mejores técnicas de aprendizaje por transferencia. A pesar del progreso impresionante en la reducción del olvido catastrófico, tenemos una comprensión limitada de cómo las diferentes arquitecturas e hiperparámetros afectan el olvido en una red. Con este estudio, nuestro objetivo es comprender los factores que causan el olvido durante el entrenamiento secuencial. Nuestro principal hallazgo es que las CNN olvidan menos que los LSTM. Demostramos que el max-pooling es la operación subyacente que ayuda a las CNN a aliviar el olvido en comparación con los LSTM. También descubrimos que el aprendizaje curricular, que coloca una tarea difícil hacia el final de la secuencia de tareas, reduce el olvido. Analizamos el efecto de ajustar las incrustaciones contextuales en el olvido catastrófico y descubrimos que el uso de incrustaciones como extractor de características es preferible al ajuste fino en la configuración de aprendizaje continuo.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastroofiline unustamine - kus ühe ülesande täitmiseks koolitatud mudel muutub teise ülesandega peeneks ja seeläbi kannatab esimese ülesandega võrreldes "katastroofilise" tulemuslikkuse languse - on takistus paremate siirdeõppetehnikate arendamisel. Hoolimata muljetavaldavatest edusammudest katastroofilise unustamise vähendamisel on meil piiratud arusaam sellest, kuidas erinevad arhitektuurid ja hüperparameetrid mõjutavad unustamist võrgus. Selle uuringuga püüame mõista tegureid, mis põhjustavad unustamist järjestikusel treeningul. Meie peamine leid on, et CNN unustab vähem kui LSTMd. Näitame, et maksimaalne koondamine on alusoperatsioon, mis aitab CNN-del leevendada unustust võrreldes LSTMdega. Samuti leidsime, et õppekava õppimine, raske ülesande asetamine ülesannete jada lõppu, vähendab unustamist. Analüüsisime kontekstipõhiste manustamiste mõju katastroofilisele unustamisele ja leidsime, et manustamiste kasutamine funktsioonide ekstraktorina on eelistatav kui peenhäälestamine pidevas õppeseadistuses.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>فراموش کردن ناراحتی - که یک مدل آموزش یافته روی یک کار در یک ثانیه کامل ساخته می شود، و در این صورت، در نتیجه، افزایش ناراحتی در عملکرد اولین وظیفه رنج می گیرد - در توسعه تکنیک یادگیری بهتر انتقال است. با وجود پیشرفت تحت تاثیر انگیز در کاهش فراموش های فاجعه، ما درک محدودیت داریم که چگونه معماری های مختلف و پارامترهای زیادی بر فراموش کردن در شبکه تاثیر می دهند. با این مطالعه، ما هدف داریم که فاکتورها را درک کنیم که باعث می شود در زمان آموزش های سطح فراموش کنند. اولین پیدا کردن ما این است که CNN ها کمتر از LSTMs فراموش می کنند. ما نشان می دهیم که مکس جمع کردن عملیات پایه‌ای است که به CNN کمک می‌کند که فراموش کردن در مقایسه با LSTMs را کمک کند. همچنین فهمیدیم که یادگیری برنامه آموزش، یک کار سخت به پایان برنامه کار، فراموش کردن را کاهش می‌دهد. ما تاثیر تغییر تغییر موقعیتی را بر فراموش ناگوار تحلیل کردیم و فهمیدیم که استفاده از استفاده از استفاده از استفاده از استفاده از استفاده از استفاده از استفاده از استفاده از ویژه‌های ویژه ترجیح می‌دهد تا تغییر‌سازی در سازمان</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofinen unohtaminen - jossa yhteen tehtävään koulutettua mallia hienosäädetään toiseen tehtävään ja näin ollen se kärsii "katastrofaalisesta" suorituskyvyn laskusta ensimmäiseen tehtävään verrattuna - on este parempien siirtooppimistekniikoiden kehittämiselle. Huolimatta vaikuttavasta edistyksestä katastrofaalisen unohduksen vähentämisessä meillä on rajallinen käsitys siitä, miten erilaiset arkkitehtuurit ja hyperparametrit vaikuttavat unohtamiseen verkossa. Tämän tutkimuksen avulla pyrimme ymmärtämään tekijöitä, jotka aiheuttavat unohtamista peräkkäisessä harjoittelussa. Pääasiallinen havaintomme on, että CNN:t unohtavat vähemmän kuin LSTMs:t. Osoitamme, että max-pooling on taustalla oleva operaatio, joka auttaa CNN:iä lievittämään unohdusta verrattuna LSTMiin. Huomasimme myös, että opetussuunnitelman oppiminen, vaikean tehtävän asettaminen tehtäväjärjestyksen loppuun, vähentää unohtamista. Analysoimme kontekstuaalisten upotusten hienosäätöjen vaikutusta katastrofaaliseen unohtamiseen ja totesimme, että upotusten käyttäminen ominaisuusuuttajana on parempi kuin hienosäätö jatkuvassa oppimisessa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>L'oubli catastrophique — où un modèle formé à une tâche est affiné sur une seconde et subit ainsi une baisse « catastrophique » de ses performances par rapport à la première tâche — constitue un obstacle au développement de meilleures techniques d'apprentissage par transfert. Malgré les progrès impressionnants réalisés dans la réduction de l'oubli catastrophique, nous ne comprenons pas très bien comment les différentes architectures et les hyperparamètres affectent l'oubli dans un réseau. Avec cette étude, nous visons à comprendre les facteurs qui provoquent l'oubli lors d'un entraînement séquentiel. Notre principale constatation est que les CNN oublient moins que les LSTM. Nous montrons que le max-pooling est l'opération sous-jacente qui aide les CNN à réduire l'oubli par rapport aux LSTM. Nous avons également constaté que l'apprentissage du curriculum, qui place une tâche difficile vers la fin de la séquence de tâches, réduit l'oubli. Nous avons analysé l'effet des intégrations contextuelles affinées sur l'oubli catastrophique et avons découvert que l'utilisation des intégrations comme extracteur de caractéristiques est préférable au réglage fin dans une configuration d'apprentissage continu.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Is bac é dearmad tubaisteach — ina ndéantar mionchoigeartú ar mhúnla a oiltear ar thasc amháin ar an soicind, agus trí laghdú “tubaisteach” a fhulaingt ar fheidhmíocht thar an gcéad tasc — a bheith ina bac ar fhorbairt teicnící foghlama aistrithe níos fearr. In ainneoin dul chun cinn suntasach maidir le dearmad tubaisteach a laghdú, tá tuiscint theoranta againn ar conas a théann ailtireachtaí agus hipearpharaiméadair éagsúla i bhfeidhm ar dearmad a dhéanamh i líonra. Leis an staidéar seo, tá sé mar aidhm againn na fachtóirí is cúis le dearmad le linn oiliúna seicheamhach a thuiscint. Is é an príomhthoradh atá againn ná go ndéanann CNNs dearmad ar níos lú ná LSTManna. Léirímid gurb é an chomhthiomsú uasta an oibríocht bhunúsach a chuidíonn le CNNanna dearmad a dhéanamh i gcomparáid le LSTManna. Fuaireamar amach freisin go laghdaíonn foghlaim curaclaim, trí thasc crua a chur i dtreo dheireadh an tseichimh thasc, an dearmad. Rinneamar anailís ar éifeacht mionchoigeartuithe leabaithe comhthéacsúla ar dhearmadú tubaisteach agus fuarthas amach go bhfuil sé níos fearr úsáid a bhaint as leabaithe mar shainghné-ghné ná mionchoigeartú i socrú na foghlama leanúnaí.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Babu mai manta - inda an sanar da wani misalin wanda aka yi wa aikin wani aikin da shi mai kyau ne a sakan guda, kuma idan yana aikata shi, zai sakar da cewa wani "kataroki" da ɓacewa a kan aikin na farkon - shi ne wani abu mai rauni a ƙarami da zanen shige mafiya alhẽri. Babu da jarrabi mai kyau ga ƙaranci ga mantawa na mai tsanani, sai mun sami fahimta yana da jinsi musamman ko da surar-parameteri za'a yi amfani da mantawa a cikin jerin. Ga wannan littafin, Munã nufin mu fahimta masu fakta da ke halatar da mantawa a lokacin da ake yi wa danganta. Babu ganinmu na farko ne cewa CNNS ke manta da mafi ƙaranci daga LSM. Tuna nũna cewa max poopoopool shi ne aikin da ke ƙara da ke taimakon CNN ya sauƙaƙe mantawa da aka sammenliki da LSAM. Kayya, na sami karatun karatun, kuma ya sami aikin mai tsanani zuwa ƙarshen aikin, kuma ya manta. Ba mu yi anayyar amfani da matsayin da ke samu'a da sauri a cikin masaukaci na manta, kuma muka gane cewa ya fi zama mafiya cancantar a gyara a cikin tsarin da za'a karanta ta daidai.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>שכחות קטסטרופית — שבה מודל מאומן במשימה אחת מתאים בשנייה, ובעשייתו זאת, סובל נפילה "קטסטרופית" ביצועים במהלך המשימה הראשונה — היא מחסומת בפיתוח של טכניקות לימוד העברה טובות יותר. למרות התקדמות מרשים בהפחית שכחות קטסטרופית, יש לנו הבנה מוגבלת על איך ארכיטקטורות שונות והיפר-פרמטרים משפיעים על שכחות ברשת. עם המחקר הזה, אנחנו מתכוונים להבין גורמים שגורמים לשכוח במהלך אימון רצופי. המצאה העיקרית שלנו היא ש CNN שוכחים פחות מLSTMs. אנו מראים שמקס-בריצה היא המבצע הנוסף שמעזר לסי.אן.איי להקל את השכחות בהשוואה ל-LSTMs. מצאנו גם שלמדת תוכנית לימודים, שמשמרת משימה קשה אל סוף רצף המשימה, מפחידה לשכוח. We analysed the effect of fine-tuning contextual embeddings on catastrophic forgetting and found that using embeddings as feature extractor is preferable to fine-tuning in continual learning setup.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>भयावह भूलना - जिससे एक कार्य पर प्रशिक्षित एक मॉडल को दूसरे पर ठीक किया जाता है, और ऐसा करने में, पहले कार्य पर प्रदर्शन में "भयावह" गिरावट का सामना करना पड़ता है - बेहतर हस्तांतरण सीखने की तकनीकों के विकास में एक बाधा है। विनाशकारी भूल को कम करने में प्रभावशाली प्रगति के बावजूद, हमारे पास सीमित समझ है कि विभिन्न आर्किटेक्चर और हाइपर-पैरामीटर एक नेटवर्क में भूलने को कैसे प्रभावित करते हैं। इस अध्ययन के साथ, हम उन कारकों को समझने का लक्ष्य रखते हैं जो अनुक्रमिक प्रशिक्षण के दौरान भूलने का कारण बनते हैं। हमारी प्राथमिक खोज यह है कि CNN LSTMs से कम भूल जाते हैं। हम दिखाते हैं कि अधिकतम-पूलिंग अंतर्निहित ऑपरेशन है जो एलएसटीएम की तुलना में सीएनएन को भूलने में मदद करता है। हमने यह भी पाया कि पाठ्यक्रम सीखना, कार्य अनुक्रम के अंत में एक कठिन कार्य रखना, भूलने को कम करता है। हमने भयावह भूलने पर ठीक-ट्यूनिंग प्रासंगिक एम्बेडिंग के प्रभाव का विश्लेषण किया और पाया कि फीचर एक्सट्रैक्टर के रूप में एम्बेडिंग का उपयोग करना निरंतर सीखने के सेटअप में ठीक-ट्यूनिंग के लिए बेहतर है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofski zaboravljanje - s kojim se model obučen na jednom zadatku dobro ispravlja na sekundu, a u tome pati "katastrofski" pad ekspluatacije na prvom zadatku - je prepreka u razvoju boljih tehnika učenja transfera. Uprkos impresivnom napretku u smanjenju katastrofskog zaboravljanja, imamo ograničeno razumijevanje kako različite arhitekture i hiper-parametre utječu na zaboravljanje u mreži. S ovim proučavanjem ciljamo razumjeti faktore koji uzrokuju zaboravljanje tijekom sekvencijskog treninga. Naš primarni nalaz je da CNN zaboravlja manje od LSTMs. Pokazujemo da je maksimalno skupljanje temeljna operacija koja pomaže CNN-ima ublažiti zaboravljanje u usporedbi s LSTMsom. Također smo otkrili da učenje nastavnih programa, stavljanje težak zadatak ka kraju poslovne sekvence, smanjuje zaboravljanje. Analizirali smo učinak određenog kontekstnog ugrađenja na katastrofsko zaboravljanje i saznali da je koristiti ugrađenje kao ekstraktor karakteristike bolje da se ispravi u nastavku učenja.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A katasztrofális felejtés - amelynek során az egyik feladatra képzett modellt finomhangolják a második feladatra, és ennek során "katasztrofális" teljesítménycsökkenést szenved az első feladatnál - akadályozza a jobb transzfer tanulási technikák kifejlesztését. Annak ellenére, hogy a katasztrofális felejtés csökkentése terén elért lenyűgöző előrelépés történt, korlátozott a megértésünk arról, hogy a különböző architektúrák és hiperparaméterek hogyan befolyásolják a felejtést egy hálózatban. Ezzel a tanulmánnyal arra törekszünk, hogy megértsük azokat a tényezőket, amelyek a szekvenciális edzés során felejtést okoznak. Az elsődleges megállapításunk, hogy a CNN-ek kevesebbet felejtenek el, mint az LSTM-ek. Megmutatjuk, hogy a maximális pooling az alapul szolgáló művelet, amely segít a CNN-ek enyhíteni a felejtést az LSTM-ekhez képest. Azt is megállapítottuk, hogy a tanterv tanulása, amely nehéz feladatot helyez a feladatsorozat vége felé, csökkenti a felejtést. Elemeztük a kontextuális beágyazások finomhangolásának hatását a katasztrofális felejtésre, és megállapítottuk, hogy a beágyazások funkciókivonóként való használata előnyösebb a folyamatos tanulási beállítások finomhangolásával szemben.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Կատաստերոֆիկ մոռանալը, որի միջոցով մեկ խնդրի վրա վարժեցված մոդելը մեկ վայրկյանում լավ կազմակերպված է, և դա անելով, առաջին խնդրի ընթացքում առաջացած արտադրողականությունը "կոտրաֆիկ" նվազում է, խոչընդոտը ավելի լավ փոխանցման ուսուցման տեխնիկաների Չնայած զարմանահրաշ առաջընթացին քանդակային մոռացման մեջ, մենք սահմանափակ հասկացություն ունենք այն մասին, թե ինչպես տարբեր ճարտարապետությունները և հիպեր-պարամետրերը ազդում են ցանցի մոռացման վրա: With this study, we aim to understand factors which cause forgetting during sequential training. Our primary finding is that CNNs forget less than LSTMs. Մենք ցույց ենք տալիս, որ մաքսային հավաքածությունը հիմնական գործողությունն է, որը օգնում է CNN-ներին նվազեցնել մոռացումը համեմատած LSMT-ների հետ: Մենք նաև հայտնաբերեցինք, որ ուսումնական ծրագրերի ուսումնասիրությունը, դժվար առաջադրանք դնելով առաջադրանքի վերջում, նվազեցնում է մոռացումը: We analysed the effect of fine-tuning contextual embeddings on catastrophic forgetting and found that using embeddings as feature extractor is preferable to fine-tuning in continual learning setup.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Catastrophic forgetting - whereby a model trained on one task is fine-tuned on a second, and in doing so, suffers a 'catastrophic' drop in performance over the first task - is a hurdle in the development of better transfer learning techniques. Meskipun kemajuan yang mengesankan dalam mengurangi melupakan bencana, kita memiliki pemahaman terbatas bagaimana arsitektur dan parameter-hyper berbeda mempengaruhi melupakan dalam jaringan. Dengan penelitian ini, kita bermaksud untuk memahami faktor yang menyebabkan melupakan selama latihan sekuensial. Penemuan utama kami adalah bahwa CNN lupa kurang dari LSTM. Kami menunjukkan bahwa max-pooling adalah operasi dasar yang membantu CNN mengurangi melupakan dibandingkan LSTM. Kami juga menemukan bahwa pelajaran kurikulum, menempatkan tugas sulit menuju akhir urutan tugas, mengurangi melupakan. Kami menganalisis efek penyesuaian isi kontekstual pada melupakan bencana dan menemukan bahwa menggunakan isi sebagai ekstraktor fitur lebih baik daripada penyesuaian dalam pengaturan belajar terus menerus.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>L'oblio catastrofico - per cui un modello addestrato su un compito viene perfezionato su un secondo, e così facendo subisce un calo "catastrofico" delle prestazioni sul primo compito - è un ostacolo allo sviluppo di migliori tecniche di apprendimento di trasferimento. Nonostante i progressi impressionanti nella riduzione dell'oblio catastrofico, abbiamo una comprensione limitata di come diverse architetture e iperparametri influenzano l'oblio in una rete. Con questo studio, miriamo a comprendere i fattori che causano l'oblio durante l'allenamento sequenziale. La nostra principale scoperta è che le CNN dimenticano meno degli LSTMs. Mostriamo che il max-pooling è l'operazione sottostante che aiuta le CNN ad alleviare l'oblio rispetto agli LSTMs. Abbiamo anche scoperto che l'apprendimento del curriculum, ponendo un compito difficile verso la fine della sequenza di compiti, riduce l'oblio. Abbiamo analizzato l'effetto della messa a punto di incorporazioni contestuali sull'oblio catastrofico e abbiamo scoperto che l'utilizzo di incorporazioni come estrattore di funzionalità è preferibile alla messa a punto di una configurazione di apprendimento continuo.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>1つのタスクで訓練されたモデルが1秒で微調整され、それによって1つ目のタスクよりもパフォーマンスが「壊滅的」に低下するという大惨事の忘却は、より良い転送学習テクニックの開発におけるハードルです。壊滅的な忘れ物を減らすことで印象的な進歩を遂げたにもかかわらず、異なるアーキテクチャとハイパーパラメータがネットワーク内の忘れ物にどのように影響するかについての理解は限られています。本研究では、連続したトレーニング中に忘れてしまう要因を理解することを目指しています。我々の主要な発見は、CNNはLSTMよりも少ないことを忘れているということです。最大プーリングは、CNNがLSTMと比較して忘れを緩和するのに役立つ基礎的な操作であることを示しています。また、課題シーケンスの終わりに向かって難しい課題を配置するカリキュラム学習は、忘れを減らすことも見出した。私たちは、壊滅的な忘却に対するコンテキスト埋め込みの微調整の効果を分析し、継続的な学習設定の微調整よりも、埋め込みを機能抽出として使用する方が望ましいことを発見しました。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Catasaro kuwi alih sing paling-paling maneh Maski Genjer-genjer saiki iki, awak dhéwé ngerasakno karo nggawe barang sliramu kuwi mau. Ndheke perusahaan punika dipunangé punika dipunangé KENs kuwi mau maning manung LA Awak dhéwé ngerasakno karo maximum-pool kuwi nggawe operasi dipun-perusahaan sing nyebutaké KNs nggawe barang nggawe barang nggawe tarjamahan karo LA Kita uga ono wektu nggoleki curric program seneng pisan, sithik sedhaya bantuan ngajar ujaran Awak dhéwé énjelisih efek karo embedding contextual</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>კატატრატური დახსოვრება - რომელიც ერთი რაოდენობაში მოდელი განაკეთებული იქნება სწორედ, და ამას გავაკეთებთ, რომ პირველი რაოდენობაზე 'კატატრატური' გამოკეთება - უკეთესი გასწავლების ტექნონიკების განვითარებაში კატატროფიკას დაბრუნდეს შემდეგ ინტერფექციური პროგრესი, ჩვენ გვაქვს განსხვავებული არქტიქტურების და ჰიპერ-პარამეტრების შესახებ ქსელში დაბრუნდეს. ამ სწავლებით ჩვენ მინდა გავიგოთ ფაქტორები, რომლებიც წარმოიდგენა სეკენტიური განაკლების განმავლობაში. ჩვენი პირველური შესაძლებლობა არის, რომ CNNs დაბრუნდება LSTMs. ჩვენ ჩვენ აჩვენებთ, რომ max-pooling არის ქვემოთ პერაცია, რომელიც CNNs-ს დახმარებს დახმარებას LSTMs-თან. ჩვენ ასევე აღმოჩნეთ, რომ სწავლა სწავლა სწავლა, ძალიან ძალიან რაოდენობა სამუშაო სამუშაო სამუშაო დავასრულება, დაბრუნება ჩვენ ანალიზიციეთ კონტექსტური კონტექსტური დაბრუნებისთვის ეფექტის გამოყენება და აღმოჩნეთ, რომ ინბედინგის გამოყენება როგორც ფუნქციების ექსტრაკტორი უფრო უფრო მეტია</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Қатастрофиялық ұмыттау - бір тапсырманың бір үлгісін бір ретінде бақылау үлгісі жақсы түзетіледі, және бұл істеу үшін бірінші тапсырманың үстінде 'катастрофиялық' дегенді өзгерту - жақсы аудару оқыту техникаларын жасау Қатастрофиялық ұмыттарды көшірмелеу үшін, біз желінде айырмашылық архитектуралар мен гипер параметрлерді ұмыттауға қандай нәтижесін түсініп тұрмыз. Бұл зерттеу үшін біз келесі оқыту кезінде ұмытты факторларды түсінуге мақсатымыз. Біздің негізгі табуымыз - CNN- лер LSTMs дан аз ұмытты. Біз максималдық жинақтау - CNN- лерін LSTMs- мен салыстырып ұмыттауға көмектеседі. Біз сондай-ақ оқыту бағдарламаларды тапсырмалардың соңына қатты тапсырманы орналастырып, ұмыттауды азайтады. Біз катастрофиялық ұмыттау үшін контекстік ендірудің эффектін анализ және ендіру құралы ретінде құрастырушыларды қолдану үшін дұрыс оқыту баптауларының артықшылығын баптауға мүмкіндік береді.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>재난적 망각은 더 나은 이동 학습 기술을 발전시키는 장애물이다. 재난적 망각은 한 임무에서 훈련하는 모델이 두 번째 임무에서 미세하게 조정되는 것을 말한다. 이렇게 할 때 첫 번째 임무의 표현은'재난적'이 떨어진다.비록 재난적인 망각을 줄이는 데 인상적인 진전을 거두었지만, 우리는 서로 다른 체계 구조와 초파라미터가 네트워크에서의 망각에 어떻게 영향을 미치는지에 대해 아직 아는 것이 매우 적다.이 연구를 통해 우리는 순서 훈련에서 잊혀지는 요인을 이해하고자 한다.우리의 주요 발견은 CNN이 LSTM보다 잊어버린 것이 더 적다는 것이다.LSTM에 비해 가장 큰 풀은 CNN이 망각을 완화하는 데 도움을 주는 기본적인 작업이라는 것을 알 수 있다.우리는 또 과정 학습이 어려운 임무를 임무 서열의 끝에 두면 잊어버리는 것을 줄일 수 있다는 것을 발견했다.우리는 미세한 언어 환경의 삽입이 재난성 망각에 미치는 영향을 분석한 결과 지속적인 학습 환경에서 삽입을 특징으로 하는 추출이 미세한 언어 환경보다 더욱 바람직하다는 것을 발견했다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Catastrophic forgetting - whereby a model trained on one task is fine-tuned on a second, and in doing so, suffers a 'catastrophic' drop in performance over the first task - is a hurdle in the development of better transfer learning techniques. Nepaisant įspūdingos pažangos mažinant nelaimingą pamiršimą, turime ribotą supratimą, kaip skirtingos architektūros ir hiperparatoriai veikia pamiršimą tinkle. Šiuo tyrimu siekiame suprasti veiksnius, kurie sukelia pamiršimą sekacinio mokymo metu. Pagrindinė išvada yra ta, kad CNN pamiršta mažiau nei LSTM. Mes rodome, kad maksimalus susijungimas yra pagrindinė operacija, kuri padeda CNN palengvinti užmiršimą, palyginti su LSTM. Taip pat nustatėme, kad mokymasis mokymosi programose, užduodant sunkią užduotį užduočių sekos pabaigoje, mažina pamiršimą. Išanalizavome patobulintų kontekstinių įdėjimų poveikį katastrofiniam pamirštui ir nustatėme, kad įdėjimų naudojimas kaip savybių ekstraktorius yra geresnis už patobulintą nuolatinio mokymosi sistemoje.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Катастрофското заборавање - со кое моделот обучен за една задача е финетизиран за секунда, и во тоа, страдаат од „катастрофален“ пад на перформансата во текот на првата задача - е пречка во развојот на подобри техники за пренесување на училиште. И покрај импресивниот напредок во намалувањето на катастрофалните заборави, имаме ограничено разбирање како различните архитектури и хиперпараметри влијаат на заборавањето во мрежата. Со оваа студија, имаме за цел да ги разбереме факторите кои предизвикуваат заборавување за време на секвенционалната обука. Нашиот примарен откритие е дека СНН забораваат помалку од ЛСТМ. We show that max-pooling is the underlying operation which helps CNNs alleviate forgetting compared to LSTMs. Исто така, откривме дека учењето на наставните планови, ставањето тешка задача кон крајот на секвенцијата на задачите, го намалува заборавањето. Го анализиравме ефектот на подобрувањето на контекстните вградувања на катастрофалното заборавување и откривме дека користењето на вградувањата како екстрактор на карактеристики е преферибилно од подобрување во постојаното научување поставување.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ഭീകരമായ മറക്കുന്നത് - ഒരു ജോലിയില്‍ ഒരു മോഡല്‍ പരിശീലിക്കപ്പെടുന്നത് ഒരു സെക്കന്റിനുള്ളില്‍ മാത്രമാണ്. അങ്ങനെ ചെയ്യുമ്പോള്‍ ആദ്യത്തെ ജോലിക്ക് മുകളില്‍ ഒരു പ്രകടനത വ്യത്യസ്ത്രീകരണങ്ങളും ഹൈപ്പര്‍ പരാമീറ്ററുകളും ഒരു നെറ്റ്‌വര്‍ക്കില്‍ മറക്കുന്നത് എങ്ങനെയാണെന്ന് ഞങ്ങള്‍ക്ക് മനസ്സിലാക്കാന്‍ പരിധിയുണ് ഈ പഠനത്തിന്‍റെ കാര്യത്തില്‍ നമുക്ക് മനസ്സിലാക്കാന്‍ ഉദ്ദേശിക്കുന്നു. അത് പിന്നീട് പരിശീലനത്തിന്‍ നമ്മുടെ പ്രധാനപ്പെട്ട കണ്ടുപിടിക്കുന്നത് എന്താണെന്നാല്‍ CNNs എല്‍സ്റ്റിഎസിനെക്കാള്‍ ക നമ്മള്‍ കാണിക്കുന്നത് ഏറ്റവും കൂടുതല്‍ പൂളിങ് ആണെന്നാണ് കാണിക്കുന്നത്. അത് CNNNs ലോകത്തെ മറക്കാന്‍ സഹായിക്കുന്നു. പണിയുടെ അവസാനത്തിലേക്ക് കഠിനമായ ജോലി വെക്കുന്നതും മറക്കുന്നതും ഞങ്ങള്‍ കണ്ടെത്തി. We analysed the effect of fine-tuning contextual embeddings on catastrophic forgetting and found that using embeddings as feature extractor is preferable to fine-tuning in continual learning setup.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Гайхалтай мартах нь - нэг ажил дээр сургалтын загвар нэг секундэд сайн зохицуулагддаг. Үүнийг хийхэд, анхны ажил дээр "гамшигтай" үйл ажиллагаа үйлдвэрлэгддэг нь илүү сайн шилжүүлэх технологиудын хөгжлийн бэрхшээл юм. Гайхалтай мартахын тулд гайхалтай хөгжлийг багасгаж байгаа ч бид өөр архитектур болон гипер параметр сүлжээнд мартах нөлөөлдөг талаар хязгаарлагддаг. Энэ судалгаанд бид дараагийн сургалтын үед мартагдаж буй хүчин зүйлсийг ойлгохын тулд зориулсан. Бидний анхны олж мэдэх зүйл бол CNN-үүд LSTMs ээс бага мартах юм. Бид хамгийн их цуглуулалт нь СНН-д LSTMs-тэй харьцуулахад тусалдаг үйл ажиллагаа юм. Мөн бид сургалтын хөтөлбөр суралцаж, ажлын дарааллын төгсгөлд хэцүү ажил хийж, мартахыг багасгаж байна. Бид гамшигтай мартаж буй гамшигтай орчинд сайхан сорилтуудын үр дүнг шинжилгээ хийсэн. Ингээд суралцах үйл ажиллагааг ашиглах нь үргэлжилсэн суралцах үйл ажиллагаанд сайхан зохицуулах илүү сайхан.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Catastrophic forgetting - whereby a model trained on one task is fine-tuned on a second, and in doing so, suffers a 'catastrophic' drop in performance over the first task - is a hurdle in the development of better transfer learning techniques. Walaupun kemajuan yang menakjubkan dalam mengurangkan melupakan bencana, kita mempunyai pemahaman terbatas bagaimana arkitektur dan parameter-hyper berbeza mempengaruhi melupakan dalam rangkaian. Dengan kajian ini, kita bermaksud untuk memahami faktor yang menyebabkan melupakan semasa latihan berturut-turut. Our primary finding is that CNNs forget less than LSTMs. Kami menunjukkan bahawa pengumpulan maksimum adalah operasi yang didasarkan yang membantu CNN mengurangi melupakan dibandingkan LSTM. Kami juga mendapati bahawa pelajaran kurikulum, meletakkan tugas sukar menuju akhir urutan tugas, mengurangkan lupaan. Kami menganalisis kesan penyesuaian penyelesaian kontekstual pada lupaan bencana dan mendapati bahawa menggunakan penyelesaian sebagai ekstraktor ciri lebih baik daripada penyesuaian dalam seting belajar terus menerus.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Insa katastrofika - li permezz tagħha mudell imħarreġ fuq kompitu wieħed jiġi rfinut f'sekonda, u meta jsir dan, tbati minn tnaqqis 'katastrofiku' fil-prestazzjoni matul l-ewwel kompitu - hija ostaklu fl-iżvilupp ta' tekniki a ħjar ta' tagħlim ta' trasferiment. Minkejja l-progress impressjonanti fit-tnaqqis tal-insenza katastrofika, għandna fehim limitat ta’ kif arkitetturi u parametri iperparatteristiċi differenti jaffettwaw l-insenza f’netwerk. With this study, we aim to understand factors which cause forgetting during sequential training. Is-sejba primarja tagħna hija li s-CNNs jinsabu inqas minn LSTMs. Aħna nuru li l-aggregazzjoni massima hija l-operazzjoni sottostanti li tgħin lis-CNNs itaffu l-insenza meta mqabbla mal-LSTMs. Instabna wkoll li t-tagħlim tal-kurrikulu, li jpoġġi kompitu diffiċli lejn it-tmiem tas-sekwenza tal-kompiti, inaqqas l-insenza. Aħna analizzaw l-effett tal-aġġustament tal-inkorporazzjonijiet kuntestwali fuq l-inseminazzjoni katastrofika u sabu li l-użu tal-inkorporazzjonijiet bħala estrattur tal-karatteristiċi huwa preferibbli milli l-aġġustament fin fit-tfassil kontinwu tat-tagħlim.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofisch vergeten, waarbij een model dat op één taak is getraind, op een tweede wordt verfijnd, en daarbij een 'catastrofale' daling in prestaties ondervindt ten opzichte van de eerste taak, is een obstakel in de ontwikkeling van betere transferleertechnieken. Ondanks indrukwekkende vooruitgang in het verminderen van catastrofale vergeten, hebben we weinig inzicht in hoe verschillende architecturen en hyperparameters het vergeten in een netwerk beïnvloeden. Met deze studie willen we factoren begrijpen die het vergeten veroorzaken tijdens sequentiële training. Onze belangrijkste bevinding is dat CNN's minder vergeten dan LSTMs. We tonen aan dat max-pooling de onderliggende operatie is die CNN's helpt vergeten te verminderen in vergelijking met LSTMs. We ontdekten ook dat curriculum leren, het plaatsen van een moeilijke taak tegen het einde van de taakvolg, het vergeten vermindert. We analyseerden het effect van fine-tuning contextuele embeddings op catastrofale vergeten en ontdekten dat het gebruik van embeddings als feature extractor de voorkeur verdient boven fine-tuning in continue learning setup.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofisk glemmering – der eit modell trent på ei oppgåve er fint oppsett på ein sekund, og i å gjøre det fører ein «katastrofisk» slepp i utviklinga over den første oppgåva – er ein hjelp i utviklinga av betre læringsteknikker for overføring. Til tross av uttrykkelig framgang i å redusera katastrofisk glemme, har vi begrenset forståelse av korleis ulike arkitekturar og hyper-parametrar påvirkar å glemme i eit nettverk. Med denne studien må vi forstå faktorer som fører til å glemme under sequential trening. Vår primærfinning er at CNN glemmer mindre enn LSTMs. Vi viser at max- pooling er den underliggende operasjonen som hjelper til CNN å alleviare glemme sammenlignet med LSTMs. Vi har også funne at læring av curriculum, plassering av ei vanskelig oppgåve mot slutten av oppgåvesekvensen, reduserer å glemme. Vi analysere effekten av finnstillingskontekst innbygging på katastrofisk glemme og funne at bruk av innbygging som funksjonsekstraktor er foretrukkeleg å finnstillinga i kontinuerlært læringsoppsett.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastroficzne zapomnienie, polegające na tym, że model trenowany w jednym zadaniu jest dostrojony na drugim, a w ten sposób doświadcza "katastroficznego" spadku wydajności nad pierwszym zadaniem, jest przeszkodą w rozwoju lepszych technik uczenia się transferu. Pomimo imponujących postępów w redukcji katastrofalnego zapomnienia, mamy ograniczone zrozumienie tego, jak różne architektury i hiperparametry wpływają na zapomnienie w sieci. W tym opracowaniu dążymy do zrozumienia czynników, które powodują zapomnienie podczas treningu sekwencyjnego. Nasze główne ustalenie jest takie, że CNN zapominają mniej niż LSTMs. Pokazujemy, że maksymalne pooling jest podstawową operacją, która pomaga CNN złagodzić zapomnienie w porównaniu z LSTMs. Odkryliśmy również, że nauka programu nauczania, umieszczanie trudnego zadania pod koniec sekwencji zadań, zmniejsza zapomnienie. Analizowaliśmy wpływ dostrajania kontekstowych osadzeń na katastrofalne zapomnienie i stwierdziliśmy, że korzystanie z osadzeń jako ekstraktora funkcji jest lepsze niż dostrajanie w ustawieniach ciągłego uczenia się.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>O esquecimento catastrófico – pelo qual um modelo treinado em uma tarefa é ajustado em uma segunda e, ao fazê-lo, sofre uma queda “catastrófica” no desempenho em relação à primeira tarefa – é um obstáculo no desenvolvimento de melhores técnicas de aprendizado de transferência. Apesar do impressionante progresso na redução do esquecimento catastrófico, temos uma compreensão limitada de como diferentes arquiteturas e hiperparâmetros afetam o esquecimento em uma rede. Com este estudo, pretendemos compreender os fatores que causam o esquecimento durante o treinamento sequencial. Nossa principal descoberta é que as CNNs esquecem menos que as LSTMs. Mostramos que o max-pooling é a operação subjacente que ajuda as CNNs a aliviar o esquecimento em comparação com as LSTMs. Também descobrimos que a aprendizagem curricular, colocando uma tarefa difícil no final da sequência de tarefas, reduz o esquecimento. Analisamos o efeito do ajuste fino de embeddings contextuais no esquecimento catastrófico e descobrimos que o uso de embeddings como extrator de recursos é preferível ao ajuste fino na configuração de aprendizado contínuo.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Uitarea catastrofală - prin care un model instruit pentru o sarcină este reglat fin pe o a doua sarcină și, făcând acest lucru, suferă o scădere "catastrofală" a performanțelor în ceea ce privește prima sarcină - reprezintă un obstacol în dezvoltarea unor tehnici mai bune de transfer de învățare. În ciuda progreselor impresionante în reducerea uitarii catastrofale, avem o înțelegere limitată a modului în care diferitele arhitecturi și hiperparametrii afectează uitarea într-o rețea. Cu acest studiu, ne propunem să înțelegem factorii care cauzează uitarea în timpul antrenamentului secvențial. Principala noastră descoperire este că CNN uită mai puțin decât LSTMs. Noi arătăm că max-pooling este operațiunea de bază care ajută CNN-urile să atenueze uitarea în comparație cu LSTMs. De asemenea, am descoperit că învățarea curriculumului, plasând o sarcină grea spre sfârșitul secvenței sarcinilor, reduce uitarea. Am analizat efectul reglării fine a încorporărilor contextuale asupra uitării catastrofale și am constatat că utilizarea încorporărilor ca extractor de caracteristici este preferabilă în locul reglării fine în configurarea continuă a învățării.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Катастрофическое забывчивость — когда модель, обученная по одной задаче, тонко настраивается на секунду, и при этом страдает от «катастрофического» падения производительности по сравнению с первой задачей — является препятствием в разработке лучших методов обучения передаче. Несмотря на впечатляющий прогресс в сокращении катастрофического забывания, мы имеем ограниченное понимание того, как различные архитектуры и гиперпараметры влияют на забывание в сети. С помощью этого исследования мы стремимся понять факторы, которые вызывают забывчивость во время последовательного обучения. Наш основной вывод заключается в том, что CNN забывают меньше, чем LSTM. Мы показываем, что макс-пулинг является основной операцией, которая помогает CNN облегчить забывчивость по сравнению с LSTM. Мы также обнаружили, что обучение по учебной программе, ставя сложную задачу к концу последовательности задач, уменьшает забывчивость. Мы проанализировали влияние тонкой настройки контекстных вложений на катастрофическое забывание и обнаружили, что использование вложений в качестве извлекателя признаков предпочтительнее тонкой настройки в настройках непрерывного обучения.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>අමතක වෙලා තියෙන්නේ - එක වැඩේ ප්‍රධානය කරපු මොඩේල් එකක් තත්වයකට හොඳින් සැකසුම් කරනවා, ඒ වගේම කරපු වෙලාවට, පළවෙනි වැඩේ සැලසුම් වෙනුවෙන් 'අනත ප්‍රශ්නයක් වෙනුවෙන් අමතක වෙලා තියෙනවා නමුත් අපිට ජාලයේ අමතක වෙලා තියෙන්න ප්‍රශ්නයක් තියෙනවා කියලා තේරුම් ගන්න ත මේ අධ්‍යානය සමඟ, අපි අදහස් කරනවා අධ්‍යානයක් තේරුම් ගන්න, ඒ විදියට අමතක කරන්න පුළුවන්. අපේ ප්‍රධාන සොයාගන්නේ CNN නිසා LSTMs වඩා අමතක කරන්න. අපි පෙන්වන්නේ විශේෂ පූලින් එක තමයි පහළින් ඉන්න තියෙන්නේ, ඒකෙන් CNNs ට LSTMs එක්ක අමතක කරන්න උදව් කරනවා. අපි හොයාගත්තා ඒ වගේම විද්‍යාලය ඉගෙන ගන්න, අමාරු වැඩක් අවසානයට දාන්න, අමතක වෙන්න. අපි විශ්ලේෂණය කරලා හොඳ සම්බන්ධ සම්බන්ධ සම්බන්ධ සම්බන්ධ විශ්ලේෂණය කරලා අමතක වෙලා තියෙනවා ඒ වගේම හොයාගත්තා ඒ වගේම සං</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofsko pozabljanje - pri katerem se model, usposobljen za eno nalogo, natančno nastavi v drugi nalogi in s tem utrpi "katastrofalno" zmanjšanje uspešnosti v primerjavi s prvo nalogo - je ovira pri razvoju boljših tehnik prenosa učenja. Kljub impresivnemu napredku pri zmanjševanju katastrofalnega pozabljanja imamo omejeno razumevanje, kako različne arhitekture in hiperparametri vplivajo na pozabljanje v omrežju. S to študijo želimo razumeti dejavnike, ki povzročajo pozabljanje med zaporednim treningom. Naša glavna ugotovitev je, da CNN pozabi manj kot LSTMs. Pokazali smo, da je največja združitev osnovna operacija, ki pomaga CNN ublažiti pozabljanje v primerjavi z LSTMi. Ugotovili smo tudi, da učenje učnega načrta, ki postavlja težko nalogo proti koncu zaporedja nalog, zmanjšuje pozabljanje. Analizirali smo učinek natančnega nastavitve kontekstualnih vdelav na katastrofalno pozabljanje in ugotovili, da je uporaba vdelav kot ekstraktor funkcij bolj primerna kot natančna nastavitev pri stalnem učenju.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Halmaamaha dhibaatada- marka lagu sameynayo tusaale lagu tababariyey shaqo keliya, marka lagu sameeyo, wuxuu ku xanuunsadaa dhaqdhaqaaqa 'dhibaatada' oo ku dhacda shaqada ugu horraysa - waa dhibaato ku jirta horumarinta qalabka waxbarashada bedelka wanaagsan. Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different architectures and hyper-parameters affect forgetting in a network. Waxbarashadan waxaynu ku talo galaynaa inaan fahno arrimaha sababta halmaanshada xiliga waxbarashada ka dambeeya. Helitaankeena asalka ah waa in CNNs ay halmaamaan in ka yar LSTMs. Waxan tusnaynaa in max-pooling waa hawlaha hoose ah oo caawinaya CNNNs inuu fududeeyo halmaanshada halmaanshada barbarka LSTMs. Sidoo kale waxaynu helnay in waxbarashada waxbarashada, in shaqada aad u adag uu dhamaado dhamaadka shaqada, uu halmaamay. Waxaannu baaraynay saamaynta xiliga hore ee hagaajinta, taasoo ah halmaanka dhibaatada, waxaana ogaanay in isticmaalka firaaqada ah oo kharashka ah ay ka wanaagsan tahay mid si fiican u barashada oo joogtada ah.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Harrimi katastrofik - me të cilin një model i trajnuar në një detyrë është i përshtatur në një sekondë, dhe duke e bërë këtë, vuan një rënie 'katastrofike' në performancë gjatë detyrës së parë - është një pengesë në zhvillimin e teknikave më të mira të mësimit transferues. Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different architectures and hyper-parameters affect forgetting in a network. Me këtë studim, ne synojmë të kuptojmë faktorë që shkaktojnë harrimin gjatë stërvitjes sekuencore. Our primary finding is that CNNs forget less than LSTMs. We show that max-pooling is the underlying operation which helps CNNs alleviate forgetting compared to LSTMs. Gjetëm gjithashtu se mësimi i kurikullit, duke vënë një detyrë të vështirë drejt fundit të sekuencës së detyrave, redukton harrimin. Analizuam efektin e rregullimit të përfshirjeve kontekstuale në harrimin katastrofik dhe zbuluam se përdorimi i përfshirjeve si nxjerrës i funksioneve është më i preferuar se rregullimi në konfigurimin e mësimit të vazhdueshëm.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofski zaboravljanje - s kojim je model obučen na jednom zadatku dobro određen na sekundu, a u tome pati "katastrofski" pad ekspluatacije na prvom zadatku - je prepreka u razvoju boljih tehnika učenja transfera. Uprkos impresivnom napretku u smanjenju katastrofskog zaboravljanja, imamo ograničeno razumevanje kako različite arhitekture i hiper-parametre utiču na zaboravljanje u mreži. Sa ovim studijom, ciljamo da razumemo faktore koji uzrokuju zaboravljanje tokom sekvencijskog treninga. Naš prvi nalaz je da CNN zaboravlja manje od LSTMs. Mi pokazujemo da je maksimalno okupljanje temeljna operacija koja pomaže CNN-ima da ublaži zaboravljanje u usporedbi sa LSTMsom. Takođe smo otkrili da učenje nastavnog programa, stavljanje težak zadatak ka kraju poslovne sekvence, smanjuje zaboravljanje. Analizirali smo učinak suštinskih kontekstualnih ugrađenja na katastrofsko zaboravljanje i saznali da je koristiti ugrađenje kao ekstraktor karakteristike bolje da se finalizuje u nastavku nastavljanja učenja.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofisk glömska - där en modell som tränats på en uppgift finjusteras på en annan och därmed drabbas av en "katastrofal" minskning av prestanda under den första uppgiften - är ett hinder för utvecklingen av bättre överföringsinlärningstekniker. Trots imponerande framsteg i att minska katastrofal glömska har vi begränsad förståelse för hur olika arkitekturer och hyperparametrar påverkar glömska i ett nätverk. Med denna studie syftar vi till att förstå faktorer som orsakar glömska under sekventiell träning. Vårt primära resultat är att CNN glömmer mindre än LSTM. Vi visar att max-pooling är den underliggande operationen som hjälper CNN att lindra glömska jämfört med LSTM. Vi fann också att läroplaner lärande, som placerar en svår uppgift mot slutet av uppgiftssekvensen, minskar glömska. Vi analyserade effekten av finjustering av kontextuella inbäddningar på katastrofal glömska och fann att användning av inbäddningar som funktionsextraktor är att föredra framför finjustering i kontinuerlig inlärning setup.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Matangazo ya kusahau – ambapo modeli inayofundishwa katika kazi moja imetunzwa vizuri kwa sekunde, na kwa kufanya hivyo, inatumia kupungua kwa 'janga' katika kazi ya kwanza - ni mzunguko katika maendeleo ya teknolojia bora ya kujifunza. Pamoja na maendeleo mazuri katika kupunguza kusahau maafa, tuna uelewa mdogo wa jinsi majengo tofauti na parameter za juu yanavyoathiri kusahau kwenye mtandao wa intaneti. Kwa utafiti huu, tunalenga kuelewa sababu ambazo zinasababisha kusahau wakati wa mafunzo ya mfululizo. Ugunduzi wetu wa msingi ni kuwa CNN wanasahau chini ya LST. Tunaonyesha kwamba kupunguza kwa kiwango kikubwa ni operesheni ya msingi inayosaidia watu wa CNN kupunguza kusahau ukilinganishwa na LSTMs. Pia tuligundua kuwa elimu ya elimu, kuweka kazi ngumu kwa ajili ya mwisho wa mfululizo wa kazi, kupunguza kusahau. Tulichambua athari za matukio ya vizuri yanayotumika katika kutambua maafa ya ajabu na tukagundua kuwa kwa kutumia mabomu kama mtengenezaji anafaa kuendelea vizuri katika mfumo wa kujifunza kwa muda mrefu.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>மறந்து விடுகிறது - அதைக் கொண்டு ஒரு செயல்பாட்டில் ஒரு மாதிரி பயிற்சி ஒரு மாதிரி ஒரு நொடியில் நன்றாக முடிக்கப்படுகிறது, அப்படிச் செய்தால், முதல் செயல்பாட்டில் ஒர துன்பத்தை மறந்து விடுவதற்கு முன்னேற்றத்தை குறைக்கும் போதும், வேறு வித்தியாசமான அடைவுகள் மற்றும் உயர் அளபுருக்கள் ஒரு வலைப்பின்ன இந்த ஆராய்ச்சியில், நாம் காரணிகளை புரிந்து கொள்ள நினைக்கிறோம். பின்வரும் பயிற்சியில் மறந்த எங்கள் முதல் கண்டுபிடிப்பு என்னவென்றால் CNNs LSTMs ஐ விட குறைவாக மறந்து விடுகிறது. அதிகபட்ச குழுக்கம் என்பதை நாம் காண்பிக்கிறோம் என்பது அடிப்படையில் உள்ள செயல்பாடு அது CNNs மறந்து விடுவதை எளிதாக்க மேலும் நாம் கண்டுபிடித்துள்ளோம் என்று பார்த்தோம், பணி தொடர்ந்து முடிவிற்கு ஒரு கடினமான பணியை வைத்து, மறக்க நாங்கள் துன்பத்தை மறந்து விட்டு நல்ல துண்டிக்கும் நிலையான உள்ளீடுகளின் விளைவுகளை ஆய்வு செய்தோம் மற்றும் தெரிந்து கொண்டிருந்தால் கு</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastrofik 첵atdan 챌ykyp bilen - bir i힊e gelin첵채n bir nusga ikinji gezek gowy d체zle힊iril첵채r we 힊onu흫 체챌in birinji i힊i흫 체st체nde 'catastrofik' d체힊체rip bolup ge챌iril첵채r - gowy g철챌체r 철wrenme tekniklerini흫 geli힊megidir. Katastrofi첵a 첵atdan 챌ykarmak 체챌in t채sirli ilerlemek isle첵채n 첵철ne, sy첵ada d체rli arhitektura we hiper-parameterleri흫 n채hili 첵atdan 챌ykarmagyny d체힊체n첵채ndigini 챌ykarypdyk. Bu okuw bilen so흫ky okuw wagtlary 첵atdan 챌ykarmak 체챌in faktorlara d체힊체nmek ama챌landyrys. Bizi흫 ilkinji tapylygymyz, CNN-ler LSTMsden az 첵atdan 챌ykar첵andyr. Biz i흫 k철p pooling (max-pooling) CNN'leri흫 LSTMsleri흫 첵agda첵ynda unutmagy 체챌in k철mekle첵채n i힊leridir. Biz hem programlary 철wrenmek 체챌in tapdyk, i힊lerin so흫unda kyn zady 챌ykaryp, unutmagy azaltdyk. Biz catastrophik 첵atdan 챌ykarmak 체챌in g철zlenen contextual ta첵첵arlary흫 etkisini analyzdyk we tapdyk ki da흫a d체힊en 철wrenme d체z체mlerini ta첵첵arlamak 체챌in ta첵첵arlanmagy gowy g철r첵채r.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>مصیبت بھول جاتی ہے - جس کے ذریعہ ایک کام پر آموزش کی ایک نمونڈل ایک دوسرے پر ٹھیک ٹھیک ٹھیک ہے، اور اس کے ذریعہ سے پہلی کام پر 'مصیبت' ڈوب رہی ہے - بہترین ترنسیٹر سیکھنے کی تکنیکیوں کی پیشرفت میں ایک پردہ ہے۔ ہمیں مصیبت بھول جانے کے لئے اثر انگیز پیشرفت کے باوجود، ہمیں معلوم ہے کہ ایک نیٹ ورک میں بھول جانے کے لئے کس طرح مختلف معمار اور ہیر پارامیٹر اثر دیتے ہیں۔ اس مطالعہ کے ساتھ ہم سمجھنے کا ارادہ رکھتے ہیں کہ فاکتوروں کو سمجھ سکتے ہیں جن کی تعلیم کے بعد بھول جاتی ہیں ہماری اصلی پیدا کرنا یہ ہے کہ CNN اس سے کم بھول جاتے ہیں ہم دکھاتے ہیں کہ سب سے زیادہ پولینگ کا عملیات ہے جو CNN کو LSTMs کے مقابلہ میں بھول جانے کی کمزوری کی مدد کرتی ہے ہم نے بھی پایا کہ تعلیم کی تعلیم، ایک سخت کام کو کام کی تعلیم کے پانے کی طرف رکھتا ہے، بھولنے کو کم کر دیتا ہے. ہم نے مصیبت بھول جانے پر اچھی تنظیم کی کنٹیکسٹ بنڈینگ کا اثر تحقیق کیا اور دیکھا کہ انڈینگ کو اضافہ کرنے والے کے طور پر استعمال کرنا ہمیشہ سیکھنے کے ساتھ اچھی تنظیم کرنا پسند کرتا ہے.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katastroq o'zgartirib qolganda - bir vazifa o'rganish modeli soniyada yaxshi o'xshash bo'ladi, va shunday qilib, birinchi vazifa orqali "katarok" bajarishini o'zgartiradi - yaxshi o'rganish texnologiyani tajriba qilishda juda harakat. Katastroqni o'zgartirish orqali juda ajoyib muvaffaqiyatli bo'lsa, biz har xil muammolar va hyper parametrlar tarmoqda o'zgartirish qanday qo'llanmagamiz. Bu o'qituvni bilan, biz keyingi taʼminlovchi paytda o'xshash sabablarni tushunishni tushunamiz. Bizning asosiy aniqlashimiz, CNNNs LSTS'dan yaroqni saqlab qoladi. Biz ko'rinishimizni ko'rsatganimiz, eng kichkina ko'paytirish asosiy operatsi, CNNS bilan o'xshash o'zgarishga yordam beradi. Biz o'rganishni o'rganishni topdik. Vazifaning davomida qiyin ishni o'tib qo'yishni o'zgartiradi. Biz paytda o'xshash o'zgartirib chiqqan paydo bo'lgan narsalarning natijasini aniqlab ko'rib chiqqamiz va bu narsalarning tashkilotlarini foydalanishi davom etishni o'rganishga o'xshash kerak.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Thảm họa quên mất rằng- theo đó một mô hình được huấn luyện trên một nhiệm vụ được chỉnh chính xác trong một giây, và làm vậy, chịu một sự giảm kinh nghiệm'thảm họa'trên nhiệm vụ đầu tiên- là một trở ngại trong việc phát triển kỹ thuật học chuyển nhượng tốt hơn. Mặc dù tiến bộ ấn tượng trong việc giảm đi sự quên lãng thảm họa, chúng tôi có giới hạn hiểu kiến trúc và siêu tham số khác nhau tác động đến quên lãng trong mạng lưới. Với nghiên cứu này, chúng tôi hướng tới sự thấu hiểu những yếu tố gây quên lãng trong suốt quá trình đào tạo. Điểm phát hiện chính của chúng tôi là CNN quên nhiều so với LSD. Chúng tôi cho thấy Max-ping là hoạt động cơ bản giúp CNN giảm bớt sự quên lãng so với LSD. Chúng tôi cũng nhận ra rằng học hành, đặt một nhiệm vụ khó khăn vào giai đoạn kết thúc của công việc, làm giảm sự quên lãng. Chúng tôi đã phân tích kết quả của sự nhúng tay vào sự quên lãng thảm họa và phát hiện ra rằng sử dụng sự nhúng tay vào như là bộ dẫn chức năng là thích hợp hơn là chỉnh sửa lại thiết lập học liên tục.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>灾难性遗忘- 于一任之中微于二务,而为此者,以首受灾难性之性降 - 开善迁学之碍也。 损灾难性得深进,异架构参数网络。 以此论之,吾所以知序练之所以遗忘也。 大抵 CNN 遗忘少于 LSTM 。 明与 LSTM 比,大池化助 CNN 轻忘之基也。 又见课程学以一剧之务置事之末,可以省忘。 论其微调上下文嵌于灾难性忘,见续学嵌以为特征提取器愈于微调。</span></div></div><dl><dt>Anthology ID:</dt><dd>U19-1011</dd><dt>Volume:</dt><dd><a href=/volumes/U19-1/>Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association</a></dd><dt>Month:</dt><dd>4--6 December</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Sydney, Australia</dd><dt>Venue:</dt><dd><a href=/venues/alta/>ALTA</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Australasian Language Technology Association</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>77–86</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/U19-1011>https://aclanthology.org/U19-1011</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">arora-etal-2019-lstm</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Gaurav Arora, Afshin Rahimi, and Timothy Baldwin. 2019. <a href=https://aclanthology.org/U19-1011>Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLPLSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP</a>. In <i>Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association</i>, pages 77–86, Sydney, Australia. Australasian Language Technology Association.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/U19-1011>Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLPLSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP</a> (Arora et al., ALTA 2019)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/U19-1011.pdf>https://aclanthology.org/U19-1011.pdf</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/U19-1011.pdf title="Open PDF of 'Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLPLSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Does+an+LSTM+forget+more+than+a+CNN%3F+An+empirical+study+of+catastrophic+forgetting+in+NLPLSTM+forget+more+than+a+CNN%3F+An+empirical+study+of+catastrophic+forgetting+in+NLP" title="Search for 'Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLPLSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLPLSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLPLSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP](https://aclanthology.org/U19-1011) (Arora et al., ALTA 2019)</p><ul class=mt-2><li><a href=https://aclanthology.org/U19-1011>Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLPLSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP</a> (Arora et al., ALTA 2019)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Gaurav Arora, Afshin Rahimi, and Timothy Baldwin. 2019. <a href=https://aclanthology.org/U19-1011>Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLPLSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP</a>. In <i>Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association</i>, pages 77–86, Sydney, Australia. Australasian Language Technology Association.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>