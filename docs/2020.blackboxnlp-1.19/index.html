<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation" name=citation_title><meta content="Rajiv Movva" name=citation_author><meta content="Jason Zhao" name=citation_author><meta content="Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP" name=citation_conference_title><meta content="2020/11" name=citation_publication_date><meta content="https://aclanthology.org/2020.blackboxnlp-1.19.pdf" name=citation_pdf_url><meta content="193" name=citation_firstpage><meta content="203" name=citation_lastpage><meta content="10.18653/v1/2020.blackboxnlp-1.19" name=citation_doi><meta property="og:title" content="Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation"><meta property="og:image" content="https://aclanthology.org/thumb/2020.blackboxnlp-1.19.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2020.blackboxnlp-1.19"><meta property="og:description" content="Rajiv Movva, Jason Zhao. Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. 2020."><link rel=canonical href=https://aclanthology.org/2020.blackboxnlp-1.19></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation</a>
<a id=af_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Skakel Lotterie Tikket Transformeerders: Struktuurale en Gedrag Studie van Sparse Neurale Masjien Vertaling</a>
<a id=am_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Transformers: Structural and Behavior Study of Sparse Neural Machine translation</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>تشريح محولات تذاكر اليانصيب: دراسة هيكلية وسلوكية لترجمة الآلة العصبية المتفرقة</a>
<a id=az_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Lotteri etiket transformatçıları parçalayır: Sərər nöral maşına çevirilməsinin strukturlu və davranışlıq öyrənməsi</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Разпространяване на лотарийни билетни трансформатори: структурно и поведенческо изследване на оскъдния неврален машинен превод</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>লোটারি টিকেট ট্রান্সফর্মার বিভক্ত করা হচ্ছে: স্পের্স নিউরাল মেশিন অনুবাদ</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Raspoređivanje Loterijskih preobraćaja kartica: Strukturalno i ponašanje proučavanja neuralnog preobraćanja</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Transformers de bitllets de loteria disseccionants: Estudio estructural i comportamental de traducció de màquines neuronals ràpides</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Disekce transformátorů loterijních lístků: Strukturální a behaviorální studie řídkého neuronového strojového překladu</a>
<a id=da_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Dissecting Lottery Ticket Transformers: Strukturel og adfærdsmæssig undersøgelse af sparsom neural maskinoversættelse</a>
<a id=de_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Analyse von Lotteriescheintransformatoren: Struktur- und Verhaltensstudie zur spärlichen neuronalen maschinellen Übersetzung</a>
<a id=el_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Διάταξη μετασχηματιστών λαχειοφόρων εισιτηρίων: Δομική και Συμπεριφερειακή Μελέτη Σπανιών Νευρικών Μηχανικών Μεταφράσεων</a>
<a id=es_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Disección de transformadores de billetes de lotería: estudio estructural y de comportamiento de la traducción automática neuronal dispersa</a>
<a id=et_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Loteripiletite muundajate levitamine: hõreda neuroalse masintõlke struktuuri- ja käitumisuuring</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>تقسیم کردن تغییرات برچسب‌های لوتری: مطالعه ساختاری و رفتاری از ترجمه ماشین عصبی فضایی</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Lottery Ticket Transformers: Harvoin hermojen konekäännöksen rakenne- ja käyttäytymistutkimus</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Dissection de transformateurs de billets de loterie : étude structurelle et comportementale de la traduction automatique neuronale clairsemée</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Claochladáin Ticéad Crannchuir a Dhíroinnt: Staidéar Struchtúrtha agus Iompraíochta ar Aistriúchán Meaisíní Néaracha Gann</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Transformers: Fractural and Atomic Research of spaspastic Neural Machine Translate</a>
<a id=he_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>משתני כרטיסי לוטריה מתפרקים: מחקר מבנה ותנהגות של תרגום של מכונות נוירות נמוכות</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>विच्छेदन लॉटरी टिकट ट्रांसफॉर्मर: विरल तंत्रिका मशीन अनुवाद के संरचनात्मक और व्यवहार अध्ययन</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Raskomadanje Loterijskih preobraćaja kartica: Strukturalno i ponašanje ispitivanja prijevoza neuroloških strojeva</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Lottószelvény transzformátorok disszekciója: a ritka neurális gépi fordítás strukturális és viselkedési vizsgálata</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Բաժանող վիճակախաղի տոմսերի փոխակերպողները. Փոքր նյարդային մեքենայի թարգմանման կառուցվածքային և վարքագծային ուսումնասիրություն</a>
<a id=id_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Transformer Tiket Loteri Memotong: Pengelajaran Struktur dan Perilaku Terjemahan Mesin Neural Tanpa</a>
<a id=is_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Dissettare i trasformatori dei biglietti della lotteria: studio strutturale e comportamentale della traduzione automatica neurale sparsa</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>解剖宝くじトランスフォーマー：まばらな神経機械翻訳の構造と行動の研究</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Dissection</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>ლოტერიის ტიკეტის ტრანფორმაციების გაყოფილი: სტრუქტურული და ქეგურაციის შესწავლება</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Лотериялық беттерді түрлендірушілерді бөліп тастау: Қосымшалық нейралық машинаның аудармасының структуралық және қасиеттерді зерттеу</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>복권 변압기 분석: 희소 신경 기계 번역의 구조와 행위 연구</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Loterijos bilietų keitikliai: struktūrinis ir elgsenos tyrimas sparčiojo nervinės mašinos vertimo būdu</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Трансформирачи на лотарски билети: Структурна и однесувачка студија за преведување на нервозни машини</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>ലോട്ടറി ടിക്കറ്റി ട്രാന്‍സ്ഫോര്‍മാര്‍ വിതരണം ചെയ്യുന്നു: സ്പെയിന്‍സ് നെയുറല്‍ മെഷീന്‍ പരിഭാഷ</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Лотерийн салбарын шилжүүлэгчид бөлөөлөх: Структурал болон Behavioral Study of Sparse Neural Machine Translation</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Penukar Tiket Loteri Mempecah: Pelajaran Struktur dan Perilaku Terjemahan Mesin Neural Tersingkat</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Trasformaturi tal-Karti tal-Lotterija li Jqassmu: Studju Strutturali u ta’ l-Imġiba ta’ Traduzzjoni ta’ Magni Newrali Sparse</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Het ontleden van loterijkaarttransformaties: Structurele en Gedragsstudie van schaarse neuronale machinevertaling</a>
<a id=no_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Avskjæring av lotteriske kartomformarar: Struktural og oppførsel- studie av omsetjing av avslag neuralmaskin</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Analiza transformatorów losów loterii: badanie strukturalne i behawioralne rzadkiego neuronowego tłumaczenia maszynowego</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Dissecando Transformadores de Bilhetes de Loteria: Estudo Estrutural e Comportamental da Tradução Automática Neural Esparsa</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Disectarea transformatorilor de bilete de loterie: Studiul structural și comportamental al traducerii automate neurale rare</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Рассекающие трансформаторы лотерейных билетов: структурное и поведенческое исследование редкого нейронного машинного перевода</a>
<a id=si_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>ලොට්‍රියි ටිකෙට් වෙනස් කරනවා: ස්ට්‍රූක්ටරුල් සහ ව්‍යාපෘතික අභ්‍යාසයේ ස්පාර්ස් න්‍යූරාල් මැෂ</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Disekacija loterijskih vstopnic transformatorjev: strukturna in vedenjska študija redkega živčnega strojnega prevajanja</a>
<a id=so_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Dissecting Lottery Ticket Transformers: structural and Behavior Study of Sparse Neural Machine Translation</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Transformuesit e biletave të shpërndarë të lotisë: Studimi strukturor dhe sjelljeje i përkthimit të makinës së shpejtë nervore</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Razvajanje Loterijskih kartica: Strukturalno i ponašanje istraživanja prijevoza neuroloških mašina</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Dissecting Lottery Ticket Transformers: Struktur- och beteendestudie av sparsam neural maskinöversättning</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Utafiti wa Kiteknolojia: Utafiti wa Miundombinu na Utafiti wa Utafiti wa Kihispania Utafiti wa Mashine ya Kiurahisi</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>முழுமையான குறிப்பு மாற்றுபவர்களை விட்டுவிடு</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Loteriýa bilen terjimelerini pozmak: structural and Behavioral Study of Sparse Neural Machine Translation</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>لوٹری ٹیکٹ تبدیل کرنے والوں کو تقسیم کرتا ہے: اسپارس نیورال ماشین تعلیم کی ساختاری اور رفتاری تحقیق</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>Dịch biến hình Vé nhòe nhoẹt: Nghiên cứu cấu trúc và hành vi về máy móc thần kinh</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>析彩票变形金刚:疏神经机器译,行也</a></h2><p class=lead><a href=/people/r/rajiv-movva/>Rajiv Movva</a>,
<a href=/people/j/jason-zhao/>Jason Zhao</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for <a href=https://en.wikipedia.org/wiki/NMT>NMT</a> while maintaining <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. However, it is unclear how such pruning techniques affect a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more <a href=https://en.wikipedia.org/wiki/Code>encoding</a>. Attention mechanisms remain remarkably consistent as sparsity increases.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Onlangse werk op die loterie belet hipotees het baie sparse transformeerders vir NMT produseer terwyl BLEU behou. Maar dit is onbekend hoe sodanige pruning teknike 'n model se leer verteenwoordighede beïnvloor. Deur te probeer Transformers met meer en meer lae magnitude gewigte wat weg uitgebreek is, vind ons dat kompleks semantiese inligting eerste is om afgebreek te word. Analiseer van interne aktiwiteite vertoon dat hoëre lage mees oor die loop van pruning verskuif word, gradief minder kompleks word as hulle dense kunstenaars. Name Aangaande mekanisme bly remarkante konsistent as sparsiteit vergroot word.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>በሎትራ ቲኪት hypothesis ላይ የሚሠራ ሥራ BLEU በመጠበቅ ጊዜ ለNMT የተለየ ፍላጻዎችን እጅግ ያሳያል፡፡ ነገር ግን እንዲህ ያሉ ብልሃት የሞዴል ተማሪዎችን እንዴት እንደሚያስጨንቁበት አይገለጽም፡፡ በተጨማሪና በሚያነካው ሚዛን በመፈታት፣ የተጨማሪው የsemantic መረጃ በመጀመሪያ እንዲያሳፍር እናገኛለን፡፡ የውስብ አካባቢዎች ትምህርት፣ ከፍተኛ ደረጃዎች በአስተዋይ ክፍል ላይ አብዛኛውን ይለየቃሉ፡፡ በዚያን ጊዜም፣ የቀድሞው ደረጃዎች የጭብጥ ምርጫዎች አካባቢ አካባቢ ማድረግ ይጀምራሉ፡፡ የጥያቄ አካባቢዎች እየጨመረ ቁጥጥር እንደሚያበዛ ይኖራል፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>أنتج العمل الأخير على فرضية بطاقة اليانصيب محولات متفرقة للغاية لـ NMT مع الحفاظ على BLEU. ومع ذلك ، فمن غير الواضح كيف تؤثر تقنيات التقليم هذه على التمثيلات المكتسبة للنموذج. من خلال فحص المحولات التي تحتوي على المزيد والمزيد من الأوزان المنخفضة الحجم التي تم تشذيبها بعيدًا ، نجد أن المعلومات الدلالية المعقدة هي أولًا يتم تدهورها. يكشف تحليل التنشيطات الداخلية أن الطبقات العليا تتباعد أكثر على مدار عملية التقليم ، وتصبح تدريجياً أقل تعقيدًا من نظيراتها الكثيفة. وفي الوقت نفسه ، تبدأ الطبقات المبكرة من النماذج المتفرقة في أداء المزيد من التشفير. تظل آليات الانتباه متسقة بشكل ملحوظ مع زيادة التباين.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Qısa zamanda loteriya bileti hipotezi üzərində NMT üçün çox küçük Transformer yaratdı. Ancaq bu təklif metodların modelinin öyrəndiyi təsirlərinin necə etkisini bilmir. Transformers daha çox və daha düşük böyüklük ağırlığını təşviq edirək, kompleks semantik məlumatları ilk dəfə rüsvay edilməlidir. İçəri fəaliyyətlərin analizi göstərir ki, yüksək səviyyələr dəyişmək yolunda daha çox fəaliyyət edir, yavaş-yavaş onların yoxluq yoldaşlarından daha az kompleks olur. Bu sırada, əkin modellərin əvvəlki səviyyələri daha çox kodlamağa başlar. Dikkati mehānismi çoxluğunda çoxluğunda mövcuddur.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Последната работа по хипотезата за лотарийни билети доведе до изключително редки трансформатори за НМТ, като същевременно се поддържа Блеу. Въпреки това, не е ясно как такива техники за подрязване влияят на наученото представяне на модела. Чрез сондиране на трансформатори с все повече и повече тежести с ниска магнитуда, откриваме, че сложната семантична информация първо трябва да бъде деградирана. Анализът на вътрешните активирания разкрива, че по-високите слоеве се различават най-много в хода на подрязването, постепенно стават по-малко сложни от техните плътни колеги. Междувременно ранните слоеве от редки модели започват да изпълняват повече кодиране. Механизмите за внимание остават забележително последователни с увеличаването на оскъдността.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>লোটারি টিকিট হিপাইথিসিসের সম্প্রতি কাজ বিলিউ রাখার সময় এনএমটির জন্য অনেক বেশী স্প্যারাস্ফার্মার তৈরি করেছে। তবে এটা পরিষ্কার নয় যে কিভাবে এই ধরনের বুদ্ধিমান প্রযুক্তিগুলো একটি মডেলের শিক্ষা প্রতিনিধিত্বের উপর প্ ট্রান্সফর্মারকে পরীক্ষা করার মাধ্যমে বেশী এবং কম মাত্রার ওজন ছিনিয়ে দেয়া হয়েছে, আমরা দেখতে পাচ্ছি যে জটিল সেম্যান্টিক তথ্য Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. এদিকে, স্প্যাস মডেলের প্রাথমিক ক্ষেত্রে আরো এনকোডিং করা শুরু করে। মনোযোগ প্রদান করা মেকিনসমূহ চমৎকার ব্যাপারে একত্রিত থাকে।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>འཕྲལ་ཁམས་ཀྱི་སྐོར་གྱི་ལྕགས་གླེང་སྡུད་པར་བཟོ་བར་མཐུན་པ་ཁག་གིས་མཐུན་བཟོ་བཅོས་པ་དེ་ཡིན། ཡིན་ནའང་། དབྱིན་རྩལ་གྱི་ཐབས་ལམ་དེ་གིས་མི་ཤེས་པས་རྣམ་གྲངས་བསྡུར་བྱེད་ཀྱི་ཡོད། དབྱིབས་བཟོ་བ་དག་པ་ཞིག་གིས་མཐོ་དམའ་བའི་ཚད་ལྡན་བ་ཞིག་ནས དབྱེ་ཞིབ་ཀྱི་ནང་འཁོད་བྱ་འགུལ་གྱི་དཔྱད་ཞིག་ནི་བགོ་རིམ་མཐོ་ཚད་རྒྱ་ཆེ་མཐོ་ཁག་གི་ཡིག་རྟགས་ལ་ཕར་ཆེ་བ་བསྐྱེད་ཚད་འདྲ་བྱེད་ཀྱི་ ད་ནའང་ཡང་སྔོན་གྱི་བང་རིམ་པ་ཁང་གི་དབྱེ་རིས་མང་ཙམ་ཨང་ཀོ་གཏོང་འགོ་འཛུགས་བྱེད་ཀྱི་ཡོད། ཆེད་འཛིན་གྱི་ཐབས་ལམ་དེ་ཚོ་ཆེད་པོ་ཞིག་ཏུ་ཉར་ཆེན་པོ་ཞིག་ཏུ་ཉར་ཡོད།</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nedavni rad na hipotezi karte loterije proizveo je visoko rezervne transformere NMT-a dok održava BLEU. Međutim, nije jasno kako takve tehnike pružanja utječu na naučene predstave model a. Probajući transformatore sa visokom i visokom težinom niskog veličine, otkrivamo da će prvo biti smanjena kompleksna semantička informacija. Analiza unutrašnjih aktivacija pokazuje da se viši slojevi najviše razlikuju tijekom pružanja, postupno postaju manje kompleksni od njihovih gustih kolega. U međuvremenu, rani sloji rezervnih modela počinju izvršiti više kodiranja. Mehanizmi pažnje ostaju nevjerojatno konsistentni dok se povećava sparsitet.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. No obstant això, no és clar com aquestes tècniques de pruning afecten les representacions aprengutes d'un model. Investigant els Transformers amb més i més pesos de baixa magnitud, trobem que la informació semàntica complexa s'ha de degradar primer. L'anàlisi de l'activació interna revela que les capes més altes divergeixen més al llarg del pruning, tornant-se gradualment menys complexes que les seves denses contrapartides. Mentrestant, les primeres capes de models poc codificats comencen a fer més codificació. Els mecanismes d'atenció segueixen sorprenentment consistents a mesura que l'escassetat augmenta.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nedávná práce na hypotéze loterijních lístků vytvořila velmi řídké transformátory pro NMT při zachování BLEU. Není však jasné, jak tyto techniky prořezávání ovlivňují naučené reprezentace modelu. Zkoumáním transformátorů s více a více nízkými velikostmi odříznutými hmotnostmi zjišťujeme, že složité sémantické informace jsou nejprve degradovány. Analýza vnitřních aktivací ukazuje, že vyšší vrstvy se v průběhu prořezávání nejvíce liší a postupně se stávají méně složitými než jejich husté protějšky. Mezitím rané vrstvy řídkých modelů začínají provádět více kódování. Mechanismy pozornosti zůstávají pozoruhodně konzistentní s rostoucí řídkostí.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nyligt arbejde med lotteri billet hypotesen har produceret meget sparsomme Transformers til NMT samtidig med at BLEU opretholdes. Det er imidlertid uklart, hvordan sådanne beskæringsteknikker påvirker en models lærte repræsentationer. Ved at undersøge Transformers med flere og flere lav størrelse vægte beskåret væk, finder vi, at komplekse semantiske oplysninger er først til at blive nedbrudt. Analyse af interne aktiveringer viser, at højere lag adskiller sig mest i løbet af beskæringen og gradvist bliver mindre komplekse end deres tætte modstykker. I mellemtiden begynder tidlige lag af sparsomme modeller at udføre mere kodning. Opmærksomhedsmekanismerne forbliver bemærkelsesværdigt konsekvente, efterhånden som sparsomheden stiger.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Jüngste Arbeiten an der Lotteriescheinhypothese haben sehr spärliche Transformatoren für NMT unter Beibehaltung der BLEU produziert. Es ist jedoch unklar, wie solche Beschneidungstechniken die erlernten Darstellungen eines Modells beeinflussen. Indem wir Transformatoren mit immer mehr niederen Gewichten abtasten, stellen wir fest, dass komplexe semantische Informationen zuerst degradiert werden. Die Analyse der inneren Aktivierungen zeigt, dass höhere Schichten im Laufe des Beschnitts am meisten divergieren und allmählich weniger komplex werden als ihre dichten Pendants. Inzwischen beginnen frühe Schichten von spärlichen Modellen, mehr Codierung durchzuführen. Aufmerksamkeitsmechanismen bleiben bemerkenswert konsistent, wenn die Sparsität zunimmt.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Πρόσφατες εργασίες σχετικά με την υπόθεση λαχειοφόρων λαχειοφόρων εισιτηρίων έχουν δημιουργήσει εξαιρετικά σπάνιους μετασχηματιστές για NMT διατηρώντας παράλληλα BLEU. Ωστόσο, δεν είναι σαφές πώς τέτοιες τεχνικές κλαδέματος επηρεάζουν τις διδαγμένες αναπαραστάσεις ενός μοντέλου. Εξετάζοντας τους μετασχηματιστές με όλο και περισσότερα βάρη χαμηλού μεγέθους που κόβονται μακριά, διαπιστώνουμε ότι οι σύνθετες σημασιολογικές πληροφορίες πρέπει πρώτα να υποβαθμιστούν. Η ανάλυση των εσωτερικών ενεργοποιήσεων αποκαλύπτει ότι τα υψηλότερα στρώματα αποκλίνουν περισσότερο κατά τη διάρκεια του κλαδέματος, γίνονται σταδιακά λιγότερο πολύπλοκα από τα πυκνά ομόλογά τους. Εν τω μεταξύ, τα πρώτα στρώματα των αραίων μοντέλων αρχίζουν να εκτελούν περισσότερη κωδικοποίηση. Οι μηχανισμοί προσοχής παραμένουν αξιοσημείωτα συνεπείς καθώς αυξάνεται η σπανιότητα.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>El trabajo reciente sobre la hipótesis del billete de lotería ha producido Transformers muy dispersos para NMT mientras se mantiene BLEU. Sin embargo, no está claro cómo estas técnicas de poda afectan las representaciones aprendidas de un modelo. Al sondear Transformers con más y más pesos de baja magnitud eliminados, descubrimos que la información semántica compleja es la primera en degradarse. El análisis de las activaciones internas revela que las capas superiores divergen más a lo largo de la poda, convirtiéndose gradualmente en menos complejas que sus contrapartes densas. Mientras tanto, las primeras capas de modelos dispersos comienzan a realizar más codificación. Los mecanismos de atención siguen siendo notablemente consistentes a medida que aumenta la dispersión.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Hiljutine töö loteriipileti hüpoteesi on toonud NMT jaoks väga hõredaid transformaatoreid, säilitades samas BLEU. Siiski on ebaselge, kuidas sellised pügamismeetodid mõjutavad mudeli õppitud representatsioone. Uurides üha enam madala suurusega kaaluga transformaatoreid, leiame, et keeruline semantiline informatsioon tuleb esmalt halveneda. Sisemise aktiveerimise analüüs näitab, et kõrgemad kihid erinevad kõige enam pügamise käigus, muutudes järk-järgult vähem keeruliseks kui nende tihedad kolleegid. Samal ajal hakkavad hõredate mudelite varajased kihid rohkem kodeerima. Tähelepanu mehhanismid jäävad märkimisväärselt järjepidevaks, sest hõredus suureneb.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>اخیراً کار روی فرضیه بلیط لوتری در زمان حفظ BLEU، تغییر‌دهنده‌های زیادی برای NMT تولید کرده است. با این حال، این تکنیک‌های پاره‌گیری چگونه بر نمایش‌های یاد گرفته‌ی یک مدل تأثیر می‌دهد، مشخص نیست. با امتحان تغییر‌دهندگان با وزن‌های بیشتر و کمتر از ارتفاع پایین، می‌بینیم که اول اطلاعات سنتی‌های پیچیده‌ای نابود شده است. تحلیل فعالیت های داخلی نشان می دهد که لایه های بالاتر بیشتر در مسیر تغییر کردن و به تدریج کمتر از همکاران dense آنها پیچیده می شوند. در ضمن، لایه‌های اولین مدل‌های خاکستری شروع می‌کنند که کودکان بیشتری انجام دهند. مکانیسم توجه به طور کامل هماهنگی می‌ماند، در حالی که آرامش افزایش می‌یابد.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Viimeaikainen työ lottokuponin hypoteesista on tuottanut erittäin harvoja muuntajia NMT:lle säilyttäen BLEU:n. On kuitenkin epäselvää, miten tällaiset karsimistekniikat vaikuttavat mallin opittuihin representaatioihin. Tutkimalla muuntajia, joilla on yhä enemmän pienikokoisia painoja, huomaamme, että monimutkaista semanttista tietoa on ensin hajotettava. Sisäisten aktivaatioiden analyysi paljastaa, että korkeammat kerrokset eroavat eniten karsimisen aikana, vähitellen vähemmän monimutkaisia kuin niiden tiheät vastineet. Samaan aikaan harvojen mallien varhaiset kerrokset alkavat suorittaa enemmän koodausta. Huomiomekanismit ovat edelleen huomattavan johdonmukaisia, kun harvaluus kasvaa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Des travaux récents sur l'hypothèse du billet de loterie ont produit des transformateurs très clairsemés pour NMT tout en maintenant l'UEBL. Cependant, il n'est pas clair comment ces techniques d'élagage affectent les représentations apprises d'un modèle. En sondant les Transformers avec de plus en plus de poids de faible magnitude éliminés, nous découvrons que les informations sémantiques complexes sont les premières à être dégradées. L'analyse des activations internes révèle que les couches supérieures divergent le plus au cours de la taille, devenant progressivement moins complexes que leurs homologues denses. Pendant ce temps, les premières couches de modèles épars commencent à effectuer davantage d'encodage. Les mécanismes d'attention restent remarquablement cohérents à mesure que la clairsemence augmente.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tá obair le déanaí ar an hipitéis ticéad crannchuir tar éis Claochladáin an-bheag a tháirgeadh do NMT agus BLEU á chothabháil. Níl sé soiléir, áfach, conas a théann a leithéid de theicnící bearradh i bhfeidhm ar léirithe foghlamtha samhla. Trí iniúchadh a dhéanamh ar Chlaochladáin a bhfuil níos mó agus níos mó meáchain íseal-mhéid acu á ngearradh ar shiúl, feicimid go bhfuil faisnéis shéimeantach chasta le díghrádú ar dtús. Léiríonn anailís ar ghníomhartha inmheánacha gurb iad na sraitheanna níos airde an difríocht is mó le linn bearradh, ag éirí níos lú casta de réir a chéile ná a gcomhghleacaithe dlúth. Idir an dá linn, tosaíonn sraitheanna luath de mhúnlaí tanaí ag déanamh ionchódú níos mó. Fanann meicníochtaí aird thar a bheith comhsheasmhach de réir mar a mhéadaíonn an teimhneacht.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Yin aikin nan da aka samu a kan mala'a mai Lottori ya sami mai girma Transformers wa NMT a lokacin da ke tsare BLEU. Amma, bã shi da gane yadda misãlai masu fahimta ke amfani da misãlai. Ga a jarraba Transformers da sikẽli masu ƙaranci ko ƙaranci, sai za mu gane cewa masu adadi na semantic ya zama kwanza a kunyatar. Anarari ga aikin aiki na guda ya bayyana cewa abubuwa masu sarrafa za'a gaura mafi yawa a tsakanin hankalin, kuma yana kasa sakan kammala musamman da tarakin nan bakin. A lokacin da za'a fara ƙananan masu motsi na tsumarni. Akwai matsayin saurãre yana daidai kamar an ƙara zafi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>העבודה האחרונה על ההיפתוזיה של כרטיסי הלוטו יצרה Transformers מאוד נדיר עבור NMT בזמן שמירה BLEU. בכל אופן, אינו ברור איך טכניקות טיפול כאלה משפיעות על מייצגים למדוגמנים. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more encoding. מנגנוני תשומת לב נשארים עקביים באופן יוצא דופן בזמן שהנדירות עולה.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>लॉटरी टिकट परिकल्पना पर हाल के काम ने BLEU को बनाए रखते हुए NMT के लिए अत्यधिक विरल ट्रांसफॉर्मर का उत्पादन किया है। हालांकि, यह स्पष्ट नहीं है कि इस तरह की छंटाई तकनीकें मॉडल के सीखे गए अभ्यावेदन को कैसे प्रभावित करती हैं। अधिक से अधिक कम परिमाण वजन के साथ ट्रांसफॉर्मर की जांच करके, हम पाते हैं कि जटिल शब्दार्थ जानकारी को सबसे पहले नीचा दिखाया जाना है। आंतरिक सक्रियणों के विश्लेषण से पता चलता है कि उच्च परतें छंटाई के दौरान सबसे अधिक अलग हो जाती हैं, धीरे-धीरे उनके घने समकक्षों की तुलना में कम जटिल हो जाती हैं। इस बीच, विरल मॉडल की शुरुआती परतें अधिक एन्कोडिंग करना शुरू कर देती हैं। ध्यान तंत्र उल्लेखनीय रूप से सुसंगत रहते हैं क्योंकि स्पार्सिटी बढ़ जाती है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nedavni rad na hipotezi karte loterije proizveo je vrlo rezervne transformere NMT tijekom održavanja BLEU-a. Međutim, nije jasno kako takve brižne tehnike utječu na naučene predstave model a. Provjeravajući transformere sa visokom i visokom težinom niskog veličine, otkrili smo da će prvo biti smanjena kompleksna semantička informacija. Analiza unutrašnjih aktivacija pokazuje da se viši slojevi najviše razlikuju tijekom pružanja, postupno postaju manje složeni od njihovih gustih kolega. U međuvremenu, rani sloji rezervnih modela počinju izvršiti više kodiranja. Mehanizmi pažnje ostaju izvanredno odgovarajući kad povećava rezervnost.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A lottószelvény hipotézisének közelmúltbeli munkája rendkívül ritka transzformátorokat hozott létre NMT-hez, miközben fenntartja a BLEU-t. Nem világos azonban, hogy az ilyen metszési technikák hogyan befolyásolják a modell tanult reprezentációit. Egyre több és több alacsony nagyságrendű transzformátort vizsgálva úgy találjuk, hogy az összetett szemantikai információk elsőként romlanak le. A belső aktivációk elemzése azt mutatja, hogy a magasabb rétegek a metszés során a legtöbbször eltérnek, fokozatosan kevésbé bonyolultak, mint sűrű társaik. Eközben a ritka modellek korai rétegei elkezdenek több kódolást végezni. A figyelmeztetési mechanizmusok továbbra is rendkívül következetesek maradnak, ahogy a ritkaság növekszik.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. Այնուամենայնիվ, անհասկանալի չէ, թե ինչպես են նմանատիպ խզբզելու մեթոդները ազդում մոդելի սովոր ներկայացումների վրա: Երբ փորձում ենք վերափոխողներին ավելի ու ավելի ցածր քաշի վրա, մենք հայտնաբերում ենք, որ բարդ սեմանտիկ ինֆորմացիան առաջին հերթին դեգրոդացվում է: Ներքին ակտիվացիաների վերլուծությունը բացահայտում է, որ բարձր շերտերը ամենաբարձր տարբերակում են մաքրման ընթացքում, դառնալով ավելի քիչ բարդ, քան իրենց խտուն համեմատությունները: Մինչդեռ, փոքր մոդելների վաղ շերտերը սկսում են ավելի շատ կոդավորել: Ուշադրություն դարձնելու մեխանիզմները շարունակում են նշանակալի համապատասխան լինել, մինչ արագությունը աճում է:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Pekerjaan baru-baru ini pada hipotesis tiket loteri telah menghasilkan Transformers sangat jarang untuk NMT sementara mempertahankan BLEU. Namun, tidak jelas bagaimana teknik pemotong tersebut mempengaruhi representation belajar model. Dengan menyelidiki Transformers dengan berat badan yang semakin rendah, kami menemukan bahwa informasi semantis kompleks adalah pertama untuk menjadi rendah. Analisi aktivasi interna mengungkapkan bahwa lapisan yang lebih tinggi paling bergerak sepanjang perjalanan pemotongan, secara perlahan-lahan menjadi lebih rumit dari rekan-rekan yang padat mereka. Sementara itu, lapisan awal dari model kecil mulai melakukan lebih banyak pengekodan. Mekanisme perhatian tetap sangat konsisten saat kecepatan meningkat.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Il recente lavoro sull'ipotesi dei biglietti della lotteria ha prodotto Transformers molto scarsi per NMT pur mantenendo BLEU. Tuttavia, non è chiaro come tali tecniche di potatura influenzino le rappresentazioni apprese di un modello. Analizzando Transformers con sempre più pesi di bassa magnitudine tagliati via, scopriamo che le informazioni semantiche complesse vengono prima degradate. L'analisi delle attivazioni interne rivela che gli strati più alti differiscono maggiormente nel corso della potatura, diventando gradualmente meno complessi rispetto alle loro controparti dense. Nel frattempo, i primi livelli di modelli rari iniziano ad eseguire più codifica. I meccanismi di attenzione rimangono notevolmente coerenti man mano che aumenta la scarsità.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>宝くじの仮説に関する最近の研究では、BLEUを維持しながらNMTのための非常にまばらなトランスフォーマーが作られています。しかしながら、そのような枝刈り技術がモデルの学習された表現にどのように影響するかは不明である。より多くの低マグニチュードの重みを刈り取ったトランスフォーマーを探索することで、複雑な意味情報が最初に劣化することがわかります。内部活性化の分析により、高層層は枝刈りの過程で最も発散し、密度の高い層よりも徐々に複雑になっていくことが明らかになりました。一方、まばらなモデルの初期の層は、より多くの符号化を実行し始めます。注目メカニズムは、希少性が増加するにつれて著しく一貫したままである。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Olo-luwih nggawe barang karo akeh kapan-kapan kuwi kapan kawit bagian sing nganggep bantuan Transformer kanggo NMT, sanes memperbudhakan CLUE Nanging, kuwi ora ngerti piye ngerti, teknik kuwi susahe nêmêr kuwi model model sing apik nyeanye Dijejer-jejer Ndelengke alam sing akeh tanggal dipunangke dipunangke alam luwih luwih-luwih jenis diolah punika dipunangke punika dipunangke dipunangke kapan politenessoffpolite"), and when there is a change ("assertivepoliteness Desaturan</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>მხოლოდ ლოტერიის ჰიპოტეზაზე სამუშაო სამუშაო ტრანფორმეტრები NMT-სთვის გამოყენება BLEU-სთვის. მაგრამ არ არის წარმოიდგინელი, როგორ ასეთი წარმოდგინული ტექნოგიები მოდელის გასწავლილი რესპენტირებების შესახებ. ტრანფორმენტირების შესაბამისთვის უფრო და უფრო მეტი მანგნიტური მანგნიტური გაზრუნებით, ჩვენ ვიღებთ, რომ კომპლექსი სიმენტიკური ინფორმაცია პირველი და ინტერული აქტივაციების ანალიზი აღმოჩნდება, რომ უფრო მეტი სინამდვილეები უფრო მეტი სინამდვილეების განსხვავება, რომელიც უფრო მეტი სინამდვილეების განსხვავება საშუალოდ, საწყისო მოდელეების მონაცემები უფრო კოდირებას დაიწყება. მექანიზმი დაახლოებით უფრო შესაძლებელია, როგორც სწრაფად სწრაფად სწრაფად სწორებულია.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Жуырдағы лотериялық билеттер гипотезасының жұмысы BLEU сақтау кезінде NMT үшін артық түрлендірушілерді жасады. Бірақ үлгісінің оқылған түсініктеріне қалай әсер ететін техникалар білмейді. Трансформацияларды біріншіден көп және төмен үлкендердің теңдігін теңдіру арқылы, біз комплекс семантикалық мәліметтер біріншіден деградицияланады. Ішкі белсенділіктердің анализиясы, қабаттардың көпшілігін бұл қабаттардың көпшілігін бұл көпшіліктерінде айырып, тұтас партнерінен артық көпшілікті Осы уақытта, кеңістік моделдердің алдыңғы қабаттары көп кодтамасын орындау бастады. Қарапайым механизмтері көтерілген кезде қарапайым болады.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>최근 로또 가설에 관한 작업은 NMT에 고도로 드문 변압기를 만들어내면서 BLEU를 유지했다.그러나 이런 가위질 기술이 모델의 학습 표시에 어떻게 영향을 미치는지는 아직 분명하지 않다.점점 더 많은 저량급 권한을 가진 변압기를 탐측함으로써 우리는 복잡한 의미 정보가 먼저 강등되는 것을 발견했다.내부 활성화에 대한 분석에 따르면 더 높은 층은 가위질 과정에서 가장 크게 분화되고 밀집층보다 점점 복잡하지 않게 변한다.또한 희소 모형의 초기 층에서 더 많은 인코딩을 실행하기 시작합니다.희소도가 증가함에 따라 주의력 메커니즘은 현저한 일치성을 유지한다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Neseniai atliktas loterijos bilietų hipotezės tyrimas sukėlė labai retas NMT transformatorius ir kartu išlaikė BLEU. However, it is unclear how such pruning techniques affect a model's learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Vidaus aktyvinimo analizė rodo, kad aukštesni sluoksniai labiausiai svyruojant skiriasi, laipsniškai tampa mažiau sudėtingi nei jų tankūs lygiaverčiai sluoksniai. Tuo tarpu pradedami naudoti ankstyvuosius nedidelių modelių sluoksnius. Atsižvelgiant į tai, kad sparčiai didėja, dėmesio mechanizmai išlieka nepaprastai nuoseklūs.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Неодамнешната работа на хипотезата за лотарски билети произведе многу ретки Трансформери за НМТ, истовремено одржувајќи го БЛЕ. Сепак, не е јасно како ваквите техники на исцепување влијаат на научените претставувања на еден модел. Со истражување на Трансформерите со сé повеќе и пониски тегови, откриваме дека комплексните семантични информации прво се деградираат. Анализата на внатрешните активации открива дека повисоките слоеви се разликуваат најмногу во текот на прскањето, постепено станувајќи помалку комплексни од нивните густи колеги. Во меѓувреме, раните слоеви на мали модели почнуваат да извршуваат повеќе кодирање. Механизмите за внимание остануваат исклучително константни со зголемувањето на реткоста.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ലോട്ടറി ടിക്കറ്റ് ഹൈപ്പിറ്റസിസിന്റെ അടുത്ത പ്രവര്‍ത്തിയ്ക്കുന്നത് ബിലിയു സൂക്ഷിച്ചുകൊണ്ടിരിക്കുമ്പോ എങ്കിലും ഒരു മോഡലിന്റെ പഠിച്ച പ്രതിനിധികളെ എങ്ങനെ ബാധിക്കുന്നുവെന്ന് അത് വ്യക്തമായിട്ടില്ല. ട്രാന്‍സ്ഫോര്‍മാര്‍ പരിശോധിക്കുന്നത് കൂടുതല്‍ കുറച്ചും കൂടുതല്‍ വലിപ്പമുള്ള തൂക്കങ്ങള്‍ നീക്കം ചെയ്യുന്നതാണെന്ന് നമുക ആന്തരീക പ്രവര്‍ത്തനങ്ങളുടെ അന്വേഷണം വെളിപ്പെടുത്തുന്നു, കൂടുതല്‍ തട്ടുകള്‍ ബുദ്ധിമുട്ടിയുടെ കാര്യത്തില്‍ ഏറ്റവും വികസിച് അതുകൊണ്ട്, സ്പാസ് മോഡലുകളുടെ ആദ്യമായ തലകള്‍ കൂടുതല്‍ കോഡിങ് പ്രവര്‍ത്തിപ്പിക്കാന്‍ തുടങ്ങി. ശ്രദ്ധിക്കുന്ന മെക്കിനസികങ്ങള്‍ സ്പെയിസിറ്റി വര്‍ദ്ധിപ്പിക്കുന്നത് പോലെയാണ്.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Сүүлийн үед Лотерийн бичлэгийн таамаглалын талаар ажиллах нь БЛУ-г хадгалах үед NMT-ийн төлөөлөгчийн төлөөлөгчийн төлөөлөгчийг бүтээсэн. Гэхдээ ийм загварын сурсан үзүүлэлтийг хэрхэн нөлөөлж байгааг мэдэхгүй. Трансформацуудыг илүү бага хэмжээтэй судалгаагаар бид комплекс семантик мэдээллийг эхлээд ухамсарлах болно. Дотоод дахь үйл ажиллагааны шинжилгээс илүү өндөр давхар нь хамгийн их хэмжээний хувьд хуваагдаж, мөргөн хамтрагчдаас бага цогц болж байна. Гэвч эхний загварын давхар нь илүү кодлого хийж эхэлсэн. Хэрэв анхаарлын механизм нэмэгдэхэд гайхалтай байдаг.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kerja baru-baru ini pada hipotesis tiket loteri telah menghasilkan Transformers yang sangat jarang untuk NMT semasa mengekalkan BLEU. Namun, tidak jelas bagaimana teknik pemotongan seperti ini mempengaruhi perwakilan belajar model. Dengan menyelidiki Transformers dengan berat-berat yang lebih rendah dan lebih rendah dipotong, kita mendapati bahawa maklumat semantik kompleks adalah pertama untuk dihina. Analisi aktivasi dalaman menunjukkan bahawa lapisan yang lebih tinggi bergerak paling dalam perjalanan pemotongan, secara perlahan-lahan menjadi lebih rumit daripada rakan-rakan yang padat. Sementara itu, lapisan awal model jarang mula melakukan pengekodan lebih. Mekanisme perhatian tetap sangat konsisten semasa kecepatan meningkat.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Xogħol reċenti dwar l-ipoteżi tal-biljetti tal-lotterija pproduċa Transformers rari ħafna għall-NMT filwaqt li żammet il-BLEU. Madankollu, mhuwiex ċar kif dawn it-tekniki ta’ pruning jaffettwaw ir-rappreżentazzjonijiet miksuba ta’ mudell. Billi jinstabu Transformers b’piżijiet ta’ daqs dejjem aktar baxx imnaqqsa ’l bogħod, isibu li l-informazzjoni semantika kumplessa l-ewwel trid tiġi degradata. L-analiżi tal-attivazzjonijiet interni turi li saffi ogħla jvarjaw l-aktar matul il-pruning, u gradwalment isiru inqas kumplessi mill-kontropartijiet densi tagħhom. Sadanittant, saffi bikrin ta’ mudelli żgħar jibdew iwettqu aktar kodifikazzjoni. Il-mekkaniżmi ta’ attenzjoni jibqgħu konsistenti b’mod notevoli hekk kif tiżdied l-iskarsezza.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Recent werk aan de loterij lot hypothese heeft zeer schaarse Transformers voor NMT geproduceerd met behoud van BLEU. Het is echter onduidelijk hoe dergelijke snoei technieken invloed hebben op de geleerde representaties van een model. Door Transformers te onderzoeken met steeds meer gewichten van lage grootte weggesneden, ontdekken we dat complexe semantische informatie eerst wordt gedegradeerd. Analyse van interne activeringen toont aan dat hogere lagen het meest uiteenlopen in de loop van het snoeien, geleidelijk minder complex worden dan hun dichte tegenhangers. Ondertussen beginnen vroege lagen van schaarse modellen meer codering uit te voeren. Aandacht mechanismen blijven opmerkelijk consistent naarmate de schaarste toeneemt.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nyleg arbeidet på lotteringshypotesien har produsert svært sparse transformerer for NMT under behandling av BLEU. Det er imidlertid ukjent korleis slike teknikk påvirkar ein modell lærte representasjonar. Ved å prøve transformerer med meir og meir låg størrelsesvekt, finn vi at komplekse semantiske informasjon først skal degraderast. Analyser av interne aktivasjonar viser at høgare lag forskjeller dei fleste over trekking, og gradvis blir mindre komplekse enn dei tette trekantane. I mellomtida startar tidlegare lag med sparse modeller med meir koding. Merknadsmekanismar blir merkelig konsistent når sparsitet øker.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ostatnie prace nad hipotezą losów loterii stworzyły bardzo rzadkie transformatory dla NMT przy jednoczesnym utrzymaniu BLEU. Nie jest jednak jasne, w jaki sposób takie techniki przycinania wpływają na nauczone reprezentacje modelu. Badając transformatory z coraz większą liczbą ciężarów niskiej wielkości, odkrywamy, że złożone informacje semantyczne są najpierw degradowane. Analiza aktywacji wewnętrznych ujawnia, że wyższe warstwy najbardziej różnią się w trakcie przycinania, stopniowo stając się mniej złożone niż ich gęste odpowiedniki. Tymczasem wczesne warstwy rzadkich modeli zaczynają wykonywać więcej kodowania. Mechanizmy uwagi pozostają niezwykle spójne wraz ze wzrostem słabości.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Trabalhos recentes sobre a hipótese do bilhete de loteria produziram Transformers altamente esparsos para NMT, mantendo o BLEU. No entanto, não está claro como essas técnicas de poda afetam as representações aprendidas de um modelo. Ao sondar Transformadores com pesos cada vez mais de baixa magnitude removidos, descobrimos que a informação semântica complexa é a primeira a ser degradada. A análise das ativações internas revela que as camadas mais altas divergem mais ao longo da poda, tornando-se gradualmente menos complexas do que suas contrapartes densas. Enquanto isso, as primeiras camadas de modelos esparsos começam a realizar mais codificação. Os mecanismos de atenção permanecem notavelmente consistentes à medida que a dispersão aumenta.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Lucrările recente asupra ipotezei biletelor de loterie au produs transformatoare foarte rare pentru NMT, menținând în același timp BLEU. Cu toate acestea, nu este clar cum astfel de tehnici de tăiere afectează reprezentările învățate ale unui model. Prin sondarea Transformers cu greutăți din ce în ce mai mici tăiate departe, descoperim că informațiile semantice complexe sunt prima care sunt degradate. Analiza activărilor interne arată că straturile superioare diferă cel mai mult pe parcursul tăierii, devenind treptat mai puțin complexe decât omologii lor densi. Între timp, straturile timpurii de modele rare încep să efectueze mai multe codări. Mecanismele de atenție rămân remarcabil de consecvente pe măsură ce cantitatea scăzută crește.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Недавняя работа над гипотезой лотерейных билетов произвела крайне редкие Трансформеры для NMT при сохранении BLEU. Однако неясно, как такие методы обрезки влияют на полученные представления модели. Зондируя Трансформаторы с все большим и большим количеством урезанных весов малой величины, мы обнаруживаем, что сложная семантическая информация сначала деградирует. Анализ внутренних активаций показывает, что более высокие слои расходятся больше всего в ходе обрезки, постепенно становясь менее сложными, чем их плотные аналоги. Между тем, ранние слои разреженных моделей начинают выполнять больше кодирования. Механизмы привлечения внимания остаются в высшей степени последовательными по мере увеличения их скудности.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ලොට්‍රියි ටිකෙට් විශ්වාසයෙන් අලුත් වැඩේ නැවත NMT වෙනුවෙන් ගොඩක් ප්‍රමාණයක් නිර්මාණය කරලා තියෙ නමුත්, ඒ වගේම නැහැ මොඩල් එක්ක ඉගෙන ගත්ත ප්‍රතිනිධානයකට කොහොමද ප්‍රතිකාර කරන්නේ කියලා. ප්‍රමාණ කරණාකරුවන්ට වැඩිය හා වැඩියෙන් ප්‍රමාණය කරන්න, අපිට හොයාගන්න පුළුවන් සැමැන්තික තොරතුරු විනාශ කරනව ඇතුළු සක්‍රීය විශ්ලේෂණය ප්‍රකාශ කරනවා විශ්ලේෂණයෙන් විශ්ලේෂණය කරනවා කියලා වඩා විශ්වාස කරනවා කියලා වඩා ව අනුවෙන් වෙලාවෙන්, පළවෙනි ස්පර්ස් මෝඩේල්ස් වලින් තරම් සංකේතනය කරන්න පටන් ගන්නවා. බලාපොරොත්තු පද්ධතිය සාමාන්‍ය විශ්වාස කරන්න පුළුවන් වෙනවා.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nedavno delo na hipotezi loterijskih vstopnic je ustvarilo zelo redke transformatorje za NMT, medtem ko je ohranilo BLEU. Vendar pa ni jasno, kako takšne tehnike obrezovanja vplivajo na znane predstavitve modela. S sondiranjem transformatorjev z vedno več težami nizke magnitude, ugotovimo, da je treba kompleksne semantične informacije najprej razgraditi. Analiza notranjih aktivacij razkriva, da se višje plasti med obrezovanjem najbolj razlikujejo in postopoma postajajo manj kompleksne od njihovih gostih kolegov. Medtem pa zgodnje plasti redkih modelov začnejo izvajati več kodiranja. Mehanizmi pozornosti ostajajo izjemno usklajeni, saj se redkost povečuje.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Shaqo ku saabsan tijaatarka tigidhada lottery waxay sameeyeen tarjumayaal aad u yar oo ay u dhaqdhaqaaqaan BLEU. Si kastaba ha ahaatee ma ahan sida qaababka caqliga ah ay u saameyso noocyada lagu bartay. Marka lagu imtixaamo turjumayaasha iyo miisaan badan oo hoos u yar, waxaynu aragnaa in macluumaad adag ee semantika marka hore la hoosaysiiyo. Analyska waxqabadka gudaha ah wuxuu muujiyaa in qasnadaha sare aad u kala bedelaan xiliga waxgarashada, si taxadar ah waxay u noqdaan wax ka yar murugaysan saaxiibbadooda hoose. Wakhtigaas waxaa bilaabaya inay sameeyaan codsiga badan. Meherka daryeelka waxyaabaha la'aanta ah waxay u sii socon yihiin sida kordhiya cimriga.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. However, it is unclear how such pruning techniques affect a model's learned representations. Duke vëzhguar Transformuesit me pesha më të vogla dhe më të vogla të rrënuara larg, ne gjejmë se informacioni kompleks semantik është i pari për të degraduar. Analiza e aktiviteteve të brendshme tregon se shtresa më të larta ndryshojnë më shumë gjatë rrjedhës së rrjedhjes, duke u bërë gradualisht më pak komplekse se homologët e tyre të dendur. Ndërkohë, nivelet e hershme të modeleve të vogla fillojnë të kryejnë më shumë kodim. Mekanizmat e vëmendjes mbeten jashtëzakonisht të konsistenta ndërsa pakësia rritet.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nedavni rad na hipotezi karte loterije proizveo je vrlo rezervne transformere NMT-a dok održava BLEU. Međutim, nije jasno kako takve brižne tehnike utiču na naučene predstave model a. Probajući transformatore sa većim i visokim težinama niskog veličine, otkrivamo da će prvo biti smanjena kompleksna semantična informacija. Analiza unutrašnjih aktivacija pokazuje da se viši slojevi najviše razlikuju tokom pružanja, postupno postaju manje kompleksni od njihovih gustih kolega. U međuvremenu, rani sloji rezervnih modela počinju izvršiti više kodiranja. Mehanizmi pažnje ostaju izvanredno konsistentni dok se povećava sparsitet.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nyligen arbete med lotteri lotter hypotesen har producerat mycket glesa Transformers för NMT samtidigt som BLEU upprätthålls. Det är dock oklart hur sådana beskärningstekniker påverkar en modells lärda representationer. Genom att sondera Transformers med fler och fler låga vikter avskurna, finner vi att komplex semantisk information först försämras. Analys av interna aktiveringar visar att högre skikt skiljer sig mest under beskärningen och gradvis blir mindre komplexa än deras täta motsvarigheter. Samtidigt börjar tidiga lager av glesa modeller utföra mer kodning. Uppmärksamhetsmekanismerna förblir anmärkningsvärt konsekventa i takt med att sparnivån ökar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kazi za hivi karibuni kuhusu nadharia ya tiketi ya madini imetengeneza WaTransformers kwa ajili ya NMT wakati wa kuendelea BLEU. Hata hivyo, haijulikani jinsi mbinu hizi za akili zinavyoathiri uwakilishi wa modeli waliojifunza. Kwa kuwajaribu WaTransfers na mizani yenye kiwango cha chini imeondolewa, tunagundua kuwa taarifa tatizo ni ya kwanza ya kupunguza. Uchambuzi wa shughuli za ndani unaonyesha kuwa vipande vya juu vinavyotofautiana zaidi katika kipindi cha kuelewa, kwa taratibu kinakuwa tatizo kuliko wapenzi wao wenye msingi. Wakati huo huo, vipande vya mwanzo vya mifano ya uchimbaji vinaanza kufanya kodi zaidi. Attention mechanisms remain remarkably consistent as sparsity increases.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>சமீபத்தில் லாட்டரி டிக்கெட் துப்பாக்கத்தில் செயல்பாடு BLEU வைத்திருக்கும் போது NMT மாற்றுபவர்களை மிகவும் வெற ஆனால், இவ்வாறு புதிய தொழில்நுட்பம் எப்படி ஒரு மாதிரியின் கற்றப்பட்ட பங்குகளை பாதிக்கும் என்பது தெளிவா மாற்றுபவர்களை மேலும் அதிக குறைந்த அளவுகளை நீக்கி விட்டதால், சிக்கலான பெம்பெண்டிக் தகவல் முதலில் குறைந்துவிடும் என்பதை நா உள்ளார்ந்த செயல்பாடுகளின் ஆய்வு அதிக குறியீட்டை செய்ய மாதிரிகளின் ஆரம்ப அடுக்குகள் கவனம் முறைமைகள் மிகவும் பொருத்தமாக இருக்கும் வெளிப்பாடு அதிகரிக்கும் போது.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ýakynda loteriýan bilet teorisinde işlenýän işi BLEU tutulanda NMT üçin gaty uly gaýd edilen Transformerçiler üretildi. Ama bu kadar akıllı teknolojiler modelinin öğrenmiş temsillerine nähili etkisi yaratmaz. Transformerleri köp we azaltrak ağırlıklar bilen denedip, ilkinji gezek kompleks semantik maglumaty azaltmak üçin pikir edýäris. Daşary janlaşdyrmalaryň analizi ýokary katlaryň ýokary ýokary ýokary ýokary düşürip, ýokary ýokary ýakynlaşyklaryndan az karmaşık bolup görünýär. Bu arada, irden depler nusgalarynyň ködlemeleri başarmak üçin başlaýar. Seresap mekanizmalary ýuwaşlyk bilen aýratyn bir şekilde dowam edýärler.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>لوتری ٹیکٹ کی فرضی پر اچھا کام NMT کے لئے بہت اچھا ترفنسر پیدا کیا گیا ہے جبکہ BLEU حفاظت کرتا ہے۔ لیکن یہ معلوم نہیں کہ ایک موڈل کی تعلیم کا کس طرح اثر کرتا ہے۔ ٹرانسفور کو آزمائش کے ذریعہ سے زیادہ اور زیادہ کم بڑائی وزن سے دور کر دیا گیا ہے، ہم دیکھتے ہیں کہ پیچیدہ سیمانٹی معلومات پہلے ذلیل ہونے والی ہے. داخلی فعالیت کا تحلیل ظاہر کرتا ہے کہ بالاترین لائٹوں سے زیادہ تغییر کرتا ہے اور ان کے گہرے کنٹوروں سے کم پیچیدہ ہوجاتا ہے۔ یہاں تک کہ اسپرس موڈل کے پہلے لہروں سے زیادہ اکڈینڈ کرنا شروع ہوتا ہے۔ توجه کے مکانیزوں باقی رہتے ہیں جب تکلیف اضافہ ہوتی ہے۔</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Name Lekin, bu narsalar modelning o'rganilgan tashkilotlariga qanday qo'llanmaydi. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Ichki harakatlarni taʼminlovchi, yuqori qatlamlarning ko'p chegarasini o'zgartiradi, tez qismlaridan murakkab bo'ladi. Hozirga, kichkina modellarning birinchi qatlam kodlash usulini ishga tushirishni boshlaydi. Aniqlik mechanislari kichkina ko'paytirishda juda qiziqarli bo'ladi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Công trình gần đây về giả thuyết vé số đã sản xuất ra các biến hình của công ty NMT rất ít trong khi còn nguyên tiếng bíp. Tuy nhiên, không rõ làm thế nào các kỹ thuật cắt tỉa ảnh hưởng đến các biểu tượng được học hỏi. Bằng cách dò tìm các Transformers với các lượng trọng lượng thấp bị tỉa dần, chúng tôi thấy thông tin cơ bản đầu tiên bị thoái hoá. Phân tích kích hoạt nội bộ cho thấy các lớp cao khác biệt nhiều nhất trong quá trình cắt tỉa, dần dần trở nên phức tạp hơn so với các lớp đông đúc. Trong khi đó, lớp đầu của các mô hình rải rác bắt đầu đặt thêm mã số. Các cơ quan chú ý vẫn ổn định mỗi khi số lượng hẹp tăng.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>近彩票伪事NMT生疏变形金刚,兼守BLEU。 然尚未详此类修剪之术,何以加于模形之学也。 探剪愈多低量级权变形金刚见语义先降。 内激活之分析表明,剪剪之际,高者最大,渐不如密者应物杂。 同时疏早期,更行更多编码。 随疏增益,意机犹一。</span></div></div><dl><dt>Anthology ID:</dt><dd>2020.blackboxnlp-1.19</dd><dt>Volume:</dt><dd><a href=/volumes/2020.blackboxnlp-1/>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/blackboxnlp/>BlackboxNLP</a>
| <a href=/venues/emnlp/>EMNLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>193–203</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.blackboxnlp-1.19>https://aclanthology.org/2020.blackboxnlp-1.19</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2020.blackboxnlp-1.19 title="To the current version of the paper by DOI">10.18653/v1/2020.blackboxnlp-1.19</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">movva-zhao-2020-dissecting</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Rajiv Movva and Jason Zhao. 2020. <a href=https://aclanthology.org/2020.blackboxnlp-1.19>Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation</a>. In <i>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</i>, pages 193–203, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2020.blackboxnlp-1.19>Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation</a> (Movva & Zhao, BlackboxNLP 2020)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf>https://aclanthology.org/2020.blackboxnlp-1.19.pdf</a></dd><dt class=acl-button-row>Optional supplementary material:</dt><dd class=acl-button-row><a href=https://aclanthology.org/attachments/2020.blackboxnlp-1.19.OptionalSupplementaryMaterial.pdf class="btn btn-attachment btn-sm"><i class="fas fa-file"></i>
&nbsp;2020.blackboxnlp-1.19.OptionalSupplementaryMaterial.pdf</a></dd><dt class=acl-button-row>Video:</dt><dd class=acl-button-row><a href=https://slideslive.com/38939765 class="btn btn-attachment btn-sm"><i class="fas fa-video"></i>&nbsp;https://slideslive.com/38939765</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf title="Open PDF of 'Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Dissecting+Lottery+Ticket+Transformers+%3A+Structural+and+Behavioral+Study+of+Sparse+Neural+Machine+Translation" title="Search for 'Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a>
<a class="btn btn-attachment d-flex flex-wrap justify-content-center" href=https://aclanthology.org/attachments/2020.blackboxnlp-1.19.OptionalSupplementaryMaterial.pdf title="Open optional supplementary material for 'Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation'"><span class="align-self-center px-1"><i class="fas fa-file"></i></span>
<span class=px-1>Optional supplementary material</span></a>
<a class="btn btn-attachment d-flex flex-wrap justify-content-center" href=https://slideslive.com/38939765 title="Open video for 'Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation'"><span class="align-self-center px-1"><i class="fas fa-video"></i></span>
<span class=px-1>Video</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation](https://aclanthology.org/2020.blackboxnlp-1.19) (Movva & Zhao, BlackboxNLP 2020)</p><ul class=mt-2><li><a href=https://aclanthology.org/2020.blackboxnlp-1.19>Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation</a> (Movva & Zhao, BlackboxNLP 2020)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Rajiv Movva and Jason Zhao. 2020. <a href=https://aclanthology.org/2020.blackboxnlp-1.19>Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation</a>. In <i>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</i>, pages 193–203, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>