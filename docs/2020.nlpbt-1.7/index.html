<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention" name=citation_title><meta content="Aman Khullar" name=citation_author><meta content="Udit Arora" name=citation_author><meta content="Proceedings of the First International Workshop on Natural Language Processing Beyond Text" name=citation_conference_title><meta content="2020/11" name=citation_publication_date><meta content="https://aclanthology.org/2020.nlpbt-1.7.pdf" name=citation_pdf_url><meta content="60" name=citation_firstpage><meta content="69" name=citation_lastpage><meta content="10.18653/v1/2020.nlpbt-1.7" name=citation_doi><meta property="og:title" content="MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention"><meta property="og:image" content="https://aclanthology.org/thumb/2020.nlpbt-1.7.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2020.nlpbt-1.7"><meta property="og:description" content="Aman Khullar, Udit Arora. Proceedings of the First International Workshop on Natural Language Processing Beyond Text. 2020."><link rel=canonical href=https://aclanthology.org/2020.nlpbt-1.7></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical Attention<span class=acl-fixed-case>MAST</span>: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</a>
<a id=af_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodal Abstractive Opsomming met Trimodal Hierarchical Aangaande</a>
<a id=am_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>Multimodal Abstractive Summary with Trimodal Hierarchical Attention</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: تلخيص تجريدي متعدد الوسائط مع اهتمام هرمي ثلاثي الوسائط</a>
<a id=az_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Trimodal Hierarchical Attention ilə Multimodal Abstractive Summarization</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>МАСТ: Мултимодално абстрактивно обобщение с тримодално йерархично внимание</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: ত্রিমোডাল হিরেরার্কিক মনোযোগ দিয়ে বহুমোডাল আবত্ত্রিক সামারিজেশন</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Trimodal Hierarchical Attention</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodalna abstraktivna sažetka sa trimodalnom hijerarhičkom pažnjom</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Resumen Abstractiu Multimodal amb Atenció Hierarquica Trimodal</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodální abstraktní shrnutí s trimodální hierarchickou pozorností</a>
<a id=da_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodal Abstraktiv Resumé med Trimodal Hierarkisk Opmærksomhed</a>
<a id=de_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodale abstrakte Zusammenfassung mit trimodaler hierarchischer Aufmerksamkeit</a>
<a id=el_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>Πολυmodale αφηρημένη Σύνοψη με Τριmodale Ιεραρχική Προσοχή</a>
<a id=es_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Resumen abstractivo multimodal con atención jerárquica trimodal</a>
<a id=et_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodaalne abstraktne kokkuvõte koos kolmemodaalse hierarhilise tähelepanuga</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: جمع‌آوری بیش‌modal abstractive with Trimodal Hierarchical Attention</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST : Synthèse abstraite multimodale avec attention hiérarchique trimodale</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Achoimre Ilmhódach Teibí le Aird Ordlathach Trimodal</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>KCharselect unicode block name</a>
<a id=he_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>סדרה מורכבת אסטרקטיבית עם תשומת לב הייררכית טרימודלית</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>मस्तूल: Trimodal पदानुक्रमित ध्यान के साथ मल्टीमॉडल अमूर्त Summarization</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodalna abstraktivna sažetka s tromodalnom hijerarskom pažnjom</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodális Absztraktív Összefoglalás Trimodális Hierarchikus Figyelemmel</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>ՄԱՍՏ. Մոլիմոդալ Աբլաստրատիվ համառոտագրություն, որը ունի Երիմոդալ Հիերախիկ ուշադրություն</a>
<a id=id_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodal Abstractive Summarization dengan Trimodal Hierarchical Attention</a>
<a id=is_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Sintesi astratta multimodale con attenzione gerarchica trimodale</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST ：三相階層的な注意を伴う多相抽象的な要約</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MASTA: Multimodal absolutetraction Cummariation with Trimodal Hiaradical Attention</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: მულტიმედიალური აბსტრაქტიური კომპანიზაცია რრიმოდეალური hiერაქტიური დაახლოებით</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Үш модел гиерархикалық назардағы көптеген абстрактивті тұжырымдамасы</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: 삼모드 차원 주의가 있는 다중모드 추상 요약</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: ട്രിമോഡാല്‍ ഹീരാര്‍ക്കിക്കല്‍ ശ്രദ്ധ കൊണ്ട് മള്‍മോഡല്‍ അബ്ട്രാക്ട്രാക്ടിവിന്‍റെ ചുരുക്കം</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>МАСТ: Олон моделийн абстрактив хэмжээ гурван гиерархик анхаарал</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>Perhatian Hierarkik Trimodal</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodale abstracte samenvatting met trimodale hiërarchische aandacht</a>
<a id=no_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodal abstraktiv samandrag med trimodal hierarkisk merking</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodalna abstrakcyjna streszczenie z uwagą hierarchiczną trimodalną</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Resumo Abstrativo Multimodal com Atenção Hierárquica Trimodal</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Rezumat abstractiv multimodal cu atenție ierarhică trimodală</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Мультимодальное абстрактное обобщение с тримодальным иерархическим вниманием</a>
<a id=si_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: ත්‍රිමෝඩාල් හියාර්චිකල් අවධානය සමග ග ගොඩක් අවධානය</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodalni abstraktivni povzetek s trimodalno hierarhično pozornostjo</a>
<a id=so_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodal Abstractive Summary with Trimodal Hierarchical Attention</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Summarization Multimodal Abstractive with Trimodal Hierarchical Attention</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodalna abstraktivna sažetka sa trimodalnom hijerarhičkom pažnjom</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Multimodal Abstraktiv Sammanfattning med Trimodal Hierarkisk Uppmärksamhet</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Ujumbe wa mfululizo wa kidini na uangalizi wa Taifa</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Trimodal Hierarchical Attention with Multimodal Abstractive Summary</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST: Üç modal Hiyerarşik Dikkati ile Çoklumodal Abstraktiv Toplaşum</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>ماسٹ: ٹریموڈال حیرارشیک حفاظت کے ساتھ بہت سی مڈیل آب تراکٹیو جماریز</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>Sơ suất đa phương với kính thiên hà ba chiều chú ý</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2020.nlpbt-1.7.pdf>MAST:三模态多模态抽摘要</a></h2><p class=lead><a href=/people/a/aman-khullar/>Aman Khullar</a>,
<a href=/people/u/udit-arora/>Udit Arora</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities text, audio and video in a multimodal video. Prior work on multimodal abstractive text summarization only utilized information from the text and video modalities. We examine the usefulness and challenges of deriving information from the <a href=https://en.wikipedia.org/wiki/Audio_signal>audio modality</a> and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Hierdie papier vertoon Mast, 'n nuwe model vir Multimodal Abstractive Teks Opsomming wat inligting gebruik van alle drie modaliteite - teks, oudio en video - in 'n multimodaal video. Vorige werk op multimodale abstraktiewe teks opsomming slegs gebruikte inligting van die teks en video modaliteite. Ons ondersoek die bruikbaarheid en uitdagings van afgeleide inligting van die oudio modaliteit en voorsien 'n volgorde-na-volgorde trimodaal hierarkiese aandag-gebaseerde model wat hierdie uitdagings oorwin deur die model meer aandag aan die teks modaliteit te laat maak. Mas uitvoer die huidige staat van die kuns model (video- text) deur 2. 51 punte in terms van Inhoud F1 telling en 1. 00 punte in terms van Rouge- L telling op die How2 datastel vir multimodale taal verstanding.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ይህ ገጽ አዲስ የፊደል አዲስ ምሳሌ ለመብሎዶል Abstractive ጽሑፍ ማጠቃለያ ነው፡፡ የጽሑፍ እና የቪዲዮ ድርጅቶች በተጠቃሚ የጽሑፍ ማስታወሻ ብቻ ነው፡፡ የድምፅ አካባቢ መረጃዎችን ለማግኘት ጥቅም እና ውቀቶችን እናፈትናለን፡፡ የፊደል ቅርጽ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>تقدم هذه الورقة MAST ، نموذج جديد لتلخيص النص التجريدي متعدد الوسائط الذي يستخدم المعلومات من جميع الأساليب الثلاثة - النص والصوت والفيديو - في فيديو متعدد الوسائط. العمل المسبق على تلخيص النص التجريدي متعدد الوسائط استخدم فقط المعلومات من طرائق النص والفيديو. ندرس فائدة وتحديات استخلاص المعلومات من الطريقة الصوتية ونقدم نموذجًا متسلسلًا متسلسلًا هرميًا قائمًا على الاهتمام يتغلب على هذه التحديات من خلال السماح للنموذج بإيلاء مزيد من الاهتمام لطريقة النص. يتفوق MAST على الحالة الحالية للنموذج الفني (نص الفيديو) بمقدار 2.51 نقطة من حيث درجة المحتوى F1 و 1.00 نقطة من حيث درجة Rouge-L على مجموعة بيانات How2 لفهم اللغة متعددة الوسائط.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bu kağıt çoxlu modal Abstraktiv Metin Toplaşdırması üçün yeni bir modeli MAST'i göstərir ki, çoxlu modal videoda məlumatları - metin, audio və video ilə istifadə edir. Çoxlu modal abstraktiv metin qeyd edilməsindən əvvəlki işlər yalnız metin və video modüllərindən istifadə edilmişdir. Biz audio modaliyyətindən məlumatların faydalanılığını və çətinliklərini incidirik və sıralama-sıralama-sıralama-sıralama-sıralama-sıralama-sıralama-sıralama-sıralama-sıralama-sıralama-sıralama-sıralama-sıralama-sıralama-sıralama modelini göstəririk ki, bu çətinliklərə üstün gəlir MAST sanat model in in (video-text) ağımdaki durumunu 2.51 pünkt ilə Content F1 score ilə və çox modal dil anlaşılması üçün How2 veri qutusu ilə Rouge-L score ilə 1.00 pünkt göstərir.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Настоящата статия представя нов модел за мултимодално абстрактивно обобщаване на текста, който използва информация от трите модала - текст, аудио и видео - в мултимодално видео. Предишната работа по мултимодалното абстрактно обобщаване на текста използва само информация от текстовите и видео модалите. Проучваме полезността и предизвикателствата при извличането на информация от аудио модалността и представяме тримодален йерархичен модел последователност към последователност, базиран на вниманието, който преодолява тези предизвикателства, като позволява на модела да обърне повече внимание на текстовия модал. МАСТ превъзхожда съвременния модел (видео-текст) с 2.51 точки по отношение на резултата Съдържание 1 и 1.00 точки по отношение на резултата Руж-Л в набора данни за мултимодално разбиране на езика.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>এই পত্রিকাটি মাল্টিমোডাল আবট্রাক্টিভ টেক্সট সামারিজেশনের জন্য একটি নতুন মডেল মাস্টিকে উপস্থাপন করেছে, যা সব তিন মোডেল থেকে তথ্য ব্যবহার করে - মাল্টিমোডাল অক্ষরিক টেক্সট সংক্ষেপের প্রাথমিক কাজ শুধুমাত্র লেখা এবং ভিডিও মডেল থেকে তথ্য ব্যবহার করা হয় আমরা অডিও মোডেল থেকে তথ্য পাওয়ার কার্যকর এবং চ্যালেঞ্জ পরীক্ষা করি এবং ত্রিমোডাল হিয়ারার্কিকাল ভিত্তিক মডেল উপস্থাপন করি যা এই চ্যালেঞ্জের উপর জয়ী হয়েছে এবং মড মাস্টির বর্তমান শিল্প মডেল (ভিডিও- টেক্সট) দ্বারা ২. 51 পয়েন্ট প্রদর্শন করে বিষয়বস্তু F1 স্কোর এবং ১. ০০ পয়েন্টের মাধ্যমে রোউজ- L স্কোর বিভিন্ন মানুষে</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ཤོག་བྱང་འདིས་ MAST སྟོན་པ་དང་རྣམ་གྲངས་འདྲ་བའི་སྣ་ཚོགས་རྣམ་གྲངས་སྒྲིག་མཛོད་ཁུངས་ཀྱི་མ་དབྱིབས་གསར་པ་ཞིག་སྟོན་ཐུབ་པའི་ཐབས་ལམ་གསུམ Multimodal abstractive text summarization་གི་སྔོན་གྱི་ལས་སྤྱོད་རྒྱུ་དངོས་ཡིག་གེ་དང་བརྙན་རིས་ཐབས་ལམ་ནང་ལས་ལག་ལེན་འཐབ་པའི་བརྡ་སྟོན་དགོས We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ovaj papir predstavlja MAST, novi model za rezervaciju multimodalnog abstraktivnog teksta koji koristi informacije iz svih tri moda - tekst, audio i video - u multimodalnom video. Prije rad na sažetku multimodalnog abstraktivnog teksta koristio je samo informacije iz teksta i video modaliteta. Provjeravamo korisnost i izazove izvlačenja informacija iz audio modaliteta i predstavljamo trimodalni model na osnovu tri-sekvence na osnovu hijerarhijske pažnje koji prevlada te izazove tako što omogućavamo modelu da obratite više pažnje na tekstualnu modalitetu. MAST iznosi trenutno stanje umjetničkog modela (video-teksta) sa 2,51 bodova u smislu rezultata Content F1 i 1,00 bodova u smislu rezultata Rouge-L na setu podataka How2 za razumijevanje multimodalnog jezika.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities - text, audio and video - in a multimodal video. Prior work on multimodal abstractive text summarization only utilized information from the text and video modalities. We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tento článek představuje MAST, nový model multimodální abstrakční textové shrnutí, který využívá informace ze všech tří modalit textu, audio a videa v multimodálním videu. Předchozí práce na multimodální abstraktivní shrnutí textu využívaly pouze informace z textových a video modalit. Zkoumáme užitečnost a výzvy odvození informací z audio modality a představujeme trimodální hierarchický model založený na pozornosti, který tyto výzvy překonává tím, že model nechá věnovat větší pozornost modalitě textu. MAST překonává současný model (video-text) o 2,51 body z hlediska skóre obsahu F1 a 1,00 bodů z hlediska skóre Rouge-L na datové sadě How2 pro multimodální porozumění jazyků.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Denne artikel præsenterer MAST, en ny model for multimodal abstraktiv tekst summering, der bruger information fra alle tre modaliteter - tekst, lyd og video - i en multimodal video. Tidligere arbejde med multimodal abstraktiv tekst resuméering anvendte kun oplysninger fra tekst og video modaliteter. Vi undersøger nytten og udfordringerne ved at udlede information fra lydmodaliteten og præsenterer en sekvens-til-sekvens trimodal hierarkisk opmærksomhedsbaseret model, der overvinder disse udfordringer ved at lade modellen være mere opmærksom på tekstmodaliteten. MAST overgår den aktuelle state of te art model (video-tekst) med 2,51 point i form af Content F1 score og 1,00 point i form af Rouge-L score på How2 datasættet til multimodal sprogforståelse.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dieser Beitrag stellt MAST vor, ein neues Modell für multimodale abstrakte Textzusammenfassung, das Informationen aus allen drei Modalitäten Text, Audio und Video in einem multimodalen Video nutzt. Bisherige Arbeiten zur multimodalen abstraktiven Textzusammenfassung nutzten ausschließlich Informationen aus den Text- und Videomodalitäten. Wir untersuchen die Nützlichkeit und Herausforderungen der Ableitung von Informationen aus der Audiomodalität und stellen ein sequenz-zu-sequenz trimodales hierarchisches aufmerksamkeitsbasiertes Modell vor, das diese Herausforderungen überwindet, indem das Modell mehr Aufmerksamkeit auf die Textmodalität lenkt. MAST übertrifft den aktuellen Stand der Technik Modell (Video-Text) um 2,51 Punkte in Bezug auf Content F1 Score und 1,00 Punkte in Bezug auf Rouge-L Score auf dem How2 Datensatz für multimodales Sprachverständnis.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Η παρούσα εργασία παρουσιάζει ένα νέο μοντέλο για την Πολυmodale Περίληψη Κειμένου που χρησιμοποιεί πληροφορίες και από τις τρεις λειτουργίες του κειμένου, του ήχου και του βίντεο σε ένα πολυμοδικό βίντεο. Προηγουμένες εργασίες για την πολυπροπική αφηρημένη σύνοψη κειμένου χρησιμοποιούσαν μόνο πληροφορίες από τις λεπτομέρειες κειμένου και βίντεο. Εξετάζουμε τη χρησιμότητα και τις προκλήσεις της απόκτησης πληροφοριών από την ακουστική τροπικότητα και παρουσιάζουμε ένα τριμοντικό ιεραρχικό μοντέλο που βασίζεται στην προσοχή ακολουθίας-ακολουθίας που ξεπερνά αυτές τις προκλήσεις αφήνοντας το μοντέλο να δώσει μεγαλύτερη προσοχή στην τροπικότητα κειμένου. Το MAST ξεπερνά το σημερινό μοντέλο (βίντεο-κείμενο) κατά 2.51 πόντους όσον αφορά την βαθμολογία περιεχομένου F1 και 1.00 πόντους όσον αφορά την βαθμολογία Rouge-L στο σύνολο δεδομένων How2 για την κατανόηση της πολυμορφικής γλώσσας.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Este artículo presenta MAST, un nuevo modelo de resumen de texto abstractivo multimodal que utiliza información de las tres modalidades (texto, audio y vídeo) en un vídeo multimodal. Los trabajos anteriores sobre la sumarización de textos abstractivos multimodales solo utilizaban información de las modalidades de texto y video. Examinamos la utilidad y los desafíos de derivar información de la modalidad de audio y presentamos un modelo jerárquico trimodal basado en la atención de secuencia a secuencia que supera estos desafíos al permitir que el modelo preste más atención a la modalidad de texto. MAST supera al modelo actual de vanguardia (video-texto) en 2,51 puntos en términos de puntuación de Content F1 y 1,00 puntos en términos de puntuación Rouge-L en el conjunto de datos How2 para la comprensión multimodal del lenguaje.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Käesolevas töös tutvustatakse MAST-i, uut mudelit multimodaalse teksti kokkuvõtmiseks, mis kasutab informatsiooni kõigist kolmest mudelist - teksti, heli ja video - multimodaalses videos. Mitmeliigilise abstraktse teksti kokkuvõtte varasem töö kasutas ainult teksti- ja videomudelite infot. Uurime audiomodaalsusest teabe saamise kasulikkust ja väljakutseid ning esitame järjestusest järjestuseni trimodaalse hierarhilise tähelepanu põhineva mudeli, mis lahendab need väljakutsed, lubades mudelil pöörata rohkem tähelepanu tekstimodaalsusele. MAST ületab praeguse tehnika mudeli (video-tekst) 2,51 punkti võrra sisu F1 skoori ja 1,00 punkti Rouge-L skoori How2 andmekogumi mitmeliigilise keele mõistmise kohta.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>این کاغذ MAST را نشان می‌دهد، یک مدل جدید برای جمع کردن متن آب‌تراکتی چندیmodal که اطلاعات را از همه سه حالت - متن، صدا و ویدئو - در یک ویدئو چندیmodal استفاده می‌کند. کارهای پیشینه روی جمع کردن متن متن چندmodal abstractive تنها اطلاعات استفاده از متن و modalities ویدئو استفاده می‌شود. ما مطالبی و چالش‌های استفاده از دسترسی اطلاعات از مدل صدا را تحقیق می‌کنیم و یک مدل توجه به مدل سه مدل از مدل‌های مختلف به عنوان مدل توجه به مدل متن را پیشنهاد می‌کنیم که این چالش‌ها را از دست می‌دهد، با اجازه دادن مدل توجه بیشتری به مدل مت MAST موقعیت فعلی مدل هنر (ویدئو-متن) با 2.51 نقطه به عنوان نقطه محتوای F1 و 1.00 نقطه به عنوان نقطه‌ای Rouge-L بر روی مجموعه داده‌های How2 برای درک زبان‌های متعددی بیشتر انجام می‌دهد.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tässä artikkelissa esitellään MAST, uusi malli multimodaalisen abstraktin tekstin yhteenvetoon, joka hyödyntää tietoa kaikista kolmesta modaalista - teksti, ääni ja video - multimodaalisessa videossa. Aikaisemmassa multimodaalisessa abstraktiivisessa tekstiyhteenvedossa hyödynnettiin vain teksti- ja videomodaaleista saatavaa tietoa. Tutkimme audiomodaalista tiedon tuottamisen hyödyllisyyttä ja haasteita ja esittelemme sekvenssi-sekvenssiin trimodaalisen hierarkkisen huomiopohjaisen mallin, joka voittaa nämä haasteet antamalla mallin kiinnittää enemmän huomiota tekstimodaalisuuteen. MAST ylittää nykytekniikan mallin (video-teksti) 2,51 pisteellä Content F1 -pisteellä ja Rouge-L-pisteellä multimodaalisen kielen ymmärtämisen How2-aineistossa 1,00 pisteellä.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Cet article présente MAST, un nouveau modèle de synthèse de texte abstrait multimodal qui utilise des informations provenant des trois modalités — texte, audio et vidéo — dans une vidéo multimodale. Les travaux antérieurs sur la synthèse de textes abstraits multimodaux n'utilisaient que les informations provenant des modalités texte et vidéo. Nous examinons l'utilité et les défis liés à la dérivation d'informations à partir de la modalité audio et présentons un modèle hiérarchique trimodal basé sur l'attention séquence-à-séquence qui surmonte ces défis en laissant le modèle accorder plus d'attention à la modalité texte. MAST surpasse le modèle actuel de pointe (vidéo-texte) de 2,51 points en termes de score Content F1 et de 1,00 point en termes de score Rouge-L sur le jeu de données How2 pour la compréhension multimodale des langues.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Cuireann an páipéar seo i láthair MAST, samhail nua le haghaidh Achoimriú Ilmhódúil Téacs Teibí a úsáideann faisnéis ó na trí mhódúlacht - téacs, fuaime agus físe - i bhfíseán ilmhódach. Níor úsáideadh réamhobair ar achoimriú téacs ilmhódach teibí ach amháin as faisnéis ó na módúlachtaí téacs agus físe. Scrúdaímid a úsáidí agus na dúshláin a bhaineann le faisnéis a fháil ón modhúlacht fuaime agus cuirimid i láthair múnla ordlathach aird-bhunaithe triantánach seicheamh-go-seicheamh a sháraíonn na dúshláin seo trí ligean don mhúnla aird níos mó a thabhairt ar mhodhúlacht an téacs. Tá 2.51 pointe níos fearr ag MAST ná an tsamhail úrscothach (fístéacs) i dtéarmaí scór Ábhar F1 agus 1.00 pointe i dtéarmaí scór Rouge-L ar thacar sonraí How2 maidir le tuiscint teanga ilmhódach.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Wannan takardar gaske na gaurar MAS, wata wata shekara na ƙarami wa masu multi-modal Abtractive Text Summariɗar da ke amfani da information daga duk tsari uku - rubutu, sauna da video - cikin wani video mai yawa. Kayyar aikin aiki na multi-multi-dural separation of text only used in amfani da information from modalities na text and video. Tuna jarraba amfani da masu kanana ga motsi daga tsari na saurãre kuma muna gaura wata misali mai sauri-sauri-sauri-dubi-sauri, wanda ke rinjãya masu hanyoyin wannan, da kuma ya yarda motel ya zama masu sauna ga tsarin matsayin. MASA na ƙididdige halin da ke yanzu kamar misalin sanar (video-text) da 2.51 points cikin muhimman F1 score da 1.0 points cikin muhimman kwanan rubutun Rouge-L kan tsarin maɓallin How2 wa gane multi-multi-language.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities - text, audio and video - in a multimodal video. עבודה קודמת על סדרת טקסט מולטלמודלית אוסטרקטיבית השתמשה רק במידע מהטקסט והוידאו מודליות. We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST מציג את המצב הנוכחי של מודל האומנות (וידאו-טקסט) ב-2.51 נקודות במונחים של תוכן F1 ו-1.00 נקודות במונחים של תואר רוג-L על קבוצת נתונים How2 להבנה multimodal לשפה.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>यह पेपर MAST प्रस्तुत करता है, मल्टीमॉडल अमूर्त पाठ सारांशीकरण के लिए एक नया मॉडल जो सभी तीन तौर-तरीकों से जानकारी का उपयोग करता है - पाठ, ऑडियो और वीडियो - एक बहुआयामी वीडियो में। Multimodal abstractive text summarization पर पहले का काम केवल पाठ और वीडियो तौर-तरीकों से जानकारी का उपयोग करता है। हम ऑडियो रूपरेखा से जानकारी प्राप्त करने की उपयोगिता और चुनौतियों की जांच करते हैं और एक अनुक्रम-से-अनुक्रम ट्राइमोडल पदानुक्रमित ध्यान-आधारित मॉडल प्रस्तुत करते हैं जो मॉडल को पाठ पद्धति पर अधिक ध्यान देकर इन चुनौतियों को दूर करता है। MAST सामग्री F1 स्कोर के संदर्भ में 2.51 अंक और मल्टीमॉडल भाषा की समझ के लिए How2 डेटासेट पर रूज-एल स्कोर के संदर्भ में 1.00 अंकों द्वारा कला मॉडल (वीडियो-टेक्स्ट) की वर्तमान स्थिति को मात देता है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ovaj papir predstavlja MAST, novi model za sažetak multimodalnog abstraktivnog teksta koji koristi informacije iz svih tri moda - tekst, audio i video - u multimodalnom snimku. Prije rad na sažetku multimodalnog abstraktivnog teksta korišteno je samo informacije iz teksta i video modaliteta. Provjeravamo korisnost i izazove izvlačenja informacija iz zvukovnog modaliteta i predstavljamo trimodalni model na osnovu tri modalne hijerarhijske pažnje koji prevlada te izazove tako što omogućavamo model obratiti više pažnje na modalitet teksta. MAST iznosi trenutno stanje umjetničkog modela (video-teksta) za 2,51 bodova u smislu rezultata Content F1 i 1,00 bodova u smislu rezultata Rouge-L na kompletu How2 podataka za razumijevanje multimodalnog jezika.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A tanulmány bemutatja a MAST-t, a multimodális absztraktív szövegösszefoglaló új modelljét, amely mindhárom módszerből származó információkat használ fel multimodális videóban. A multimodális absztraktív szövegösszefoglalással kapcsolatos korábbi munkák kizárólag a szövegből és a videóból származó információkat használták fel. Vizsgáljuk az audiomódszerből származó információk hasznosságát és kihívásait, és bemutatunk egy szekvenciás, hierarchikus figyelem-alapú trimodális modellt, amely ezeket a kihívásokat leküzdi azzal, hogy a modell nagyobb figyelmet fordít a szövegmódszerre. A MAST 2,51 ponttal haladja meg a jelenlegi korszerű modellt (videó-szöveg) az F1 tartalom és 1,00 ponttal a Rouge-L pontszám tekintetében a How2 adatkészleten a multimodális nyelvértés érdekében.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Այս հոդվածը ներկայացնում է MASTA-ը, մի նոր մոդել, որը օգտագործում է բազմամոդալ վերացական տեքստի համառոտացման համար տեղեկատվությունը բոլոր երեք մոդելներից՝ տեքստից, ձայնից և տեսահոլովակից, բազմամոդալ տեսահոլովակում: Անցյալ աշխատանքը բազմամոդալ վերացական տեքստի համառոտագրման վրա օգտագործեց միայն տեքստի և տեսագրական մեթոդների տեղեկատվությունը: Մենք ուսումնասիրում ենք հնչյունների միջոցով տեղեկատվության ստանալու օգտակարությունը և մարտահրավերները և ներկայացնում ենք հաջորդականությունից հաջորդականություն առնող տրամոդային հիերարխիկ ուշադրության հիմնված մոդել, որը հաղթահարում է այս մարտահրավերները, թույլ տալով մոդելը ավելի ու MASTA-ն արտադրում է ներկայիս ամենահարվեստի մոդելը (տեսագրական տեքստը) 2.51 միավորով F1-ի պարունակության և 1.00 միավորով Rուգ-L-ի պարունակության առումով, որը կազմում է բազմամոդալ լեզվի հասկանալու մասին Հոյ2 տվյալների համակարգում:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kertas ini mempersembahkan MAST, model baru untuk Penapisan Teks Abstraktif Multimodal yang menggunakan informasi dari tiga modalitas - teks, audio dan video - dalam video multimodal. Pekerjaan sebelumnya pada penghasilan teks abstraktif multimodal hanya menggunakan informasi dari modalitas teks dan video. Kami memeriksa kebaikan dan tantangan untuk mendapatkan informasi dari modalitas audio dan mempersembahkan model berkurunan-ke-urutan yang berdasarkan perhatian trimodal yang mengatasi tantangan-tantangan ini dengan membiarkan model memperhatikan modalitas teks lebih banyak. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Questo articolo presenta MAST, un nuovo modello di sintesi multimodale del testo astratto che utilizza informazioni provenienti da tutte e tre le modalità - testo, audio e video - in un video multimodale. I lavori precedenti sulla sintesi multimodale astratta del testo utilizzavano solo informazioni provenienti dalle modalità testuali e video. Esaminiamo l'utilità e le sfide di ricavare informazioni dalla modalità audio e presentiamo un modello gerarchico di attenzione trimodale sequenza-sequenza che supera queste sfide lasciando che il modello presti maggiore attenzione alla modalità testuale. MAST supera lo stato dell'arte attuale del modello (video-testo) di 2,51 punti in termini di punteggio Content F1 e di 1,00 punti in termini di punteggio Rouge-L sul set di dati How2 per la comprensione multimodale della lingua.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>本稿では、マルチモーダルビデオにおいて、テキスト、オーディオ、ビデオの3つのモダリティすべてからの情報を利用する、マルチモーダル抽象的テキスト要約の新しいモデルであるMASTを紹介する。マルチモーダル抽象的テキストの要約に関する以前の仕事は、テキストおよびビデオモードからの情報のみを利用していた。私たちは、オーディオモダリティから情報を導き出すことの有用性と課題を検討し、モデルがテキストモダリティにより注意を払うことによって、これらの課題を克服するシーケンスツーシーケンスのトリモーダル階層的注意ベースのモデルを提示します。MASTは、マルチモーダル言語の理解のために、コンテンツF 1スコアで2.51ポイント、How 2データセットのRouge - Lスコアで1.00ポイント、現在の最先端モデル（ビデオテキスト）を上回る。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Gambar iki nambah MART, model sing bagu kanggo Multimodal absolute Text Samurasi sing nggawe informasi ning saben telu modalitat - teks, video lan video - ning video multimodal. Samsul Awak dhéwé éntuk sistem usahaan lan susahé awak dhéwé nggambar informasi ning modalité surat lan gabung ning acara sekène-to-sekène trimodal karo akeh nyong lanjut model sing isiné berarti dhéwé kuwi nggawe modalité teka. MART iso nggambar kalagayet coro Mulalat (video-text) 2.31 punti, ditambalo Content F1 punti lan 1.00 punti</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ამ დოკუმენტი MAST- ს ახალი მოდელი მულტიმოდიალური აბსტრაქტიგური ტექსტის კომპანიზაციაში, რომელიც ყველა სამი მოდიალური ინფორმაციის გამოყენება - ტექსტი, ასეთო და ვიდეო - მულტიმოდიალური აბსტრაქტიური ტექსტის კუნძიზაციაზე მხოლოდ ტექსტის და ვიდეო მოდილიტების გამოყენებული ინფორმაცია. ჩვენ აუდიო მოდიალობიდან ინფორმაციის გამოიყენება და გამოსახულებების გამოყენება და აუდიო მოდიალობიდან მივიღეთ სიკეცემალური თერაქტიკური ინფორმაციის მოდელი, რომელიც ამ გამოსახულებების გამოსახულება, რომელი MAST მოდელის მიმდინარე სტატის მოდულის (ვიდეო- ტექსტის) სტატის განმავლობაზე 2. 51 წერტილით Content F1 წერტილის და 1. 00 წერტილის განმავლობაში Rouge- L წერტილის განმავლობაში How2 მონაცემების მონაცემების მოდულებ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Бұл қағаз MAST, көпModal Абстрактивті мәтін тұжырымдамасының жаңа үлгісін көрсетеді. Бұл көпModal видеонда мәліметті - мәтін, аудио және видео - барлық үш әдістерден қолданаты Көптеген абстрактивті мәтін тұжырымдамасындағы алдыңғы жұмыс тек мәтін мен видео әдістерінен қолданылған мәліметтер. Мәліметті аудио модулінен алу үшін пайдаланушы мен өзгерістерді тексереміз және реттеу үшін тримодалдық иерархиялық түрінде негізделген үлгісін таңдаймыз. Бұл өзгерістерді өзгерту үшін модулінің мәтін модулін MAST Сурет үлгісінің қазіргі күйі (видео- мәтін) 2. 51 нүкте Content F1 нүктесі мен 1. 00 нүкте бірнеше тілді ойлау үшін How2 деректер жиынындағы Rouge- L нүктесі жоғарылады.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>본고는 다중모드 추상적 텍스트 요약 모델인 MAST를 제시했는데 다중모드 영상의 모든 세 가지 모드(텍스트, 오디오, 영상)의 정보를 이용했다.이전에 다중모드 추상적인 텍스트 요약에 대한 작업은 텍스트와 영상 모드의 정보만 이용했다.우리는 오디오 모드에서 정보를 추출하는 유용성과 도전을 연구했고 서열을 바탕으로 하는 삼모드 차원 주의 모델을 제시했다. 이 모델은 모델이 텍스트 모드에 더욱 관심을 가지게 함으로써 이러한 도전을 극복한다.다중모드 언어 이해에 사용되는 How2 데이터 집합에서 MAST는 콘텐츠 F1 득점 면에서 현재 가장 선진적인 모델(영상 텍스트)보다 2.51점, Rouge-L 득점 면에서 현재보다 1.00점 높았다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities - text, audio and video - in a multimodal video. Ankstesniame daugiarūšio pobūdžio abstrakcinio teksto santraukos darbe buvo naudojama tik teksto ir vaizdo būdų informacija. We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Овој документ претставува MAST, нов модел за мултимодилна апстрактивна резултатација на текстот кој користи информации од сите три модели - текст, аудио и видео - во мултимодилно видео. Претходната работа за мултимодална апстрактивна резултатација на текстот користеше само информации од текстот и видео моделите. Ние ја испитуваме корисноста и предизвиците од извлекувањето информации од аудио модијалноста и претставуваме тримодален хиерархички модел базиран на секвенца на внимание кој ги надминува овие предизвици со дозвола моделот да привлече поголемо внимание на текстот модијалноста. МАСТ го надминува сегашниот најнов модел (видео-текст) за 2,51 поени во поглед на резултатот на Содржината F1 и 1,00 поени во поглед на резултатот на Руџ-Л на податоците How2 за мултимодилно разбирање на јазикот.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ഈ പത്രത്തില്‍ മൂന്നു രീതികളില്‍ നിന്നും വിവരങ്ങള്‍ ഉപയോഗിക്കുന്ന എല്ലാ വിവരങ്ങളില്‍ നിന്നും മുള്‍ട്ടിമോഡല്‍ അബ്ട്രാക്ട്രാക്ടിവിന്റെ പു പദാവലിയും വീഡിയോ മോഡിറ്റുകളില്‍ നിന്നും വിവരങ്ങളില്‍ നിന്നും ഉപയോഗിക്കുന്ന വിവരങ്ങള്‍ മുമ്പുള്ള ജോലി ഓഡിയോ മോഡിയില്‍ നിന്നും വിവരങ്ങള്‍ ലഭ്യമാക്കുന്നതിന്റെ ഉപയോഗവും വ്യാല്‍ക്രവങ്ങളും ഞങ്ങള്‍ പരിശോധിക്കുന്നു. ട്രിമോണിക്ക് ട്രിമോഡാല്‍ ഹിയെരാര്‍ക്കിക്കല മുള്‍ട്ടിമോഡാല്‍ എഫ്1 സ്കോരും 1. 00 പോയിന്റുകളും ഹൌ2 ഡാറ്റാസ്റ്റേറ്റ് ഗ്രഹിക്കുന്നതിനുള്ള മാതൃകയില്‍ നിലവിലുള്ള സ്ഥിതിയില്‍ 2. 51 പോയിന്റുകള്‍ 2. 51 പ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Энэ цаас MAST-г олон моделийн бичлэгээс мэдээллийг хэрэглэдэг олон моделийн Abstractive Text Summarization-ийн шинэ загвар болгодог. Бид олон моделийн абстрактив текст цуглуулалт дээр өмнөх ажил зөвхөн текст болон видео хувилбараас хэрэглэгдсэн мэдээлэл. Бид аудио хувилбараас мэдээллийг гаргах хэрэгтэй болон сорилтуудыг шалгаж, дарааллаар дарааллаар давтагдсан гурван төрлийн анхаарал төвлөрүүлэх загварыг илүү анхаарлаа хандуулж, эдгээр сорилтуудыг даван авч, загварыг текст хувилбарт илүү анха MAST Урлагийн загварын орчин үеийг 2.51 цэгээр үржүүлдэг. Content F1 score болон 1.00 цэгээр олон модель хэлний ойлголтын How2 өгөгдлийн сангийн тоо хэлбэрээр Rouge-L score дээр гаргадаг.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kertas ini memperkenalkan MAST, model baru untuk Penapisan Teks Abstratif Multimodal yang menggunakan maklumat dari semua tiga modaliti - teks, audio dan video - dalam video multimodal. Kerja terdahulu pada pengringkasan teks abstraktif multimodal hanya digunakan maklumat dari modaliti teks dan video. We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dan id-dokument jippreżenta MAST, mudell ġdid għas-Sommarju tat-Test Abstrattiv Multimodali li juża l-informazzjoni mit-tliet modalitajiet kollha - test, awdjo u vidjo - f’vidjo multimodali. Ħidma preċedenti dwar is-sommarju tat-test astrattiv multimodali użat biss informazzjoni mit-test u l-modalitajiet tal-vidjo. Aħna teżamina l-utilità u l-isfidi tad-derivazzjoni tal-informazzjoni mill-modalità awdjo u nippreżentaw mudell trimodali b’sekwenza għal sekwenza bbażat fuq l-attenzjoni ġerarkika li jegħleb dawn l-isfidi billi l-mudell jitħalla jagħti aktar attenzjoni lill-modalità tat-test. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Deze paper presenteert MAST, een nieuw model voor multimodale abstracte tekstsamenvatting dat informatie uit alle drie de modaliteiten van tekst, audio en video in een multimodale video gebruikt. Voorafgaand werk aan multimodale abstracte tekstsamenvatting gebruikte alleen informatie uit de tekst- en videomodaliteiten. We onderzoeken het nut en de uitdagingen van het afleiden van informatie uit de audiomodaliteit en presenteren een sequentie-to-sequence trimodaal hiërarchisch aandachtsgebaseerd model dat deze uitdagingen overwinnt door het model meer aandacht te laten besteden aan de tekstmodaliteit. MAST overtreft het huidige state-of-the-art model (video-tekst) met 2.51 punten in termen van Content F1 score en 1.00 punten in termen van Rouge-L score op de How2 dataset voor multimodaal taalbegrip.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Denne papiret viser MAST, eit ny modell for multimodal abstraktiv tekstsamandrag som brukar informasjon frå alle tre modular – tekst, lyd og video – i eit multimodal video. Førre arbeid på multimodal abstraktiv tekstsammendering er berre brukt informasjon frå teksten og videomodusane. Vi undersøker nødvendigheten og utfordringar for å få informasjon frå lyd-modulitet og presenterer ein trimodal hierarkisk oppmerksbasert modell som overfører desse utfordringane ved å la modellen få meir oppmerksomhet til tekstmodulitet. MAST utfører den gjeldande tilstanden til kunstmodellen (video-tekst) med 2,51 punkt i forhold til innhaldet F1- poeng og 1,00 punkt i forhold til Rouge- L- poeng på How2- datasettet for multimodal språksforståelse.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>W artykule przedstawiono MAST, nowy model multimodalnego podsumowania tekstu abstrakcyjnego, który wykorzystuje informacje ze wszystkich trzech modalności tekstu, audio i wideo w multimodalnym wideo. Wcześniejsze prace nad multimodalną abstrakcyjną streszczeniem tekstu wykorzystywały jedynie informacje z modalności tekstu i wideo. Badamy przydatność i wyzwania związane z pozyskiwaniem informacji z modalności audio i przedstawiamy trimodalny model oparty na uwadze sekwencji na hierarchicznym modelu, który pokonuje te wyzwania poprzez pozwolenie modelowi zwrócić większą uwagę na modalność tekstową. MAST przewyższa aktualny model (wideo-tekst) o 2,51 punkty pod względem wyniku Content F1 oraz 1,00 punkty pod względem wyniku Rouge-L na zbiorze danych How2 dla zrozumienia języka multimodalnego.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Este artigo apresenta o MAST, um novo modelo de sumarização de texto abstrato multimodal que utiliza informações de todas as três modalidades – texto, áudio e vídeo – em um vídeo multimodal. Trabalhos anteriores sobre sumarização de texto abstrato multimodal utilizaram apenas informações das modalidades de texto e vídeo. Examinamos a utilidade e os desafios de derivar informações da modalidade de áudio e apresentamos um modelo baseado em atenção hierárquica trimodal sequência a sequência que supera esses desafios, permitindo que o modelo preste mais atenção à modalidade de texto. O MAST supera o atual modelo de última geração (vídeo-texto) em 2,51 pontos em termos de pontuação F1 de conteúdo e 1,00 pontos em termos de pontuação Rouge-L no conjunto de dados How2 para compreensão de linguagem multimodal.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Această lucrare prezintă MAST, un nou model de Rezumare Multimodală a Textului Abstractiv care utilizează informații din toate cele trei modalități - text, audio și video - într-un video multimodal. Lucrările anterioare privind rezumarea textului multimodal abstractiv utilizează doar informații din modalitățile text și video. Examinăm utilitatea și provocările obținerii informațiilor din modalitatea audio și prezentăm un model ierarhic secvență la secvență bazat pe atenție trimodală, care depășește aceste provocări permițând modelului să acorde mai multă atenție modalității text. MAST depășește modelul actual de ultimă generație (video-text) cu 2,51 puncte în ceea ce privește scorul F1 conținut și 1,00 puncte în ceea ce privește scorul Rouge-L pe setul de date How2 pentru înțelegerea limbilor multimodale.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>В этой статье представлена МАЧТА, новая модель для мультимодального абстрактного текстового суммирования, которая использует информацию из всех трех способов – текста, аудио и видео – в мультимодальном видео. В предыдущей работе по мультимодальному абстрактному обобщению текста использовалась только информация из текстовых и видеомодулей. Мы изучаем полезность и проблемы извлечения информации из аудиомодальности и представляем тримодальную иерархическую модель, основанную на иерархическом внимании, которая преодолевает эти проблемы, позволяя модели уделять больше внимания текстовой модальности. MAST опережает текущую современную модель (видеотекст) на 2,51 балла по оценке Content F1 и на 1,00 балла по оценке Rouge-L в наборе данных How2 для мультимодального понимания языка.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>මේ පැත්ත MAST වෙනුවෙන්, අළුත් මොඩියෝල් අවස්ථාවක් පැත්තක් සංවේදනය සඳහා අළුත් මොඩියෝල් එකක් පෙනුවෙන් පෙනුවෙන් ති පාළුවන් සහ විඩියෝ මොඩියෝල් විධානයෙන් විතරයි ප්‍රභාවිත විතරයි. අපි ප්‍රයෝජනය සහ අවධානය පරීක්ෂා කරනවා ඔඩියෝ මෝඩියෝලියෙන් තොරතුරු ගන්න සහ ප්‍රයෝජනය සඳහා පරීක්ෂණය සඳහා පරීක්ෂණය සඳහා ත්‍රිමෝඩාල MAST ප්‍රස්තූති විද්‍යාත්මක (විඩියෝ- පාට්ස්) 2.51 පින්තුවක් වලින් විද්‍යාත්මක F1 ස්කෝර් වලින් සහ 1.00 පින්තුවක් වලින් Rouge- L ස්කෝර් වලි</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ta prispevek predstavlja MAST, nov model za multimodalno povzetek besedila, ki uporablja informacije iz vseh treh modalitet - besedila, avdio in video - v multimodalnem videu. Predhodno delo na multimodalnem abstraktivnem povzetku besedila je uporabljalo le informacije iz besedilnih in video modalih. Preučujemo uporabnost in izzive pridobivanja informacij iz avdio modalnosti in predstavljamo trimodalni hierarhični model, ki temelji na pozornosti, ki te izzive premaga tako, da model več pozornosti nameni besedilni modalnosti. MAST presega trenutno najsodobnejši model (video-besedilo) za 2,51 točke v smislu rezultata vsebine F1 in 1,00 točke v smislu rezultata Rouge-L na naboru podatkov How2 za razumevanje multimodalnega jezika.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Warqaddaas wuxuu MAST, model cusub oo u qoran qoraalka hoose-dhexe ee multimodal Abstractive, kaas oo macluumaadka ka isticmaalaya saddexda qaab oo dhan - text, audio iyo fiidiyo ah fiidiyo badan. Shaqo horay ah oo ku saabsan qoraalka la'aanta oo kala duduwan oo la isticmaalay macluumaad la isticmaalay qoraalka iyo fiidiyowga. Waxaan baaraynaa faa'iidada iyo dhibaatooyin ku saabsan helitaanka macluumaadka qaababka codka, waxaana keenaynaa model aad u fiirsaneyso marxaladda hierarchiga ah oo ka adkaysanaya dhibaatooyinkaas si aad u fiirsato qaababka qoraalka. MAST wuxuu ku soo bandhigaa xaaladda muusikada farshaxanka (fiidiyo-text) 2.51 barxad oo ku qoran kooxda F1 iyo 1.00 barxad oo lagu qorayo kooxda Rouge-L ee kooxda aqoonta luuqada kala duduwan ee How2.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ky dokument paraqet MAST, një model i ri për përmbledhjen e tekstit abstraktiv multimodal që përdor informacionin nga të tre modalitetet - teksti, audio dhe video - në një video multimodal. Prior work on multimodal abstractive text summarization only utilized information from the text and video modalities. Ne shqyrtojmë dobinë dhe sfidat e marrjes së informacionit nga modaliteti audio dhe paraqesim një model trimodal sekuencë-në-sekuencë të bazuar në vëmendjen hierarkike që i kapërcen këto sfida duke lejuar modelit të kushtojë më shumë vëmendje modalitetit të tekstit. MAST ekzekuton gjendjen aktuale të modelit të artit (video-text) me 2.51 pikë lidhur me pikën e përmbajtjes F1 dhe 1.00 pikë lidhur me pikën e kuqe-L në kompletin e të dhënave How2 për kuptimin multimodal të gjuhës.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ovaj papir predstavlja MAST, novi model za sažetak multimodalnog abstraktivnog teksta koji koristi informacije iz svih tri moda - tekst, audio i video - u multimodalnom video. Prije rad na sažetku multimodalnog abstraktivnog teksta korišteno je samo informacije iz teksta i video modaliteta. Ispitujemo korisnost i izazove izvlačenja informacija iz audio modaliteta i predstavljamo sekvencu trimodalnog hijerarhičkog model a baziranog na sekvenci, koji prevlada te izazove tako što omogućavamo model da obratite više pažnje na tekstualnu modalitetu. MAST iznosi trenutno stanje umjetničkog modela (video-tekst) sa 2,51 poena u smislu rezultata Content F1 i 1,00 poena u smislu rezultata Rouge-L na setu podataka How2 za razumevanje multimodalnog jezika.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Denna uppsats presenterar MAST, en ny modell för multimodal abstraktiv textsammanfattning som använder information från alla tre modaliteterna - text, ljud och video - i en multimodal video. Tidigare arbete med multimodal abstraktiv textsammanfattning utnyttjade endast information från text- och videomodaliteter. Vi undersöker nyttan och utmaningarna med att härleda information från ljudmodaliteten och presenterar en sekvens-till-sekvens trimodal uppmärksamhetsbaserad modell som övervinner dessa utmaningar genom att låta modellen ägna mer uppmärksamhet åt textmodaliteten. MAST överträffar den nuvarande toppmoderna modellen (video-text) med 2,51 poäng när det gäller Content F1 poäng och 1,00 poäng när det gäller Rouge-L poäng på How2 datauppsättningen för multimodal språkförståelse.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Makala hii inaonyesha MAST, mfano mpya kwa ajili ya Ujumbe wa Matambo ya Kimulmodal Abstractive unaotumia taarifa kutoka katika a in a tatu - maandishi, sauti na video - katika video mbalimbali. Kazi ya awali kuhusu muhtasari wa ujumbe wa maandishi yasiyo na maandishi mengi yaliyotumika tu kwa kutumia taarifa kutoka kwenye njia za maandishi na video. Tunajaribu matumizi na changamoto za kupata taarifa kutoka katika mfumo wa sauti na kuweka muundo wa ufuatiliaji wa mifumo ya mitatu kwa mfululizo ambao unashinda changamoto hizi kwa kuruhusu model kusikiliza zaidi kwa namna ya maandishi. MAST inaonyesha hali ya sasa ya muundo wa sanaa (ujumbe wa video-text) kwa pointi 2.51 kwa maana ya score F1 na pointi 1.00 kwa mujibu wa score ya Rouge-L kwenye seti ya data ya How2 kwa ajili ya kuelewa lugha nyingine.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>இந்த தாள் MAST, ஒரு புதிய மாதிரியை கொடுக்கும் பல மாதிரி Abstractive Text சுருக்கமாக்கும் அது அனைத்து மூன்று வகைகளிலிருந்தும் தகவலை பயன்படுத்துகிறது - பலவிதமான செயல்பாடு உரை சுருக்கம் மட்டும் உரை மற்றும் வீடியோ வகைகளிலிருந்து பயன்படுத்தப்பட்ட தகவல் முன்னிருப்பு பணி கேட்பொலி வகையிலிருந்து தகவல் பெறுவதற்கான பயன்பாட்டுகளையும் சவால்களையும் நாம் பரிசோதிக்கிறோம் மற்றும் ஒரு வரிசையில் இருந்து மூன்று தொடர்ந்து மூன்று ம MAST தற்போதைய கலைப்பாட்டு மாதிரியின் தற்போதைய நிலையை 2. 51 புள்ளிகளால் செயல்படுத்தும் உள்ளடக்க F1 மதிப்பு மற்றும் 1. 00 புள்ளி</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bu käze MAST'i, Multimodal Abstraktiw Metin Toplaýyşy üçin täze bir nusga görkezýär ki bu multimodal wideoda maglumat ullanýar. Multimodal abstraktiv metin holasasy üstünde öňki işlem diňe metin we video modlerden ullanýar Biz ses modalitatyndan informasiýa çykarmak üçin ullanlygyny we çözgütlerini barlaýarys we bu kynçylyklaryň üstüne çykan trimodal iýerarhiýa tabanly nusgasyny çykarýarys we modaliýasyna köp üns ber. MAST sanat düzümleriniň häzirki durumyny 2.51 puç diýipdir. Mazmunlar F1 अ'da we 1.00 puç diýipdir.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that uses information from all three modalities - text, audio and video - in a multimodal video. multimodal abstractive text summarization پر پہلے کام صرف متن اور ویڈیو موڈلیٹ سے استعمال کی جاتی ہے. ہم آڈیو موڈلیٹ سے اطلاعات اٹھانے کے کامیابی اور چالوں کے مطابق تحقیق کرتے ہیں اور آڈیو موڈلیٹ سے اٹھانے کے لئے تیموڈالی توجه کی مدل کو پیش کرتے ہیں جو ان چالوں پر غالب آتا ہے اور مدل کو متن موڈلیٹ کے لئے زیادہ توجه کرنا اجازت دیتے ہیں. MAST آرت موڈل (ویڈیو-ٹکسٹ) کی موجود موجود موجود موجود کو 2.51 پوینٹ کے مطابق موجود F1 پوینٹ اور 1.00 پوینٹ کے مطابق موجود زبان سمجھنے کے لئے Hauge-L پوینٹ کے مطابق موجود موجود ہے.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities - text, audio and video - in a multimodal video. Name Biz audio modulidan maʼlumotni olish uchun foydalanuvchi va muammolarni tekshirib ko'rib chiqaramiz va bu muammolarni matn moduliga qo'llash mumkin, bu muammolarni oshirish mumkin. MAST multimodal tilni tushunish uchun Joriy sanam modeli (video- text) 2. 51 нуқтаи bilan ishga tushiriladi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tờ giấy này giới thiệu MAST, một mô hình mới cho Sơ sài Văn bản đa phương, dùng thông tin từ ba phương thức - văn bản, âm thanh và video- trong một đoạn video đa phương. Việc tổng hợp văn bản trừu tượng đa phương chỉ sử dụng thông tin từ phương thức văn bản và đoạn video. Chúng tôi xem xét sự hữu ích và thử thách của việc lấy thông tin từ chế độ âm thanh và trình bày một mô hình cấp độ phân cấp cấp ba phân loại theo quy trình sắp xếp phân loại để vượt qua những thử thách này bằng cách để mô hình chú ý nhiều hơn đến kiểu văn bản. MAST thực hiện hiện hiện trạng thái hiện thời của mô hình nghệ thuật (đoạn video) bởi 2.51 Point theo như vẫn còn trong nội dung F1.00 theo giá trị ghi số Rouge-L trên tập tin Howl2 để hiểu ngôn ngữ đa phương.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>本文言MAST,此多模态象摘要之新模也,可于多模态视频中利用三文(本,音频视频)信息。 前此多模抽象摘要惟用文本及视频文信息。 论音频模态之有用性挑战性者获取信息,为次第之三模态以为形势,使模形多模态以胜之。 多模态言解者How2数集上,MAST先F1得分者(视频文本)高2.51分,Rouge-L得分先1.00分。</span></div></div><dl><dt>Anthology ID:</dt><dd>2020.nlpbt-1.7</dd><dt>Volume:</dt><dd><a href=/volumes/2020.nlpbt-1/>Proceedings of the First International Workshop on Natural Language Processing Beyond Text</a></dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/nlpbt/>nlpbt</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>60–69</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.nlpbt-1.7>https://aclanthology.org/2020.nlpbt-1.7</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2020.nlpbt-1.7 title="To the current version of the paper by DOI">10.18653/v1/2020.nlpbt-1.7</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">khullar-arora-2020-mast</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Aman Khullar and Udit Arora. 2020. <a href=https://aclanthology.org/2020.nlpbt-1.7>MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</a>. In <i>Proceedings of the First International Workshop on Natural Language Processing Beyond Text</i>, pages 60–69, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2020.nlpbt-1.7>MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</a> (Khullar & Arora, nlpbt 2020)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.nlpbt-1.7.pdf>https://aclanthology.org/2020.nlpbt-1.7.pdf</a></dd><dt class=acl-button-row>Video:</dt><dd class=acl-button-row><a href=https://slideslive.com/38939781 class="btn btn-attachment btn-sm"><i class="fas fa-video"></i>&nbsp;https://slideslive.com/38939781</a></dd><dt>Code</dt><dd><a href=https://github.com/amankhullar/mast><i class="fab fa-github"></i>&nbsp;amankhullar/mast</a></dd><dt>Data</dt><dd><a href=https://paperswithcode.com/dataset/how2>How2</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.nlpbt-1.7.pdf title="Open PDF of 'MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=MAST+%3A+Multimodal+Abstractive+Summarization+with+Trimodal+Hierarchical+AttentionMAST%3A+Multimodal+Abstractive+Summarization+with+Trimodal+Hierarchical+Attention" title="Search for 'MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-secondary d-flex flex-wrap justify-content-center" href="https://paperswithcode.com/paper/?acl=2020.nlpbt-1.7" title="Code for 'MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention' on Papers with Code"><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-big" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg><span class="pl-sm-2 d-none d-sm-inline">Code</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a>
<a class="btn btn-attachment d-flex flex-wrap justify-content-center" href=https://slideslive.com/38939781 title="Open video for 'MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention'"><span class="align-self-center px-1"><i class="fas fa-video"></i></span>
<span class=px-1>Video</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention](https://aclanthology.org/2020.nlpbt-1.7) (Khullar & Arora, nlpbt 2020)</p><ul class=mt-2><li><a href=https://aclanthology.org/2020.nlpbt-1.7>MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</a> (Khullar & Arora, nlpbt 2020)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Aman Khullar and Udit Arora. 2020. <a href=https://aclanthology.org/2020.nlpbt-1.7>MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical AttentionMAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</a>. In <i>Proceedings of the First International Workshop on Natural Language Processing Beyond Text</i>, pages 60–69, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>